{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cca01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from numpy import array\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.utils import resample\n",
    "from matplotlib import pyplot\n",
    "from sklearn.svm import SVC\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee45db",
   "metadata": {},
   "source": [
    "# 1. Import and understand the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a548d23a",
   "metadata": {},
   "source": [
    "1.A. Import ‘signal-data.csv’ as DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a5cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('signal-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0305b6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 592)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e588f04",
   "metadata": {},
   "source": [
    "1.B. Print 5 point summary and share at least 2 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dbe8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "430e0d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>Pass/Fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-07-19 11:55:00</td>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-07-19 12:32:00</td>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>...</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-07-19 13:17:00</td>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>...</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-07-19 14:43:00</td>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>...</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-07-19 15:22:00</td>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008-07-19 17:53:00</td>\n",
       "      <td>2946.25</td>\n",
       "      <td>2432.84</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5287</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0077</td>\n",
       "      <td>0.4949</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>3.8276</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>44.0077</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008-07-19 19:44:00</td>\n",
       "      <td>3030.27</td>\n",
       "      <td>2430.12</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.5816</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5010</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>2.8515</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>44.0077</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008-07-19 19:45:00</td>\n",
       "      <td>3058.88</td>\n",
       "      <td>2690.15</td>\n",
       "      <td>2248.9000</td>\n",
       "      <td>1004.4692</td>\n",
       "      <td>0.7884</td>\n",
       "      <td>100.0</td>\n",
       "      <td>106.2400</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>1.5153</td>\n",
       "      <td>...</td>\n",
       "      <td>95.0310</td>\n",
       "      <td>0.4984</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>2.1261</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>95.0310</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008-07-19 20:24:00</td>\n",
       "      <td>2967.68</td>\n",
       "      <td>2600.47</td>\n",
       "      <td>2248.9000</td>\n",
       "      <td>1004.4692</td>\n",
       "      <td>0.7884</td>\n",
       "      <td>100.0</td>\n",
       "      <td>106.2400</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>1.5358</td>\n",
       "      <td>...</td>\n",
       "      <td>111.6525</td>\n",
       "      <td>0.4993</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>3.4456</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>111.6525</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2008-07-19 21:35:00</td>\n",
       "      <td>3016.11</td>\n",
       "      <td>2428.37</td>\n",
       "      <td>2248.9000</td>\n",
       "      <td>1004.4692</td>\n",
       "      <td>0.7884</td>\n",
       "      <td>100.0</td>\n",
       "      <td>106.2400</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>1.5381</td>\n",
       "      <td>...</td>\n",
       "      <td>90.2294</td>\n",
       "      <td>0.4967</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>3.0687</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>90.2294</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 592 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Time        0        1          2          3       4      5  \\\n",
       "0  2008-07-19 11:55:00  3030.93  2564.00  2187.7333  1411.1265  1.3602  100.0   \n",
       "1  2008-07-19 12:32:00  3095.78  2465.14  2230.4222  1463.6606  0.8294  100.0   \n",
       "2  2008-07-19 13:17:00  2932.61  2559.94  2186.4111  1698.0172  1.5102  100.0   \n",
       "3  2008-07-19 14:43:00  2988.72  2479.90  2199.0333   909.7926  1.3204  100.0   \n",
       "4  2008-07-19 15:22:00  3032.24  2502.87  2233.3667  1326.5200  1.5334  100.0   \n",
       "5  2008-07-19 17:53:00  2946.25  2432.84  2233.3667  1326.5200  1.5334  100.0   \n",
       "6  2008-07-19 19:44:00  3030.27  2430.12  2230.4222  1463.6606  0.8294  100.0   \n",
       "7  2008-07-19 19:45:00  3058.88  2690.15  2248.9000  1004.4692  0.7884  100.0   \n",
       "8  2008-07-19 20:24:00  2967.68  2600.47  2248.9000  1004.4692  0.7884  100.0   \n",
       "9  2008-07-19 21:35:00  3016.11  2428.37  2248.9000  1004.4692  0.7884  100.0   \n",
       "\n",
       "          6       7       8  ...       581     582     583     584      585  \\\n",
       "0   97.6133  0.1242  1.5005  ...       NaN  0.5005  0.0118  0.0035   2.3630   \n",
       "1  102.3433  0.1247  1.4966  ...  208.2045  0.5019  0.0223  0.0055   4.4447   \n",
       "2   95.4878  0.1241  1.4436  ...   82.8602  0.4958  0.0157  0.0039   3.1745   \n",
       "3  104.2367  0.1217  1.4882  ...   73.8432  0.4990  0.0103  0.0025   2.0544   \n",
       "4  100.3967  0.1235  1.5031  ...       NaN  0.4800  0.4766  0.1045  99.3032   \n",
       "5  100.3967  0.1235  1.5287  ...   44.0077  0.4949  0.0189  0.0044   3.8276   \n",
       "6  102.3433  0.1247  1.5816  ...       NaN  0.5010  0.0143  0.0042   2.8515   \n",
       "7  106.2400  0.1185  1.5153  ...   95.0310  0.4984  0.0106  0.0034   2.1261   \n",
       "8  106.2400  0.1185  1.5358  ...  111.6525  0.4993  0.0172  0.0046   3.4456   \n",
       "9  106.2400  0.1185  1.5381  ...   90.2294  0.4967  0.0152  0.0038   3.0687   \n",
       "\n",
       "      586     587     588       589  Pass/Fail  \n",
       "0     NaN     NaN     NaN       NaN         -1  \n",
       "1  0.0096  0.0201  0.0060  208.2045         -1  \n",
       "2  0.0584  0.0484  0.0148   82.8602          1  \n",
       "3  0.0202  0.0149  0.0044   73.8432         -1  \n",
       "4  0.0202  0.0149  0.0044   73.8432         -1  \n",
       "5  0.0342  0.0151  0.0052   44.0077         -1  \n",
       "6  0.0342  0.0151  0.0052   44.0077         -1  \n",
       "7  0.0204  0.0194  0.0063   95.0310         -1  \n",
       "8  0.0111  0.0124  0.0045  111.6525         -1  \n",
       "9  0.0212  0.0191  0.0073   90.2294         -1  \n",
       "\n",
       "[10 rows x 592 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "334b1fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>3014.452896</td>\n",
       "      <td>73.621787</td>\n",
       "      <td>2743.2400</td>\n",
       "      <td>2966.260000</td>\n",
       "      <td>3011.49000</td>\n",
       "      <td>3056.650000</td>\n",
       "      <td>3356.3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>2495.850231</td>\n",
       "      <td>80.407705</td>\n",
       "      <td>2158.7500</td>\n",
       "      <td>2452.247500</td>\n",
       "      <td>2499.40500</td>\n",
       "      <td>2538.822500</td>\n",
       "      <td>2846.4400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>2200.547318</td>\n",
       "      <td>29.513152</td>\n",
       "      <td>2060.6600</td>\n",
       "      <td>2181.044400</td>\n",
       "      <td>2201.06670</td>\n",
       "      <td>2218.055500</td>\n",
       "      <td>2315.2667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>1396.376627</td>\n",
       "      <td>441.691640</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1081.875800</td>\n",
       "      <td>1285.21440</td>\n",
       "      <td>1591.223500</td>\n",
       "      <td>3715.0417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>4.197013</td>\n",
       "      <td>56.355540</td>\n",
       "      <td>0.6815</td>\n",
       "      <td>1.017700</td>\n",
       "      <td>1.31680</td>\n",
       "      <td>1.525700</td>\n",
       "      <td>1114.5366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>101.112908</td>\n",
       "      <td>6.237214</td>\n",
       "      <td>82.1311</td>\n",
       "      <td>97.920000</td>\n",
       "      <td>101.51220</td>\n",
       "      <td>104.586700</td>\n",
       "      <td>129.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.121822</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.12240</td>\n",
       "      <td>0.123800</td>\n",
       "      <td>0.1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>1.462862</td>\n",
       "      <td>0.073897</td>\n",
       "      <td>1.1910</td>\n",
       "      <td>1.411200</td>\n",
       "      <td>1.46160</td>\n",
       "      <td>1.516900</td>\n",
       "      <td>1.6564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>-0.000841</td>\n",
       "      <td>0.015116</td>\n",
       "      <td>-0.0534</td>\n",
       "      <td>-0.010800</td>\n",
       "      <td>-0.00130</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.0749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.009302</td>\n",
       "      <td>-0.0349</td>\n",
       "      <td>-0.005600</td>\n",
       "      <td>0.00040</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.0530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.964353</td>\n",
       "      <td>0.012452</td>\n",
       "      <td>0.6554</td>\n",
       "      <td>0.958100</td>\n",
       "      <td>0.96580</td>\n",
       "      <td>0.971300</td>\n",
       "      <td>0.9848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>199.956809</td>\n",
       "      <td>3.257276</td>\n",
       "      <td>182.0940</td>\n",
       "      <td>198.130700</td>\n",
       "      <td>199.53560</td>\n",
       "      <td>202.007100</td>\n",
       "      <td>272.0451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>9.005371</td>\n",
       "      <td>2.796596</td>\n",
       "      <td>2.2493</td>\n",
       "      <td>7.094875</td>\n",
       "      <td>8.96700</td>\n",
       "      <td>10.861875</td>\n",
       "      <td>19.5465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>413.086035</td>\n",
       "      <td>17.221095</td>\n",
       "      <td>333.4486</td>\n",
       "      <td>406.127400</td>\n",
       "      <td>412.21910</td>\n",
       "      <td>419.089275</td>\n",
       "      <td>824.9271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>9.907603</td>\n",
       "      <td>2.403867</td>\n",
       "      <td>4.4696</td>\n",
       "      <td>9.567625</td>\n",
       "      <td>9.85175</td>\n",
       "      <td>10.128175</td>\n",
       "      <td>102.8677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>0.971444</td>\n",
       "      <td>0.012062</td>\n",
       "      <td>0.5794</td>\n",
       "      <td>0.968200</td>\n",
       "      <td>0.97260</td>\n",
       "      <td>0.976800</td>\n",
       "      <td>0.9848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>190.047354</td>\n",
       "      <td>2.781041</td>\n",
       "      <td>169.1774</td>\n",
       "      <td>188.299825</td>\n",
       "      <td>189.66420</td>\n",
       "      <td>192.189375</td>\n",
       "      <td>215.5977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>12.481034</td>\n",
       "      <td>0.217965</td>\n",
       "      <td>9.8773</td>\n",
       "      <td>12.460000</td>\n",
       "      <td>12.49960</td>\n",
       "      <td>12.547100</td>\n",
       "      <td>12.9898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>1.405054</td>\n",
       "      <td>0.016737</td>\n",
       "      <td>1.1797</td>\n",
       "      <td>1.396500</td>\n",
       "      <td>1.40600</td>\n",
       "      <td>1.415000</td>\n",
       "      <td>1.4534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>-5618.393610</td>\n",
       "      <td>626.822178</td>\n",
       "      <td>-7150.2500</td>\n",
       "      <td>-5933.250000</td>\n",
       "      <td>-5523.25000</td>\n",
       "      <td>-5356.250000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>2699.378435</td>\n",
       "      <td>295.498535</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2578.000000</td>\n",
       "      <td>2664.00000</td>\n",
       "      <td>2841.750000</td>\n",
       "      <td>3656.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>-3806.299734</td>\n",
       "      <td>1380.162148</td>\n",
       "      <td>-9986.7500</td>\n",
       "      <td>-4371.750000</td>\n",
       "      <td>-3820.75000</td>\n",
       "      <td>-3352.750000</td>\n",
       "      <td>2363.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>-298.598136</td>\n",
       "      <td>2902.690117</td>\n",
       "      <td>-14804.5000</td>\n",
       "      <td>-1476.000000</td>\n",
       "      <td>-78.75000</td>\n",
       "      <td>1377.250000</td>\n",
       "      <td>14106.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>1.203845</td>\n",
       "      <td>0.177600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.094800</td>\n",
       "      <td>1.28300</td>\n",
       "      <td>1.304300</td>\n",
       "      <td>1.3828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>1.938477</td>\n",
       "      <td>0.189495</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.906500</td>\n",
       "      <td>1.98650</td>\n",
       "      <td>2.003200</td>\n",
       "      <td>2.0528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>6.638628</td>\n",
       "      <td>1.244249</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.263700</td>\n",
       "      <td>7.26470</td>\n",
       "      <td>7.329700</td>\n",
       "      <td>7.6588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>69.499532</td>\n",
       "      <td>3.461181</td>\n",
       "      <td>59.4000</td>\n",
       "      <td>67.377800</td>\n",
       "      <td>69.15560</td>\n",
       "      <td>72.266700</td>\n",
       "      <td>77.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>2.366197</td>\n",
       "      <td>0.408694</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>2.088900</td>\n",
       "      <td>2.37780</td>\n",
       "      <td>2.655600</td>\n",
       "      <td>3.5111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.184159</td>\n",
       "      <td>0.032944</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.161700</td>\n",
       "      <td>0.18670</td>\n",
       "      <td>0.207100</td>\n",
       "      <td>0.2851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>3.673189</td>\n",
       "      <td>0.535322</td>\n",
       "      <td>2.0698</td>\n",
       "      <td>3.362700</td>\n",
       "      <td>3.43100</td>\n",
       "      <td>3.531300</td>\n",
       "      <td>4.8044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>85.337469</td>\n",
       "      <td>2.026549</td>\n",
       "      <td>83.1829</td>\n",
       "      <td>84.490500</td>\n",
       "      <td>85.13545</td>\n",
       "      <td>85.741900</td>\n",
       "      <td>105.6038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>8.960279</td>\n",
       "      <td>1.344456</td>\n",
       "      <td>7.6032</td>\n",
       "      <td>8.580000</td>\n",
       "      <td>8.76980</td>\n",
       "      <td>9.060600</td>\n",
       "      <td>23.3453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>50.582639</td>\n",
       "      <td>1.182618</td>\n",
       "      <td>49.8348</td>\n",
       "      <td>50.252350</td>\n",
       "      <td>50.39640</td>\n",
       "      <td>50.578800</td>\n",
       "      <td>59.7711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>64.555787</td>\n",
       "      <td>2.574749</td>\n",
       "      <td>63.6774</td>\n",
       "      <td>64.024800</td>\n",
       "      <td>64.16580</td>\n",
       "      <td>64.344700</td>\n",
       "      <td>94.2641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>49.417370</td>\n",
       "      <td>1.182619</td>\n",
       "      <td>40.2289</td>\n",
       "      <td>49.421200</td>\n",
       "      <td>49.60360</td>\n",
       "      <td>49.747650</td>\n",
       "      <td>50.1652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>66.221274</td>\n",
       "      <td>0.304141</td>\n",
       "      <td>64.9193</td>\n",
       "      <td>66.040650</td>\n",
       "      <td>66.23180</td>\n",
       "      <td>66.343275</td>\n",
       "      <td>67.9586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>86.836577</td>\n",
       "      <td>0.446756</td>\n",
       "      <td>84.7327</td>\n",
       "      <td>86.578300</td>\n",
       "      <td>86.82070</td>\n",
       "      <td>87.002400</td>\n",
       "      <td>88.4188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>118.679554</td>\n",
       "      <td>1.807221</td>\n",
       "      <td>111.7128</td>\n",
       "      <td>118.015600</td>\n",
       "      <td>118.39930</td>\n",
       "      <td>118.939600</td>\n",
       "      <td>133.3898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>67.904909</td>\n",
       "      <td>24.062943</td>\n",
       "      <td>1.4340</td>\n",
       "      <td>74.800000</td>\n",
       "      <td>78.29000</td>\n",
       "      <td>80.200000</td>\n",
       "      <td>86.1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>3.353066</td>\n",
       "      <td>2.360425</td>\n",
       "      <td>-0.0759</td>\n",
       "      <td>2.690000</td>\n",
       "      <td>3.07400</td>\n",
       "      <td>3.521000</td>\n",
       "      <td>37.8800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.00000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>355.538904</td>\n",
       "      <td>6.234706</td>\n",
       "      <td>342.7545</td>\n",
       "      <td>350.801575</td>\n",
       "      <td>353.72090</td>\n",
       "      <td>360.772250</td>\n",
       "      <td>377.2973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>10.031165</td>\n",
       "      <td>0.175038</td>\n",
       "      <td>9.4640</td>\n",
       "      <td>9.925425</td>\n",
       "      <td>10.03485</td>\n",
       "      <td>10.152475</td>\n",
       "      <td>11.0530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>136.743060</td>\n",
       "      <td>7.849247</td>\n",
       "      <td>108.8464</td>\n",
       "      <td>130.728875</td>\n",
       "      <td>136.40000</td>\n",
       "      <td>142.098225</td>\n",
       "      <td>176.3136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>733.672811</td>\n",
       "      <td>12.170315</td>\n",
       "      <td>699.8139</td>\n",
       "      <td>724.442300</td>\n",
       "      <td>733.45000</td>\n",
       "      <td>741.454500</td>\n",
       "      <td>789.7523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>1.177958</td>\n",
       "      <td>0.189637</td>\n",
       "      <td>0.4967</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>1.25105</td>\n",
       "      <td>1.340350</td>\n",
       "      <td>1.5111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>139.972231</td>\n",
       "      <td>4.524251</td>\n",
       "      <td>125.7982</td>\n",
       "      <td>136.926800</td>\n",
       "      <td>140.00775</td>\n",
       "      <td>143.195700</td>\n",
       "      <td>163.2509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>632.254197</td>\n",
       "      <td>8.643985</td>\n",
       "      <td>607.3927</td>\n",
       "      <td>625.928425</td>\n",
       "      <td>631.37090</td>\n",
       "      <td>638.136325</td>\n",
       "      <td>667.7418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>157.420991</td>\n",
       "      <td>60.925108</td>\n",
       "      <td>40.2614</td>\n",
       "      <td>115.508975</td>\n",
       "      <td>183.31815</td>\n",
       "      <td>206.977150</td>\n",
       "      <td>258.5432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>4.592971</td>\n",
       "      <td>0.054950</td>\n",
       "      <td>3.7060</td>\n",
       "      <td>4.574000</td>\n",
       "      <td>4.59600</td>\n",
       "      <td>4.617000</td>\n",
       "      <td>4.7640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>4.838523</td>\n",
       "      <td>0.059581</td>\n",
       "      <td>3.9320</td>\n",
       "      <td>4.816000</td>\n",
       "      <td>4.84300</td>\n",
       "      <td>4.869000</td>\n",
       "      <td>5.0110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>2856.172105</td>\n",
       "      <td>25.749317</td>\n",
       "      <td>2801.0000</td>\n",
       "      <td>2836.000000</td>\n",
       "      <td>2854.00000</td>\n",
       "      <td>2874.000000</td>\n",
       "      <td>2936.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.928849</td>\n",
       "      <td>0.006807</td>\n",
       "      <td>0.8755</td>\n",
       "      <td>0.925450</td>\n",
       "      <td>0.93100</td>\n",
       "      <td>0.933100</td>\n",
       "      <td>0.9378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.949215</td>\n",
       "      <td>0.004176</td>\n",
       "      <td>0.9319</td>\n",
       "      <td>0.946650</td>\n",
       "      <td>0.94930</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.9598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>4.593312</td>\n",
       "      <td>0.085095</td>\n",
       "      <td>4.2199</td>\n",
       "      <td>4.531900</td>\n",
       "      <td>4.57270</td>\n",
       "      <td>4.668600</td>\n",
       "      <td>4.8475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>2.960241</td>\n",
       "      <td>9.532220</td>\n",
       "      <td>-28.9882</td>\n",
       "      <td>-1.871575</td>\n",
       "      <td>0.94725</td>\n",
       "      <td>4.385225</td>\n",
       "      <td>168.1455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>355.159094</td>\n",
       "      <td>6.027889</td>\n",
       "      <td>324.7145</td>\n",
       "      <td>350.596400</td>\n",
       "      <td>353.79910</td>\n",
       "      <td>359.673600</td>\n",
       "      <td>373.8664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>10.423143</td>\n",
       "      <td>0.274877</td>\n",
       "      <td>9.4611</td>\n",
       "      <td>10.283000</td>\n",
       "      <td>10.43670</td>\n",
       "      <td>10.591600</td>\n",
       "      <td>11.7849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>116.502329</td>\n",
       "      <td>8.629022</td>\n",
       "      <td>81.4900</td>\n",
       "      <td>112.022700</td>\n",
       "      <td>116.21180</td>\n",
       "      <td>120.927300</td>\n",
       "      <td>287.1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>13.989927</td>\n",
       "      <td>7.119863</td>\n",
       "      <td>1.6591</td>\n",
       "      <td>10.364300</td>\n",
       "      <td>13.24605</td>\n",
       "      <td>16.376100</td>\n",
       "      <td>188.0923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>20.542109</td>\n",
       "      <td>4.977467</td>\n",
       "      <td>6.4482</td>\n",
       "      <td>17.364800</td>\n",
       "      <td>20.02135</td>\n",
       "      <td>22.813625</td>\n",
       "      <td>48.9882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>27.131816</td>\n",
       "      <td>7.121703</td>\n",
       "      <td>4.3080</td>\n",
       "      <td>23.056425</td>\n",
       "      <td>26.26145</td>\n",
       "      <td>29.914950</td>\n",
       "      <td>118.0836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>706.668523</td>\n",
       "      <td>11.623078</td>\n",
       "      <td>632.4226</td>\n",
       "      <td>698.770200</td>\n",
       "      <td>706.45360</td>\n",
       "      <td>714.597000</td>\n",
       "      <td>770.6084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>16.715444</td>\n",
       "      <td>307.502293</td>\n",
       "      <td>0.4137</td>\n",
       "      <td>0.890700</td>\n",
       "      <td>0.97830</td>\n",
       "      <td>1.065000</td>\n",
       "      <td>7272.8283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>147.437578</td>\n",
       "      <td>4.240095</td>\n",
       "      <td>87.0255</td>\n",
       "      <td>145.237300</td>\n",
       "      <td>147.59730</td>\n",
       "      <td>149.959100</td>\n",
       "      <td>167.8309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>619.101687</td>\n",
       "      <td>9.539190</td>\n",
       "      <td>581.7773</td>\n",
       "      <td>612.774500</td>\n",
       "      <td>619.03270</td>\n",
       "      <td>625.170000</td>\n",
       "      <td>722.6018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>104.329033</td>\n",
       "      <td>31.651899</td>\n",
       "      <td>21.4332</td>\n",
       "      <td>87.484200</td>\n",
       "      <td>102.60430</td>\n",
       "      <td>115.498900</td>\n",
       "      <td>238.4775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>773.0</td>\n",
       "      <td>150.361552</td>\n",
       "      <td>18.388481</td>\n",
       "      <td>-59.4777</td>\n",
       "      <td>145.305300</td>\n",
       "      <td>152.29720</td>\n",
       "      <td>158.437800</td>\n",
       "      <td>175.4132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>773.0</td>\n",
       "      <td>468.020404</td>\n",
       "      <td>17.629886</td>\n",
       "      <td>456.0447</td>\n",
       "      <td>464.458100</td>\n",
       "      <td>466.08170</td>\n",
       "      <td>467.889900</td>\n",
       "      <td>692.4256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.106190</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>-0.006903</td>\n",
       "      <td>0.022292</td>\n",
       "      <td>-0.1049</td>\n",
       "      <td>-0.019550</td>\n",
       "      <td>-0.00630</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.2315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>-0.029390</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>-0.1862</td>\n",
       "      <td>-0.051900</td>\n",
       "      <td>-0.02890</td>\n",
       "      <td>-0.006500</td>\n",
       "      <td>0.0723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>-0.007041</td>\n",
       "      <td>0.031368</td>\n",
       "      <td>-0.1046</td>\n",
       "      <td>-0.029500</td>\n",
       "      <td>-0.00990</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.1331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>-0.013643</td>\n",
       "      <td>0.047872</td>\n",
       "      <td>-0.3482</td>\n",
       "      <td>-0.047600</td>\n",
       "      <td>-0.01250</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.2492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.003458</td>\n",
       "      <td>0.023080</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>-0.010800</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.1013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>-0.018531</td>\n",
       "      <td>0.049226</td>\n",
       "      <td>-0.1437</td>\n",
       "      <td>-0.044500</td>\n",
       "      <td>-0.00870</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.1186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>-0.021153</td>\n",
       "      <td>0.017021</td>\n",
       "      <td>-0.0982</td>\n",
       "      <td>-0.027200</td>\n",
       "      <td>-0.01960</td>\n",
       "      <td>-0.012000</td>\n",
       "      <td>0.0584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.006055</td>\n",
       "      <td>0.036074</td>\n",
       "      <td>-0.2129</td>\n",
       "      <td>-0.018000</td>\n",
       "      <td>0.00760</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.1437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>7.452067</td>\n",
       "      <td>0.516251</td>\n",
       "      <td>5.8257</td>\n",
       "      <td>7.104225</td>\n",
       "      <td>7.46745</td>\n",
       "      <td>7.807625</td>\n",
       "      <td>8.9904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1555.0</td>\n",
       "      <td>0.133108</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.1174</td>\n",
       "      <td>0.129800</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.136300</td>\n",
       "      <td>0.1505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>226.0</td>\n",
       "      <td>0.112783</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.110725</td>\n",
       "      <td>0.11355</td>\n",
       "      <td>0.114900</td>\n",
       "      <td>0.1184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>2.401872</td>\n",
       "      <td>0.037332</td>\n",
       "      <td>2.2425</td>\n",
       "      <td>2.376850</td>\n",
       "      <td>2.40390</td>\n",
       "      <td>2.428600</td>\n",
       "      <td>2.5555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.982420</td>\n",
       "      <td>0.012848</td>\n",
       "      <td>0.7749</td>\n",
       "      <td>0.975800</td>\n",
       "      <td>0.98740</td>\n",
       "      <td>0.989700</td>\n",
       "      <td>0.9935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>1807.815021</td>\n",
       "      <td>53.537262</td>\n",
       "      <td>1627.4714</td>\n",
       "      <td>1777.470300</td>\n",
       "      <td>1809.24920</td>\n",
       "      <td>1841.873000</td>\n",
       "      <td>2105.1823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1516.0</td>\n",
       "      <td>0.188703</td>\n",
       "      <td>0.052373</td>\n",
       "      <td>0.1113</td>\n",
       "      <td>0.169375</td>\n",
       "      <td>0.19010</td>\n",
       "      <td>0.200425</td>\n",
       "      <td>1.4727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1516.0</td>\n",
       "      <td>8827.536865</td>\n",
       "      <td>396.313662</td>\n",
       "      <td>7397.3100</td>\n",
       "      <td>8564.689975</td>\n",
       "      <td>8825.43510</td>\n",
       "      <td>9065.432400</td>\n",
       "      <td>10746.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.087683</td>\n",
       "      <td>-0.3570</td>\n",
       "      <td>-0.042900</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.3627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.003231</td>\n",
       "      <td>-0.0126</td>\n",
       "      <td>-0.001200</td>\n",
       "      <td>0.00040</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.0281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>-0.000541</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>-0.0171</td>\n",
       "      <td>-0.001600</td>\n",
       "      <td>-0.00020</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.0133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>-0.0020</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.017127</td>\n",
       "      <td>0.219578</td>\n",
       "      <td>-1.4803</td>\n",
       "      <td>-0.088600</td>\n",
       "      <td>0.00390</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>2.5093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>-0.018143</td>\n",
       "      <td>0.427110</td>\n",
       "      <td>-5.2717</td>\n",
       "      <td>-0.218800</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.189300</td>\n",
       "      <td>2.5698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.062740</td>\n",
       "      <td>-0.5283</td>\n",
       "      <td>-0.029800</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.8854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>-0.0030</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.0023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>-0.0024</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.062968</td>\n",
       "      <td>-0.5353</td>\n",
       "      <td>-0.035700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.2979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>-0.009789</td>\n",
       "      <td>0.003065</td>\n",
       "      <td>-0.0329</td>\n",
       "      <td>-0.011800</td>\n",
       "      <td>-0.01010</td>\n",
       "      <td>-0.008200</td>\n",
       "      <td>0.0203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>-0.0119</td>\n",
       "      <td>-0.000400</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.0071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>-0.000498</td>\n",
       "      <td>0.003202</td>\n",
       "      <td>-0.0281</td>\n",
       "      <td>-0.001900</td>\n",
       "      <td>-0.00020</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.0127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.0133</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.0172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>-0.001766</td>\n",
       "      <td>0.087475</td>\n",
       "      <td>-0.5226</td>\n",
       "      <td>-0.048600</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.4856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>-0.010789</td>\n",
       "      <td>0.086758</td>\n",
       "      <td>-0.3454</td>\n",
       "      <td>-0.064900</td>\n",
       "      <td>-0.01120</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.3938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>549.0</td>\n",
       "      <td>0.979993</td>\n",
       "      <td>0.008695</td>\n",
       "      <td>0.7848</td>\n",
       "      <td>0.978800</td>\n",
       "      <td>0.98100</td>\n",
       "      <td>0.982300</td>\n",
       "      <td>0.9842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>549.0</td>\n",
       "      <td>101.318253</td>\n",
       "      <td>1.880087</td>\n",
       "      <td>88.1938</td>\n",
       "      <td>100.389000</td>\n",
       "      <td>101.48170</td>\n",
       "      <td>102.078100</td>\n",
       "      <td>106.9227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>549.0</td>\n",
       "      <td>231.818898</td>\n",
       "      <td>2.105318</td>\n",
       "      <td>213.0083</td>\n",
       "      <td>230.373800</td>\n",
       "      <td>231.20120</td>\n",
       "      <td>233.036100</td>\n",
       "      <td>236.9546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>852.0</td>\n",
       "      <td>0.457538</td>\n",
       "      <td>0.048939</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.459300</td>\n",
       "      <td>0.46285</td>\n",
       "      <td>0.466425</td>\n",
       "      <td>0.4885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.945424</td>\n",
       "      <td>0.012133</td>\n",
       "      <td>0.8534</td>\n",
       "      <td>0.938600</td>\n",
       "      <td>0.94640</td>\n",
       "      <td>0.952300</td>\n",
       "      <td>0.9763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>747.383792</td>\n",
       "      <td>48.949250</td>\n",
       "      <td>544.0254</td>\n",
       "      <td>721.023000</td>\n",
       "      <td>750.86140</td>\n",
       "      <td>776.781850</td>\n",
       "      <td>924.5318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.987130</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>0.8900</td>\n",
       "      <td>0.989500</td>\n",
       "      <td>0.99050</td>\n",
       "      <td>0.990900</td>\n",
       "      <td>0.9924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>58.625908</td>\n",
       "      <td>6.485174</td>\n",
       "      <td>52.8068</td>\n",
       "      <td>57.978300</td>\n",
       "      <td>58.54910</td>\n",
       "      <td>59.133900</td>\n",
       "      <td>311.7344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.598412</td>\n",
       "      <td>0.008102</td>\n",
       "      <td>0.5274</td>\n",
       "      <td>0.594100</td>\n",
       "      <td>0.59900</td>\n",
       "      <td>0.603400</td>\n",
       "      <td>0.6245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.970777</td>\n",
       "      <td>0.008949</td>\n",
       "      <td>0.8411</td>\n",
       "      <td>0.964800</td>\n",
       "      <td>0.96940</td>\n",
       "      <td>0.978300</td>\n",
       "      <td>0.9827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>6.310863</td>\n",
       "      <td>0.124304</td>\n",
       "      <td>5.1259</td>\n",
       "      <td>6.246400</td>\n",
       "      <td>6.31360</td>\n",
       "      <td>6.375850</td>\n",
       "      <td>7.5220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>15.796425</td>\n",
       "      <td>0.099618</td>\n",
       "      <td>15.4600</td>\n",
       "      <td>15.730000</td>\n",
       "      <td>15.79000</td>\n",
       "      <td>15.860000</td>\n",
       "      <td>16.0700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>3.898390</td>\n",
       "      <td>0.904120</td>\n",
       "      <td>1.6710</td>\n",
       "      <td>3.202000</td>\n",
       "      <td>3.87700</td>\n",
       "      <td>4.392000</td>\n",
       "      <td>6.8890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>15.829660</td>\n",
       "      <td>0.108315</td>\n",
       "      <td>15.1700</td>\n",
       "      <td>15.762500</td>\n",
       "      <td>15.83000</td>\n",
       "      <td>15.900000</td>\n",
       "      <td>16.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>15.794705</td>\n",
       "      <td>0.114144</td>\n",
       "      <td>15.4300</td>\n",
       "      <td>15.722500</td>\n",
       "      <td>15.78000</td>\n",
       "      <td>15.870000</td>\n",
       "      <td>16.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>1.184956</td>\n",
       "      <td>0.280555</td>\n",
       "      <td>0.3122</td>\n",
       "      <td>0.974400</td>\n",
       "      <td>1.14400</td>\n",
       "      <td>1.338000</td>\n",
       "      <td>2.4650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>2.750728</td>\n",
       "      <td>0.253471</td>\n",
       "      <td>2.3400</td>\n",
       "      <td>2.572000</td>\n",
       "      <td>2.73500</td>\n",
       "      <td>2.873000</td>\n",
       "      <td>3.9910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.648478</td>\n",
       "      <td>0.135409</td>\n",
       "      <td>0.3161</td>\n",
       "      <td>0.548900</td>\n",
       "      <td>0.65390</td>\n",
       "      <td>0.713500</td>\n",
       "      <td>1.1750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>3.192182</td>\n",
       "      <td>0.264175</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.074000</td>\n",
       "      <td>3.19500</td>\n",
       "      <td>3.311000</td>\n",
       "      <td>3.8950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>-0.554228</td>\n",
       "      <td>1.220479</td>\n",
       "      <td>-3.7790</td>\n",
       "      <td>-0.898800</td>\n",
       "      <td>-0.14190</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>2.4580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.744976</td>\n",
       "      <td>0.082531</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.688700</td>\n",
       "      <td>0.75875</td>\n",
       "      <td>0.814500</td>\n",
       "      <td>0.8884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.997808</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.9936</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.99775</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>1.0190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1559.0</td>\n",
       "      <td>2.318545</td>\n",
       "      <td>0.053181</td>\n",
       "      <td>2.1911</td>\n",
       "      <td>2.277300</td>\n",
       "      <td>2.31240</td>\n",
       "      <td>2.358300</td>\n",
       "      <td>2.4723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1559.0</td>\n",
       "      <td>1004.043093</td>\n",
       "      <td>6.537701</td>\n",
       "      <td>980.4510</td>\n",
       "      <td>999.996100</td>\n",
       "      <td>1004.05000</td>\n",
       "      <td>1008.670600</td>\n",
       "      <td>1020.9944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1559.0</td>\n",
       "      <td>39.391979</td>\n",
       "      <td>2.990476</td>\n",
       "      <td>33.3658</td>\n",
       "      <td>37.347250</td>\n",
       "      <td>38.90260</td>\n",
       "      <td>40.804600</td>\n",
       "      <td>64.1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1562.0</td>\n",
       "      <td>117.960948</td>\n",
       "      <td>57.544627</td>\n",
       "      <td>58.0000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>994.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>138.194747</td>\n",
       "      <td>53.909792</td>\n",
       "      <td>36.1000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>134.60000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>295.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>122.692949</td>\n",
       "      <td>52.253015</td>\n",
       "      <td>19.2000</td>\n",
       "      <td>81.300000</td>\n",
       "      <td>117.70000</td>\n",
       "      <td>161.600000</td>\n",
       "      <td>334.7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>57.603025</td>\n",
       "      <td>12.345358</td>\n",
       "      <td>19.8000</td>\n",
       "      <td>50.900100</td>\n",
       "      <td>55.90010</td>\n",
       "      <td>62.900100</td>\n",
       "      <td>141.7998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>416.766964</td>\n",
       "      <td>263.300614</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>243.786000</td>\n",
       "      <td>339.56100</td>\n",
       "      <td>502.205900</td>\n",
       "      <td>1770.6909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>26.077904</td>\n",
       "      <td>506.922106</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.131700</td>\n",
       "      <td>0.23580</td>\n",
       "      <td>0.439100</td>\n",
       "      <td>9998.8944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>6.641565</td>\n",
       "      <td>3.552254</td>\n",
       "      <td>1.7400</td>\n",
       "      <td>5.110000</td>\n",
       "      <td>6.26000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>103.3900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.00390</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.0121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.120008</td>\n",
       "      <td>0.061343</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.083900</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.132700</td>\n",
       "      <td>0.6253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.063621</td>\n",
       "      <td>0.026541</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.05860</td>\n",
       "      <td>0.071800</td>\n",
       "      <td>0.2507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.055010</td>\n",
       "      <td>0.021844</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.2479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.017411</td>\n",
       "      <td>0.027123</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01590</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.9783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>8.471308</td>\n",
       "      <td>18.740631</td>\n",
       "      <td>1.4208</td>\n",
       "      <td>6.359900</td>\n",
       "      <td>7.91730</td>\n",
       "      <td>9.585300</td>\n",
       "      <td>742.9421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>6.814268</td>\n",
       "      <td>3.241843</td>\n",
       "      <td>1.3370</td>\n",
       "      <td>4.459250</td>\n",
       "      <td>5.95100</td>\n",
       "      <td>8.275000</td>\n",
       "      <td>22.3180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>14.047403</td>\n",
       "      <td>31.002541</td>\n",
       "      <td>2.0200</td>\n",
       "      <td>8.089750</td>\n",
       "      <td>10.99350</td>\n",
       "      <td>14.347250</td>\n",
       "      <td>536.5640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>1.196733</td>\n",
       "      <td>23.364063</td>\n",
       "      <td>0.1544</td>\n",
       "      <td>0.373750</td>\n",
       "      <td>0.46870</td>\n",
       "      <td>0.679925</td>\n",
       "      <td>924.3780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>0.011926</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.01110</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.2389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>7.697971</td>\n",
       "      <td>5.239219</td>\n",
       "      <td>1.2438</td>\n",
       "      <td>5.926950</td>\n",
       "      <td>7.51270</td>\n",
       "      <td>9.054675</td>\n",
       "      <td>191.5478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>0.507171</td>\n",
       "      <td>1.122427</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.32000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>12.7100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.058089</td>\n",
       "      <td>0.079174</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.036250</td>\n",
       "      <td>0.04870</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>2.2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>138.0</td>\n",
       "      <td>0.047104</td>\n",
       "      <td>0.039538</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.027050</td>\n",
       "      <td>0.03545</td>\n",
       "      <td>0.048875</td>\n",
       "      <td>0.2876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>138.0</td>\n",
       "      <td>1039.650738</td>\n",
       "      <td>406.848810</td>\n",
       "      <td>234.0996</td>\n",
       "      <td>721.675050</td>\n",
       "      <td>1020.30005</td>\n",
       "      <td>1277.750125</td>\n",
       "      <td>2505.2998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>882.680511</td>\n",
       "      <td>983.043021</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>623.00000</td>\n",
       "      <td>966.000000</td>\n",
       "      <td>7791.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>555.346326</td>\n",
       "      <td>574.808588</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>438.00000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>4170.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>4066.850479</td>\n",
       "      <td>4239.245058</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1321.000000</td>\n",
       "      <td>2614.00000</td>\n",
       "      <td>5034.000000</td>\n",
       "      <td>37943.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>4797.154633</td>\n",
       "      <td>6553.569317</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>451.000000</td>\n",
       "      <td>1784.00000</td>\n",
       "      <td>6384.000000</td>\n",
       "      <td>36871.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.140204</td>\n",
       "      <td>0.121989</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.9570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.127942</td>\n",
       "      <td>0.242534</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.08900</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>1.8170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.252026</td>\n",
       "      <td>0.407329</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.18400</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>3.2860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>2.788882</td>\n",
       "      <td>1.119756</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>2.60000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>21.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>1.235783</td>\n",
       "      <td>0.632767</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.20000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>16.3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.124397</td>\n",
       "      <td>0.047639</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.11900</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.7250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.400454</td>\n",
       "      <td>0.197918</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.41200</td>\n",
       "      <td>0.536000</td>\n",
       "      <td>1.1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.684330</td>\n",
       "      <td>0.157468</td>\n",
       "      <td>0.2979</td>\n",
       "      <td>0.575600</td>\n",
       "      <td>0.68600</td>\n",
       "      <td>0.797300</td>\n",
       "      <td>1.1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.120064</td>\n",
       "      <td>0.060785</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.11250</td>\n",
       "      <td>0.140300</td>\n",
       "      <td>0.4940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.320113</td>\n",
       "      <td>0.071243</td>\n",
       "      <td>0.1287</td>\n",
       "      <td>0.276600</td>\n",
       "      <td>0.32385</td>\n",
       "      <td>0.370200</td>\n",
       "      <td>0.5484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.576192</td>\n",
       "      <td>0.095734</td>\n",
       "      <td>0.2538</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.57760</td>\n",
       "      <td>0.634500</td>\n",
       "      <td>0.8643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.320113</td>\n",
       "      <td>0.071247</td>\n",
       "      <td>0.1287</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.32385</td>\n",
       "      <td>0.370200</td>\n",
       "      <td>0.5484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.778044</td>\n",
       "      <td>0.116322</td>\n",
       "      <td>0.4616</td>\n",
       "      <td>0.692200</td>\n",
       "      <td>0.76820</td>\n",
       "      <td>0.843900</td>\n",
       "      <td>1.1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.244718</td>\n",
       "      <td>0.074918</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.196250</td>\n",
       "      <td>0.24290</td>\n",
       "      <td>0.293925</td>\n",
       "      <td>0.4411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.394760</td>\n",
       "      <td>0.282903</td>\n",
       "      <td>0.0470</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>0.29900</td>\n",
       "      <td>0.423000</td>\n",
       "      <td>1.8580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>19.013257</td>\n",
       "      <td>3.311632</td>\n",
       "      <td>9.4000</td>\n",
       "      <td>16.850000</td>\n",
       "      <td>18.69000</td>\n",
       "      <td>20.972500</td>\n",
       "      <td>48.6700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.546770</td>\n",
       "      <td>0.224402</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>0.378000</td>\n",
       "      <td>0.52400</td>\n",
       "      <td>0.688750</td>\n",
       "      <td>3.5730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>10.780543</td>\n",
       "      <td>4.164051</td>\n",
       "      <td>3.1700</td>\n",
       "      <td>7.732500</td>\n",
       "      <td>10.17000</td>\n",
       "      <td>13.337500</td>\n",
       "      <td>55.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>26.661170</td>\n",
       "      <td>6.836101</td>\n",
       "      <td>5.0140</td>\n",
       "      <td>21.171500</td>\n",
       "      <td>27.20050</td>\n",
       "      <td>31.687000</td>\n",
       "      <td>72.9470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.144815</td>\n",
       "      <td>0.110198</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.169150</td>\n",
       "      <td>3.2283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>7.365741</td>\n",
       "      <td>7.188720</td>\n",
       "      <td>1.9400</td>\n",
       "      <td>5.390000</td>\n",
       "      <td>6.73500</td>\n",
       "      <td>8.450000</td>\n",
       "      <td>267.9100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>17.936290</td>\n",
       "      <td>8.609912</td>\n",
       "      <td>6.2200</td>\n",
       "      <td>14.505000</td>\n",
       "      <td>17.86500</td>\n",
       "      <td>20.860000</td>\n",
       "      <td>307.9300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>43.211418</td>\n",
       "      <td>21.711876</td>\n",
       "      <td>6.6130</td>\n",
       "      <td>24.711000</td>\n",
       "      <td>40.20950</td>\n",
       "      <td>57.674750</td>\n",
       "      <td>191.8300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.287084</td>\n",
       "      <td>0.395187</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>0.218000</td>\n",
       "      <td>0.25900</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>4.8380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>8.688487</td>\n",
       "      <td>15.720926</td>\n",
       "      <td>1.7500</td>\n",
       "      <td>5.040000</td>\n",
       "      <td>6.78000</td>\n",
       "      <td>9.555000</td>\n",
       "      <td>396.1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>20.092710</td>\n",
       "      <td>10.552162</td>\n",
       "      <td>9.2200</td>\n",
       "      <td>17.130000</td>\n",
       "      <td>19.37000</td>\n",
       "      <td>21.460000</td>\n",
       "      <td>252.8700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.557359</td>\n",
       "      <td>0.537705</td>\n",
       "      <td>0.0900</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.42400</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>10.0170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>11.532056</td>\n",
       "      <td>16.445556</td>\n",
       "      <td>2.7700</td>\n",
       "      <td>6.740000</td>\n",
       "      <td>8.57000</td>\n",
       "      <td>11.460000</td>\n",
       "      <td>390.1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>17.600192</td>\n",
       "      <td>8.690718</td>\n",
       "      <td>3.2100</td>\n",
       "      <td>14.155000</td>\n",
       "      <td>17.23500</td>\n",
       "      <td>20.162500</td>\n",
       "      <td>199.6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>7.839359</td>\n",
       "      <td>5.104495</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.020000</td>\n",
       "      <td>6.76000</td>\n",
       "      <td>9.490000</td>\n",
       "      <td>126.5300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>10.170463</td>\n",
       "      <td>14.622904</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.094000</td>\n",
       "      <td>8.46200</td>\n",
       "      <td>11.953000</td>\n",
       "      <td>490.5610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>30.073143</td>\n",
       "      <td>17.461798</td>\n",
       "      <td>7.7280</td>\n",
       "      <td>24.653000</td>\n",
       "      <td>30.09700</td>\n",
       "      <td>33.506000</td>\n",
       "      <td>500.3490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>32.218169</td>\n",
       "      <td>565.101239</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>0.114300</td>\n",
       "      <td>0.15820</td>\n",
       "      <td>0.230700</td>\n",
       "      <td>9998.4483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>9.050122</td>\n",
       "      <td>11.541083</td>\n",
       "      <td>2.3000</td>\n",
       "      <td>6.040000</td>\n",
       "      <td>7.74000</td>\n",
       "      <td>9.940000</td>\n",
       "      <td>320.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.050621</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>20.376176</td>\n",
       "      <td>17.497556</td>\n",
       "      <td>4.0100</td>\n",
       "      <td>16.350000</td>\n",
       "      <td>19.72000</td>\n",
       "      <td>22.370000</td>\n",
       "      <td>457.6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>73.264316</td>\n",
       "      <td>28.067143</td>\n",
       "      <td>5.3590</td>\n",
       "      <td>56.158000</td>\n",
       "      <td>73.24800</td>\n",
       "      <td>90.515000</td>\n",
       "      <td>172.3490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.029564</td>\n",
       "      <td>1.168074</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.088866</td>\n",
       "      <td>0.042065</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.07970</td>\n",
       "      <td>0.099450</td>\n",
       "      <td>0.5164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.056755</td>\n",
       "      <td>0.025005</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.043800</td>\n",
       "      <td>0.05320</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.3227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.051432</td>\n",
       "      <td>0.031578</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.04160</td>\n",
       "      <td>0.062450</td>\n",
       "      <td>0.5941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.060346</td>\n",
       "      <td>0.053030</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.05600</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>1.2837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.083268</td>\n",
       "      <td>0.056456</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>0.07540</td>\n",
       "      <td>0.093550</td>\n",
       "      <td>0.7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.081076</td>\n",
       "      <td>0.030437</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>0.08250</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.3429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.083484</td>\n",
       "      <td>0.025764</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.069550</td>\n",
       "      <td>0.08460</td>\n",
       "      <td>0.097550</td>\n",
       "      <td>0.2828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.071635</td>\n",
       "      <td>0.046283</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.06170</td>\n",
       "      <td>0.086350</td>\n",
       "      <td>0.6744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>3.771465</td>\n",
       "      <td>1.170436</td>\n",
       "      <td>1.0340</td>\n",
       "      <td>2.946100</td>\n",
       "      <td>3.63075</td>\n",
       "      <td>4.404750</td>\n",
       "      <td>8.8015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>1555.0</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.00300</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.0163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>226.0</td>\n",
       "      <td>0.009213</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.00895</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.0240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.060718</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.06090</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>0.2305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.008821</td>\n",
       "      <td>0.055937</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.00230</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.9911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>122.846571</td>\n",
       "      <td>55.156003</td>\n",
       "      <td>32.2637</td>\n",
       "      <td>95.147350</td>\n",
       "      <td>119.43600</td>\n",
       "      <td>144.502800</td>\n",
       "      <td>1768.8802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1516.0</td>\n",
       "      <td>0.059370</td>\n",
       "      <td>0.071211</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.029775</td>\n",
       "      <td>0.03980</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>1.4361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>1516.0</td>\n",
       "      <td>1041.056588</td>\n",
       "      <td>433.170076</td>\n",
       "      <td>168.7998</td>\n",
       "      <td>718.725350</td>\n",
       "      <td>967.29980</td>\n",
       "      <td>1261.299800</td>\n",
       "      <td>3601.2998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.019125</td>\n",
       "      <td>0.010756</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.01650</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.1541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.017844</td>\n",
       "      <td>0.010745</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.01550</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.2133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.004791</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.00460</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.0244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.00440</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.0236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>549.0</td>\n",
       "      <td>0.005755</td>\n",
       "      <td>0.084618</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.00170</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>1.9844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>549.0</td>\n",
       "      <td>1.729723</td>\n",
       "      <td>4.335614</td>\n",
       "      <td>0.2914</td>\n",
       "      <td>0.911500</td>\n",
       "      <td>1.18510</td>\n",
       "      <td>1.761800</td>\n",
       "      <td>99.9022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>549.0</td>\n",
       "      <td>4.148742</td>\n",
       "      <td>10.045084</td>\n",
       "      <td>1.1022</td>\n",
       "      <td>2.725900</td>\n",
       "      <td>3.67300</td>\n",
       "      <td>4.479700</td>\n",
       "      <td>237.1837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>852.0</td>\n",
       "      <td>0.053374</td>\n",
       "      <td>0.066880</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.02700</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.025171</td>\n",
       "      <td>0.049235</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.02100</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.9732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.015771</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>109.650967</td>\n",
       "      <td>54.597274</td>\n",
       "      <td>21.0107</td>\n",
       "      <td>76.132150</td>\n",
       "      <td>103.09360</td>\n",
       "      <td>131.758400</td>\n",
       "      <td>1119.7042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.004285</td>\n",
       "      <td>0.037472</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.9909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>4.645115</td>\n",
       "      <td>64.354756</td>\n",
       "      <td>0.7673</td>\n",
       "      <td>2.205650</td>\n",
       "      <td>2.86460</td>\n",
       "      <td>3.795050</td>\n",
       "      <td>2549.9885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.033216</td>\n",
       "      <td>0.022425</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.03080</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.4517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.013943</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.01500</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.0787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.403848</td>\n",
       "      <td>0.120334</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>0.40510</td>\n",
       "      <td>0.480950</td>\n",
       "      <td>0.9255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>1559.0</td>\n",
       "      <td>0.070587</td>\n",
       "      <td>0.029649</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.07060</td>\n",
       "      <td>0.091650</td>\n",
       "      <td>0.1578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>1559.0</td>\n",
       "      <td>19.504677</td>\n",
       "      <td>7.344404</td>\n",
       "      <td>6.0980</td>\n",
       "      <td>13.828000</td>\n",
       "      <td>17.97700</td>\n",
       "      <td>24.653000</td>\n",
       "      <td>40.8550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>1559.0</td>\n",
       "      <td>3.777866</td>\n",
       "      <td>1.152329</td>\n",
       "      <td>1.3017</td>\n",
       "      <td>2.956500</td>\n",
       "      <td>3.70350</td>\n",
       "      <td>4.379400</td>\n",
       "      <td>10.1529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1562.0</td>\n",
       "      <td>29.260291</td>\n",
       "      <td>8.402013</td>\n",
       "      <td>15.5471</td>\n",
       "      <td>24.982300</td>\n",
       "      <td>28.77350</td>\n",
       "      <td>31.702200</td>\n",
       "      <td>158.5260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>46.056598</td>\n",
       "      <td>17.866438</td>\n",
       "      <td>10.4015</td>\n",
       "      <td>30.013900</td>\n",
       "      <td>45.67650</td>\n",
       "      <td>59.594700</td>\n",
       "      <td>132.6479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>41.298147</td>\n",
       "      <td>17.737513</td>\n",
       "      <td>6.9431</td>\n",
       "      <td>27.092725</td>\n",
       "      <td>40.01925</td>\n",
       "      <td>54.277325</td>\n",
       "      <td>122.1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>20.181246</td>\n",
       "      <td>3.830463</td>\n",
       "      <td>8.6512</td>\n",
       "      <td>18.247100</td>\n",
       "      <td>19.58090</td>\n",
       "      <td>22.097300</td>\n",
       "      <td>43.5737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>136.292426</td>\n",
       "      <td>85.607784</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>81.215600</td>\n",
       "      <td>110.60140</td>\n",
       "      <td>162.038200</td>\n",
       "      <td>659.1696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>8.693213</td>\n",
       "      <td>168.949413</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.07840</td>\n",
       "      <td>0.144900</td>\n",
       "      <td>3332.5964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>2.210744</td>\n",
       "      <td>1.196437</td>\n",
       "      <td>0.5615</td>\n",
       "      <td>1.697700</td>\n",
       "      <td>2.08310</td>\n",
       "      <td>2.514300</td>\n",
       "      <td>32.1709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.0034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.041057</td>\n",
       "      <td>0.020289</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.03720</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.1884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.018034</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.01690</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.0755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.015094</td>\n",
       "      <td>0.005545</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.01390</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.0597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.005770</td>\n",
       "      <td>0.008550</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.00530</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.3083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>2.803984</td>\n",
       "      <td>5.864324</td>\n",
       "      <td>0.5050</td>\n",
       "      <td>2.210400</td>\n",
       "      <td>2.65800</td>\n",
       "      <td>3.146200</td>\n",
       "      <td>232.8049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>2.119795</td>\n",
       "      <td>0.962923</td>\n",
       "      <td>0.4611</td>\n",
       "      <td>1.438175</td>\n",
       "      <td>1.87515</td>\n",
       "      <td>2.606950</td>\n",
       "      <td>6.8698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>4.260018</td>\n",
       "      <td>9.763829</td>\n",
       "      <td>0.7280</td>\n",
       "      <td>2.467200</td>\n",
       "      <td>3.36005</td>\n",
       "      <td>4.311425</td>\n",
       "      <td>207.0161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>0.367529</td>\n",
       "      <td>7.386343</td>\n",
       "      <td>0.0513</td>\n",
       "      <td>0.114875</td>\n",
       "      <td>0.13895</td>\n",
       "      <td>0.198450</td>\n",
       "      <td>292.2274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.00360</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.0749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>2.578596</td>\n",
       "      <td>1.616993</td>\n",
       "      <td>0.3960</td>\n",
       "      <td>2.092125</td>\n",
       "      <td>2.54900</td>\n",
       "      <td>3.024525</td>\n",
       "      <td>59.5187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>0.123427</td>\n",
       "      <td>0.270987</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.064900</td>\n",
       "      <td>0.08330</td>\n",
       "      <td>0.118100</td>\n",
       "      <td>4.4203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.019926</td>\n",
       "      <td>0.025549</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.01690</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.6915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>138.0</td>\n",
       "      <td>0.014487</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.008725</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.0831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>138.0</td>\n",
       "      <td>335.551157</td>\n",
       "      <td>137.692483</td>\n",
       "      <td>82.3233</td>\n",
       "      <td>229.809450</td>\n",
       "      <td>317.86710</td>\n",
       "      <td>403.989300</td>\n",
       "      <td>879.2260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>401.814750</td>\n",
       "      <td>477.050076</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>185.089800</td>\n",
       "      <td>278.67190</td>\n",
       "      <td>428.554500</td>\n",
       "      <td>3933.7550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>252.999118</td>\n",
       "      <td>283.530702</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>130.220300</td>\n",
       "      <td>195.82560</td>\n",
       "      <td>273.952600</td>\n",
       "      <td>2005.8744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>1879.228369</td>\n",
       "      <td>1975.111365</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>603.032900</td>\n",
       "      <td>1202.41210</td>\n",
       "      <td>2341.288700</td>\n",
       "      <td>15559.9525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>2342.826978</td>\n",
       "      <td>3226.924298</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>210.936600</td>\n",
       "      <td>820.09880</td>\n",
       "      <td>3190.616400</td>\n",
       "      <td>18520.4683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.063804</td>\n",
       "      <td>0.064225</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.05280</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.5264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.060267</td>\n",
       "      <td>0.130825</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>0.04000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>1.0312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.118386</td>\n",
       "      <td>0.219147</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.08280</td>\n",
       "      <td>0.115500</td>\n",
       "      <td>1.8123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.910146</td>\n",
       "      <td>0.331982</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.717200</td>\n",
       "      <td>0.86040</td>\n",
       "      <td>1.046400</td>\n",
       "      <td>5.7110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.403342</td>\n",
       "      <td>0.197514</td>\n",
       "      <td>0.1118</td>\n",
       "      <td>0.295800</td>\n",
       "      <td>0.38080</td>\n",
       "      <td>0.477000</td>\n",
       "      <td>5.1549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.040344</td>\n",
       "      <td>0.014511</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.03880</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.2258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.132076</td>\n",
       "      <td>0.064867</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>0.13720</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>0.3337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.264917</td>\n",
       "      <td>0.057387</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.26430</td>\n",
       "      <td>0.307500</td>\n",
       "      <td>0.4750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.048623</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.04480</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.2246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.128921</td>\n",
       "      <td>0.027468</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.12950</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.2112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.218414</td>\n",
       "      <td>0.033593</td>\n",
       "      <td>0.0913</td>\n",
       "      <td>0.197600</td>\n",
       "      <td>0.21945</td>\n",
       "      <td>0.237900</td>\n",
       "      <td>0.3239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.128921</td>\n",
       "      <td>0.027470</td>\n",
       "      <td>0.0549</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.12950</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.2112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.304752</td>\n",
       "      <td>0.043460</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.278550</td>\n",
       "      <td>0.30290</td>\n",
       "      <td>0.331900</td>\n",
       "      <td>0.4438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.097344</td>\n",
       "      <td>0.028796</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.077600</td>\n",
       "      <td>0.09770</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.1784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.160051</td>\n",
       "      <td>0.117316</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.091500</td>\n",
       "      <td>0.12150</td>\n",
       "      <td>0.160175</td>\n",
       "      <td>0.7549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>5.976977</td>\n",
       "      <td>1.018629</td>\n",
       "      <td>2.7882</td>\n",
       "      <td>5.301525</td>\n",
       "      <td>5.83150</td>\n",
       "      <td>6.547800</td>\n",
       "      <td>13.0958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.172629</td>\n",
       "      <td>0.072392</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>0.117375</td>\n",
       "      <td>0.16340</td>\n",
       "      <td>0.218100</td>\n",
       "      <td>1.0034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>3.188770</td>\n",
       "      <td>1.215930</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>2.319725</td>\n",
       "      <td>2.89890</td>\n",
       "      <td>4.021250</td>\n",
       "      <td>15.8934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>7.916036</td>\n",
       "      <td>2.179059</td>\n",
       "      <td>1.6574</td>\n",
       "      <td>6.245150</td>\n",
       "      <td>8.38880</td>\n",
       "      <td>9.481100</td>\n",
       "      <td>20.0455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.043105</td>\n",
       "      <td>0.031885</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.03985</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.9474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>2.263727</td>\n",
       "      <td>2.116994</td>\n",
       "      <td>0.6114</td>\n",
       "      <td>1.670075</td>\n",
       "      <td>2.07765</td>\n",
       "      <td>2.633350</td>\n",
       "      <td>79.1515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>5.393420</td>\n",
       "      <td>2.518859</td>\n",
       "      <td>1.7101</td>\n",
       "      <td>4.272950</td>\n",
       "      <td>5.45880</td>\n",
       "      <td>6.344875</td>\n",
       "      <td>89.1917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>13.332172</td>\n",
       "      <td>6.615850</td>\n",
       "      <td>2.2345</td>\n",
       "      <td>7.578600</td>\n",
       "      <td>12.50450</td>\n",
       "      <td>17.925175</td>\n",
       "      <td>51.8678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.083232</td>\n",
       "      <td>0.063425</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.08480</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>1.0959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>2.593485</td>\n",
       "      <td>5.645226</td>\n",
       "      <td>0.5373</td>\n",
       "      <td>1.546550</td>\n",
       "      <td>2.06270</td>\n",
       "      <td>2.790525</td>\n",
       "      <td>174.8944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>6.215866</td>\n",
       "      <td>3.403447</td>\n",
       "      <td>2.8372</td>\n",
       "      <td>5.453900</td>\n",
       "      <td>5.98010</td>\n",
       "      <td>6.549500</td>\n",
       "      <td>90.5159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.168364</td>\n",
       "      <td>0.172883</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>0.089400</td>\n",
       "      <td>0.12940</td>\n",
       "      <td>0.210400</td>\n",
       "      <td>3.4125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>3.426925</td>\n",
       "      <td>5.781558</td>\n",
       "      <td>0.7899</td>\n",
       "      <td>2.035700</td>\n",
       "      <td>2.51350</td>\n",
       "      <td>3.360400</td>\n",
       "      <td>172.7119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>9.736386</td>\n",
       "      <td>7.556131</td>\n",
       "      <td>5.2151</td>\n",
       "      <td>8.288525</td>\n",
       "      <td>9.07355</td>\n",
       "      <td>10.041625</td>\n",
       "      <td>214.8628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>2.327482</td>\n",
       "      <td>1.699435</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.542850</td>\n",
       "      <td>2.05445</td>\n",
       "      <td>2.785475</td>\n",
       "      <td>38.8995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>3.037580</td>\n",
       "      <td>5.645022</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.901350</td>\n",
       "      <td>2.56085</td>\n",
       "      <td>3.405450</td>\n",
       "      <td>196.6880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>9.328958</td>\n",
       "      <td>6.074702</td>\n",
       "      <td>2.2001</td>\n",
       "      <td>7.588900</td>\n",
       "      <td>9.47420</td>\n",
       "      <td>10.439900</td>\n",
       "      <td>197.4988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>14.673507</td>\n",
       "      <td>261.738451</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.04640</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>5043.8789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>2.732094</td>\n",
       "      <td>3.667902</td>\n",
       "      <td>0.5741</td>\n",
       "      <td>1.911800</td>\n",
       "      <td>2.37730</td>\n",
       "      <td>2.985400</td>\n",
       "      <td>97.7089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.011319</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>6.198508</td>\n",
       "      <td>5.371825</td>\n",
       "      <td>1.2565</td>\n",
       "      <td>4.998900</td>\n",
       "      <td>6.00560</td>\n",
       "      <td>6.885200</td>\n",
       "      <td>156.3360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>23.217146</td>\n",
       "      <td>8.895221</td>\n",
       "      <td>2.0560</td>\n",
       "      <td>17.860900</td>\n",
       "      <td>23.21470</td>\n",
       "      <td>28.873100</td>\n",
       "      <td>59.3241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>773.0</td>\n",
       "      <td>7.958376</td>\n",
       "      <td>17.512965</td>\n",
       "      <td>1.7694</td>\n",
       "      <td>4.440600</td>\n",
       "      <td>5.56700</td>\n",
       "      <td>6.825500</td>\n",
       "      <td>257.0106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>773.0</td>\n",
       "      <td>5.770212</td>\n",
       "      <td>17.077498</td>\n",
       "      <td>1.0177</td>\n",
       "      <td>2.532700</td>\n",
       "      <td>3.04640</td>\n",
       "      <td>4.085700</td>\n",
       "      <td>187.7589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.008914</td>\n",
       "      <td>0.352186</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.9147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.024706</td>\n",
       "      <td>0.011862</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.02260</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.2200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.025252</td>\n",
       "      <td>0.010603</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.02400</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.1339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.023202</td>\n",
       "      <td>0.014326</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.01880</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.2914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.027584</td>\n",
       "      <td>0.024563</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.02530</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.6188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.023356</td>\n",
       "      <td>0.013157</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.02200</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.1429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.040331</td>\n",
       "      <td>0.015499</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>0.04210</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.1535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.041921</td>\n",
       "      <td>0.013068</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.034850</td>\n",
       "      <td>0.04420</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.1344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.034543</td>\n",
       "      <td>0.022307</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.02940</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.2789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>1.298629</td>\n",
       "      <td>0.386918</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>1.025475</td>\n",
       "      <td>1.25530</td>\n",
       "      <td>1.533325</td>\n",
       "      <td>2.8348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>1555.0</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.00090</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.0052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>226.0</td>\n",
       "      <td>0.002443</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.019840</td>\n",
       "      <td>0.007136</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.01960</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.0888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.020003</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.00070</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.4090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>39.936406</td>\n",
       "      <td>17.056304</td>\n",
       "      <td>10.7204</td>\n",
       "      <td>32.168700</td>\n",
       "      <td>39.69610</td>\n",
       "      <td>47.079200</td>\n",
       "      <td>547.1722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>1516.0</td>\n",
       "      <td>0.018383</td>\n",
       "      <td>0.021644</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.01250</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.4163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>1516.0</td>\n",
       "      <td>333.319601</td>\n",
       "      <td>138.801928</td>\n",
       "      <td>60.9882</td>\n",
       "      <td>228.682525</td>\n",
       "      <td>309.83165</td>\n",
       "      <td>412.329775</td>\n",
       "      <td>1072.2031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.005199</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.00460</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.0368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.004814</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.00430</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.0392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.003773</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.00320</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.0357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.003172</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.00280</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.0334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.00160</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.0082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.00150</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.0077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>549.0</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.026740</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.6271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>549.0</td>\n",
       "      <td>0.541040</td>\n",
       "      <td>1.341020</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.295500</td>\n",
       "      <td>0.37260</td>\n",
       "      <td>0.541200</td>\n",
       "      <td>30.9982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>549.0</td>\n",
       "      <td>1.285448</td>\n",
       "      <td>3.168427</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>0.842300</td>\n",
       "      <td>1.10630</td>\n",
       "      <td>1.386600</td>\n",
       "      <td>74.8445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>852.0</td>\n",
       "      <td>0.011427</td>\n",
       "      <td>0.014366</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.00680</td>\n",
       "      <td>0.011325</td>\n",
       "      <td>0.2073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.008281</td>\n",
       "      <td>0.015488</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.00680</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.3068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>35.155091</td>\n",
       "      <td>17.227003</td>\n",
       "      <td>6.3101</td>\n",
       "      <td>24.386550</td>\n",
       "      <td>32.53070</td>\n",
       "      <td>42.652450</td>\n",
       "      <td>348.8293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.011816</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.3127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>1.431868</td>\n",
       "      <td>20.326415</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.675150</td>\n",
       "      <td>0.87730</td>\n",
       "      <td>1.148200</td>\n",
       "      <td>805.3936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.010956</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.01020</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.1375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.00490</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.0229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.133990</td>\n",
       "      <td>0.038408</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.13390</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1559.0</td>\n",
       "      <td>0.024208</td>\n",
       "      <td>0.010728</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.02390</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>0.0514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1559.0</td>\n",
       "      <td>6.730615</td>\n",
       "      <td>2.829583</td>\n",
       "      <td>2.0545</td>\n",
       "      <td>4.547600</td>\n",
       "      <td>5.92010</td>\n",
       "      <td>8.585200</td>\n",
       "      <td>14.7277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1559.0</td>\n",
       "      <td>1.231997</td>\n",
       "      <td>0.364711</td>\n",
       "      <td>0.4240</td>\n",
       "      <td>0.966500</td>\n",
       "      <td>1.23970</td>\n",
       "      <td>1.416700</td>\n",
       "      <td>3.3128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1562.0</td>\n",
       "      <td>5.340932</td>\n",
       "      <td>2.578118</td>\n",
       "      <td>2.7378</td>\n",
       "      <td>4.127800</td>\n",
       "      <td>4.92245</td>\n",
       "      <td>5.787100</td>\n",
       "      <td>44.3100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>4.580430</td>\n",
       "      <td>1.776843</td>\n",
       "      <td>1.2163</td>\n",
       "      <td>3.012800</td>\n",
       "      <td>4.48970</td>\n",
       "      <td>5.936700</td>\n",
       "      <td>9.5765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>4.929344</td>\n",
       "      <td>2.122978</td>\n",
       "      <td>0.7342</td>\n",
       "      <td>3.265075</td>\n",
       "      <td>4.73275</td>\n",
       "      <td>6.458300</td>\n",
       "      <td>13.8071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>2.616086</td>\n",
       "      <td>0.551474</td>\n",
       "      <td>0.9609</td>\n",
       "      <td>2.321300</td>\n",
       "      <td>2.54810</td>\n",
       "      <td>2.853200</td>\n",
       "      <td>6.2150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>30.911316</td>\n",
       "      <td>18.413622</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>18.407900</td>\n",
       "      <td>26.15690</td>\n",
       "      <td>38.139700</td>\n",
       "      <td>128.2816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>25.612690</td>\n",
       "      <td>47.308463</td>\n",
       "      <td>4.0416</td>\n",
       "      <td>11.375800</td>\n",
       "      <td>20.25510</td>\n",
       "      <td>29.307300</td>\n",
       "      <td>899.1190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1553.0</td>\n",
       "      <td>6.630616</td>\n",
       "      <td>3.958371</td>\n",
       "      <td>1.5340</td>\n",
       "      <td>4.927400</td>\n",
       "      <td>6.17660</td>\n",
       "      <td>7.570700</td>\n",
       "      <td>116.8615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>3.404349</td>\n",
       "      <td>1.035433</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.660100</td>\n",
       "      <td>3.23400</td>\n",
       "      <td>4.010700</td>\n",
       "      <td>9.6900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>8.190905</td>\n",
       "      <td>4.054515</td>\n",
       "      <td>2.1531</td>\n",
       "      <td>5.765500</td>\n",
       "      <td>7.39560</td>\n",
       "      <td>9.168800</td>\n",
       "      <td>39.0376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>320.259235</td>\n",
       "      <td>287.704482</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>302.17760</td>\n",
       "      <td>524.002200</td>\n",
       "      <td>999.3160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>309.061299</td>\n",
       "      <td>325.448391</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>272.44870</td>\n",
       "      <td>582.935200</td>\n",
       "      <td>998.6813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>1.821261</td>\n",
       "      <td>3.057692</td>\n",
       "      <td>0.4411</td>\n",
       "      <td>1.030400</td>\n",
       "      <td>1.64510</td>\n",
       "      <td>2.214700</td>\n",
       "      <td>111.4956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>4.174524</td>\n",
       "      <td>6.913855</td>\n",
       "      <td>0.7217</td>\n",
       "      <td>3.184200</td>\n",
       "      <td>3.94310</td>\n",
       "      <td>4.784300</td>\n",
       "      <td>273.0952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>77.660446</td>\n",
       "      <td>32.596933</td>\n",
       "      <td>23.0200</td>\n",
       "      <td>55.976675</td>\n",
       "      <td>69.90545</td>\n",
       "      <td>92.911500</td>\n",
       "      <td>424.2152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>3.315469</td>\n",
       "      <td>6.325365</td>\n",
       "      <td>0.4866</td>\n",
       "      <td>1.965250</td>\n",
       "      <td>2.66710</td>\n",
       "      <td>3.470975</td>\n",
       "      <td>103.1809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>6.796312</td>\n",
       "      <td>23.257716</td>\n",
       "      <td>1.4666</td>\n",
       "      <td>3.766200</td>\n",
       "      <td>4.76440</td>\n",
       "      <td>6.883500</td>\n",
       "      <td>898.6085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>1.233858</td>\n",
       "      <td>0.995620</td>\n",
       "      <td>0.3632</td>\n",
       "      <td>0.743425</td>\n",
       "      <td>1.13530</td>\n",
       "      <td>1.539500</td>\n",
       "      <td>24.9904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>1564.0</td>\n",
       "      <td>4.058501</td>\n",
       "      <td>3.042144</td>\n",
       "      <td>0.6637</td>\n",
       "      <td>3.113225</td>\n",
       "      <td>3.94145</td>\n",
       "      <td>4.768650</td>\n",
       "      <td>113.2230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>1557.0</td>\n",
       "      <td>4.220747</td>\n",
       "      <td>10.632730</td>\n",
       "      <td>1.1198</td>\n",
       "      <td>1.935500</td>\n",
       "      <td>2.53410</td>\n",
       "      <td>3.609000</td>\n",
       "      <td>118.7533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>4.171844</td>\n",
       "      <td>6.435390</td>\n",
       "      <td>0.7837</td>\n",
       "      <td>2.571400</td>\n",
       "      <td>3.45380</td>\n",
       "      <td>4.755800</td>\n",
       "      <td>186.6164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>18.421600</td>\n",
       "      <td>36.060084</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.999700</td>\n",
       "      <td>11.10560</td>\n",
       "      <td>17.423100</td>\n",
       "      <td>400.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>22.358305</td>\n",
       "      <td>36.395408</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.059000</td>\n",
       "      <td>16.38100</td>\n",
       "      <td>21.765200</td>\n",
       "      <td>400.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>99.367633</td>\n",
       "      <td>126.188715</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>31.032400</td>\n",
       "      <td>57.96930</td>\n",
       "      <td>120.172900</td>\n",
       "      <td>994.2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>205.519304</td>\n",
       "      <td>225.778870</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.027100</td>\n",
       "      <td>151.11560</td>\n",
       "      <td>305.026300</td>\n",
       "      <td>995.7447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>14.733945</td>\n",
       "      <td>34.108854</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.550700</td>\n",
       "      <td>10.19770</td>\n",
       "      <td>12.754200</td>\n",
       "      <td>400.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>9.370666</td>\n",
       "      <td>34.369789</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.494400</td>\n",
       "      <td>4.55110</td>\n",
       "      <td>5.822800</td>\n",
       "      <td>400.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>7.513266</td>\n",
       "      <td>34.557804</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.950900</td>\n",
       "      <td>2.76430</td>\n",
       "      <td>3.822200</td>\n",
       "      <td>400.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>4.016785</td>\n",
       "      <td>1.611274</td>\n",
       "      <td>1.1568</td>\n",
       "      <td>3.070700</td>\n",
       "      <td>3.78090</td>\n",
       "      <td>4.678600</td>\n",
       "      <td>32.2740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>54.701052</td>\n",
       "      <td>34.108051</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>36.290300</td>\n",
       "      <td>49.09090</td>\n",
       "      <td>66.666700</td>\n",
       "      <td>851.6129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>70.643942</td>\n",
       "      <td>38.376178</td>\n",
       "      <td>14.1206</td>\n",
       "      <td>48.173800</td>\n",
       "      <td>65.43780</td>\n",
       "      <td>84.973400</td>\n",
       "      <td>657.7621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>11.526617</td>\n",
       "      <td>6.169471</td>\n",
       "      <td>1.0973</td>\n",
       "      <td>5.414100</td>\n",
       "      <td>12.08590</td>\n",
       "      <td>15.796400</td>\n",
       "      <td>33.0580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.802081</td>\n",
       "      <td>0.184213</td>\n",
       "      <td>0.3512</td>\n",
       "      <td>0.679600</td>\n",
       "      <td>0.80760</td>\n",
       "      <td>0.927600</td>\n",
       "      <td>1.2771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>1.345259</td>\n",
       "      <td>0.659195</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.907650</td>\n",
       "      <td>1.26455</td>\n",
       "      <td>1.577825</td>\n",
       "      <td>5.1317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.633941</td>\n",
       "      <td>0.143552</td>\n",
       "      <td>0.2169</td>\n",
       "      <td>0.550500</td>\n",
       "      <td>0.64350</td>\n",
       "      <td>0.733425</td>\n",
       "      <td>1.0851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.895043</td>\n",
       "      <td>0.155522</td>\n",
       "      <td>0.3336</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>0.90270</td>\n",
       "      <td>0.988800</td>\n",
       "      <td>1.3511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.647090</td>\n",
       "      <td>0.141252</td>\n",
       "      <td>0.3086</td>\n",
       "      <td>0.555800</td>\n",
       "      <td>0.65110</td>\n",
       "      <td>0.748400</td>\n",
       "      <td>1.1087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>1.175003</td>\n",
       "      <td>0.176158</td>\n",
       "      <td>0.6968</td>\n",
       "      <td>1.046800</td>\n",
       "      <td>1.16380</td>\n",
       "      <td>1.272300</td>\n",
       "      <td>1.7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.281895</td>\n",
       "      <td>0.086461</td>\n",
       "      <td>0.0846</td>\n",
       "      <td>0.226100</td>\n",
       "      <td>0.27970</td>\n",
       "      <td>0.338825</td>\n",
       "      <td>0.5085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.332270</td>\n",
       "      <td>0.236275</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.25120</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>1.4754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>5.346816</td>\n",
       "      <td>0.919196</td>\n",
       "      <td>2.6709</td>\n",
       "      <td>4.764200</td>\n",
       "      <td>5.27145</td>\n",
       "      <td>5.913000</td>\n",
       "      <td>13.9776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>5.460971</td>\n",
       "      <td>2.250804</td>\n",
       "      <td>0.9037</td>\n",
       "      <td>3.747875</td>\n",
       "      <td>5.22710</td>\n",
       "      <td>6.902475</td>\n",
       "      <td>34.4902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>7.883742</td>\n",
       "      <td>3.059660</td>\n",
       "      <td>2.3294</td>\n",
       "      <td>5.806525</td>\n",
       "      <td>7.42490</td>\n",
       "      <td>9.576775</td>\n",
       "      <td>42.0703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>3.636633</td>\n",
       "      <td>0.938372</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>2.899675</td>\n",
       "      <td>3.72450</td>\n",
       "      <td>4.341925</td>\n",
       "      <td>10.1840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>12.325685</td>\n",
       "      <td>8.125876</td>\n",
       "      <td>3.0489</td>\n",
       "      <td>8.816575</td>\n",
       "      <td>11.35090</td>\n",
       "      <td>14.387900</td>\n",
       "      <td>232.1258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>5.263666</td>\n",
       "      <td>4.537737</td>\n",
       "      <td>1.4428</td>\n",
       "      <td>3.827525</td>\n",
       "      <td>4.79335</td>\n",
       "      <td>6.089450</td>\n",
       "      <td>164.1093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>2.838380</td>\n",
       "      <td>1.345576</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>2.291175</td>\n",
       "      <td>2.83035</td>\n",
       "      <td>3.309225</td>\n",
       "      <td>47.7772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>29.197414</td>\n",
       "      <td>13.335189</td>\n",
       "      <td>7.9534</td>\n",
       "      <td>20.221850</td>\n",
       "      <td>26.16785</td>\n",
       "      <td>35.278800</td>\n",
       "      <td>149.3851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>1563.0</td>\n",
       "      <td>6.252091</td>\n",
       "      <td>8.673724</td>\n",
       "      <td>1.7163</td>\n",
       "      <td>4.697500</td>\n",
       "      <td>5.64500</td>\n",
       "      <td>6.386900</td>\n",
       "      <td>109.0074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>224.173047</td>\n",
       "      <td>230.766915</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>38.472775</td>\n",
       "      <td>150.34010</td>\n",
       "      <td>335.922400</td>\n",
       "      <td>999.8770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>5.662293</td>\n",
       "      <td>3.151685</td>\n",
       "      <td>2.6009</td>\n",
       "      <td>4.847200</td>\n",
       "      <td>5.47240</td>\n",
       "      <td>6.005700</td>\n",
       "      <td>77.8007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>5.367752</td>\n",
       "      <td>4.983367</td>\n",
       "      <td>0.8325</td>\n",
       "      <td>2.823300</td>\n",
       "      <td>4.06110</td>\n",
       "      <td>7.006800</td>\n",
       "      <td>87.1347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>9.638797</td>\n",
       "      <td>10.174117</td>\n",
       "      <td>2.4026</td>\n",
       "      <td>5.807300</td>\n",
       "      <td>7.39600</td>\n",
       "      <td>9.720200</td>\n",
       "      <td>212.6557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>137.888406</td>\n",
       "      <td>47.698041</td>\n",
       "      <td>11.4997</td>\n",
       "      <td>105.525150</td>\n",
       "      <td>138.25515</td>\n",
       "      <td>168.410125</td>\n",
       "      <td>492.7718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>39.426847</td>\n",
       "      <td>22.457104</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>24.900800</td>\n",
       "      <td>34.24675</td>\n",
       "      <td>47.727850</td>\n",
       "      <td>358.9504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>37.637050</td>\n",
       "      <td>24.822918</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>23.156500</td>\n",
       "      <td>32.82005</td>\n",
       "      <td>45.169475</td>\n",
       "      <td>415.4355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>4.262573</td>\n",
       "      <td>2.611174</td>\n",
       "      <td>1.1011</td>\n",
       "      <td>3.494500</td>\n",
       "      <td>4.27620</td>\n",
       "      <td>4.741800</td>\n",
       "      <td>79.1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>20.132155</td>\n",
       "      <td>14.939590</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.577100</td>\n",
       "      <td>15.97380</td>\n",
       "      <td>23.737200</td>\n",
       "      <td>274.8871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>6.257921</td>\n",
       "      <td>10.185026</td>\n",
       "      <td>1.6872</td>\n",
       "      <td>4.105400</td>\n",
       "      <td>5.24220</td>\n",
       "      <td>6.703800</td>\n",
       "      <td>289.8264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.128123</td>\n",
       "      <td>5.062075</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>200.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>3.283394</td>\n",
       "      <td>2.638608</td>\n",
       "      <td>0.6459</td>\n",
       "      <td>2.627700</td>\n",
       "      <td>3.18450</td>\n",
       "      <td>3.625300</td>\n",
       "      <td>63.3336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>75.538131</td>\n",
       "      <td>35.752493</td>\n",
       "      <td>8.8406</td>\n",
       "      <td>52.894500</td>\n",
       "      <td>70.43450</td>\n",
       "      <td>93.119600</td>\n",
       "      <td>221.9747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>318.418448</td>\n",
       "      <td>281.011323</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>293.51850</td>\n",
       "      <td>514.585900</td>\n",
       "      <td>999.4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>206.564196</td>\n",
       "      <td>192.864413</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>81.316150</td>\n",
       "      <td>148.31750</td>\n",
       "      <td>262.865250</td>\n",
       "      <td>989.4737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>215.288948</td>\n",
       "      <td>213.126638</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>76.455400</td>\n",
       "      <td>138.77550</td>\n",
       "      <td>294.667050</td>\n",
       "      <td>996.8586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>201.111728</td>\n",
       "      <td>218.690015</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>50.383550</td>\n",
       "      <td>112.95340</td>\n",
       "      <td>288.893450</td>\n",
       "      <td>994.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>302.506186</td>\n",
       "      <td>287.364070</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>249.92700</td>\n",
       "      <td>501.607450</td>\n",
       "      <td>999.4911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>239.455326</td>\n",
       "      <td>263.837645</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>55.555150</td>\n",
       "      <td>112.27550</td>\n",
       "      <td>397.506100</td>\n",
       "      <td>995.7447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>352.616477</td>\n",
       "      <td>252.043751</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>139.914350</td>\n",
       "      <td>348.52940</td>\n",
       "      <td>510.647150</td>\n",
       "      <td>997.5186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>272.169707</td>\n",
       "      <td>228.046702</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>112.859250</td>\n",
       "      <td>219.48720</td>\n",
       "      <td>377.144200</td>\n",
       "      <td>994.0035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>51.354045</td>\n",
       "      <td>18.048612</td>\n",
       "      <td>13.7225</td>\n",
       "      <td>38.391100</td>\n",
       "      <td>48.55745</td>\n",
       "      <td>61.494725</td>\n",
       "      <td>142.8436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>1555.0</td>\n",
       "      <td>2.442673</td>\n",
       "      <td>1.224283</td>\n",
       "      <td>0.5558</td>\n",
       "      <td>1.747100</td>\n",
       "      <td>2.25080</td>\n",
       "      <td>2.839800</td>\n",
       "      <td>12.7698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>226.0</td>\n",
       "      <td>8.170943</td>\n",
       "      <td>1.759262</td>\n",
       "      <td>4.8882</td>\n",
       "      <td>6.924650</td>\n",
       "      <td>8.00895</td>\n",
       "      <td>9.078900</td>\n",
       "      <td>21.0443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>2.530046</td>\n",
       "      <td>0.973948</td>\n",
       "      <td>0.8330</td>\n",
       "      <td>1.663750</td>\n",
       "      <td>2.52910</td>\n",
       "      <td>3.199100</td>\n",
       "      <td>9.4024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.956442</td>\n",
       "      <td>6.615200</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.23250</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>127.5728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>6.807826</td>\n",
       "      <td>3.260019</td>\n",
       "      <td>1.7720</td>\n",
       "      <td>5.274600</td>\n",
       "      <td>6.60790</td>\n",
       "      <td>7.897200</td>\n",
       "      <td>107.6926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1516.0</td>\n",
       "      <td>29.865896</td>\n",
       "      <td>24.621586</td>\n",
       "      <td>4.8135</td>\n",
       "      <td>16.342300</td>\n",
       "      <td>22.03910</td>\n",
       "      <td>32.438475</td>\n",
       "      <td>219.6436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1516.0</td>\n",
       "      <td>11.821030</td>\n",
       "      <td>4.956647</td>\n",
       "      <td>1.9496</td>\n",
       "      <td>8.150350</td>\n",
       "      <td>10.90655</td>\n",
       "      <td>14.469050</td>\n",
       "      <td>40.2818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>263.195864</td>\n",
       "      <td>324.771342</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>536.204600</td>\n",
       "      <td>1000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>240.981377</td>\n",
       "      <td>323.003410</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>505.401000</td>\n",
       "      <td>999.2337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>55.763508</td>\n",
       "      <td>37.691736</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>35.322200</td>\n",
       "      <td>46.98610</td>\n",
       "      <td>64.248700</td>\n",
       "      <td>451.4851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>275.979457</td>\n",
       "      <td>329.664680</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>555.294100</td>\n",
       "      <td>1000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>1561.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>549.0</td>\n",
       "      <td>0.678898</td>\n",
       "      <td>10.783880</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.17470</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>252.8604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>549.0</td>\n",
       "      <td>1.738902</td>\n",
       "      <td>4.890663</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.890300</td>\n",
       "      <td>1.15430</td>\n",
       "      <td>1.759700</td>\n",
       "      <td>113.2758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>549.0</td>\n",
       "      <td>1.806273</td>\n",
       "      <td>4.715894</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>1.171200</td>\n",
       "      <td>1.58910</td>\n",
       "      <td>1.932800</td>\n",
       "      <td>111.3495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>852.0</td>\n",
       "      <td>11.728440</td>\n",
       "      <td>15.814420</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.160300</td>\n",
       "      <td>5.83295</td>\n",
       "      <td>10.971850</td>\n",
       "      <td>184.3488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>2.695999</td>\n",
       "      <td>5.702366</td>\n",
       "      <td>0.3121</td>\n",
       "      <td>1.552150</td>\n",
       "      <td>2.22100</td>\n",
       "      <td>2.903700</td>\n",
       "      <td>111.7365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>11.610080</td>\n",
       "      <td>103.122996</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>14.728866</td>\n",
       "      <td>7.104435</td>\n",
       "      <td>2.6811</td>\n",
       "      <td>10.182800</td>\n",
       "      <td>13.74260</td>\n",
       "      <td>17.808950</td>\n",
       "      <td>137.9838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.453896</td>\n",
       "      <td>4.147581</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>0.073050</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>111.3330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>5.687782</td>\n",
       "      <td>20.663414</td>\n",
       "      <td>1.3104</td>\n",
       "      <td>3.769650</td>\n",
       "      <td>4.87710</td>\n",
       "      <td>6.450650</td>\n",
       "      <td>818.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>1543.0</td>\n",
       "      <td>5.560397</td>\n",
       "      <td>3.920370</td>\n",
       "      <td>1.5400</td>\n",
       "      <td>4.101500</td>\n",
       "      <td>5.13420</td>\n",
       "      <td>6.329500</td>\n",
       "      <td>80.0406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>1.443457</td>\n",
       "      <td>0.958428</td>\n",
       "      <td>0.1705</td>\n",
       "      <td>0.484200</td>\n",
       "      <td>1.55010</td>\n",
       "      <td>2.211650</td>\n",
       "      <td>8.2037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>6.395717</td>\n",
       "      <td>1.888698</td>\n",
       "      <td>2.1700</td>\n",
       "      <td>4.895450</td>\n",
       "      <td>6.41080</td>\n",
       "      <td>7.594250</td>\n",
       "      <td>14.4479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>1558.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>1559.0</td>\n",
       "      <td>3.034235</td>\n",
       "      <td>1.252913</td>\n",
       "      <td>0.8516</td>\n",
       "      <td>1.889900</td>\n",
       "      <td>3.05480</td>\n",
       "      <td>3.947000</td>\n",
       "      <td>6.5803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>1559.0</td>\n",
       "      <td>1.942828</td>\n",
       "      <td>0.731928</td>\n",
       "      <td>0.6144</td>\n",
       "      <td>1.385300</td>\n",
       "      <td>1.78550</td>\n",
       "      <td>2.458350</td>\n",
       "      <td>4.0825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>1559.0</td>\n",
       "      <td>9.611628</td>\n",
       "      <td>2.896376</td>\n",
       "      <td>3.2761</td>\n",
       "      <td>7.495750</td>\n",
       "      <td>9.45930</td>\n",
       "      <td>11.238400</td>\n",
       "      <td>25.7792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.111208</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.1184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.008471</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.00780</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.0240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.00260</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>1565.0</td>\n",
       "      <td>7.611403</td>\n",
       "      <td>1.315544</td>\n",
       "      <td>4.4294</td>\n",
       "      <td>7.116000</td>\n",
       "      <td>7.11600</td>\n",
       "      <td>8.020700</td>\n",
       "      <td>21.0443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>1307.0</td>\n",
       "      <td>1.039630</td>\n",
       "      <td>0.389066</td>\n",
       "      <td>0.4444</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>0.91110</td>\n",
       "      <td>1.285550</td>\n",
       "      <td>3.9786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1307.0</td>\n",
       "      <td>403.546477</td>\n",
       "      <td>5.063887</td>\n",
       "      <td>372.8220</td>\n",
       "      <td>400.694000</td>\n",
       "      <td>403.12200</td>\n",
       "      <td>407.431000</td>\n",
       "      <td>421.7020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>1307.0</td>\n",
       "      <td>75.679871</td>\n",
       "      <td>3.390523</td>\n",
       "      <td>71.0380</td>\n",
       "      <td>73.254000</td>\n",
       "      <td>74.08400</td>\n",
       "      <td>78.397000</td>\n",
       "      <td>83.7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>1307.0</td>\n",
       "      <td>0.663256</td>\n",
       "      <td>0.673346</td>\n",
       "      <td>0.0446</td>\n",
       "      <td>0.226250</td>\n",
       "      <td>0.47100</td>\n",
       "      <td>0.850350</td>\n",
       "      <td>7.0656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>1307.0</td>\n",
       "      <td>17.013313</td>\n",
       "      <td>4.966954</td>\n",
       "      <td>6.1100</td>\n",
       "      <td>14.530000</td>\n",
       "      <td>16.34000</td>\n",
       "      <td>19.035000</td>\n",
       "      <td>131.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>1307.0</td>\n",
       "      <td>1.230712</td>\n",
       "      <td>1.361117</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>1.15000</td>\n",
       "      <td>1.370000</td>\n",
       "      <td>39.3300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>1307.0</td>\n",
       "      <td>0.276688</td>\n",
       "      <td>0.276231</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.094900</td>\n",
       "      <td>0.19790</td>\n",
       "      <td>0.358450</td>\n",
       "      <td>2.7182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>1307.0</td>\n",
       "      <td>7.703874</td>\n",
       "      <td>2.192647</td>\n",
       "      <td>2.7860</td>\n",
       "      <td>6.738100</td>\n",
       "      <td>7.42790</td>\n",
       "      <td>8.637150</td>\n",
       "      <td>56.9303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1307.0</td>\n",
       "      <td>0.503657</td>\n",
       "      <td>0.598852</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>0.47890</td>\n",
       "      <td>0.562350</td>\n",
       "      <td>17.4781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>1307.0</td>\n",
       "      <td>57.746537</td>\n",
       "      <td>35.207552</td>\n",
       "      <td>4.8269</td>\n",
       "      <td>27.017600</td>\n",
       "      <td>54.44170</td>\n",
       "      <td>74.628700</td>\n",
       "      <td>303.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>1307.0</td>\n",
       "      <td>4.216905</td>\n",
       "      <td>1.280008</td>\n",
       "      <td>1.4967</td>\n",
       "      <td>3.625100</td>\n",
       "      <td>4.06710</td>\n",
       "      <td>4.702700</td>\n",
       "      <td>35.3198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>1307.0</td>\n",
       "      <td>1.623070</td>\n",
       "      <td>1.870433</td>\n",
       "      <td>0.1646</td>\n",
       "      <td>1.182900</td>\n",
       "      <td>1.52980</td>\n",
       "      <td>1.815600</td>\n",
       "      <td>54.2917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.995009</td>\n",
       "      <td>0.083860</td>\n",
       "      <td>0.8919</td>\n",
       "      <td>0.955200</td>\n",
       "      <td>0.97270</td>\n",
       "      <td>1.000800</td>\n",
       "      <td>1.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.325708</td>\n",
       "      <td>0.201392</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>0.149825</td>\n",
       "      <td>0.29090</td>\n",
       "      <td>0.443600</td>\n",
       "      <td>1.0737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.072443</td>\n",
       "      <td>0.051578</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.05920</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.4457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>32.284956</td>\n",
       "      <td>19.026081</td>\n",
       "      <td>7.2369</td>\n",
       "      <td>15.762450</td>\n",
       "      <td>29.73115</td>\n",
       "      <td>44.113400</td>\n",
       "      <td>101.1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>1294.0</td>\n",
       "      <td>262.729683</td>\n",
       "      <td>7.630585</td>\n",
       "      <td>242.2860</td>\n",
       "      <td>259.972500</td>\n",
       "      <td>264.27200</td>\n",
       "      <td>265.707000</td>\n",
       "      <td>311.4040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>1294.0</td>\n",
       "      <td>0.679641</td>\n",
       "      <td>0.121758</td>\n",
       "      <td>0.3049</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>0.65100</td>\n",
       "      <td>0.768875</td>\n",
       "      <td>1.2988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1294.0</td>\n",
       "      <td>6.444985</td>\n",
       "      <td>2.633583</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>5.16000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>32.5800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1294.0</td>\n",
       "      <td>0.145610</td>\n",
       "      <td>0.081122</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>0.11955</td>\n",
       "      <td>0.186150</td>\n",
       "      <td>0.6892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1294.0</td>\n",
       "      <td>2.610870</td>\n",
       "      <td>1.032761</td>\n",
       "      <td>0.4122</td>\n",
       "      <td>2.090200</td>\n",
       "      <td>2.15045</td>\n",
       "      <td>3.098725</td>\n",
       "      <td>14.0141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1294.0</td>\n",
       "      <td>0.060086</td>\n",
       "      <td>0.032761</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.04865</td>\n",
       "      <td>0.075275</td>\n",
       "      <td>0.2932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1294.0</td>\n",
       "      <td>2.452417</td>\n",
       "      <td>0.996644</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>1.884400</td>\n",
       "      <td>1.99970</td>\n",
       "      <td>2.970850</td>\n",
       "      <td>12.7462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>1294.0</td>\n",
       "      <td>21.117674</td>\n",
       "      <td>10.213294</td>\n",
       "      <td>3.2504</td>\n",
       "      <td>15.466200</td>\n",
       "      <td>16.98835</td>\n",
       "      <td>24.772175</td>\n",
       "      <td>84.8024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>530.523623</td>\n",
       "      <td>17.499736</td>\n",
       "      <td>317.1964</td>\n",
       "      <td>530.702700</td>\n",
       "      <td>532.39820</td>\n",
       "      <td>534.356400</td>\n",
       "      <td>589.5082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>2.101836</td>\n",
       "      <td>0.275112</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>1.982900</td>\n",
       "      <td>2.11860</td>\n",
       "      <td>2.290650</td>\n",
       "      <td>2.7395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>28.450165</td>\n",
       "      <td>86.304681</td>\n",
       "      <td>3.5400</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>8.65000</td>\n",
       "      <td>10.130000</td>\n",
       "      <td>454.5600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.345636</td>\n",
       "      <td>0.248478</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.242250</td>\n",
       "      <td>0.29340</td>\n",
       "      <td>0.366900</td>\n",
       "      <td>2.1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>9.162315</td>\n",
       "      <td>26.920150</td>\n",
       "      <td>1.0395</td>\n",
       "      <td>2.567850</td>\n",
       "      <td>2.97580</td>\n",
       "      <td>3.492500</td>\n",
       "      <td>170.0204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.104729</td>\n",
       "      <td>0.067791</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>0.08950</td>\n",
       "      <td>0.112150</td>\n",
       "      <td>0.5502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>5.563747</td>\n",
       "      <td>16.921369</td>\n",
       "      <td>0.6636</td>\n",
       "      <td>1.408450</td>\n",
       "      <td>1.62450</td>\n",
       "      <td>1.902000</td>\n",
       "      <td>90.4235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>16.642363</td>\n",
       "      <td>12.485267</td>\n",
       "      <td>4.5820</td>\n",
       "      <td>11.501550</td>\n",
       "      <td>13.81790</td>\n",
       "      <td>17.080900</td>\n",
       "      <td>96.9601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>618.0</td>\n",
       "      <td>0.021615</td>\n",
       "      <td>0.011730</td>\n",
       "      <td>-0.0169</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.02040</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>618.0</td>\n",
       "      <td>0.016829</td>\n",
       "      <td>0.009640</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.01480</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.0799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>618.0</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.00470</td>\n",
       "      <td>0.006475</td>\n",
       "      <td>0.0286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>618.0</td>\n",
       "      <td>97.934373</td>\n",
       "      <td>87.520966</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>46.184900</td>\n",
       "      <td>72.28890</td>\n",
       "      <td>116.539150</td>\n",
       "      <td>737.3048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.500096</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.4778</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.50020</td>\n",
       "      <td>0.502375</td>\n",
       "      <td>0.5098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.015318</td>\n",
       "      <td>0.017180</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.01380</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.4766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.00360</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>3.067826</td>\n",
       "      <td>3.578033</td>\n",
       "      <td>1.1975</td>\n",
       "      <td>2.306500</td>\n",
       "      <td>2.75765</td>\n",
       "      <td>3.295175</td>\n",
       "      <td>99.3032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.021458</td>\n",
       "      <td>0.012358</td>\n",
       "      <td>-0.0169</td>\n",
       "      <td>0.013425</td>\n",
       "      <td>0.02050</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.016475</td>\n",
       "      <td>0.008808</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.01480</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.0799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.00460</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.0286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>99.670066</td>\n",
       "      <td>93.891919</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>44.368600</td>\n",
       "      <td>71.90050</td>\n",
       "      <td>114.749700</td>\n",
       "      <td>737.3048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pass/Fail</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.867262</td>\n",
       "      <td>0.498010</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count         mean          std         min          25%  \\\n",
       "0          1561.0  3014.452896    73.621787   2743.2400  2966.260000   \n",
       "1          1560.0  2495.850231    80.407705   2158.7500  2452.247500   \n",
       "2          1553.0  2200.547318    29.513152   2060.6600  2181.044400   \n",
       "3          1553.0  1396.376627   441.691640      0.0000  1081.875800   \n",
       "4          1553.0     4.197013    56.355540      0.6815     1.017700   \n",
       "5          1553.0   100.000000     0.000000    100.0000   100.000000   \n",
       "6          1553.0   101.112908     6.237214     82.1311    97.920000   \n",
       "7          1558.0     0.121822     0.008961      0.0000     0.121100   \n",
       "8          1565.0     1.462862     0.073897      1.1910     1.411200   \n",
       "9          1565.0    -0.000841     0.015116     -0.0534    -0.010800   \n",
       "10         1565.0     0.000146     0.009302     -0.0349    -0.005600   \n",
       "11         1565.0     0.964353     0.012452      0.6554     0.958100   \n",
       "12         1565.0   199.956809     3.257276    182.0940   198.130700   \n",
       "13         1564.0     0.000000     0.000000      0.0000     0.000000   \n",
       "14         1564.0     9.005371     2.796596      2.2493     7.094875   \n",
       "15         1564.0   413.086035    17.221095    333.4486   406.127400   \n",
       "16         1564.0     9.907603     2.403867      4.4696     9.567625   \n",
       "17         1564.0     0.971444     0.012062      0.5794     0.968200   \n",
       "18         1564.0   190.047354     2.781041    169.1774   188.299825   \n",
       "19         1557.0    12.481034     0.217965      9.8773    12.460000   \n",
       "20         1567.0     1.405054     0.016737      1.1797     1.396500   \n",
       "21         1565.0 -5618.393610   626.822178  -7150.2500 -5933.250000   \n",
       "22         1565.0  2699.378435   295.498535      0.0000  2578.000000   \n",
       "23         1565.0 -3806.299734  1380.162148  -9986.7500 -4371.750000   \n",
       "24         1565.0  -298.598136  2902.690117 -14804.5000 -1476.000000   \n",
       "25         1565.0     1.203845     0.177600      0.0000     1.094800   \n",
       "26         1565.0     1.938477     0.189495      0.0000     1.906500   \n",
       "27         1565.0     6.638628     1.244249      0.0000     5.263700   \n",
       "28         1565.0    69.499532     3.461181     59.4000    67.377800   \n",
       "29         1565.0     2.366197     0.408694      0.6667     2.088900   \n",
       "30         1565.0     0.184159     0.032944      0.0341     0.161700   \n",
       "31         1565.0     3.673189     0.535322      2.0698     3.362700   \n",
       "32         1566.0    85.337469     2.026549     83.1829    84.490500   \n",
       "33         1566.0     8.960279     1.344456      7.6032     8.580000   \n",
       "34         1566.0    50.582639     1.182618     49.8348    50.252350   \n",
       "35         1566.0    64.555787     2.574749     63.6774    64.024800   \n",
       "36         1566.0    49.417370     1.182619     40.2289    49.421200   \n",
       "37         1566.0    66.221274     0.304141     64.9193    66.040650   \n",
       "38         1566.0    86.836577     0.446756     84.7327    86.578300   \n",
       "39         1566.0   118.679554     1.807221    111.7128   118.015600   \n",
       "40         1543.0    67.904909    24.062943      1.4340    74.800000   \n",
       "41         1543.0     3.353066     2.360425     -0.0759     2.690000   \n",
       "42         1566.0    70.000000     0.000000     70.0000    70.000000   \n",
       "43         1566.0   355.538904     6.234706    342.7545   350.801575   \n",
       "44         1566.0    10.031165     0.175038      9.4640     9.925425   \n",
       "45         1566.0   136.743060     7.849247    108.8464   130.728875   \n",
       "46         1566.0   733.672811    12.170315    699.8139   724.442300   \n",
       "47         1566.0     1.177958     0.189637      0.4967     0.985000   \n",
       "48         1566.0   139.972231     4.524251    125.7982   136.926800   \n",
       "49         1566.0     1.000000     0.000000      1.0000     1.000000   \n",
       "50         1566.0   632.254197     8.643985    607.3927   625.928425   \n",
       "51         1566.0   157.420991    60.925108     40.2614   115.508975   \n",
       "52         1566.0     0.000000     0.000000      0.0000     0.000000   \n",
       "53         1563.0     4.592971     0.054950      3.7060     4.574000   \n",
       "54         1563.0     4.838523     0.059581      3.9320     4.816000   \n",
       "55         1563.0  2856.172105    25.749317   2801.0000  2836.000000   \n",
       "56         1563.0     0.928849     0.006807      0.8755     0.925450   \n",
       "57         1563.0     0.949215     0.004176      0.9319     0.946650   \n",
       "58         1563.0     4.593312     0.085095      4.2199     4.531900   \n",
       "59         1560.0     2.960241     9.532220    -28.9882    -1.871575   \n",
       "60         1561.0   355.159094     6.027889    324.7145   350.596400   \n",
       "61         1561.0    10.423143     0.274877      9.4611    10.283000   \n",
       "62         1561.0   116.502329     8.629022     81.4900   112.022700   \n",
       "63         1560.0    13.989927     7.119863      1.6591    10.364300   \n",
       "64         1560.0    20.542109     4.977467      6.4482    17.364800   \n",
       "65         1560.0    27.131816     7.121703      4.3080    23.056425   \n",
       "66         1561.0   706.668523    11.623078    632.4226   698.770200   \n",
       "67         1561.0    16.715444   307.502293      0.4137     0.890700   \n",
       "68         1561.0   147.437578     4.240095     87.0255   145.237300   \n",
       "69         1561.0     1.000000     0.000000      1.0000     1.000000   \n",
       "70         1561.0   619.101687     9.539190    581.7773   612.774500   \n",
       "71         1561.0   104.329033    31.651899     21.4332    87.484200   \n",
       "72          773.0   150.361552    18.388481    -59.4777   145.305300   \n",
       "73          773.0   468.020404    17.629886    456.0447   464.458100   \n",
       "74         1561.0     0.002688     0.106190      0.0000     0.000000   \n",
       "75         1543.0    -0.006903     0.022292     -0.1049    -0.019550   \n",
       "76         1543.0    -0.029390     0.033203     -0.1862    -0.051900   \n",
       "77         1543.0    -0.007041     0.031368     -0.1046    -0.029500   \n",
       "78         1543.0    -0.013643     0.047872     -0.3482    -0.047600   \n",
       "79         1543.0     0.003458     0.023080     -0.0568    -0.010800   \n",
       "80         1543.0    -0.018531     0.049226     -0.1437    -0.044500   \n",
       "81         1543.0    -0.021153     0.017021     -0.0982    -0.027200   \n",
       "82         1543.0     0.006055     0.036074     -0.2129    -0.018000   \n",
       "83         1566.0     7.452067     0.516251      5.8257     7.104225   \n",
       "84         1555.0     0.133108     0.005051      0.1174     0.129800   \n",
       "85          226.0     0.112783     0.002928      0.1053     0.110725   \n",
       "86         1567.0     2.401872     0.037332      2.2425     2.376850   \n",
       "87         1567.0     0.982420     0.012848      0.7749     0.975800   \n",
       "88         1567.0  1807.815021    53.537262   1627.4714  1777.470300   \n",
       "89         1516.0     0.188703     0.052373      0.1113     0.169375   \n",
       "90         1516.0  8827.536865   396.313662   7397.3100  8564.689975   \n",
       "91         1561.0     0.002440     0.087683     -0.3570    -0.042900   \n",
       "92         1565.0     0.000507     0.003231     -0.0126    -0.001200   \n",
       "93         1565.0    -0.000541     0.003010     -0.0171    -0.001600   \n",
       "94         1561.0    -0.000029     0.000174     -0.0020    -0.000100   \n",
       "95         1561.0     0.000060     0.000104     -0.0009     0.000000   \n",
       "96         1561.0     0.017127     0.219578     -1.4803    -0.088600   \n",
       "97         1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "98         1561.0    -0.018143     0.427110     -5.2717    -0.218800   \n",
       "99         1561.0     0.001540     0.062740     -0.5283    -0.029800   \n",
       "100        1561.0    -0.000021     0.000356     -0.0030    -0.000200   \n",
       "101        1561.0    -0.000007     0.000221     -0.0024    -0.000100   \n",
       "102        1561.0     0.001115     0.062968     -0.5353    -0.035700   \n",
       "103        1565.0    -0.009789     0.003065     -0.0329    -0.011800   \n",
       "104        1565.0    -0.000015     0.000851     -0.0119    -0.000400   \n",
       "105        1561.0    -0.000498     0.003202     -0.0281    -0.001900   \n",
       "106        1561.0     0.000540     0.002988     -0.0133    -0.001000   \n",
       "107        1561.0    -0.001766     0.087475     -0.5226    -0.048600   \n",
       "108        1561.0    -0.010789     0.086758     -0.3454    -0.064900   \n",
       "109         549.0     0.979993     0.008695      0.7848     0.978800   \n",
       "110         549.0   101.318253     1.880087     88.1938   100.389000   \n",
       "111         549.0   231.818898     2.105318    213.0083   230.373800   \n",
       "112         852.0     0.457538     0.048939      0.0000     0.459300   \n",
       "113        1567.0     0.945424     0.012133      0.8534     0.938600   \n",
       "114        1567.0     0.000123     0.001668      0.0000     0.000000   \n",
       "115        1567.0   747.383792    48.949250    544.0254   721.023000   \n",
       "116        1567.0     0.987130     0.009497      0.8900     0.989500   \n",
       "117        1567.0    58.625908     6.485174     52.8068    57.978300   \n",
       "118        1543.0     0.598412     0.008102      0.5274     0.594100   \n",
       "119        1567.0     0.970777     0.008949      0.8411     0.964800   \n",
       "120        1567.0     6.310863     0.124304      5.1259     6.246400   \n",
       "121        1558.0    15.796425     0.099618     15.4600    15.730000   \n",
       "122        1558.0     3.898390     0.904120      1.6710     3.202000   \n",
       "123        1558.0    15.829660     0.108315     15.1700    15.762500   \n",
       "124        1558.0    15.794705     0.114144     15.4300    15.722500   \n",
       "125        1558.0     1.184956     0.280555      0.3122     0.974400   \n",
       "126        1558.0     2.750728     0.253471      2.3400     2.572000   \n",
       "127        1558.0     0.648478     0.135409      0.3161     0.548900   \n",
       "128        1558.0     3.192182     0.264175      0.0000     3.074000   \n",
       "129        1558.0    -0.554228     1.220479     -3.7790    -0.898800   \n",
       "130        1558.0     0.744976     0.082531      0.4199     0.688700   \n",
       "131        1558.0     0.997808     0.002251      0.9936     0.996400   \n",
       "132        1559.0     2.318545     0.053181      2.1911     2.277300   \n",
       "133        1559.0  1004.043093     6.537701    980.4510   999.996100   \n",
       "134        1559.0    39.391979     2.990476     33.3658    37.347250   \n",
       "135        1562.0   117.960948    57.544627     58.0000    92.000000   \n",
       "136        1561.0   138.194747    53.909792     36.1000    90.000000   \n",
       "137        1560.0   122.692949    52.253015     19.2000    81.300000   \n",
       "138        1553.0    57.603025    12.345358     19.8000    50.900100   \n",
       "139        1553.0   416.766964   263.300614      0.0000   243.786000   \n",
       "140        1553.0    26.077904   506.922106      0.0319     0.131700   \n",
       "141        1553.0     0.000000     0.000000      0.0000     0.000000   \n",
       "142        1553.0     6.641565     3.552254      1.7400     5.110000   \n",
       "143        1558.0     0.004169     0.001282      0.0000     0.003300   \n",
       "144        1565.0     0.120008     0.061343      0.0324     0.083900   \n",
       "145        1565.0     0.063621     0.026541      0.0214     0.048000   \n",
       "146        1565.0     0.055010     0.021844      0.0227     0.042300   \n",
       "147        1565.0     0.017411     0.027123      0.0043     0.010000   \n",
       "148        1565.0     8.471308    18.740631      1.4208     6.359900   \n",
       "149        1564.0     0.000000     0.000000      0.0000     0.000000   \n",
       "150        1564.0     6.814268     3.241843      1.3370     4.459250   \n",
       "151        1564.0    14.047403    31.002541      2.0200     8.089750   \n",
       "152        1564.0     1.196733    23.364063      0.1544     0.373750   \n",
       "153        1564.0     0.011926     0.009346      0.0036     0.007275   \n",
       "154        1564.0     7.697971     5.239219      1.2438     5.926950   \n",
       "155        1557.0     0.507171     1.122427      0.1400     0.240000   \n",
       "156        1567.0     0.058089     0.079174      0.0111     0.036250   \n",
       "157         138.0     0.047104     0.039538      0.0118     0.027050   \n",
       "158         138.0  1039.650738   406.848810    234.0996   721.675050   \n",
       "159        1565.0   882.680511   983.043021      0.0000   411.000000   \n",
       "160        1565.0   555.346326   574.808588      0.0000   295.000000   \n",
       "161        1565.0  4066.850479  4239.245058      0.0000  1321.000000   \n",
       "162        1565.0  4797.154633  6553.569317      0.0000   451.000000   \n",
       "163        1565.0     0.140204     0.121989      0.0000     0.091000   \n",
       "164        1565.0     0.127942     0.242534      0.0000     0.068000   \n",
       "165        1565.0     0.252026     0.407329      0.0000     0.132000   \n",
       "166        1565.0     2.788882     1.119756      0.8000     2.100000   \n",
       "167        1565.0     1.235783     0.632767      0.3000     0.900000   \n",
       "168        1565.0     0.124397     0.047639      0.0330     0.090000   \n",
       "169        1565.0     0.400454     0.197918      0.0460     0.230000   \n",
       "170        1566.0     0.684330     0.157468      0.2979     0.575600   \n",
       "171        1566.0     0.120064     0.060785      0.0089     0.079800   \n",
       "172        1566.0     0.320113     0.071243      0.1287     0.276600   \n",
       "173        1566.0     0.576192     0.095734      0.2538     0.516800   \n",
       "174        1566.0     0.320113     0.071247      0.1287     0.276500   \n",
       "175        1566.0     0.778044     0.116322      0.4616     0.692200   \n",
       "176        1566.0     0.244718     0.074918      0.0735     0.196250   \n",
       "177        1566.0     0.394760     0.282903      0.0470     0.222000   \n",
       "178        1543.0     0.000000     0.000000      0.0000     0.000000   \n",
       "179        1566.0     0.000000     0.000000      0.0000     0.000000   \n",
       "180        1566.0    19.013257     3.311632      9.4000    16.850000   \n",
       "181        1566.0     0.546770     0.224402      0.0930     0.378000   \n",
       "182        1566.0    10.780543     4.164051      3.1700     7.732500   \n",
       "183        1566.0    26.661170     6.836101      5.0140    21.171500   \n",
       "184        1566.0     0.144815     0.110198      0.0297     0.102200   \n",
       "185        1566.0     7.365741     7.188720      1.9400     5.390000   \n",
       "186        1566.0     0.000000     0.000000      0.0000     0.000000   \n",
       "187        1566.0    17.936290     8.609912      6.2200    14.505000   \n",
       "188        1566.0    43.211418    21.711876      6.6130    24.711000   \n",
       "189        1566.0     0.000000     0.000000      0.0000     0.000000   \n",
       "190        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "191        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "192        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "193        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "194        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "195        1563.0     0.287084     0.395187      0.0800     0.218000   \n",
       "196        1560.0     8.688487    15.720926      1.7500     5.040000   \n",
       "197        1561.0    20.092710    10.552162      9.2200    17.130000   \n",
       "198        1561.0     0.557359     0.537705      0.0900     0.296000   \n",
       "199        1561.0    11.532056    16.445556      2.7700     6.740000   \n",
       "200        1560.0    17.600192     8.690718      3.2100    14.155000   \n",
       "201        1560.0     7.839359     5.104495      0.0000     5.020000   \n",
       "202        1560.0    10.170463    14.622904      0.0000     6.094000   \n",
       "203        1561.0    30.073143    17.461798      7.7280    24.653000   \n",
       "204        1561.0    32.218169   565.101239      0.0429     0.114300   \n",
       "205        1561.0     9.050122    11.541083      2.3000     6.040000   \n",
       "206        1561.0     0.001281     0.050621      0.0000     0.000000   \n",
       "207        1561.0    20.376176    17.497556      4.0100    16.350000   \n",
       "208        1561.0    73.264316    28.067143      5.3590    56.158000   \n",
       "209        1561.0     0.029564     1.168074      0.0000     0.000000   \n",
       "210        1543.0     0.088866     0.042065      0.0319     0.065600   \n",
       "211        1543.0     0.056755     0.025005      0.0022     0.043800   \n",
       "212        1543.0     0.051432     0.031578      0.0071     0.032500   \n",
       "213        1543.0     0.060346     0.053030      0.0037     0.036400   \n",
       "214        1543.0     0.083268     0.056456      0.0193     0.056800   \n",
       "215        1543.0     0.081076     0.030437      0.0059     0.063200   \n",
       "216        1543.0     0.083484     0.025764      0.0097     0.069550   \n",
       "217        1543.0     0.071635     0.046283      0.0079     0.045800   \n",
       "218        1566.0     3.771465     1.170436      1.0340     2.946100   \n",
       "219        1555.0     0.003254     0.001646      0.0007     0.002300   \n",
       "220         226.0     0.009213     0.001989      0.0057     0.007800   \n",
       "221        1567.0     0.060718     0.023305      0.0200     0.040200   \n",
       "222        1567.0     0.008821     0.055937      0.0003     0.001400   \n",
       "223        1567.0   122.846571    55.156003     32.2637    95.147350   \n",
       "224        1516.0     0.059370     0.071211      0.0093     0.029775   \n",
       "225        1516.0  1041.056588   433.170076    168.7998   718.725350   \n",
       "226        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "227        1565.0     0.019125     0.010756      0.0062     0.013200   \n",
       "228        1565.0     0.017844     0.010745      0.0072     0.012600   \n",
       "229        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "230        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "231        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "232        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "233        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "234        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "235        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "236        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "237        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "238        1565.0     0.004791     0.001698      0.0013     0.003700   \n",
       "239        1565.0     0.004575     0.001441      0.0014     0.003600   \n",
       "240        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "241        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "242        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "243        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "244         549.0     0.005755     0.084618      0.0003     0.001200   \n",
       "245         549.0     1.729723     4.335614      0.2914     0.911500   \n",
       "246         549.0     4.148742    10.045084      1.1022     2.725900   \n",
       "247         852.0     0.053374     0.066880      0.0000     0.019200   \n",
       "248        1567.0     0.025171     0.049235      0.0030     0.014700   \n",
       "249        1567.0     0.001065     0.015771      0.0000     0.000000   \n",
       "250        1567.0   109.650967    54.597274     21.0107    76.132150   \n",
       "251        1567.0     0.004285     0.037472      0.0003     0.000700   \n",
       "252        1567.0     4.645115    64.354756      0.7673     2.205650   \n",
       "253        1543.0     0.033216     0.022425      0.0094     0.024500   \n",
       "254        1567.0     0.013943     0.009132      0.0017     0.004700   \n",
       "255        1567.0     0.403848     0.120334      0.1269     0.307600   \n",
       "256        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "257        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "258        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "259        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "260        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "261        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "262        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "263        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "264        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "265        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "266        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "267        1559.0     0.070587     0.029649      0.0198     0.044000   \n",
       "268        1559.0    19.504677     7.344404      6.0980    13.828000   \n",
       "269        1559.0     3.777866     1.152329      1.3017     2.956500   \n",
       "270        1562.0    29.260291     8.402013     15.5471    24.982300   \n",
       "271        1561.0    46.056598    17.866438     10.4015    30.013900   \n",
       "272        1560.0    41.298147    17.737513      6.9431    27.092725   \n",
       "273        1553.0    20.181246     3.830463      8.6512    18.247100   \n",
       "274        1553.0   136.292426    85.607784      0.0000    81.215600   \n",
       "275        1553.0     8.693213   168.949413      0.0111     0.044700   \n",
       "276        1553.0     0.000000     0.000000      0.0000     0.000000   \n",
       "277        1553.0     2.210744     1.196437      0.5615     1.697700   \n",
       "278        1558.0     0.001117     0.000340      0.0000     0.000900   \n",
       "279        1565.0     0.041057     0.020289      0.0107     0.028300   \n",
       "280        1565.0     0.018034     0.006483      0.0073     0.014200   \n",
       "281        1565.0     0.015094     0.005545      0.0069     0.011900   \n",
       "282        1565.0     0.005770     0.008550      0.0016     0.003300   \n",
       "283        1565.0     2.803984     5.864324      0.5050     2.210400   \n",
       "284        1564.0     0.000000     0.000000      0.0000     0.000000   \n",
       "285        1564.0     2.119795     0.962923      0.4611     1.438175   \n",
       "286        1564.0     4.260018     9.763829      0.7280     2.467200   \n",
       "287        1564.0     0.367529     7.386343      0.0513     0.114875   \n",
       "288        1564.0     0.003924     0.002936      0.0012     0.002400   \n",
       "289        1564.0     2.578596     1.616993      0.3960     2.092125   \n",
       "290        1557.0     0.123427     0.270987      0.0416     0.064900   \n",
       "291        1567.0     0.019926     0.025549      0.0038     0.012500   \n",
       "292         138.0     0.014487     0.011494      0.0041     0.008725   \n",
       "293         138.0   335.551157   137.692483     82.3233   229.809450   \n",
       "294        1565.0   401.814750   477.050076      0.0000   185.089800   \n",
       "295        1565.0   252.999118   283.530702      0.0000   130.220300   \n",
       "296        1565.0  1879.228369  1975.111365      0.0000   603.032900   \n",
       "297        1565.0  2342.826978  3226.924298      0.0000   210.936600   \n",
       "298        1565.0     0.063804     0.064225      0.0000     0.040700   \n",
       "299        1565.0     0.060267     0.130825      0.0000     0.030200   \n",
       "300        1565.0     0.118386     0.219147      0.0000     0.058900   \n",
       "301        1565.0     0.910146     0.331982      0.3100     0.717200   \n",
       "302        1565.0     0.403342     0.197514      0.1118     0.295800   \n",
       "303        1565.0     0.040344     0.014511      0.0108     0.030000   \n",
       "304        1565.0     0.132076     0.064867      0.0138     0.072800   \n",
       "305        1566.0     0.264917     0.057387      0.1171     0.225000   \n",
       "306        1566.0     0.048623     0.025400      0.0034     0.033100   \n",
       "307        1566.0     0.128921     0.027468      0.0549     0.113700   \n",
       "308        1566.0     0.218414     0.033593      0.0913     0.197600   \n",
       "309        1566.0     0.128921     0.027470      0.0549     0.113700   \n",
       "310        1566.0     0.304752     0.043460      0.1809     0.278550   \n",
       "311        1566.0     0.097344     0.028796      0.0328     0.077600   \n",
       "312        1566.0     0.160051     0.117316      0.0224     0.091500   \n",
       "313        1543.0     0.000000     0.000000      0.0000     0.000000   \n",
       "314        1543.0     0.000000     0.000000      0.0000     0.000000   \n",
       "315        1566.0     0.000000     0.000000      0.0000     0.000000   \n",
       "316        1566.0     5.976977     1.018629      2.7882     5.301525   \n",
       "317        1566.0     0.172629     0.072392      0.0283     0.117375   \n",
       "318        1566.0     3.188770     1.215930      0.9848     2.319725   \n",
       "319        1566.0     7.916036     2.179059      1.6574     6.245150   \n",
       "320        1566.0     0.043105     0.031885      0.0084     0.031200   \n",
       "321        1566.0     2.263727     2.116994      0.6114     1.670075   \n",
       "322        1566.0     0.000000     0.000000      0.0000     0.000000   \n",
       "323        1566.0     5.393420     2.518859      1.7101     4.272950   \n",
       "324        1566.0    13.332172     6.615850      2.2345     7.578600   \n",
       "325        1566.0     0.000000     0.000000      0.0000     0.000000   \n",
       "326        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "327        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "328        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "329        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "330        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "331        1563.0     0.083232     0.063425      0.0224     0.068800   \n",
       "332        1560.0     2.593485     5.645226      0.5373     1.546550   \n",
       "333        1561.0     6.215866     3.403447      2.8372     5.453900   \n",
       "334        1561.0     0.168364     0.172883      0.0282     0.089400   \n",
       "335        1561.0     3.426925     5.781558      0.7899     2.035700   \n",
       "336        1560.0     9.736386     7.556131      5.2151     8.288525   \n",
       "337        1560.0     2.327482     1.699435      0.0000     1.542850   \n",
       "338        1560.0     3.037580     5.645022      0.0000     1.901350   \n",
       "339        1561.0     9.328958     6.074702      2.2001     7.588900   \n",
       "340        1561.0    14.673507   261.738451      0.0131     0.034600   \n",
       "341        1561.0     2.732094     3.667902      0.5741     1.911800   \n",
       "342        1561.0     0.000286     0.011319      0.0000     0.000000   \n",
       "343        1561.0     6.198508     5.371825      1.2565     4.998900   \n",
       "344        1561.0    23.217146     8.895221      2.0560    17.860900   \n",
       "345         773.0     7.958376    17.512965      1.7694     4.440600   \n",
       "346         773.0     5.770212    17.077498      1.0177     2.532700   \n",
       "347        1561.0     0.008914     0.352186      0.0000     0.000000   \n",
       "348        1543.0     0.024706     0.011862      0.0103     0.018000   \n",
       "349        1543.0     0.025252     0.010603      0.0010     0.019600   \n",
       "350        1543.0     0.023202     0.014326      0.0029     0.014600   \n",
       "351        1543.0     0.027584     0.024563      0.0020     0.016600   \n",
       "352        1543.0     0.023356     0.013157      0.0056     0.016000   \n",
       "353        1543.0     0.040331     0.015499      0.0026     0.030200   \n",
       "354        1543.0     0.041921     0.013068      0.0040     0.034850   \n",
       "355        1543.0     0.034543     0.022307      0.0038     0.021200   \n",
       "356        1566.0     1.298629     0.386918      0.3796     1.025475   \n",
       "357        1555.0     0.000999     0.000501      0.0003     0.000700   \n",
       "358         226.0     0.002443     0.000395      0.0017     0.002200   \n",
       "359        1567.0     0.019840     0.007136      0.0076     0.013800   \n",
       "360        1567.0     0.002945     0.020003      0.0001     0.000400   \n",
       "361        1567.0    39.936406    17.056304     10.7204    32.168700   \n",
       "362        1516.0     0.018383     0.021644      0.0028     0.009500   \n",
       "363        1516.0   333.319601   138.801928     60.9882   228.682525   \n",
       "364        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "365        1565.0     0.005199     0.002656      0.0017     0.003800   \n",
       "366        1565.0     0.004814     0.002382      0.0020     0.003500   \n",
       "367        1561.0     0.003773     0.002699      0.0000     0.002600   \n",
       "368        1561.0     0.003172     0.002107      0.0000     0.002200   \n",
       "369        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "370        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "371        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "372        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "373        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "374        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "375        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "376        1565.0     0.001601     0.000534      0.0004     0.001300   \n",
       "377        1565.0     0.001571     0.000467      0.0004     0.001300   \n",
       "378        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "379        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "380        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "381        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "382         549.0     0.001826     0.026740      0.0001     0.000400   \n",
       "383         549.0     0.541040     1.341020      0.0875     0.295500   \n",
       "384         549.0     1.285448     3.168427      0.3383     0.842300   \n",
       "385         852.0     0.011427     0.014366      0.0000     0.005300   \n",
       "386        1567.0     0.008281     0.015488      0.0008     0.004800   \n",
       "387        1567.0     0.000339     0.004989      0.0000     0.000000   \n",
       "388        1567.0    35.155091    17.227003      6.3101    24.386550   \n",
       "389        1567.0     0.001338     0.011816      0.0001     0.000200   \n",
       "390        1567.0     1.431868    20.326415      0.3046     0.675150   \n",
       "391        1543.0     0.010956     0.006738      0.0031     0.008300   \n",
       "392        1567.0     0.004533     0.002956      0.0005     0.001500   \n",
       "393        1567.0     0.133990     0.038408      0.0342     0.104400   \n",
       "394        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "395        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "396        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "397        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "398        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "399        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "400        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "401        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "402        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "403        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "404        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "405        1559.0     0.024208     0.010728      0.0062     0.014000   \n",
       "406        1559.0     6.730615     2.829583      2.0545     4.547600   \n",
       "407        1559.0     1.231997     0.364711      0.4240     0.966500   \n",
       "408        1562.0     5.340932     2.578118      2.7378     4.127800   \n",
       "409        1561.0     4.580430     1.776843      1.2163     3.012800   \n",
       "410        1560.0     4.929344     2.122978      0.7342     3.265075   \n",
       "411        1553.0     2.616086     0.551474      0.9609     2.321300   \n",
       "412        1553.0    30.911316    18.413622      0.0000    18.407900   \n",
       "413        1553.0    25.612690    47.308463      4.0416    11.375800   \n",
       "414        1553.0     0.000000     0.000000      0.0000     0.000000   \n",
       "415        1553.0     6.630616     3.958371      1.5340     4.927400   \n",
       "416        1558.0     3.404349     1.035433      0.0000     2.660100   \n",
       "417        1565.0     8.190905     4.054515      2.1531     5.765500   \n",
       "418        1565.0   320.259235   287.704482      0.0000     0.000000   \n",
       "419        1565.0   309.061299   325.448391      0.0000     0.000000   \n",
       "420        1565.0     1.821261     3.057692      0.4411     1.030400   \n",
       "421        1565.0     4.174524     6.913855      0.7217     3.184200   \n",
       "422        1564.0     0.000000     0.000000      0.0000     0.000000   \n",
       "423        1564.0    77.660446    32.596933     23.0200    55.976675   \n",
       "424        1564.0     3.315469     6.325365      0.4866     1.965250   \n",
       "425        1564.0     6.796312    23.257716      1.4666     3.766200   \n",
       "426        1564.0     1.233858     0.995620      0.3632     0.743425   \n",
       "427        1564.0     4.058501     3.042144      0.6637     3.113225   \n",
       "428        1557.0     4.220747    10.632730      1.1198     1.935500   \n",
       "429        1567.0     4.171844     6.435390      0.7837     2.571400   \n",
       "430        1565.0    18.421600    36.060084      0.0000     6.999700   \n",
       "431        1565.0    22.358305    36.395408      0.0000    11.059000   \n",
       "432        1565.0    99.367633   126.188715      0.0000    31.032400   \n",
       "433        1565.0   205.519304   225.778870      0.0000    10.027100   \n",
       "434        1565.0    14.733945    34.108854      0.0000     7.550700   \n",
       "435        1565.0     9.370666    34.369789      0.0000     3.494400   \n",
       "436        1565.0     7.513266    34.557804      0.0000     1.950900   \n",
       "437        1565.0     4.016785     1.611274      1.1568     3.070700   \n",
       "438        1565.0    54.701052    34.108051      0.0000    36.290300   \n",
       "439        1565.0    70.643942    38.376178     14.1206    48.173800   \n",
       "440        1565.0    11.526617     6.169471      1.0973     5.414100   \n",
       "441        1566.0     0.802081     0.184213      0.3512     0.679600   \n",
       "442        1566.0     1.345259     0.659195      0.0974     0.907650   \n",
       "443        1566.0     0.633941     0.143552      0.2169     0.550500   \n",
       "444        1566.0     0.895043     0.155522      0.3336     0.804800   \n",
       "445        1566.0     0.647090     0.141252      0.3086     0.555800   \n",
       "446        1566.0     1.175003     0.176158      0.6968     1.046800   \n",
       "447        1566.0     0.281895     0.086461      0.0846     0.226100   \n",
       "448        1566.0     0.332270     0.236275      0.0399     0.187700   \n",
       "449        1543.0     0.000000     0.000000      0.0000     0.000000   \n",
       "450        1543.0     0.000000     0.000000      0.0000     0.000000   \n",
       "451        1566.0     0.000000     0.000000      0.0000     0.000000   \n",
       "452        1566.0     5.346816     0.919196      2.6709     4.764200   \n",
       "453        1566.0     5.460971     2.250804      0.9037     3.747875   \n",
       "454        1566.0     7.883742     3.059660      2.3294     5.806525   \n",
       "455        1566.0     3.636633     0.938372      0.6948     2.899675   \n",
       "456        1566.0    12.325685     8.125876      3.0489     8.816575   \n",
       "457        1566.0     5.263666     4.537737      1.4428     3.827525   \n",
       "458        1566.0     0.000000     0.000000      0.0000     0.000000   \n",
       "459        1566.0     2.838380     1.345576      0.9910     2.291175   \n",
       "460        1566.0    29.197414    13.335189      7.9534    20.221850   \n",
       "461        1566.0     0.000000     0.000000      0.0000     0.000000   \n",
       "462        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "463        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "464        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "465        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "466        1563.0     0.000000     0.000000      0.0000     0.000000   \n",
       "467        1563.0     6.252091     8.673724      1.7163     4.697500   \n",
       "468        1560.0   224.173047   230.766915      0.0000    38.472775   \n",
       "469        1561.0     5.662293     3.151685      2.6009     4.847200   \n",
       "470        1561.0     5.367752     4.983367      0.8325     2.823300   \n",
       "471        1561.0     9.638797    10.174117      2.4026     5.807300   \n",
       "472        1560.0   137.888406    47.698041     11.4997   105.525150   \n",
       "473        1560.0    39.426847    22.457104      0.0000    24.900800   \n",
       "474        1560.0    37.637050    24.822918      0.0000    23.156500   \n",
       "475        1561.0     4.262573     2.611174      1.1011     3.494500   \n",
       "476        1561.0    20.132155    14.939590      0.0000    11.577100   \n",
       "477        1561.0     6.257921    10.185026      1.6872     4.105400   \n",
       "478        1561.0     0.128123     5.062075      0.0000     0.000000   \n",
       "479        1561.0     3.283394     2.638608      0.6459     2.627700   \n",
       "480        1561.0    75.538131    35.752493      8.8406    52.894500   \n",
       "481        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "482        1543.0   318.418448   281.011323      0.0000     0.000000   \n",
       "483        1543.0   206.564196   192.864413      0.0000    81.316150   \n",
       "484        1543.0   215.288948   213.126638      0.0000    76.455400   \n",
       "485        1543.0   201.111728   218.690015      0.0000    50.383550   \n",
       "486        1543.0   302.506186   287.364070      0.0000     0.000000   \n",
       "487        1543.0   239.455326   263.837645      0.0000    55.555150   \n",
       "488        1543.0   352.616477   252.043751      0.0000   139.914350   \n",
       "489        1543.0   272.169707   228.046702      0.0000   112.859250   \n",
       "490        1566.0    51.354045    18.048612     13.7225    38.391100   \n",
       "491        1555.0     2.442673     1.224283      0.5558     1.747100   \n",
       "492         226.0     8.170943     1.759262      4.8882     6.924650   \n",
       "493        1567.0     2.530046     0.973948      0.8330     1.663750   \n",
       "494        1567.0     0.956442     6.615200      0.0342     0.139000   \n",
       "495        1567.0     6.807826     3.260019      1.7720     5.274600   \n",
       "496        1516.0    29.865896    24.621586      4.8135    16.342300   \n",
       "497        1516.0    11.821030     4.956647      1.9496     8.150350   \n",
       "498        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "499        1565.0   263.195864   324.771342      0.0000     0.000000   \n",
       "500        1565.0   240.981377   323.003410      0.0000     0.000000   \n",
       "501        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "502        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "503        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "504        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "505        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "506        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "507        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "508        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "509        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "510        1565.0    55.763508    37.691736      0.0000    35.322200   \n",
       "511        1565.0   275.979457   329.664680      0.0000     0.000000   \n",
       "512        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "513        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "514        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "515        1561.0     0.000000     0.000000      0.0000     0.000000   \n",
       "516         549.0     0.678898    10.783880      0.0287     0.121500   \n",
       "517         549.0     1.738902     4.890663      0.2880     0.890300   \n",
       "518         549.0     1.806273     4.715894      0.4674     1.171200   \n",
       "519         852.0    11.728440    15.814420      0.0000     4.160300   \n",
       "520        1567.0     2.695999     5.702366      0.3121     1.552150   \n",
       "521        1567.0    11.610080   103.122996      0.0000     0.000000   \n",
       "522        1567.0    14.728866     7.104435      2.6811    10.182800   \n",
       "523        1567.0     0.453896     4.147581      0.0258     0.073050   \n",
       "524        1567.0     5.687782    20.663414      1.3104     3.769650   \n",
       "525        1543.0     5.560397     3.920370      1.5400     4.101500   \n",
       "526        1567.0     1.443457     0.958428      0.1705     0.484200   \n",
       "527        1567.0     6.395717     1.888698      2.1700     4.895450   \n",
       "528        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "529        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "530        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "531        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "532        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "533        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "534        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "535        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "536        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "537        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "538        1558.0     0.000000     0.000000      0.0000     0.000000   \n",
       "539        1559.0     3.034235     1.252913      0.8516     1.889900   \n",
       "540        1559.0     1.942828     0.731928      0.6144     1.385300   \n",
       "541        1559.0     9.611628     2.896376      3.2761     7.495750   \n",
       "542        1565.0     0.111208     0.002737      0.1053     0.109600   \n",
       "543        1565.0     0.008471     0.001534      0.0051     0.007800   \n",
       "544        1565.0     0.002509     0.000296      0.0016     0.002400   \n",
       "545        1565.0     7.611403     1.315544      4.4294     7.116000   \n",
       "546        1307.0     1.039630     0.389066      0.4444     0.797500   \n",
       "547        1307.0   403.546477     5.063887    372.8220   400.694000   \n",
       "548        1307.0    75.679871     3.390523     71.0380    73.254000   \n",
       "549        1307.0     0.663256     0.673346      0.0446     0.226250   \n",
       "550        1307.0    17.013313     4.966954      6.1100    14.530000   \n",
       "551        1307.0     1.230712     1.361117      0.1200     0.870000   \n",
       "552        1307.0     0.276688     0.276231      0.0187     0.094900   \n",
       "553        1307.0     7.703874     2.192647      2.7860     6.738100   \n",
       "554        1307.0     0.503657     0.598852      0.0520     0.343800   \n",
       "555        1307.0    57.746537    35.207552      4.8269    27.017600   \n",
       "556        1307.0     4.216905     1.280008      1.4967     3.625100   \n",
       "557        1307.0     1.623070     1.870433      0.1646     1.182900   \n",
       "558        1566.0     0.995009     0.083860      0.8919     0.955200   \n",
       "559        1566.0     0.325708     0.201392      0.0699     0.149825   \n",
       "560        1566.0     0.072443     0.051578      0.0177     0.036200   \n",
       "561        1566.0    32.284956    19.026081      7.2369    15.762450   \n",
       "562        1294.0   262.729683     7.630585    242.2860   259.972500   \n",
       "563        1294.0     0.679641     0.121758      0.3049     0.567100   \n",
       "564        1294.0     6.444985     2.633583      0.9700     4.980000   \n",
       "565        1294.0     0.145610     0.081122      0.0224     0.087700   \n",
       "566        1294.0     2.610870     1.032761      0.4122     2.090200   \n",
       "567        1294.0     0.060086     0.032761      0.0091     0.038200   \n",
       "568        1294.0     2.452417     0.996644      0.3706     1.884400   \n",
       "569        1294.0    21.117674    10.213294      3.2504    15.466200   \n",
       "570        1567.0   530.523623    17.499736    317.1964   530.702700   \n",
       "571        1567.0     2.101836     0.275112      0.9802     1.982900   \n",
       "572        1567.0    28.450165    86.304681      3.5400     7.500000   \n",
       "573        1567.0     0.345636     0.248478      0.0667     0.242250   \n",
       "574        1567.0     9.162315    26.920150      1.0395     2.567850   \n",
       "575        1567.0     0.104729     0.067791      0.0230     0.075100   \n",
       "576        1567.0     5.563747    16.921369      0.6636     1.408450   \n",
       "577        1567.0    16.642363    12.485267      4.5820    11.501550   \n",
       "578         618.0     0.021615     0.011730     -0.0169     0.013800   \n",
       "579         618.0     0.016829     0.009640      0.0032     0.010600   \n",
       "580         618.0     0.005396     0.003116      0.0010     0.003400   \n",
       "581         618.0    97.934373    87.520966      0.0000    46.184900   \n",
       "582        1566.0     0.500096     0.003404      0.4778     0.497900   \n",
       "583        1566.0     0.015318     0.017180      0.0060     0.011600   \n",
       "584        1566.0     0.003847     0.003720      0.0017     0.003100   \n",
       "585        1566.0     3.067826     3.578033      1.1975     2.306500   \n",
       "586        1566.0     0.021458     0.012358     -0.0169     0.013425   \n",
       "587        1566.0     0.016475     0.008808      0.0032     0.010600   \n",
       "588        1566.0     0.005283     0.002867      0.0010     0.003300   \n",
       "589        1566.0    99.670066    93.891919      0.0000    44.368600   \n",
       "Pass/Fail  1567.0    -0.867262     0.498010     -1.0000    -1.000000   \n",
       "\n",
       "                  50%          75%         max  \n",
       "0          3011.49000  3056.650000   3356.3500  \n",
       "1          2499.40500  2538.822500   2846.4400  \n",
       "2          2201.06670  2218.055500   2315.2667  \n",
       "3          1285.21440  1591.223500   3715.0417  \n",
       "4             1.31680     1.525700   1114.5366  \n",
       "5           100.00000   100.000000    100.0000  \n",
       "6           101.51220   104.586700    129.2522  \n",
       "7             0.12240     0.123800      0.1286  \n",
       "8             1.46160     1.516900      1.6564  \n",
       "9            -0.00130     0.008400      0.0749  \n",
       "10            0.00040     0.005900      0.0530  \n",
       "11            0.96580     0.971300      0.9848  \n",
       "12          199.53560   202.007100    272.0451  \n",
       "13            0.00000     0.000000      0.0000  \n",
       "14            8.96700    10.861875     19.5465  \n",
       "15          412.21910   419.089275    824.9271  \n",
       "16            9.85175    10.128175    102.8677  \n",
       "17            0.97260     0.976800      0.9848  \n",
       "18          189.66420   192.189375    215.5977  \n",
       "19           12.49960    12.547100     12.9898  \n",
       "20            1.40600     1.415000      1.4534  \n",
       "21        -5523.25000 -5356.250000      0.0000  \n",
       "22         2664.00000  2841.750000   3656.2500  \n",
       "23        -3820.75000 -3352.750000   2363.0000  \n",
       "24          -78.75000  1377.250000  14106.0000  \n",
       "25            1.28300     1.304300      1.3828  \n",
       "26            1.98650     2.003200      2.0528  \n",
       "27            7.26470     7.329700      7.6588  \n",
       "28           69.15560    72.266700     77.9000  \n",
       "29            2.37780     2.655600      3.5111  \n",
       "30            0.18670     0.207100      0.2851  \n",
       "31            3.43100     3.531300      4.8044  \n",
       "32           85.13545    85.741900    105.6038  \n",
       "33            8.76980     9.060600     23.3453  \n",
       "34           50.39640    50.578800     59.7711  \n",
       "35           64.16580    64.344700     94.2641  \n",
       "36           49.60360    49.747650     50.1652  \n",
       "37           66.23180    66.343275     67.9586  \n",
       "38           86.82070    87.002400     88.4188  \n",
       "39          118.39930   118.939600    133.3898  \n",
       "40           78.29000    80.200000     86.1200  \n",
       "41            3.07400     3.521000     37.8800  \n",
       "42           70.00000    70.000000     70.0000  \n",
       "43          353.72090   360.772250    377.2973  \n",
       "44           10.03485    10.152475     11.0530  \n",
       "45          136.40000   142.098225    176.3136  \n",
       "46          733.45000   741.454500    789.7523  \n",
       "47            1.25105     1.340350      1.5111  \n",
       "48          140.00775   143.195700    163.2509  \n",
       "49            1.00000     1.000000      1.0000  \n",
       "50          631.37090   638.136325    667.7418  \n",
       "51          183.31815   206.977150    258.5432  \n",
       "52            0.00000     0.000000      0.0000  \n",
       "53            4.59600     4.617000      4.7640  \n",
       "54            4.84300     4.869000      5.0110  \n",
       "55         2854.00000  2874.000000   2936.0000  \n",
       "56            0.93100     0.933100      0.9378  \n",
       "57            0.94930     0.952000      0.9598  \n",
       "58            4.57270     4.668600      4.8475  \n",
       "59            0.94725     4.385225    168.1455  \n",
       "60          353.79910   359.673600    373.8664  \n",
       "61           10.43670    10.591600     11.7849  \n",
       "62          116.21180   120.927300    287.1509  \n",
       "63           13.24605    16.376100    188.0923  \n",
       "64           20.02135    22.813625     48.9882  \n",
       "65           26.26145    29.914950    118.0836  \n",
       "66          706.45360   714.597000    770.6084  \n",
       "67            0.97830     1.065000   7272.8283  \n",
       "68          147.59730   149.959100    167.8309  \n",
       "69            1.00000     1.000000      1.0000  \n",
       "70          619.03270   625.170000    722.6018  \n",
       "71          102.60430   115.498900    238.4775  \n",
       "72          152.29720   158.437800    175.4132  \n",
       "73          466.08170   467.889900    692.4256  \n",
       "74            0.00000     0.000000      4.1955  \n",
       "75           -0.00630     0.007100      0.2315  \n",
       "76           -0.02890    -0.006500      0.0723  \n",
       "77           -0.00990     0.009250      0.1331  \n",
       "78           -0.01250     0.012200      0.2492  \n",
       "79            0.00060     0.013200      0.1013  \n",
       "80           -0.00870     0.009100      0.1186  \n",
       "81           -0.01960    -0.012000      0.0584  \n",
       "82            0.00760     0.026900      0.1437  \n",
       "83            7.46745     7.807625      8.9904  \n",
       "84            0.13300     0.136300      0.1505  \n",
       "85            0.11355     0.114900      0.1184  \n",
       "86            2.40390     2.428600      2.5555  \n",
       "87            0.98740     0.989700      0.9935  \n",
       "88         1809.24920  1841.873000   2105.1823  \n",
       "89            0.19010     0.200425      1.4727  \n",
       "90         8825.43510  9065.432400  10746.6000  \n",
       "91            0.00000     0.050700      0.3627  \n",
       "92            0.00040     0.002000      0.0281  \n",
       "93           -0.00020     0.001000      0.0133  \n",
       "94            0.00000     0.000100      0.0011  \n",
       "95            0.00000     0.000100      0.0009  \n",
       "96            0.00390     0.122000      2.5093  \n",
       "97            0.00000     0.000000      0.0000  \n",
       "98            0.00000     0.189300      2.5698  \n",
       "99            0.00000     0.029800      0.8854  \n",
       "100           0.00000     0.000200      0.0023  \n",
       "101           0.00000     0.000100      0.0017  \n",
       "102           0.00000     0.033600      0.2979  \n",
       "103          -0.01010    -0.008200      0.0203  \n",
       "104           0.00000     0.000400      0.0071  \n",
       "105          -0.00020     0.001100      0.0127  \n",
       "106           0.00020     0.001600      0.0172  \n",
       "107           0.00000     0.049000      0.4856  \n",
       "108          -0.01120     0.038000      0.3938  \n",
       "109           0.98100     0.982300      0.9842  \n",
       "110         101.48170   102.078100    106.9227  \n",
       "111         231.20120   233.036100    236.9546  \n",
       "112           0.46285     0.466425      0.4885  \n",
       "113           0.94640     0.952300      0.9763  \n",
       "114           0.00000     0.000000      0.0414  \n",
       "115         750.86140   776.781850    924.5318  \n",
       "116           0.99050     0.990900      0.9924  \n",
       "117          58.54910    59.133900    311.7344  \n",
       "118           0.59900     0.603400      0.6245  \n",
       "119           0.96940     0.978300      0.9827  \n",
       "120           6.31360     6.375850      7.5220  \n",
       "121          15.79000    15.860000     16.0700  \n",
       "122           3.87700     4.392000      6.8890  \n",
       "123          15.83000    15.900000     16.1000  \n",
       "124          15.78000    15.870000     16.1000  \n",
       "125           1.14400     1.338000      2.4650  \n",
       "126           2.73500     2.873000      3.9910  \n",
       "127           0.65390     0.713500      1.1750  \n",
       "128           3.19500     3.311000      3.8950  \n",
       "129          -0.14190     0.047300      2.4580  \n",
       "130           0.75875     0.814500      0.8884  \n",
       "131           0.99775     0.998900      1.0190  \n",
       "132           2.31240     2.358300      2.4723  \n",
       "133        1004.05000  1008.670600   1020.9944  \n",
       "134          38.90260    40.804600     64.1287  \n",
       "135         109.00000   127.000000    994.0000  \n",
       "136         134.60000   181.000000    295.8000  \n",
       "137         117.70000   161.600000    334.7000  \n",
       "138          55.90010    62.900100    141.7998  \n",
       "139         339.56100   502.205900   1770.6909  \n",
       "140           0.23580     0.439100   9998.8944  \n",
       "141           0.00000     0.000000      0.0000  \n",
       "142           6.26000     7.500000    103.3900  \n",
       "143           0.00390     0.004900      0.0121  \n",
       "144           0.10750     0.132700      0.6253  \n",
       "145           0.05860     0.071800      0.2507  \n",
       "146           0.05000     0.061500      0.2479  \n",
       "147           0.01590     0.021300      0.9783  \n",
       "148           7.91730     9.585300    742.9421  \n",
       "149           0.00000     0.000000      0.0000  \n",
       "150           5.95100     8.275000     22.3180  \n",
       "151          10.99350    14.347250    536.5640  \n",
       "152           0.46870     0.679925    924.3780  \n",
       "153           0.01110     0.014900      0.2389  \n",
       "154           7.51270     9.054675    191.5478  \n",
       "155           0.32000     0.450000     12.7100  \n",
       "156           0.04870     0.066700      2.2016  \n",
       "157           0.03545     0.048875      0.2876  \n",
       "158        1020.30005  1277.750125   2505.2998  \n",
       "159         623.00000   966.000000   7791.0000  \n",
       "160         438.00000   625.000000   4170.0000  \n",
       "161        2614.00000  5034.000000  37943.0000  \n",
       "162        1784.00000  6384.000000  36871.0000  \n",
       "163           0.12000     0.154000      0.9570  \n",
       "164           0.08900     0.116000      1.8170  \n",
       "165           0.18400     0.255000      3.2860  \n",
       "166           2.60000     3.200000     21.1000  \n",
       "167           1.20000     1.500000     16.3000  \n",
       "168           0.11900     0.151000      0.7250  \n",
       "169           0.41200     0.536000      1.1430  \n",
       "170           0.68600     0.797300      1.1530  \n",
       "171           0.11250     0.140300      0.4940  \n",
       "172           0.32385     0.370200      0.5484  \n",
       "173           0.57760     0.634500      0.8643  \n",
       "174           0.32385     0.370200      0.5484  \n",
       "175           0.76820     0.843900      1.1720  \n",
       "176           0.24290     0.293925      0.4411  \n",
       "177           0.29900     0.423000      1.8580  \n",
       "178           0.00000     0.000000      0.0000  \n",
       "179           0.00000     0.000000      0.0000  \n",
       "180          18.69000    20.972500     48.6700  \n",
       "181           0.52400     0.688750      3.5730  \n",
       "182          10.17000    13.337500     55.0000  \n",
       "183          27.20050    31.687000     72.9470  \n",
       "184           0.13260     0.169150      3.2283  \n",
       "185           6.73500     8.450000    267.9100  \n",
       "186           0.00000     0.000000      0.0000  \n",
       "187          17.86500    20.860000    307.9300  \n",
       "188          40.20950    57.674750    191.8300  \n",
       "189           0.00000     0.000000      0.0000  \n",
       "190           0.00000     0.000000      0.0000  \n",
       "191           0.00000     0.000000      0.0000  \n",
       "192           0.00000     0.000000      0.0000  \n",
       "193           0.00000     0.000000      0.0000  \n",
       "194           0.00000     0.000000      0.0000  \n",
       "195           0.25900     0.296000      4.8380  \n",
       "196           6.78000     9.555000    396.1100  \n",
       "197          19.37000    21.460000    252.8700  \n",
       "198           0.42400     0.726000     10.0170  \n",
       "199           8.57000    11.460000    390.1200  \n",
       "200          17.23500    20.162500    199.6200  \n",
       "201           6.76000     9.490000    126.5300  \n",
       "202           8.46200    11.953000    490.5610  \n",
       "203          30.09700    33.506000    500.3490  \n",
       "204           0.15820     0.230700   9998.4483  \n",
       "205           7.74000     9.940000    320.0500  \n",
       "206           0.00000     0.000000      2.0000  \n",
       "207          19.72000    22.370000    457.6500  \n",
       "208          73.24800    90.515000    172.3490  \n",
       "209           0.00000     0.000000     46.1500  \n",
       "210           0.07970     0.099450      0.5164  \n",
       "211           0.05320     0.064200      0.3227  \n",
       "212           0.04160     0.062450      0.5941  \n",
       "213           0.05600     0.073700      1.2837  \n",
       "214           0.07540     0.093550      0.7615  \n",
       "215           0.08250     0.098300      0.3429  \n",
       "216           0.08460     0.097550      0.2828  \n",
       "217           0.06170     0.086350      0.6744  \n",
       "218           3.63075     4.404750      8.8015  \n",
       "219           0.00300     0.003800      0.0163  \n",
       "220           0.00895     0.010300      0.0240  \n",
       "221           0.06090     0.076500      0.2305  \n",
       "222           0.00230     0.005500      0.9911  \n",
       "223         119.43600   144.502800   1768.8802  \n",
       "224           0.03980     0.061300      1.4361  \n",
       "225         967.29980  1261.299800   3601.2998  \n",
       "226           0.00000     0.000000      0.0000  \n",
       "227           0.01650     0.021200      0.1541  \n",
       "228           0.01550     0.020000      0.2133  \n",
       "229           0.00000     0.000000      0.0000  \n",
       "230           0.00000     0.000000      0.0000  \n",
       "231           0.00000     0.000000      0.0000  \n",
       "232           0.00000     0.000000      0.0000  \n",
       "233           0.00000     0.000000      0.0000  \n",
       "234           0.00000     0.000000      0.0000  \n",
       "235           0.00000     0.000000      0.0000  \n",
       "236           0.00000     0.000000      0.0000  \n",
       "237           0.00000     0.000000      0.0000  \n",
       "238           0.00460     0.005700      0.0244  \n",
       "239           0.00440     0.005300      0.0236  \n",
       "240           0.00000     0.000000      0.0000  \n",
       "241           0.00000     0.000000      0.0000  \n",
       "242           0.00000     0.000000      0.0000  \n",
       "243           0.00000     0.000000      0.0000  \n",
       "244           0.00170     0.002600      1.9844  \n",
       "245           1.18510     1.761800     99.9022  \n",
       "246           3.67300     4.479700    237.1837  \n",
       "247           0.02700     0.051500      0.4914  \n",
       "248           0.02100     0.027300      0.9732  \n",
       "249           0.00000     0.000000      0.4138  \n",
       "250         103.09360   131.758400   1119.7042  \n",
       "251           0.00100     0.001300      0.9909  \n",
       "252           2.86460     3.795050   2549.9885  \n",
       "253           0.03080     0.037900      0.4517  \n",
       "254           0.01500     0.021300      0.0787  \n",
       "255           0.40510     0.480950      0.9255  \n",
       "256           0.00000     0.000000      0.0000  \n",
       "257           0.00000     0.000000      0.0000  \n",
       "258           0.00000     0.000000      0.0000  \n",
       "259           0.00000     0.000000      0.0000  \n",
       "260           0.00000     0.000000      0.0000  \n",
       "261           0.00000     0.000000      0.0000  \n",
       "262           0.00000     0.000000      0.0000  \n",
       "263           0.00000     0.000000      0.0000  \n",
       "264           0.00000     0.000000      0.0000  \n",
       "265           0.00000     0.000000      0.0000  \n",
       "266           0.00000     0.000000      0.0000  \n",
       "267           0.07060     0.091650      0.1578  \n",
       "268          17.97700    24.653000     40.8550  \n",
       "269           3.70350     4.379400     10.1529  \n",
       "270          28.77350    31.702200    158.5260  \n",
       "271          45.67650    59.594700    132.6479  \n",
       "272          40.01925    54.277325    122.1174  \n",
       "273          19.58090    22.097300     43.5737  \n",
       "274         110.60140   162.038200    659.1696  \n",
       "275           0.07840     0.144900   3332.5964  \n",
       "276           0.00000     0.000000      0.0000  \n",
       "277           2.08310     2.514300     32.1709  \n",
       "278           0.00110     0.001300      0.0034  \n",
       "279           0.03720     0.045800      0.1884  \n",
       "280           0.01690     0.020700      0.0755  \n",
       "281           0.01390     0.016600      0.0597  \n",
       "282           0.00530     0.007100      0.3083  \n",
       "283           2.65800     3.146200    232.8049  \n",
       "284           0.00000     0.000000      0.0000  \n",
       "285           1.87515     2.606950      6.8698  \n",
       "286           3.36005     4.311425    207.0161  \n",
       "287           0.13895     0.198450    292.2274  \n",
       "288           0.00360     0.004900      0.0749  \n",
       "289           2.54900     3.024525     59.5187  \n",
       "290           0.08330     0.118100      4.4203  \n",
       "291           0.01690     0.023600      0.6915  \n",
       "292           0.01100     0.014925      0.0831  \n",
       "293         317.86710   403.989300    879.2260  \n",
       "294         278.67190   428.554500   3933.7550  \n",
       "295         195.82560   273.952600   2005.8744  \n",
       "296        1202.41210  2341.288700  15559.9525  \n",
       "297         820.09880  3190.616400  18520.4683  \n",
       "298           0.05280     0.069200      0.5264  \n",
       "299           0.04000     0.052000      1.0312  \n",
       "300           0.08280     0.115500      1.8123  \n",
       "301           0.86040     1.046400      5.7110  \n",
       "302           0.38080     0.477000      5.1549  \n",
       "303           0.03880     0.048600      0.2258  \n",
       "304           0.13720     0.178500      0.3337  \n",
       "305           0.26430     0.307500      0.4750  \n",
       "306           0.04480     0.055200      0.2246  \n",
       "307           0.12950     0.147600      0.2112  \n",
       "308           0.21945     0.237900      0.3239  \n",
       "309           0.12950     0.147600      0.2112  \n",
       "310           0.30290     0.331900      0.4438  \n",
       "311           0.09770     0.115900      0.1784  \n",
       "312           0.12150     0.160175      0.7549  \n",
       "313           0.00000     0.000000      0.0000  \n",
       "314           0.00000     0.000000      0.0000  \n",
       "315           0.00000     0.000000      0.0000  \n",
       "316           5.83150     6.547800     13.0958  \n",
       "317           0.16340     0.218100      1.0034  \n",
       "318           2.89890     4.021250     15.8934  \n",
       "319           8.38880     9.481100     20.0455  \n",
       "320           0.03985     0.050200      0.9474  \n",
       "321           2.07765     2.633350     79.1515  \n",
       "322           0.00000     0.000000      0.0000  \n",
       "323           5.45880     6.344875     89.1917  \n",
       "324          12.50450    17.925175     51.8678  \n",
       "325           0.00000     0.000000      0.0000  \n",
       "326           0.00000     0.000000      0.0000  \n",
       "327           0.00000     0.000000      0.0000  \n",
       "328           0.00000     0.000000      0.0000  \n",
       "329           0.00000     0.000000      0.0000  \n",
       "330           0.00000     0.000000      0.0000  \n",
       "331           0.08480     0.095600      1.0959  \n",
       "332           2.06270     2.790525    174.8944  \n",
       "333           5.98010     6.549500     90.5159  \n",
       "334           0.12940     0.210400      3.4125  \n",
       "335           2.51350     3.360400    172.7119  \n",
       "336           9.07355    10.041625    214.8628  \n",
       "337           2.05445     2.785475     38.8995  \n",
       "338           2.56085     3.405450    196.6880  \n",
       "339           9.47420    10.439900    197.4988  \n",
       "340           0.04640     0.066800   5043.8789  \n",
       "341           2.37730     2.985400     97.7089  \n",
       "342           0.00000     0.000000      0.4472  \n",
       "343           6.00560     6.885200    156.3360  \n",
       "344          23.21470    28.873100     59.3241  \n",
       "345           5.56700     6.825500    257.0106  \n",
       "346           3.04640     4.085700    187.7589  \n",
       "347           0.00000     0.000000     13.9147  \n",
       "348           0.02260     0.027300      0.2200  \n",
       "349           0.02400     0.028600      0.1339  \n",
       "350           0.01880     0.028500      0.2914  \n",
       "351           0.02530     0.033900      0.6188  \n",
       "352           0.02200     0.026900      0.1429  \n",
       "353           0.04210     0.050200      0.1535  \n",
       "354           0.04420     0.050000      0.1344  \n",
       "355           0.02940     0.042300      0.2789  \n",
       "356           1.25530     1.533325      2.8348  \n",
       "357           0.00090     0.001100      0.0052  \n",
       "358           0.00240     0.002700      0.0047  \n",
       "359           0.01960     0.025000      0.0888  \n",
       "360           0.00070     0.001800      0.4090  \n",
       "361          39.69610    47.079200    547.1722  \n",
       "362           0.01250     0.018600      0.4163  \n",
       "363         309.83165   412.329775   1072.2031  \n",
       "364           0.00000     0.000000      0.0000  \n",
       "365           0.00460     0.005800      0.0368  \n",
       "366           0.00430     0.005400      0.0392  \n",
       "367           0.00320     0.004200      0.0357  \n",
       "368           0.00280     0.003600      0.0334  \n",
       "369           0.00000     0.000000      0.0000  \n",
       "370           0.00000     0.000000      0.0000  \n",
       "371           0.00000     0.000000      0.0000  \n",
       "372           0.00000     0.000000      0.0000  \n",
       "373           0.00000     0.000000      0.0000  \n",
       "374           0.00000     0.000000      0.0000  \n",
       "375           0.00000     0.000000      0.0000  \n",
       "376           0.00160     0.001900      0.0082  \n",
       "377           0.00150     0.001800      0.0077  \n",
       "378           0.00000     0.000000      0.0000  \n",
       "379           0.00000     0.000000      0.0000  \n",
       "380           0.00000     0.000000      0.0000  \n",
       "381           0.00000     0.000000      0.0000  \n",
       "382           0.00050     0.000800      0.6271  \n",
       "383           0.37260     0.541200     30.9982  \n",
       "384           1.10630     1.386600     74.8445  \n",
       "385           0.00680     0.011325      0.2073  \n",
       "386           0.00680     0.009300      0.3068  \n",
       "387           0.00000     0.000000      0.1309  \n",
       "388          32.53070    42.652450    348.8293  \n",
       "389           0.00030     0.000400      0.3127  \n",
       "390           0.87730     1.148200    805.3936  \n",
       "391           0.01020     0.012400      0.1375  \n",
       "392           0.00490     0.006900      0.0229  \n",
       "393           0.13390     0.160400      0.2994  \n",
       "394           0.00000     0.000000      0.0000  \n",
       "395           0.00000     0.000000      0.0000  \n",
       "396           0.00000     0.000000      0.0000  \n",
       "397           0.00000     0.000000      0.0000  \n",
       "398           0.00000     0.000000      0.0000  \n",
       "399           0.00000     0.000000      0.0000  \n",
       "400           0.00000     0.000000      0.0000  \n",
       "401           0.00000     0.000000      0.0000  \n",
       "402           0.00000     0.000000      0.0000  \n",
       "403           0.00000     0.000000      0.0000  \n",
       "404           0.00000     0.000000      0.0000  \n",
       "405           0.02390     0.032300      0.0514  \n",
       "406           5.92010     8.585200     14.7277  \n",
       "407           1.23970     1.416700      3.3128  \n",
       "408           4.92245     5.787100     44.3100  \n",
       "409           4.48970     5.936700      9.5765  \n",
       "410           4.73275     6.458300     13.8071  \n",
       "411           2.54810     2.853200      6.2150  \n",
       "412          26.15690    38.139700    128.2816  \n",
       "413          20.25510    29.307300    899.1190  \n",
       "414           0.00000     0.000000      0.0000  \n",
       "415           6.17660     7.570700    116.8615  \n",
       "416           3.23400     4.010700      9.6900  \n",
       "417           7.39560     9.168800     39.0376  \n",
       "418         302.17760   524.002200    999.3160  \n",
       "419         272.44870   582.935200    998.6813  \n",
       "420           1.64510     2.214700    111.4956  \n",
       "421           3.94310     4.784300    273.0952  \n",
       "422           0.00000     0.000000      0.0000  \n",
       "423          69.90545    92.911500    424.2152  \n",
       "424           2.66710     3.470975    103.1809  \n",
       "425           4.76440     6.883500    898.6085  \n",
       "426           1.13530     1.539500     24.9904  \n",
       "427           3.94145     4.768650    113.2230  \n",
       "428           2.53410     3.609000    118.7533  \n",
       "429           3.45380     4.755800    186.6164  \n",
       "430          11.10560    17.423100    400.0000  \n",
       "431          16.38100    21.765200    400.0000  \n",
       "432          57.96930   120.172900    994.2857  \n",
       "433         151.11560   305.026300    995.7447  \n",
       "434          10.19770    12.754200    400.0000  \n",
       "435           4.55110     5.822800    400.0000  \n",
       "436           2.76430     3.822200    400.0000  \n",
       "437           3.78090     4.678600     32.2740  \n",
       "438          49.09090    66.666700    851.6129  \n",
       "439          65.43780    84.973400    657.7621  \n",
       "440          12.08590    15.796400     33.0580  \n",
       "441           0.80760     0.927600      1.2771  \n",
       "442           1.26455     1.577825      5.1317  \n",
       "443           0.64350     0.733425      1.0851  \n",
       "444           0.90270     0.988800      1.3511  \n",
       "445           0.65110     0.748400      1.1087  \n",
       "446           1.16380     1.272300      1.7639  \n",
       "447           0.27970     0.338825      0.5085  \n",
       "448           0.25120     0.351100      1.4754  \n",
       "449           0.00000     0.000000      0.0000  \n",
       "450           0.00000     0.000000      0.0000  \n",
       "451           0.00000     0.000000      0.0000  \n",
       "452           5.27145     5.913000     13.9776  \n",
       "453           5.22710     6.902475     34.4902  \n",
       "454           7.42490     9.576775     42.0703  \n",
       "455           3.72450     4.341925     10.1840  \n",
       "456          11.35090    14.387900    232.1258  \n",
       "457           4.79335     6.089450    164.1093  \n",
       "458           0.00000     0.000000      0.0000  \n",
       "459           2.83035     3.309225     47.7772  \n",
       "460          26.16785    35.278800    149.3851  \n",
       "461           0.00000     0.000000      0.0000  \n",
       "462           0.00000     0.000000      0.0000  \n",
       "463           0.00000     0.000000      0.0000  \n",
       "464           0.00000     0.000000      0.0000  \n",
       "465           0.00000     0.000000      0.0000  \n",
       "466           0.00000     0.000000      0.0000  \n",
       "467           5.64500     6.386900    109.0074  \n",
       "468         150.34010   335.922400    999.8770  \n",
       "469           5.47240     6.005700     77.8007  \n",
       "470           4.06110     7.006800     87.1347  \n",
       "471           7.39600     9.720200    212.6557  \n",
       "472         138.25515   168.410125    492.7718  \n",
       "473          34.24675    47.727850    358.9504  \n",
       "474          32.82005    45.169475    415.4355  \n",
       "475           4.27620     4.741800     79.1162  \n",
       "476          15.97380    23.737200    274.8871  \n",
       "477           5.24220     6.703800    289.8264  \n",
       "478           0.00000     0.000000    200.0000  \n",
       "479           3.18450     3.625300     63.3336  \n",
       "480          70.43450    93.119600    221.9747  \n",
       "481           0.00000     0.000000      0.0000  \n",
       "482         293.51850   514.585900    999.4135  \n",
       "483         148.31750   262.865250    989.4737  \n",
       "484         138.77550   294.667050    996.8586  \n",
       "485         112.95340   288.893450    994.0000  \n",
       "486         249.92700   501.607450    999.4911  \n",
       "487         112.27550   397.506100    995.7447  \n",
       "488         348.52940   510.647150    997.5186  \n",
       "489         219.48720   377.144200    994.0035  \n",
       "490          48.55745    61.494725    142.8436  \n",
       "491           2.25080     2.839800     12.7698  \n",
       "492           8.00895     9.078900     21.0443  \n",
       "493           2.52910     3.199100      9.4024  \n",
       "494           0.23250     0.563000    127.5728  \n",
       "495           6.60790     7.897200    107.6926  \n",
       "496          22.03910    32.438475    219.6436  \n",
       "497          10.90655    14.469050     40.2818  \n",
       "498           0.00000     0.000000      0.0000  \n",
       "499           0.00000   536.204600   1000.0000  \n",
       "500           0.00000   505.401000    999.2337  \n",
       "501           0.00000     0.000000      0.0000  \n",
       "502           0.00000     0.000000      0.0000  \n",
       "503           0.00000     0.000000      0.0000  \n",
       "504           0.00000     0.000000      0.0000  \n",
       "505           0.00000     0.000000      0.0000  \n",
       "506           0.00000     0.000000      0.0000  \n",
       "507           0.00000     0.000000      0.0000  \n",
       "508           0.00000     0.000000      0.0000  \n",
       "509           0.00000     0.000000      0.0000  \n",
       "510          46.98610    64.248700    451.4851  \n",
       "511           0.00000   555.294100   1000.0000  \n",
       "512           0.00000     0.000000      0.0000  \n",
       "513           0.00000     0.000000      0.0000  \n",
       "514           0.00000     0.000000      0.0000  \n",
       "515           0.00000     0.000000      0.0000  \n",
       "516           0.17470     0.264900    252.8604  \n",
       "517           1.15430     1.759700    113.2758  \n",
       "518           1.58910     1.932800    111.3495  \n",
       "519           5.83295    10.971850    184.3488  \n",
       "520           2.22100     2.903700    111.7365  \n",
       "521           0.00000     0.000000   1000.0000  \n",
       "522          13.74260    17.808950    137.9838  \n",
       "523           0.10000     0.133200    111.3330  \n",
       "524           4.87710     6.450650    818.0005  \n",
       "525           5.13420     6.329500     80.0406  \n",
       "526           1.55010     2.211650      8.2037  \n",
       "527           6.41080     7.594250     14.4479  \n",
       "528           0.00000     0.000000      0.0000  \n",
       "529           0.00000     0.000000      0.0000  \n",
       "530           0.00000     0.000000      0.0000  \n",
       "531           0.00000     0.000000      0.0000  \n",
       "532           0.00000     0.000000      0.0000  \n",
       "533           0.00000     0.000000      0.0000  \n",
       "534           0.00000     0.000000      0.0000  \n",
       "535           0.00000     0.000000      0.0000  \n",
       "536           0.00000     0.000000      0.0000  \n",
       "537           0.00000     0.000000      0.0000  \n",
       "538           0.00000     0.000000      0.0000  \n",
       "539           3.05480     3.947000      6.5803  \n",
       "540           1.78550     2.458350      4.0825  \n",
       "541           9.45930    11.238400     25.7792  \n",
       "542           0.10960     0.113400      0.1184  \n",
       "543           0.00780     0.009000      0.0240  \n",
       "544           0.00260     0.002600      0.0047  \n",
       "545           7.11600     8.020700     21.0443  \n",
       "546           0.91110     1.285550      3.9786  \n",
       "547         403.12200   407.431000    421.7020  \n",
       "548          74.08400    78.397000     83.7200  \n",
       "549           0.47100     0.850350      7.0656  \n",
       "550          16.34000    19.035000    131.6800  \n",
       "551           1.15000     1.370000     39.3300  \n",
       "552           0.19790     0.358450      2.7182  \n",
       "553           7.42790     8.637150     56.9303  \n",
       "554           0.47890     0.562350     17.4781  \n",
       "555          54.44170    74.628700    303.5500  \n",
       "556           4.06710     4.702700     35.3198  \n",
       "557           1.52980     1.815600     54.2917  \n",
       "558           0.97270     1.000800      1.5121  \n",
       "559           0.29090     0.443600      1.0737  \n",
       "560           0.05920     0.089000      0.4457  \n",
       "561          29.73115    44.113400    101.1146  \n",
       "562         264.27200   265.707000    311.4040  \n",
       "563           0.65100     0.768875      1.2988  \n",
       "564           5.16000     7.800000     32.5800  \n",
       "565           0.11955     0.186150      0.6892  \n",
       "566           2.15045     3.098725     14.0141  \n",
       "567           0.04865     0.075275      0.2932  \n",
       "568           1.99970     2.970850     12.7462  \n",
       "569          16.98835    24.772175     84.8024  \n",
       "570         532.39820   534.356400    589.5082  \n",
       "571           2.11860     2.290650      2.7395  \n",
       "572           8.65000    10.130000    454.5600  \n",
       "573           0.29340     0.366900      2.1967  \n",
       "574           2.97580     3.492500    170.0204  \n",
       "575           0.08950     0.112150      0.5502  \n",
       "576           1.62450     1.902000     90.4235  \n",
       "577          13.81790    17.080900     96.9601  \n",
       "578           0.02040     0.027700      0.1028  \n",
       "579           0.01480     0.020000      0.0799  \n",
       "580           0.00470     0.006475      0.0286  \n",
       "581          72.28890   116.539150    737.3048  \n",
       "582           0.50020     0.502375      0.5098  \n",
       "583           0.01380     0.016500      0.4766  \n",
       "584           0.00360     0.004100      0.1045  \n",
       "585           2.75765     3.295175     99.3032  \n",
       "586           0.02050     0.027600      0.1028  \n",
       "587           0.01480     0.020300      0.0799  \n",
       "588           0.00460     0.006400      0.0286  \n",
       "589          71.90050   114.749700    737.3048  \n",
       "Pass/Fail    -1.00000    -1.000000      1.0000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().transpose() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1233f",
   "metadata": {},
   "source": [
    "Observations:\n",
    "    1.591 Featues having continous values \n",
    "    2.Few of features (( 5,13,42,49,etc...)having standard deviation as 0 which implies they have single value \n",
    "    3.Featue pass/Fail has only 2 values( 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c197e",
   "metadata": {},
   "source": [
    "# 2. Data cleansing: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b977c9",
   "metadata": {},
   "source": [
    "2.A. Write a for loop which will remove all the features with 20%+ Null values and impute rest with mean of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a01cd234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove Null value columns above threshold will be removed. \n",
    "#Also remaining Null values will be imputed as median\n",
    "def null_feature_remove(df,threshold):\n",
    "    cols = list(df)\n",
    "    for column in cols:\n",
    "        col_data = df[column]\n",
    "        missing_data=df[column].isnull().sum() /df.shape[0] * 100.00\n",
    "        if ( missing_data > threshold ):\n",
    "            df.drop(column, axis=1, inplace=True)\n",
    "        elif (missing_data > 0):\n",
    "            col_median = col_data.median()\n",
    "            col_data.fillna(col_median, inplace=True)\n",
    "            df[column] = col_data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f0512f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=null_feature_remove(df,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c9f98c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 560)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227170db",
   "metadata": {},
   "source": [
    "32 Features removed due to more than 20% Null "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8116a",
   "metadata": {},
   "source": [
    "2.B. Identify and drop the features which are having same value for all the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ed9dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_card_cols_1 =df.columns[df.nunique()==1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab39e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '13', '42', '49', '52', '69', '97', '141', '149', '178', '179', '186', '189', '190', '191', '192', '193', '194', '226', '229', '230', '231', '232', '233', '234', '235', '236', '237', '240', '241', '242', '243', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '276', '284', '313', '314', '315', '322', '325', '326', '327', '328', '329', '330', '364', '369', '370', '371', '372', '373', '374', '375', '378', '379', '380', '381', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '414', '422', '449', '450', '451', '458', '461', '462', '463', '464', '465', '466', '481', '498', '501', '502', '503', '504', '505', '506', '507', '508', '509', '512', '513', '514', '515', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538']\n"
     ]
    }
   ],
   "source": [
    "print(low_card_cols_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bed8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.drop(low_card_cols_1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f11ca117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 444)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98bc815",
   "metadata": {},
   "source": [
    "2.c.Drop other features if required using relevant functional knowledge. Clearly justify the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0305dab9",
   "metadata": {},
   "source": [
    "Time feature has unique values for each row. So it won't create any impact on output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e908b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49caba2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 443)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4436ef77",
   "metadata": {},
   "source": [
    "2.D. Check for multi-collinearity in the data and take necessary action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e855e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_collinear_features(x, threshold):\n",
    "    threshold =0.70\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "    for i in iters:\n",
    "        for j in range(i+1):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "                # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                # Print the correlated features and the correlation value\n",
    "                #print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "    drops = set(drop_cols)\n",
    "    features1 = x.drop(columns=drops)\n",
    "    return(features1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a141e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=remove_collinear_features(df,.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aafa9564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 202)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abff65",
   "metadata": {},
   "source": [
    "2.E. Make all relevant modifications on the data using both functional/logical reasoning/assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5161c6c",
   "metadata": {},
   "source": [
    "1.Features having unique value across rows removed.Becuase it has 0 Std which will not give impact on Algorithm\n",
    "2.Time feature removed becuase it won't impact target prediction\n",
    "3.Features which have multi-collinear removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a5ea3",
   "metadata": {},
   "source": [
    "# 3. Data analysis & visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c8d0b",
   "metadata": {},
   "source": [
    "3A. Perform a detailed univariate Analysis with appropriate detailed comments after each analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83d37aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0da1c",
   "metadata": {},
   "source": [
    "Observation: \n",
    "    Data is not Normal Curve.\n",
    "    It is Positve Skewd \n",
    "    Data has lot of outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1f96893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_vals = df['Pass/Fail'].unique() \n",
    "unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0f509f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [df.loc[df['Pass/Fail'] == val] for val in unique_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c93e78fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[            0         1          2          3          4         6       7  \\\n",
       " 0     3030.93  2564.000  2187.7333  1411.1265     1.3602   97.6133  0.1242   \n",
       " 1     3095.78  2465.140  2230.4222  1463.6606     0.8294  102.3433  0.1247   \n",
       " 3     2988.72  2479.900  2199.0333   909.7926     1.3204  104.2367  0.1217   \n",
       " 4     3032.24  2502.870  2233.3667  1326.5200     1.5334  100.3967  0.1235   \n",
       " 5     2946.25  2432.840  2233.3667  1326.5200     1.5334  100.3967  0.1235   \n",
       " 6     3030.27  2430.120  2230.4222  1463.6606     0.8294  102.3433  0.1247   \n",
       " 7     3058.88  2690.150  2248.9000  1004.4692     0.7884  106.2400  0.1185   \n",
       " 8     2967.68  2600.470  2248.9000  1004.4692     0.7884  106.2400  0.1185   \n",
       " 9     3016.11  2428.370  2248.9000  1004.4692     0.7884  106.2400  0.1185   \n",
       " 12    2920.07  2507.400  2195.1222  1046.1468     1.3204  103.3400  0.1223   \n",
       " 13    3051.44  2529.270  2184.4333   877.6266     1.4668  107.8711  0.1240   \n",
       " 15    2988.31  2546.260  2224.6222   947.7739     1.2924  104.8489  0.1197   \n",
       " 16    3028.02  2560.870  2270.2556  1258.4558     1.3950  104.8078  0.1207   \n",
       " 17    3032.73  2517.790  2270.2556  1258.4558     1.3950  104.8078  0.1207   \n",
       " 18    3040.34  2501.160  2207.3889   962.5317     1.2043  104.0311  0.1210   \n",
       " 19    2988.30  2519.050  2208.8556  1157.7224     1.5509  107.8022  0.1233   \n",
       " 20    2987.32  2528.810  2201.0667  1285.2144     1.3168  101.5122  0.1195   \n",
       " 21    3011.49  2481.850  2207.3889   962.5317     1.2043  104.0311  0.1210   \n",
       " 22    3002.27  2497.450  2207.3889   962.5317     1.2043  104.0311  0.1210   \n",
       " 24    3010.41  2632.800  2203.9000  1116.4129     1.2639  102.2733  0.1199   \n",
       " 25    2979.74  2446.560  2257.1667  1437.9565     1.4918  106.3400  0.1203   \n",
       " 26    3067.35  2456.330  2257.1667  1437.9565     1.4918  106.3400  0.1203   \n",
       " 27    2988.99  2607.630  2223.0333  1533.9934     1.3548  109.7067  0.1211   \n",
       " 28    2972.78  2431.570  2190.4889  1059.4390     0.8614  102.1178  0.1216   \n",
       " 29    2981.85  2529.110  2180.3778  1208.7411     1.2998  100.2789  0.1209   \n",
       " 30    2975.88  2489.700  2191.6667  1153.9011     1.2569  100.6767  0.1210   \n",
       " 31    3058.61  2492.360  2180.3778  1208.7411     1.2998  100.2789  0.1209   \n",
       " 32    3047.19  2524.180  2197.3111   969.8910     1.3015  105.3911  0.1201   \n",
       " 33    2981.31  2566.350  2197.3111   969.8910     1.3015  105.3911  0.1201   \n",
       " 34    2963.83  2457.640  2225.1777  1457.7934     1.2414  110.2789  0.1211   \n",
       " 35    3040.72  2477.350  2191.6667  1153.9011     1.2569  100.6767  0.1210   \n",
       " 36    2989.47  2445.440  2223.1667  1522.5535     1.1981  110.6333  0.1210   \n",
       " 37    2981.54  2302.460  2221.8445  1416.8211     1.1758  111.6278  0.1210   \n",
       " 39    3017.21  2530.670  2169.4667  1185.4449     1.2412  100.8444  0.1221   \n",
       " 41    3017.10  2517.180  2180.3778  1208.7411     1.2998  100.2789  0.1209   \n",
       " 42    3021.26  2503.460  2180.3778  1208.7411     1.2998  100.2789  0.1209   \n",
       " 43    3000.36  2748.670  2174.8666  1039.2291     1.0455  103.6000  0.1221   \n",
       " 44    3047.78  2490.710  2166.5222   907.0746     1.0647  104.5211  0.1221   \n",
       " 46    3076.38  2475.530  2166.5222   907.0746     1.0647  104.5211  0.1221   \n",
       " 47    3055.97  2788.400  2166.5222   907.0746     1.0647  104.5211  0.1221   \n",
       " 51    2993.14  2520.210  2180.6778  1230.6762     1.4095  103.3778  0.1205   \n",
       " 52    2938.41  2466.780  2166.5222   907.0746     1.0647  104.5211  0.1221   \n",
       " 53    3058.98  2484.310  2172.9778  1222.6067     1.3658  101.8400  0.1220   \n",
       " 54    2950.46  2398.440  2180.6778  1230.6762     1.4095  103.3778  0.1205   \n",
       " 55    3060.03  2809.790  2166.5222   907.0746     1.0647  104.5211  0.1221   \n",
       " 56    2983.09  2569.810  2206.5112  1244.1552     1.2691  101.6667  0.1229   \n",
       " 59    3020.76  2481.280  2197.3111   969.8910     1.3015  105.3911  0.1201   \n",
       " 60    3042.08  2694.310  2206.5112  1244.1552     1.2691  101.6667  0.1229   \n",
       " 61    3000.56  2541.920  2220.5445  1192.8757     1.3872  106.2567  0.1218   \n",
       " 63    3016.64  2492.800  2246.4889  1006.9548     1.0997  103.3222  0.1184   \n",
       " 65    2847.81  2461.380  2202.7111  1010.4454     1.0032  104.3067  0.1225   \n",
       " 66    3011.49  2544.520  2202.7111  1010.4454     1.0032  104.3067  0.1225   \n",
       " 67    2975.64  2508.280  2202.7111  1010.4454     1.0032  104.3067  0.1225   \n",
       " 68    3066.19  2702.640  2246.4889  1006.9548     1.0997  103.3222  0.1184   \n",
       " 69    2873.35  2565.800  2220.5445  1192.8757     1.3872  106.2567  0.1218   \n",
       " 70    3045.03  2315.760  2246.4889  1006.9548     1.0997  103.3222  0.1184   \n",
       " 71    3090.18  2375.490  2191.6667  1107.4330     1.3529  103.4233  0.1206   \n",
       " 72    2955.85  2623.100  2191.6667  1107.4330     1.3529  103.4233  0.1206   \n",
       " 73    3006.23  2424.370  2220.5445  1192.8757     1.3872  106.2567  0.1218   \n",
       " 74    2958.05  2450.250  2199.3334  1017.9399     1.2414  100.5544  0.1214   \n",
       " 75    2999.72  2492.290  2191.6667  1107.4330     1.3529  103.4233  0.1206   \n",
       " 76    2918.90  2500.410  2183.4333  1582.5646     1.3601   99.0267  0.1240   \n",
       " 77    2926.07  2505.730  2191.6667  1107.4330     1.3529  103.4233  0.1206   \n",
       " 78    3032.89  2500.810  2183.4333  1582.5646     1.3601   99.0267  0.1240   \n",
       " 79    2865.31  2531.750  2246.4889  1006.9548     1.0997  103.3222  0.1184   \n",
       " 80    2855.80  2537.350  2183.4333  1582.5646     1.3601   99.0267  0.1240   \n",
       " 81    2930.22  2417.850  2246.4889  1006.9548     1.0997  103.3222  0.1184   \n",
       " 83    2940.53  2459.870  2202.7111  1010.4454     1.0032  104.3067  0.1225   \n",
       " 84    2916.41  2503.590  2246.4889  1006.9548     1.0997  103.3222  0.1184   \n",
       " 85    3047.67  2565.030  2199.3334  1017.9399     1.2414  100.5544  0.1214   \n",
       " 86    3082.45  2739.190  2199.3334  1017.9399     1.2414  100.5544  0.1214   \n",
       " 87    3088.82  2655.370  2183.4556   955.9073     1.1048  102.6978  0.1223   \n",
       " 88    3090.42  2531.980  2265.1889  1740.3297     1.4715  100.3889  0.1222   \n",
       " 89    3031.90  2414.830  2191.9000  1936.7653     1.5291   98.1244  0.1254   \n",
       " 90    3029.16  2519.680  2265.1889  1740.3297     1.4715  100.3889  0.1222   \n",
       " 91    3040.63  2547.430  2174.8666  1039.2291     1.0455  103.6000  0.1221   \n",
       " 92    3039.93  2447.890  2246.4889  1006.9548     1.0997  103.3222  0.1184   \n",
       " 93    2922.94  2478.550  2193.1555  1172.6226     1.2752  101.1111  0.1209   \n",
       " 94    3046.33  2545.340  2172.3666  1752.3206     1.3622   97.2633  0.1269   \n",
       " 95    3081.07  2560.990  2180.1556  1822.5073     1.2579   98.1289  0.1261   \n",
       " 97    3049.31  2453.820  2265.1889  1740.3297     1.4715  100.3889  0.1222   \n",
       " 98    3011.49  2537.900  2183.4556   955.9073     1.1048  102.6978  0.1223   \n",
       " 99    2913.15  2544.520  2232.5889  1717.2750     1.6700  104.1067  0.1223   \n",
       " 100   3080.26  2526.820  2265.1889  1740.3297     1.4715  100.3889  0.1222   \n",
       " 101   3070.50  2439.130  2209.9222  1693.5732     1.3918   99.0811  0.1250   \n",
       " 102   2960.18  2459.620  2213.7667  1679.3498     1.0685   98.1978  0.1235   \n",
       " 103   2960.65  2436.170  2265.1889  1740.3297     1.4715  100.3889  0.1222   \n",
       " 104   3000.46  2523.080  2191.9000  1936.7653     1.5291   98.1244  0.1254   \n",
       " 105   2946.86  2474.050  2220.7111  1064.8446     0.9747  103.0567  0.1193   \n",
       " 106   2831.91  2433.960  2171.9000  1811.8799     1.3811   99.2200  0.1276   \n",
       " 107   2872.40  2607.750  2172.3666  1752.3206     1.3622   97.2633  0.1269   \n",
       " 108   3054.78  2479.170  2171.9000  1811.8799     1.3811   99.2200  0.1276   \n",
       " 109   3086.88  2395.850  2171.9000  1811.8799     1.3811   99.2200  0.1276   \n",
       " 110   2913.42  2437.690  2220.7111  1064.8446     0.9747  103.0567  0.1193   \n",
       " 111   3039.57  2583.780  2197.8445  1907.8909     1.1799   99.6667  0.1247   \n",
       " 112   2958.13  2485.570  2197.8667  1888.0388     1.1988  100.2811  0.1247   \n",
       " 113   2928.16  2523.210  2210.6111  1184.6481     1.2577  102.9356  0.1201   \n",
       " 114   2951.20  2418.830  2181.1555   988.7660     0.9454  101.3611  0.1215   \n",
       " 116   3046.34  2307.240  2178.1444  1074.5145     1.2260  101.0956  0.1218   \n",
       " 117   3011.49  2810.120  2232.5889  1717.2750     1.6700  104.1067  0.1223   \n",
       " 118   2985.11  2496.730  2220.7111  1064.8446     0.9747  103.0567  0.1193   \n",
       " 119   2943.58  2512.490  2171.9000  1811.8799     1.3811   99.2200  0.1276   \n",
       " 120   2992.89  2481.430  2183.4556   955.9073     1.1048  102.6978  0.1223   \n",
       " 121   2962.27  2495.350  2171.9000  1811.8799     1.3811   99.2200  0.1276   \n",
       " 122   3043.67  2518.210  2222.3222  1678.9669     1.5099   99.6944  0.1248   \n",
       " 123   3024.46  2468.330  2222.3333  1720.9033     1.3150  100.3989  0.1231   \n",
       " 124   3032.81  2534.740  2239.4223  1997.3782     1.5397   98.3356  0.1229   \n",
       " 125   3020.12  2416.730  2196.6555  1066.1908     1.2188  101.8900  0.1211   \n",
       " 126   2955.30  2460.080  2191.9000  1936.7653     1.5291   98.1244  0.1254   \n",
       " 127   3100.44  2690.990  2239.4223  1997.3782     1.5397   98.3356  0.1229   \n",
       " 128   2976.48  2502.960  2232.5889  1717.2750     1.6700  104.1067  0.1223   \n",
       " 129   3056.50  2481.090  2220.4444  1637.2946     1.3680  100.5189  0.1231   \n",
       " 130   3017.50  2634.300  2183.4556   955.9073     1.1048  102.6978  0.1223   \n",
       " 132   2924.23  2531.340  2196.6555  1066.1908     1.2188  101.8900  0.1211   \n",
       " 133   3023.86  2432.850  2172.3666  1752.3206     1.3622   97.2633  0.1269   \n",
       " 134   3020.46  2477.860  2242.2444  1695.7049     1.4215   99.9944  0.1215   \n",
       " 135   3035.71  2567.290  2222.3222  1678.9669     1.5099   99.6944  0.1248   \n",
       " 136   2917.86  2490.860  2178.1444  1074.5145     1.2260  101.0956  0.1218   \n",
       " 137   3044.46  2497.580  2178.1444  1074.5145     1.2260  101.0956  0.1218   \n",
       " 138   2959.65  2576.840  2171.9000  1811.8799     1.3811   99.2200  0.1276   \n",
       " 139   2936.30  2473.270  2181.1555   988.7660     0.9454  101.3611  0.1215   \n",
       " 140   3001.41  2581.070  2224.4778   994.9032     1.2834  101.8267  0.1191   \n",
       " 141   3072.73  2587.600  2224.4778   994.9032     1.2834  101.8267  0.1191   \n",
       " 142   2989.63  2364.730  2196.6555  1066.1908     1.2188  101.8900  0.1211   \n",
       " 143   3027.10  2374.500  2222.3222  1678.9669     1.5099   99.6944  0.1248   \n",
       " 144   2950.00  2539.370  2196.6555  1066.1908     1.2188  101.8900  0.1211   \n",
       " 145   3095.50  2394.580  2196.6555  1066.1908     1.2188  101.8900  0.1211   \n",
       " 146   3049.19  2406.220  2222.3222  1678.9669     1.5099   99.6944  0.1248   \n",
       " 147   3050.47  2515.300  2242.2444  1695.7049     1.4215   99.9944  0.1215   \n",
       " 148   2957.73  2581.930  2224.4778   994.9032     1.2834  101.8267  0.1191   \n",
       " 149   2963.35  2537.400  2222.0000  1630.1128     1.3555  100.0144  0.1231   \n",
       " 150   2983.27  2522.160  2219.8222  1032.5542     1.2795  102.3356  0.1195   \n",
       " 151   3095.06  2494.550  2180.1556  1822.5073     1.2579   98.1289  0.1261   \n",
       " 152   2999.18  2560.670  2222.3222  1678.9669     1.5099   99.6944  0.1248   \n",
       " 153   2939.56  2557.940  2242.2444  1695.7049     1.4215   99.9944  0.1215   \n",
       " 155   3052.82  2525.330  2186.2667  1088.7359     1.2723  103.5633  0.1212   \n",
       " 156   2914.47  2492.140  2181.6111  1083.3937     1.2484  104.1478  0.1216   \n",
       " 159   3045.70  2456.170  2184.8778   960.8486     1.0160  102.5333  0.1214   \n",
       " 160   3057.03  2468.410  2184.8778   960.8486     1.0160  102.5333  0.1214   \n",
       " 161   2969.52  2546.750  2221.9444  1551.6947     1.5296   99.2678  0.1222   \n",
       " 162   2892.37  2517.090  2184.8778   960.8486     1.0160  102.5333  0.1214   \n",
       " 163   3026.77  2474.320  2181.6111  1083.3937     1.2484  104.1478  0.1216   \n",
       " 164   2999.01  2447.290  2217.7556  1349.3451     1.2793  105.3100  0.1237   \n",
       " 165   3072.75  2581.180  2221.9444  1551.6947     1.5296   99.2678  0.1222   \n",
       " 166   3090.97  2469.600  2218.7889  1622.3514     1.2043   98.2667  0.1224   \n",
       " 168   2964.85  2493.320  2218.7889  1622.3514     1.2043   98.2667  0.1224   \n",
       " 170   2908.06  2465.070  2221.9444  1551.6947     1.5296   99.2678  0.1222   \n",
       " 171   2949.65  2335.060  2184.8778   960.8486     1.0160  102.5333  0.1214   \n",
       " 172   2954.44  2576.940  2221.9444  1551.6947     1.5296   99.2678  0.1222   \n",
       " 173   3018.00  2320.050  2248.4222  1418.5634     1.4905  102.6444  0.1212   \n",
       " 174   2929.20  2486.590  2234.6111  1590.1699     1.2049   98.7567  0.1208   \n",
       " 175   3071.09  2521.230  2248.4222  1418.5634     1.4905  102.6444  0.1212   \n",
       " 176   3033.95  2505.400  2248.4222  1418.5634     1.4905  102.6444  0.1212   \n",
       " 177   3075.87  2501.930  2248.4222  1418.5634     1.4905  102.6444  0.1212   \n",
       " 178   2846.07  2514.660  2234.6111  1590.1699     1.2049   98.7567  0.1208   \n",
       " 179   2916.54  2480.640  2256.1222  1226.2217     1.4656  106.3122  0.1209   \n",
       " 181   2997.07  2543.110  2256.1222  1226.2217     1.4656  106.3122  0.1209   \n",
       " 183   3028.65  2455.140  2228.0555  1816.9286     1.2244  100.2256  0.1231   \n",
       " 184   2943.34  2485.000  2234.6111  1590.1699     1.2049   98.7567  0.1208   \n",
       " 185   3022.71  2511.090  2220.7111  1064.8446     0.9747  103.0567  0.1193   \n",
       " 187   3035.20  2662.190  2221.9444  1551.6947     1.5296   99.2678  0.1222   \n",
       " 190   3024.74  2553.600  2171.8111  1007.2396     1.4598  104.7444  0.1252   \n",
       " 191   3109.01  2487.160  2171.8111  1007.2396     1.4598  104.7444  0.1252   \n",
       " 192   2956.64  2436.120  2139.2667  1123.3450     1.3699  102.2522  0.1254   \n",
       " 193   3024.15  2521.240  2191.7889  1133.3967     1.3808  106.6711  0.1237   \n",
       " 194   2997.41  2401.530  2139.2667  1123.3450     1.3699  102.2522  0.1254   \n",
       " 195   3030.17  2559.120  2220.7111  1064.8446     0.9747  103.0567  0.1193   \n",
       " 196   2967.40  2553.040  2304.2111  1857.8658     1.7719   96.9967  0.1183   \n",
       " 197   3067.34  2496.560  2177.1667   969.6185     1.0125  104.9322  0.1243   \n",
       " 198   3074.63  2538.750  2223.4000  1104.9568     1.2479  105.7289  0.1219   \n",
       " 199   2992.92  2706.540  2146.8555   917.5352     1.0638  106.8833  0.1260   \n",
       " 200   2896.63  2550.510  2165.6222   953.0040     1.3366  105.9356  0.1250   \n",
       " 201   3054.13  2511.140  2192.6889  1130.9910     1.3923  102.1067  0.1220   \n",
       " 202   3031.39  2632.880  2280.8222  1125.7334     0.6815  101.9111  0.1221   \n",
       " 203   2981.95  2279.480  2205.2889  1630.3112     1.2733   98.8056  0.1218   \n",
       " 204   2977.98  2384.660  2212.7111  1062.6288     1.3848  101.9300  0.1212   \n",
       " 205   3072.35  2551.840  2212.7111  1062.6288     1.3848  101.9300  0.1212   \n",
       " 206   2998.59  2559.070  2184.8889   982.8147     1.0124  103.1656  0.1245   \n",
       " 207   2855.96  2539.780  2187.4444   980.2436     1.0997  104.1300  0.1233   \n",
       " 208   2962.13  2540.950  2211.3889  1763.4015     1.6569  101.9133  0.1212   \n",
       " 209   3051.32  2464.390  2205.2889  1630.3112     1.2733   98.8056  0.1218   \n",
       " 210   3104.40  2506.270  2205.4667  1035.6241     1.1295  105.5600  0.1216   \n",
       " 211   2892.99  2332.390  2216.9556   907.1863     1.4472  105.0367  0.1212   \n",
       " 212   2995.76  2526.220  2230.0333  1668.6804     1.5739   99.0522  0.1204   \n",
       " 213   3034.34  2631.470  2179.0445  2028.2208     1.5552   95.4256  0.1234   \n",
       " 214   3075.76  2491.550  2185.9333  1659.6962     1.6290   98.6822  0.1227   \n",
       " 215   3043.05  2466.360  2205.2889  1630.3112     1.2733   98.8056  0.1218   \n",
       " 216   2940.50  2441.610  2205.2889  1630.3112     1.2733   98.8056  0.1218   \n",
       " 217   3065.36  2162.870  2211.3889  1763.4015     1.6569  101.9133  0.1212   \n",
       " 219   2912.76  2480.540  2233.7666  1772.4931     0.9501  100.7256  0.1215   \n",
       " 220   3091.71  2548.950  2233.7666  1772.4931     0.9501  100.7256  0.1215   \n",
       " 221   2907.96  2439.310  2225.5889  1890.2199     1.6000   97.2756  0.1202   \n",
       " 223   2971.15  2474.950  2185.9333  1659.6962     1.6290   98.6822  0.1227   \n",
       " 224   3076.52  2502.620  2197.6444  1247.0334     0.7865   99.9211  0.1203   \n",
       " 225   2999.86  2287.900  2197.6444  1247.0334     0.7865   99.9211  0.1203   \n",
       " 226   2992.52  2470.140  2197.6444  1247.0334     0.7865   99.9211  0.1203   \n",
       " 227   3109.02  2472.130  2197.6444  1247.0334     0.7865   99.9211  0.1203   \n",
       " 228   2954.10  2489.910  2179.0445  2028.2208     1.5552   95.4256  0.1234   \n",
       " 229   3006.22  2257.810  2185.9333  1659.6962     1.6290   98.6822  0.1227   \n",
       " 230   3088.73  2378.950  2185.9333  1659.6962     1.6290   98.6822  0.1227   \n",
       " 232   2948.42  2444.550  2204.9223  1787.6757     1.5138  100.4322  0.1216   \n",
       " 233   2958.39  2478.990  2231.9556  1185.0959     1.0208  102.5344  0.1192   \n",
       " 234   3025.01  2604.260  2177.3222  1089.3655     1.3101  101.1478  0.1216   \n",
       " 237   2908.04  2494.370  2185.9333  1659.6962     1.6290   98.6822  0.1227   \n",
       " 239   3061.98  2344.910  2197.2667   911.2054     1.3257  104.1722  0.1208   \n",
       " 242   2959.20  2491.600  2181.1555  1010.6897     1.3711  103.1456  0.1215   \n",
       " 245   2981.08  2616.630  2177.3222  1089.3655     1.3101  101.1478  0.1224   \n",
       " 246   2985.06  2509.800  2177.3222  1089.3655     1.3101  101.1478  0.1216   \n",
       " 247   3043.17  2483.740  2195.7666  1015.3046     1.3663  101.4600  0.1207   \n",
       " 248   3056.95  2549.730  2195.7666  1015.3046     1.3663  101.4600  0.1207   \n",
       " 249   3050.29  2402.570  2185.8111   995.3928     1.3714  102.6422  0.1218   \n",
       " 250   3079.90  2463.510  2178.7333  1039.3641     0.7367  101.4922  0.1219   \n",
       " 251   3016.16  2382.030  2185.8111   995.3928     1.3714  102.6422  0.1218   \n",
       " 252   2972.78  2513.680  2197.2667   911.2054     1.3257  104.1722  0.1208   \n",
       " 253   2872.01  2570.130  2244.1111  1676.7316     0.9197  100.8067  0.1204   \n",
       " 254   3008.98  2523.550  2197.6444  1247.0334     0.7865   99.9211  0.1203   \n",
       " 255   2821.23  2549.890  2244.1111  1676.7316     0.9197  100.8067  0.1204   \n",
       " 256   3073.67  2349.480  2244.1111  1676.7316     0.9197  100.8067  0.1204   \n",
       " 257   3012.98  2498.280  2243.7778  1502.9221     1.8160  102.0978  0.1195   \n",
       " 258   3072.31  2308.600  2176.6334  1272.4090     1.3828  100.4622  0.1216   \n",
       " 259   2968.02  2494.250  2197.2667   911.2054     1.3257  104.1722  0.1208   \n",
       " 260   3111.93  2460.940  2176.6334  1272.4090     1.3828  100.4622  0.1216   \n",
       " 261   2957.97  2483.970  2243.7778  1502.9221     1.8160  102.0978  0.1195   \n",
       " 262   2850.33  2570.420  2177.3222  1089.3655     1.3101  101.1478  0.1216   \n",
       " 263   2927.46  2464.800  2243.7778  1502.9221     1.8160  102.0978  0.1195   \n",
       " 264   3055.21  2377.270  2177.3222  1089.3655     1.3101  101.1478  0.1216   \n",
       " 265   3045.21  2561.100  2243.7778  1502.9221     1.8160  102.0978  0.1195   \n",
       " 266   3000.39  2581.670  2197.2667   911.2054     1.3257  104.1722  0.1208   \n",
       " 267   2970.87  2610.780  2185.8111   995.3928     1.3714  102.6422  0.1218   \n",
       " 268   3087.45  2503.320  2243.7778  1502.9221     1.8160  102.0978  0.1195   \n",
       " 269   3054.45  2443.680  2243.7778  1502.9221     1.8160  102.0978  0.1195   \n",
       " 270   2988.52  2291.920  2183.5777  1764.5386     1.7050  100.4478  0.1222   \n",
       " 271   3033.29  2587.270  2244.1111  1676.7316     0.9197  100.8067  0.1204   \n",
       " 272   3076.50  2554.290  2178.7333  1039.3641     0.7367  101.4922  0.1219   \n",
       " 274   3038.25  2337.960  2244.1111  1676.7316     0.9197  100.8067  0.1204   \n",
       " 275   2989.91  2575.710  2244.1111  1676.7316     0.9197  100.8067  0.1204   \n",
       " 276   3061.24  2734.330  2243.7778  1502.9221     1.8160  102.0978  0.1195   \n",
       " 278   3054.84  2488.500  2185.8111   995.3928     1.3714  102.6422  0.1218   \n",
       " 279   3010.71  2513.050  2243.7778  1502.9221     1.8160  102.0978  0.1195   \n",
       " 280   2935.73  2573.740  2201.5777   976.4791     0.7679   99.9956  0.1202   \n",
       " 281   3018.60  2374.640  2243.7778  1502.9221     1.8160  102.0978  0.1195   \n",
       " 283   2969.01  2537.880  2194.6444   999.4387     1.3259  101.9767  0.1205   \n",
       " 284   2984.32  2539.730  2214.7111  1493.8781     1.5899  103.9367  0.1202   \n",
       " 285   3017.94  2629.060  2194.6444   999.4387     1.3259  101.9767  0.1205   \n",
       " 286   2988.66  2533.490  2194.6444   999.4387     1.3259  101.9767  0.1205   \n",
       " 287   3002.46  2693.160  2214.7111  1493.8781     1.5899  103.9367  0.1202   \n",
       " 288   2962.65  2535.190  2214.7111  1493.8781     1.5899  103.9367  0.1202   \n",
       " 289   3021.41  2509.190  2218.5778  1632.2734     0.8396   98.1311  0.1211   \n",
       " 290   3073.81  2786.970  2178.7333  1039.3641     0.7367  101.4922  0.1219   \n",
       " 292   3029.19  2459.560  2214.7111  1493.8781     1.5899  103.9367  0.1202   \n",
       " 293   3082.31  2457.920  2194.6444   999.4387     1.3259  101.9767  0.1205   \n",
       " 295   3067.43  2467.110  2214.7111  1493.8781     1.5899  103.9367  0.1202   \n",
       " 296   2864.05  2533.750  2224.0000  1510.0797     1.5611   99.8300  0.1199   \n",
       " 297   3024.14  2505.030  2185.8111   995.3928     1.3714  102.6422  0.1218   \n",
       " 298   3032.29  2356.940  2218.5778  1632.2734     0.8396   98.1311  0.1211   \n",
       " 299   2968.06  2524.250  2218.5778  1632.2734     0.8396   98.1311  0.1211   \n",
       " 300   2988.95  2611.210  2202.4333  1586.1015     1.6025  100.8067  0.1214   \n",
       " 301   3052.43  2486.480  2201.5777   976.4791     0.7679   99.9956  0.1202   \n",
       " 302   3031.44  2487.460  2195.5333  1561.7164     1.4292   99.2500  0.1223   \n",
       " 303   2889.93  2499.405  2238.5444  1659.1424     0.9010   99.3100  0.1204   \n",
       " 304   3020.25  2436.770  2224.0000  1510.0797     1.5611   99.8300  0.1199   \n",
       " 305   3020.71  2483.520  2201.5777   976.4791     0.7679   99.9956  0.1202   \n",
       " 306   2933.79  2417.700  2195.5333  1561.7164     1.4292   99.2500  0.1223   \n",
       " 307   3067.25  2312.140  2214.7111  1493.8781     1.5899  103.9367  0.1202   \n",
       " 308   2973.52  2414.480  2214.7111  1493.8781     1.5899  103.9367  0.1202   \n",
       " 309   2999.81  2477.460  2178.7333  1039.3641     0.7367  101.4922  0.1219   \n",
       " 310   3098.79  2453.540  2214.7111  1493.8781     1.5899  103.9367  0.1202   \n",
       " 311   2995.26  2540.190  2214.7111  1493.8781     1.5899  103.9367  0.1202   \n",
       " 312   3065.02  2515.550  2238.5444  1659.1424     0.9010   99.3100  0.1204   \n",
       " 313   3091.11  2520.850  2238.5444  1659.1424     0.9010   99.3100  0.1204   \n",
       " 314   2941.71  2529.420  2196.6889  1593.1220     1.5925   99.1133  0.1226   \n",
       " 315   3019.05  2461.890  2196.6889  1593.1220     1.5925   99.1133  0.1226   \n",
       " 316   2885.72  2575.710  2218.5778  1632.2734     0.8396   98.1311  0.1211   \n",
       " 317   3054.70  2450.480  2201.5889   956.1617     1.3886  105.4122  0.1207   \n",
       " 318   3033.33  2512.510  2224.0000  1510.0797     1.5611   99.8300  0.1199   \n",
       " 319   2926.40  2421.450  2180.9666   998.4939     1.3990  106.3311  0.1216   \n",
       " 320   3081.75  2442.260  2196.6889  1593.1220     1.5925   99.1133  0.1226   \n",
       " 322   3093.48  2413.920  2238.5444  1659.1424     0.9010   99.3100  0.1204   \n",
       " 324   2787.49  2584.150  2224.0000  1510.0797     1.5611   99.8300  0.1199   \n",
       " 325   3039.66  2393.780  2194.2889  1631.5293     1.5119   99.6367  0.1222   \n",
       " 328   2894.04  2490.060  2207.0444  1330.6718     1.3076  101.6778  0.1216   \n",
       " 329   3042.90  2431.540  2205.2222  1427.3840     1.4633  106.8389  0.1215   \n",
       " 330   2915.83  2526.290  2197.6778  1056.7817     1.3168  102.9611  0.1203   \n",
       " 331   2967.04  2273.460  2208.4444  1089.1175     0.7665  103.7100  0.1201   \n",
       " 332   3057.33  2496.080  2196.6889  1593.1220     1.5925   99.1133  0.1226   \n",
       " 333   2904.17  2405.530  2204.2667  1475.7797     1.4053  101.2411  0.1221   \n",
       " 334   3038.41  2475.670  2224.0000  1510.0797     1.5611   99.8300  0.1199   \n",
       " 335   3035.75  2484.620  2207.0444  1330.6718     1.3076  101.6778  0.1216   \n",
       " 337   2991.98  2412.410  2197.2333  1435.1460     0.9740  104.8733  0.1226   \n",
       " 338   3065.83  2273.800  2197.2333  1435.1460     0.9740  104.8733  0.1226   \n",
       " 339   3106.85  2453.470  2197.2333  1435.1460     0.9740  104.8733  0.1226   \n",
       " 340   3033.82  2531.630  2180.9666   998.4939     1.3990  106.3311  0.1216   \n",
       " 341   3059.46  2567.680  2197.6778  1056.7817     1.3168  102.9611  0.1203   \n",
       " 342   2908.11  2512.990  2218.0555  1517.4371     0.8579  105.8133  0.1206   \n",
       " 343   3111.09  2536.560  2180.9666   998.4939     1.3990  106.3311  0.1216   \n",
       " 345   3084.00  2453.910  2194.2889  1631.5293     1.5119   99.6367  0.1222   \n",
       " 346   2959.29  2478.620  2197.2333  1435.1460     0.9740  104.8733  0.1226   \n",
       " 347   3047.54  2709.450  2238.5444  1659.1424     0.9010   99.3100  0.1204   \n",
       " 348   2990.95  2379.960  2218.5778  1632.2734     0.8396   98.1311  0.1211   \n",
       " 349   3038.81  2569.580  2198.7222  1534.2053     0.9374  104.1989  0.1224   \n",
       " 350   3028.43  2517.540  2170.5444   921.0605     1.4390  104.5300  0.1226   \n",
       " 352   3190.45  2391.470  2208.4444  1089.1175     0.7665  103.7100  0.1201   \n",
       " 353   2893.56  2408.400  2208.4444  1089.1175     0.7665  103.7100  0.1201   \n",
       " 354   3031.79  2479.690  2186.9889   949.2201     1.2981  103.3322  0.1219   \n",
       " 355   3111.28  2503.690  2218.0555  1517.4371     0.8579  105.8133  0.1206   \n",
       " 356   3100.44  2495.400  2218.0555  1517.4371     0.8579  105.8133  0.1206   \n",
       " 357   3040.45  2226.990  2218.0555  1517.4371     0.8579  105.8133  0.1206   \n",
       " 358   2942.16  2450.050  2224.0000  1510.0797     1.5611   99.8300  0.1199   \n",
       " 359   3012.18  2590.800  2197.6778  1056.7817     1.3168  102.9611  0.1203   \n",
       " 360   2973.57  2421.450  2218.5778  1632.2734     0.8396   98.1311  0.1211   \n",
       " 361   2999.40  2270.680  2198.7222  1534.2053     0.9374  104.1989  0.1224   \n",
       " 362   3018.64  2401.800  2224.0000  1510.0797     1.5611   99.8300  0.1199   \n",
       " 363   3066.07  2547.840  2186.9889   949.2201     1.2981  103.3322  0.1219   \n",
       " 364   3063.36  2333.960  2186.9889   949.2201     1.2981  103.3322  0.1219   \n",
       " 365   2988.92  2460.910  2178.0778   941.9524     0.8039  104.0167  0.1229   \n",
       " 366   3089.31  2546.270  2208.4444  1089.1175     0.7665  103.7100  0.1201   \n",
       " 367   2939.94  2516.110  2186.9889   949.2201     1.2981  103.3322  0.1219   \n",
       " 369   3206.18  2456.650  2198.7222  1534.2053     0.9374  104.1989  0.1224   \n",
       " 370   3109.46  2510.080  2197.6778  1056.7817     1.3168  102.9611  0.1203   \n",
       " 371   2998.23  2475.500  2238.5444  1659.1424     0.9010   99.3100  0.1204   \n",
       " 372   3047.65  2605.260  2196.8000  1090.0084     1.3270   99.3944  0.1212   \n",
       " 374   2993.48  2502.340  2218.0555  1517.4371     0.8579  105.8133  0.1206   \n",
       " 375   3045.73  2479.930  2197.6778  1056.7817     1.3168  102.9611  0.1203   \n",
       " 376   3067.14  2512.140  2205.2222  1427.3840     1.4633  106.8389  0.1215   \n",
       " 377   2982.49  2288.050  2198.7222  1534.2053     0.9374  104.1989  0.1224   \n",
       " 378   2927.75  2462.820  2207.0444  1330.6718     1.3076  101.6778  0.1216   \n",
       " 379   3001.50  2523.600  2197.2333  1435.1460     0.9740  104.8733  0.1226   \n",
       " 380   2875.43  2513.280  2197.2333  1435.1460     0.9740  104.8733  0.1226   \n",
       " 381   2957.88  2560.910  2196.8000  1090.0084     1.3270   99.3944  0.1212   \n",
       " 382   3047.00  2436.010  2213.7556  1113.5599     0.7217  104.1667  0.1211   \n",
       " 383   3037.81  2305.520  2186.9889   949.2201     1.2981  103.3322  0.1219   \n",
       " 384   3040.17  2375.550  2198.7222  1534.2053     0.9374  104.1989  0.1224   \n",
       " 385   3067.89  2570.930  2196.8000  1090.0084     1.3270   99.3944  0.1212   \n",
       " 386   3083.97  2311.420  2205.2222  1427.3840     1.4633  106.8389  0.1215   \n",
       " 387   2912.24  2555.490  2206.4222  1113.3443     1.2678   97.7689  0.1215   \n",
       " 388   3024.19  2474.410  2186.9889   949.2201     1.2981  103.3322  0.1219   \n",
       " 389   3002.85  2578.760  2197.6778  1056.7817     1.3168  102.9611  0.1203   \n",
       " 390   2989.82  2434.000  2180.0556  1031.0669     0.7565  104.7056  0.1226   \n",
       " 391   2924.07  2566.220  2213.7556  1113.5599     0.7217  104.1667  0.1211   \n",
       " 393   2950.24  2512.970  2205.2222  1427.3840     1.4633  106.8389  0.1215   \n",
       " 394   2821.63  2491.130  2198.7222  1534.2053     0.9374  104.1989  0.1224   \n",
       " 395   3245.00  2371.450  2196.8000  1090.0084     1.3270   99.3944  0.1212   \n",
       " 396   3127.07  2478.970  2198.7222  1534.2053     0.9374  104.1989  0.1224   \n",
       " 397   3034.08  2735.650  2213.7556  1113.5599     0.7217  104.1667  0.1211   \n",
       " 398   2953.65  2310.290  2197.2333  1435.1460     0.9740  104.8733  0.1226   \n",
       " 399   2996.53  2533.520  2202.1222  1034.5674     0.7760  104.6156  0.1219   \n",
       " 400   2951.56  2441.220  2202.1222  1034.5674     0.7760  104.6156  0.1219   \n",
       " 401   2986.34  2318.950  2213.7556  1113.5599     0.7217  104.1667  0.1211   \n",
       " 402   3105.49  2587.300  2218.5778  1632.2734     0.8396   98.1311  0.1211   \n",
       " 403   3090.09  2490.540  2213.7556  1113.5599     0.7217  104.1667  0.1211   \n",
       " 404   2986.32  2490.920  2218.0555  1517.4371     0.8579  105.8133  0.1206   \n",
       " 405   3052.97  2501.630  2218.0555  1517.4371     0.8579  105.8133  0.1206   \n",
       " 407   2981.36  2502.060  2178.6889  1657.3518     1.6603  100.8022  0.1229   \n",
       " 408   3017.11  2364.310  2197.2333  1435.1460     0.9740  104.8733  0.1226   \n",
       " 409   2940.72  2419.830  2218.0555  1517.4371     0.8579  105.8133  0.1206   \n",
       " 410   2940.17  2514.300  2186.9889   949.2201     1.2981  103.3322  0.1219   \n",
       " 411   2933.88  2488.490  2178.6889  1657.3518     1.6603  100.8022  0.1229   \n",
       " 412   2989.85  2501.880  2197.2333  1435.1460     0.9740  104.8733  0.1226   \n",
       " 413   3083.49  2536.430  2213.7556  1113.5599     0.7217  104.1667  0.1211   \n",
       " 414   2965.48  2467.940  2178.6889  1657.3518     1.6603  100.8022  0.1229   \n",
       " 415   2951.41  2577.270  2207.8111  1202.4520     1.6219  108.7689  0.1212   \n",
       " 416   3015.48  2532.880  2218.0555  1517.4371     0.8579  105.8133  0.1206   \n",
       " 417   2981.47  2434.320  2180.0556  1031.0669     0.7565  104.7056  0.1226   \n",
       " 418   2946.70  2535.820  2178.6889  1657.3518     1.6603  100.8022  0.1229   \n",
       " 419   3015.67  2539.460  2204.2889  1616.0688     0.9830  102.9922  0.1226   \n",
       " 420   3099.87  2675.580  2200.7000  1568.0936     0.9496   99.4889  0.1217   \n",
       " 421   3048.73  2460.110  2205.2222  1427.3840     1.4633  106.8389  0.1215   \n",
       " 422   3022.80  2436.690  2204.2889  1616.0688     0.9830  102.9922  0.1226   \n",
       " 423   3018.81  2281.300  2197.2333  1435.1460     0.9740  104.8733  0.1226   \n",
       " 425   3079.17  2405.560  2217.3777  1425.1041     1.7585  106.2556  0.1200   \n",
       " 426   2911.37  2541.210  2207.8111  1202.4520     1.6219  108.7689  0.1212   \n",
       " 427   3085.57  2364.780  2178.6889  1657.3518     1.6603  100.8022  0.1229   \n",
       " 428   3053.49  2457.080  2172.5333  1351.9648     1.6377  103.8800  0.1243   \n",
       " 429   3120.68  2396.400  2177.0222  1448.8499     1.5565  103.2567  0.1232   \n",
       " 430   2956.84  2592.290  2217.3777  1425.1041     1.7585  106.2556  0.1200   \n",
       " 431   3068.98  2439.000  2172.5333  1351.9648     1.6377  103.8800  0.1243   \n",
       " 432   2999.89  2506.390  2178.6889  1657.3518     1.6603  100.8022  0.1229   \n",
       " 433   3114.68  2605.480  2172.5333  1351.9648     1.6377  103.8800  0.1243   \n",
       " 434   3266.04  2417.040  2217.3777  1425.1041     1.7585  106.2556  0.1200   \n",
       " 435   3041.33  2526.730  2213.7556  1113.5599     0.7217  104.1667  0.1211   \n",
       " 436   3071.58  2489.470  2217.3777  1425.1041     1.7585  106.2556  0.1200   \n",
       " 437   2921.48  2496.540  2213.9333  1248.6209     1.8848   97.1856  0.1209   \n",
       " 438   3059.20  2467.800  2196.0889  1277.8592     1.8246   95.6322  0.1224   \n",
       " 439   3062.54  2370.500  2177.0222  1448.8499     1.5565  103.2567  0.1232   \n",
       " 440   3067.70  2500.270  2177.0222  1448.8499     1.5565  103.2567  0.1232   \n",
       " 442   2948.76  2644.320  2185.3334  1780.4149     1.7632   93.9844  0.1215   \n",
       " 443   3079.77  2354.510  2207.0444  1269.6078     1.7571   97.0189  0.1221   \n",
       " 444   3114.46  2656.840  2207.0444  1269.6078     1.7571   97.0189  0.1221   \n",
       " 445   2882.17  2601.980  2213.9333  1248.6209     1.8848   97.1856  0.1209   \n",
       " 446   3014.35  2653.230  2203.0445   996.1560     1.3660  104.6633  0.1209   \n",
       " 447   2901.07  2505.830  2207.0444  1269.6078     1.7571   97.0189  0.1221   \n",
       " 449   2882.76  2516.110  2172.5333  1351.9648     1.6377  103.8800  0.1243   \n",
       " 450   3047.76  2446.960  2187.6667  1468.8963     0.9253  101.8611  0.1234   \n",
       " 451   3133.43  2406.580  2173.4556  1433.6732     1.0304  110.5422  0.1245   \n",
       " 452   3080.01  2481.040  2172.1223  1871.2454     1.5915  100.2533  0.1247   \n",
       " 453   2967.57  2520.910  2172.1223  1871.2454     1.5915  100.2533  0.1247   \n",
       " 454   3007.38  2474.970  2170.9667  1600.3858     1.0430  104.9756  0.1249   \n",
       " 455   3012.56  2534.620  2180.7000  1159.3838     1.0177   98.9367  0.1222   \n",
       " 456   2958.91  2460.190  2191.5667  1448.8757     0.9187  104.4511  0.1261   \n",
       " 457   2940.56  2490.680  2170.9667  1600.3858     1.0430  104.9756  0.1249   \n",
       " 458   3021.45  2611.980  2113.0778  1016.6750     0.9218   97.9200  0.1265   \n",
       " 459   2996.66  2501.030  2235.0556  1302.6607     1.6347  109.9856  0.1230   \n",
       " 460   3072.41  2705.160  2183.3111  1588.5090     1.6269  102.8467  0.1248   \n",
       " 461   3200.36  2378.770  2163.5889  1448.3869     1.7014  104.8333  0.1256   \n",
       " 462   3012.41  2481.590  2141.0667  1236.5212     0.9698   98.3344  0.1238   \n",
       " 463   2961.36  2362.090  2163.5889  1448.3869     1.7014  104.8333  0.1256   \n",
       " 464   3106.55  2673.410  2202.2889  1518.2905     0.9257   94.8922  0.1213   \n",
       " 465   3043.23  2556.220  2230.7555  1281.7862     1.0038  111.5489  0.1235   \n",
       " 466   3043.34  2473.490  2170.9667  1600.3858     1.0430  104.9756  0.1249   \n",
       " 467   3044.88  2636.690  2230.7555  1281.7862     1.0038  111.5489  0.1235   \n",
       " 468   2975.62  2465.160  2198.6889  1252.1130     1.8008   98.0211  0.1218   \n",
       " 469   2997.15  2531.250  2183.1000  1175.1481     1.7881  111.8900  0.1256   \n",
       " 470   2929.84  2504.500  2183.3111  1588.5090     1.6269  102.8467  0.1248   \n",
       " 471   3036.93  2570.130  2230.7555  1281.7862     1.0038  111.5489  0.1235   \n",
       " 472   3100.12  2412.530  2231.4889  1275.3021     1.6787   94.1256  0.1193   \n",
       " 473   3038.93  2187.670  2219.1333  1167.9207     0.9002   97.9378  0.1203   \n",
       " 474   3008.92  2566.940  2235.0556  1302.6607     1.6347  109.9856  0.1230   \n",
       " 475   2981.50  2500.590  2173.6444  1458.7176     0.8811  101.5667  0.1258   \n",
       " 476   3046.93  2459.210  2179.0778  1820.0613     1.6746   99.3300  0.1261   \n",
       " 477   2957.63  2558.810  2230.7555  1281.7862     1.0038  111.5489  0.1235   \n",
       " 478   3023.34  2493.120  2208.2334  1517.0152     1.0980  110.1900  0.1247   \n",
       " 479   3042.78  2377.890  2173.4556  1433.6732     1.0304  110.5422  0.1245   \n",
       " 480   3091.64  2340.050  2200.7333  1602.8427     0.9790   95.3311  0.1220   \n",
       " 481   3008.02  2425.160  2230.7555  1281.7862     1.0038  111.5489  0.1235   \n",
       " 482   2993.53  2441.120  2180.7000  1159.3838     1.0177   98.9367  0.1222   \n",
       " 483   3003.08  2521.760  2183.3111  1588.5090     1.6269  102.8467  0.1248   \n",
       " 484   3053.95  2459.390  2183.1000  1175.1481     1.7881  111.8900  0.1256   \n",
       " 485   2933.89  2485.500  2189.6555  1200.5912     0.9898   98.0433  0.1215   \n",
       " 486   2996.95  2446.920  2163.5889  1448.3869     1.7014  104.8333  0.1256   \n",
       " 487   3070.54  2406.400  2183.3111  1588.5090     1.6269  102.8467  0.1248   \n",
       " 488   3085.99  2550.520  2179.0778  1820.0613     1.6746   99.3300  0.1261   \n",
       " 489   3094.34  2391.090  2207.0444  1269.6078     1.7571   97.0189  0.1221   \n",
       " 490   2973.71  2487.450  2163.5778  1434.2764     1.6241  105.0833  0.1249   \n",
       " 491   2934.01  2461.280  2168.7222  1212.1040     1.5175  106.7589  0.1233   \n",
       " 492   3043.22  2484.780  2191.5667  1448.8757     0.9187  104.4511  0.1261   \n",
       " 493   3037.38  2441.160  2146.1111  1792.7115     1.6513  100.7244  0.1248   \n",
       " 494   2974.93  2473.340  2200.7333  1602.8427     0.9790   95.3311  0.1220   \n",
       " 496   2964.74  2480.510  2163.5889  1448.3869     1.7014  104.8333  0.1256   \n",
       " 497   3057.56  2471.010  2180.7000  1159.3838     1.0177   98.9367  0.1222   \n",
       " 498   2954.41  2614.990  2208.2334  1517.0152     1.0980  110.1900  0.1247   \n",
       " 499   2997.27  2619.880  2148.6223  1288.4151     1.4990  107.9122  0.1254   \n",
       " 500   3069.02  2474.370  2151.2000  1089.5951     0.8860  100.5622  0.1247   \n",
       " 501   2987.72  2550.520  2180.7000  1159.3838     1.0177   98.9367  0.1222   \n",
       " 502   3057.06  2445.540  2183.3111  1588.5090     1.6269  102.8467  0.1248   \n",
       " 503   2976.40  2534.910  2185.3334  1780.4149     1.7632   93.9844  0.1215   \n",
       " 504   3120.81  2349.270  2126.6555  1015.0770     1.4381  102.4733  0.1255   \n",
       " 505   2984.05  2600.070  2235.0556  1302.6607     1.6347  109.9856  0.1230   \n",
       " 506   3071.29  2561.990  2219.1333  1167.9207     0.9002   97.9378  0.1203   \n",
       " 507   2965.67  2606.000  2155.6333  1070.0439     0.8024  101.4333  0.1241   \n",
       " 509   2970.71  2506.320  2180.8333  1435.0747     1.5082  107.3322  0.1261   \n",
       " 510   3060.07  2693.200  2219.1333  1167.9207     0.9002   97.9378  0.1203   \n",
       " 511   3108.24  2662.700  2183.3444  1111.4499     1.5548   97.5311  0.1236   \n",
       " 512   3008.50  2526.160  2173.6444  1458.7176     0.8811  101.5667  0.1258   \n",
       " 513   3034.90  2598.020  2208.2334  1517.0152     1.0980  110.1900  0.1247   \n",
       " 514   2884.65  2523.360  2208.2667  1656.2210     0.9143  102.6267  0.1243   \n",
       " 515   3028.50  2480.480  2234.5222  1590.2597     1.7889   93.1967  0.1188   \n",
       " 516   2941.25  2492.810  2219.1333  1167.9207     0.9002   97.9378  0.1203   \n",
       " 517   2980.11  2516.330  2201.8222  1288.0857     1.6769   95.9789  0.1209   \n",
       " 519   2933.27  2473.540  2183.3111  1588.5090     1.6269  102.8467  0.1248   \n",
       " 520   2792.24  2533.760  2180.7000  1159.3838     1.0177   98.9367  0.1222   \n",
       " 521   2991.74  2575.390  2173.6444  1458.7176     0.8811  101.5667  0.1258   \n",
       " 522   2960.22  2502.250  2183.3111  1588.5090     1.6269  102.8467  0.1248   \n",
       " 523   3078.79  2720.380  2167.2000  1108.9456     1.4869   94.2600  0.1249   \n",
       " 524   2953.55  2506.190  2146.1111  1792.7115     1.6513  100.7244  0.1248   \n",
       " 525   2994.56  2505.350  2234.5222  1590.2597     1.7889   93.1967  0.1188   \n",
       " 526   2946.11  2491.900  2167.2000  1108.9456     1.4869   94.2600  0.1249   \n",
       " 527   2987.43  2508.220  2124.8444  1180.2820     0.8465  100.7978  0.1257   \n",
       " 528   3100.96  2547.830  2167.2000  1108.9456     1.4869   94.2600  0.1249   \n",
       " 529   3047.28  2186.060  2235.0556  1302.6607     1.6347  109.9856  0.1230   \n",
       " 530   3076.81  2158.750  2208.2334  1517.0152     1.0980  110.1900  0.1247   \n",
       " 531   2951.62  2511.920  2253.5111  1397.5060     0.9660  109.7611  0.1210   \n",
       " 532   2930.42  2505.170  2235.0556  1302.6607     1.6347  109.9856  0.1230   \n",
       " 533   2997.28  2357.990  2141.0667  1236.5212     0.9698   98.3344  0.1238   \n",
       " 534   3025.10  2475.180  2235.0556  1302.6607     1.6347  109.9856  0.1230   \n",
       " 535   2992.15  2538.050  2162.8445  1312.3198     0.8286  100.3633  0.1242   \n",
       " 536   3055.87  2569.670  2235.0556  1302.6607     1.6347  109.9856  0.1230   \n",
       " 537   2971.23  2626.130  2183.3444  1111.4499     1.5548   97.5311  0.1236   \n",
       " 538   3011.49  2651.370  2180.7000  1159.3838     1.0177   98.9367  0.1222   \n",
       " 539   3028.53  2480.420  2220.4778  1531.6980     1.7751  107.6856  0.1249   \n",
       " 540   3072.67  2487.080  2202.2889  1518.2905     0.9257   94.8922  0.1213   \n",
       " 541   3048.78  2442.470  2183.3444  1111.4499     1.5548   97.5311  0.1236   \n",
       " 542   2962.79  2560.690  2155.6333  1070.0439     0.8024  101.4333  0.1241   \n",
       " 543   3085.35  2503.860  2201.8222  1288.0857     1.6769   95.9789  0.1209   \n",
       " 544   3037.82  2436.390  2167.2000  1108.9456     1.4869   94.2600  0.1249   \n",
       " 545   3089.53  2695.570  2242.8555  1430.7270     0.8832  106.2100  0.1216   \n",
       " 546   3106.03  2506.420  2216.4778  1242.2350     0.8379  105.1111  0.1233   \n",
       " 547   3050.22  2563.580  2242.8555  1430.7270     0.8832  106.2100  0.1216   \n",
       " 548   2929.60  2564.910  2216.4778  1242.2350     0.8379  105.1111  0.1233   \n",
       " 549   3058.60  2568.350  2228.4444  1364.0563     0.8795  106.2756  0.1216   \n",
       " 550   2961.59  2585.060  2167.2000  1108.9456     1.4869   94.2600  0.1249   \n",
       " 551   3158.88  2462.640  2216.4778  1242.2350     0.8379  105.1111  0.1233   \n",
       " 552   2945.19  2456.560  2216.4778  1242.2350     0.8379  105.1111  0.1233   \n",
       " 553   3071.43  2370.720  2216.4778  1242.2350     0.8379  105.1111  0.1233   \n",
       " 554   3026.87  2436.850  2163.8667  1106.0618     1.6273  100.8000  0.1251   \n",
       " 555   2983.83  2413.650  2235.0556  1302.6607     1.6347  109.9856  0.1230   \n",
       " 556   2923.26  2548.340  2216.4778  1242.2350     0.8379  105.1111  0.1233   \n",
       " 557   2967.53  2545.770  2155.6333  1070.0439     0.8024  101.4333  0.1241   \n",
       " 558   2996.16  2449.260  2180.7000  1159.3838     1.0177   98.9367  0.1222   \n",
       " 559   3072.99  2471.620  2126.6555  1015.0770     1.4381  102.4733  0.1255   \n",
       " 560   3017.28  2590.950  2124.8444  1180.2820     0.8465  100.7978  0.1257   \n",
       " 561   3009.40  2436.380  2164.3000  1031.4032     1.3626  102.1322  0.1242   \n",
       " 562   2979.95  2518.440  2164.3000  1031.4032     1.3626  102.1322  0.1242   \n",
       " 563   2978.67  2559.320  2253.5111  1397.5060     0.9660  109.7611  0.1210   \n",
       " 564   3108.50  2321.450  2162.8445  1312.3198     0.8286  100.3633  0.1242   \n",
       " 565   3064.81  2538.430  2167.2000  1108.9456     1.4869   94.2600  0.1249   \n",
       " 566   3023.07  2484.550  2242.8555  1430.7270     0.8832  106.2100  0.1216   \n",
       " 567   3032.98  2403.470  2180.7000  1159.3838     1.0177   98.9367  0.1222   \n",
       " 568   2972.12  2571.210  2167.2000  1108.9456     1.4869   94.2600  0.1249   \n",
       " 569   3041.89  2458.120  2242.8555  1430.7270     0.8832  106.2100  0.1216   \n",
       " 570   3006.39  2496.550  2220.4778  1531.6980     1.7751  107.6856  0.1249   \n",
       " 571   2976.75  2508.100  2242.8555  1430.7270     0.8832  106.2100  0.1216   \n",
       " 572   3009.65  2515.970  2216.4778  1242.2350     0.8379  105.1111  0.1233   \n",
       " 573   3052.16  2576.190  2141.0667  1236.5212     0.9698   98.3344  0.1238   \n",
       " 574   3033.91  2607.330  2164.3000  1031.4032     1.3626  102.1322  0.1242   \n",
       " 575   2922.65  2560.920  2201.8222  1288.0857     1.6769   95.9789  0.1209   \n",
       " 577   2973.53  2532.810  2228.4444  1364.0563     0.8795  106.2756  0.1216   \n",
       " 578   2898.08  2573.360  2159.0556  1084.3779     0.8184  100.7056  0.1249   \n",
       " 579   3042.36  2493.850  2124.8444  1180.2820     0.8465  100.7978  0.1257   \n",
       " 580   2977.46  2497.090  2183.3444  1111.4499     1.5548   97.5311  0.1236   \n",
       " 581   2994.64  2666.040  2159.0556  1084.3779     0.8184  100.7056  0.1249   \n",
       " 582   2984.68  2497.490  2179.0778  1820.0613     1.6746   99.3300  0.1261   \n",
       " 584   2968.78  2501.480  2171.8222  1010.0662     0.8599  103.6356  0.1237   \n",
       " 585   3093.75  2440.820  2160.6000  1124.5821     1.5257   98.7122  0.1246   \n",
       " 586   2973.88  2449.170  2159.0556  1084.3779     0.8184  100.7056  0.1249   \n",
       " 587   2984.30  2481.200  2238.4556  1256.5186     1.3404  103.2589  0.1222   \n",
       " 588   2931.26  2527.770  2242.8555  1430.7270     0.8832  106.2100  0.1216   \n",
       " 589   2978.18  2484.780  2159.0556  1084.3779     0.8184  100.7056  0.1249   \n",
       " 590   2890.67  2574.320  2159.0556  1084.3779     0.8184  100.7056  0.1249   \n",
       " 591   2971.43  2568.930  2167.2000  1108.9456     1.4869   94.2600  0.1249   \n",
       " 592   3022.02  2323.340  2159.0556  1084.3779     0.8184  100.7056  0.1249   \n",
       " 593   2941.23  2505.380  2160.6000  1124.5821     1.5257   98.7122  0.1246   \n",
       " 594   3042.70  2355.110  2242.8555  1430.7270     0.8832  106.2100  0.1216   \n",
       " 595   3036.34  2468.910  2179.0778  1820.0613     1.6746   99.3300  0.1261   \n",
       " 596   3045.38  2513.260  2124.8444  1180.2820     0.8465  100.7978  0.1257   \n",
       " 597   2907.52  2547.130  2159.0556  1084.3779     0.8184  100.7056  0.1249   \n",
       " 598   3033.29  2508.860  2124.8444  1180.2820     0.8465  100.7978  0.1257   \n",
       " 599   2895.71  2573.590  2173.6444  1458.7176     0.8811  101.5667  0.1258   \n",
       " 600   2960.54  2508.090  2164.3000  1031.4032     1.3626  102.1322  0.1242   \n",
       " 602   2974.20  2449.160  2231.4889  1275.3021     1.6787   94.1256  0.1193   \n",
       " 603   3114.92  2574.480  2200.0666  1012.6747     1.3954  103.0644  0.1212   \n",
       " 604   2937.63  2499.720  2200.0666  1012.6747     1.3954  103.0644  0.1212   \n",
       " 606   2985.38  2542.340  2200.0666  1012.6747     1.3954  103.0644  0.1212   \n",
       " 607   3032.07  2477.720  2172.4000  1148.4101     1.2614  102.4522  0.1227   \n",
       " 608   3065.45  2698.140  2200.0666  1012.6747     1.3954  103.0644  0.1212   \n",
       " 609   2966.94  2509.220  2113.0778  1016.6750     0.9218   97.9200  0.1265   \n",
       " 610   3010.40  2499.405  2172.4000  1148.4101     1.2614  102.4522  0.1227   \n",
       " 611   2967.54  2573.090  2160.6000  1124.5821     1.5257   98.7122  0.1246   \n",
       " 612   2891.17  2480.110  2171.8556   940.9917     1.2906  103.4733  0.1234   \n",
       " 613   2992.61  2541.130  2171.8556   940.9917     1.2906  103.4733  0.1234   \n",
       " 614   2924.96  2564.290  2171.8556   940.9917     1.2906  103.4733  0.1234   \n",
       " 615   2956.18  2435.990  2171.8556   940.9917     1.2906  103.4733  0.1234   \n",
       " 616   3020.40  2690.550  2171.8556   940.9917     1.2906  103.4733  0.1234   \n",
       " 617   3059.01  2641.110  2171.8556   940.9917     1.2906  103.4733  0.1234   \n",
       " 618   2993.11  2498.910  2171.8556   940.9917     1.2906  103.4733  0.1234   \n",
       " 619   2987.75  2574.530  2183.5000  1099.0027     1.3593  104.4156  0.1220   \n",
       " 620   3004.71  2329.660  2217.8667  1275.0917     1.5487  105.2933  0.1230   \n",
       " 621   2931.40  2573.210  2171.8556   940.9917     1.2906  103.4733  0.1234   \n",
       " 622   3108.56  2468.650  2201.0667   880.2317     1.4148  106.5478  0.1211   \n",
       " 623   3071.80  2517.650  2200.0666  1012.6747     1.3954  103.0644  0.1212   \n",
       " 624   2989.44  2487.660  2201.0667   880.2317     1.4148  106.5478  0.1211   \n",
       " 625   2996.89  2492.400  2217.8667  1275.0917     1.5487  105.2933  0.1230   \n",
       " 626   2990.85  2485.990  2167.9444   861.8041     1.4140  106.6033  0.1243   \n",
       " 627   3059.43  2473.550  2214.9333  1663.7024     1.0203  100.4456  0.1247   \n",
       " 628   3024.54  2420.250  2167.9444   861.8041     1.4140  106.6033  0.1243   \n",
       " 629   3069.44  2459.500  2183.5000  1099.0027     1.3593  104.4156  0.1220   \n",
       " 630   2989.37  2584.750  2167.9444   861.8041     1.4140  106.6033  0.1243   \n",
       " 631   3147.74  2281.350  2171.8556   940.9917     1.2906  103.4733  0.1234   \n",
       " 632   3031.65  2565.470  2184.8889   905.1501     1.3378  106.6900  0.1226   \n",
       " 633   3007.85  2685.060  2217.8667  1275.0917     1.5487  105.2933  0.1230   \n",
       " 635   3017.53  2524.090  2201.0667   880.2317     1.4148  106.5478  0.1211   \n",
       " 636   2980.53  2221.100  2167.9444   861.8041     1.4140  106.6033  0.1243   \n",
       " 637   2925.34  2521.850  2184.8889   905.1501     1.3378  106.6900  0.1226   \n",
       " 638   3048.42  2691.960  2167.9444   861.8041     1.4140  106.6033  0.1243   \n",
       " 639   2998.18  2615.960  2171.3222   966.5755     0.8066  107.1700  0.1242   \n",
       " 640   3032.45  2455.320  2172.4000  1148.4101     1.2614  102.4522  0.1227   \n",
       " 641   2986.50  2497.130  2236.0667  1680.1825     1.4834   98.6889  0.1221   \n",
       " 642   2939.60  2541.120  2233.1556  1434.9983     1.5188  102.6611  0.1235   \n",
       " 643   3041.23  2487.830  2183.5000  1099.0027     1.3593  104.4156  0.1220   \n",
       " 644   3072.21  2610.620  2214.9333  1663.7024     1.0203  100.4456  0.1247   \n",
       " 645   3017.87  2467.400  2217.8667  1275.0917     1.5487  105.2933  0.1230   \n",
       " 646   3119.97  2373.030  2184.8889   905.1501     1.3378  106.6900  0.1226   \n",
       " 647   2743.24  2614.540  2171.3222   966.5755     0.8066  107.1700  0.1242   \n",
       " 648   3068.56  2363.520  2171.3222   966.5755     0.8066  107.1700  0.1242   \n",
       " 649   3022.04  2482.880  2214.9333  1663.7024     1.0203  100.4456  0.1247   \n",
       " 650   2954.46  2449.480  2236.0667  1680.1825     1.4834   98.6889  0.1221   \n",
       " 651   2978.62  2478.810  2236.0667  1680.1825     1.4834   98.6889  0.1221   \n",
       " 652   2938.27  2447.280  2201.0667   880.2317     1.4148  106.5478  0.1211   \n",
       " 653   2976.24  2411.420  2233.1556  1434.9983     1.5188  102.6611  0.1235   \n",
       " 654   2993.04  2504.660  2229.3333  1553.3158     1.5123  102.7800  0.1235   \n",
       " 655   3083.33  2440.210  2171.8556   940.9917     1.2906  103.4733  0.1234   \n",
       " 656   2992.39  2591.830  2233.1556  1434.9983     1.5188  102.6611  0.1235   \n",
       " 657   3049.80  2512.790  2165.8666   847.7976     1.4274  108.6589  0.1236   \n",
       " 658   3034.55  2473.390  2233.1556  1434.9983     1.5188  102.6611  0.1235   \n",
       " 659   3068.38  2515.320  2165.8666   847.7976     1.4274  108.6589  0.1236   \n",
       " 660   2961.59  2513.790  2229.3333  1553.3158     1.5123  102.7800  0.1235   \n",
       " 661   3085.78  2428.950  2229.3333  1553.3158     1.5123  102.7800  0.1235   \n",
       " 662   3020.29  2433.990  2217.8111  1744.7771     0.9618  100.1789  0.1218   \n",
       " 663   3011.84  2604.990  2167.9444   861.8041     1.4140  106.6033  0.1243   \n",
       " 664   2979.45  2546.780  2217.8111  1744.7771     0.9618  100.1789  0.1218   \n",
       " 665   3098.04  2386.330  2236.0667  1680.1825     1.4834   98.6889  0.1221   \n",
       " 666   3080.57  2428.250  2217.8667  1275.0917     1.5487  105.2933  0.1230   \n",
       " 667   2938.11  2471.740  2233.1556  1434.9983     1.5188  102.6611  0.1235   \n",
       " 668   3073.57  2419.180  2171.3222   966.5755     0.8066  107.1700  0.1242   \n",
       " 669   3057.31  2481.530  2214.9333  1663.7024     1.0203  100.4456  0.1247   \n",
       " 670   2979.86  2545.480  2229.3333  1553.3158     1.5123  102.7800  0.1235   \n",
       " 671   3113.13  2459.140  2159.0556  1084.3779     0.8184  100.7056  0.1249   \n",
       " 672   3094.95  2505.700  2201.0667   880.2317     1.4148  106.5478  0.1211   \n",
       " 673   2978.89  2523.930  2217.8111  1744.7771     0.9618  100.1789  0.1218   \n",
       " 674   3005.61  2393.560  2214.9333  1663.7024     1.0203  100.4456  0.1247   \n",
       " 675   2965.50  2366.360  2184.8889   905.1501     1.3378  106.6900  0.1226   \n",
       " 676   2933.05  2514.760  2229.3333  1553.3158     1.5123  102.7800  0.1235   \n",
       " 677   2970.25  2432.020  2233.1556  1434.9983     1.5188  102.6611  0.1235   \n",
       " 678   3005.80  2669.090  2233.1556  1434.9983     1.5188  102.6611  0.1235   \n",
       " 679   3015.13  2605.970  2233.1556  1434.9983     1.5188  102.6611  0.1235   \n",
       " 680   3110.23  2397.920  2184.8889   905.1501     1.3378  106.6900  0.1226   \n",
       " 681   3018.34  2556.620  2217.8111  1744.7771     0.9618  100.1789  0.1218   \n",
       " 682   3067.95  2461.410  2233.1556  1434.9983     1.5188  102.6611  0.1235   \n",
       " 683   3046.22  2715.500  2171.3222   966.5755     0.8066  107.1700  0.1242   \n",
       " 684   2960.65  2442.710  2159.0556  1084.3779     0.8184  100.7056  0.1249   \n",
       " 685   3165.51  2380.330  2215.3889  1779.1892     0.8552  101.2178  0.1226   \n",
       " 686   3024.24  2446.060  2165.8666   847.7976     1.4274  108.6589  0.1236   \n",
       " 687   3011.84  2451.120  2215.3889  1779.1892     0.8552  101.2178  0.1226   \n",
       " 688   2951.67  2521.410  2165.8666   847.7976     1.4274  108.6589  0.1236   \n",
       " 689   3025.81  2314.350  2179.2000   875.7538     1.2708  106.9600  0.1213   \n",
       " 690   2917.17  2379.310  2215.3889  1779.1892     0.8552  101.2178  0.1226   \n",
       " 691   3122.43  2541.550  2233.1556  1434.9983     1.5188  102.6611  0.1235   \n",
       " 692   2960.04  2292.400  2215.3889  1779.1892     0.8552  101.2178  0.1226   \n",
       " 693   2991.62  2540.360  2172.2778  1085.3232     0.7955  105.7633  0.1221   \n",
       " 694   3205.08  2286.450  2184.8889   905.1501     1.3378  106.6900  0.1226   \n",
       " 695   2965.52  2539.470  2201.0667   880.2317     1.4148  106.5478  0.1211   \n",
       " 696   3046.63  2440.050  2214.9333  1663.7024     1.0203  100.4456  0.1247   \n",
       " 697   3110.14  2474.300  2242.4333  1543.8401     1.4994   98.1133  0.1218   \n",
       " 698   3005.14  2443.390  2150.5889   868.5387     1.3597  105.6089  0.1236   \n",
       " 699   3064.13  2526.220  2205.7222   906.9522     1.3443  105.6600  0.1200   \n",
       " 700   3073.93  2472.680  2227.2444  1329.8933     0.9480  107.8778  0.1220   \n",
       " 701   3010.92  2491.130  2179.2000   875.7538     1.2708  106.9600  0.1213   \n",
       " 702   3040.91  2499.405  2205.7222   906.9522     1.3443  105.6600  0.1200   \n",
       " 703   3004.63  2559.700  2220.4001  1389.2768     1.5976  100.9711  0.1249   \n",
       " 704   2988.50  2560.750  2214.9333  1663.7024     1.0203  100.4456  0.1247   \n",
       " 705   2965.28  2482.610  2205.7222   906.9522     1.3443  105.6600  0.1200   \n",
       " 706   2978.87  2503.090  2171.3222   966.5755     0.8066  107.1700  0.1242   \n",
       " 707   3045.88  2506.600  2229.3333  1553.3158     1.5123  102.7800  0.1235   \n",
       " 708   3005.53  2479.240  2171.3222   966.5755     0.8066  107.1700  0.1242   \n",
       " 710   2972.22  2519.870  2215.3889  1779.1892     0.8552  101.2178  0.1226   \n",
       " 711   3043.49  2476.750  2205.7222   906.9522     1.3443  105.6600  0.1200   \n",
       " 712   3219.60  2329.440  2201.0667   880.2317     1.4148  106.5478  0.1211   \n",
       " 713   3096.81  2476.950  2205.7222   906.9522     1.3443  105.6600  0.1200   \n",
       " 714   3101.28  2464.330  2216.5111   871.2526     1.2366  107.4867  0.1194   \n",
       " 715   3070.07  2459.220  2216.5111   871.2526     1.2366  107.4867  0.1194   \n",
       " 716   3068.81  2543.620  2221.5333  1738.1057     1.3844   99.7633  0.1228   \n",
       " 717   2943.83  2445.170  2236.0667  1680.1825     1.4834   98.6889  0.1221   \n",
       " 718   2992.33  2513.220  2229.3333  1553.3158     1.5123  102.7800  0.1235   \n",
       " 719   2949.77  2522.320  2205.7222   906.9522     1.3443  105.6600  0.1200   \n",
       " 720   3003.13  2498.300  2227.2444  1329.8933     0.9480  107.8778  0.1220   \n",
       " 721   3000.68  2432.180  2198.2667   986.5558     0.8652  105.0589  0.1209   \n",
       " 722   3056.40  2502.480  2173.6222   918.8631     1.2742  107.1889  0.1217   \n",
       " 723   2964.83  2516.950  2210.9778  1572.4698     1.0204  106.2089  0.1222   \n",
       " 724   2967.98  2512.650  2210.9778  1572.4698     1.0204  106.2089  0.1222   \n",
       " 725   3011.49  2608.110  2216.5111   871.2526     1.2366  107.4867  0.1194   \n",
       " 726   2994.75  2583.360  2210.9778  1572.4698     1.0204  106.2089  0.1222   \n",
       " 727   2984.80  2508.030  2250.7445   996.4071     0.8572  106.2956  0.1172   \n",
       " 728   3048.25  2523.940  2242.4333  1543.8401     1.4994   98.1133  0.1218   \n",
       " 729   3063.44  2510.760  2216.6778  1567.4646     1.3617   96.5556  0.1230   \n",
       " 730   3136.34  2442.450  2250.7445   996.4071     0.8572  106.2956  0.1172   \n",
       " 731   3048.54  2353.210  2210.9778  1572.4698     1.0204  106.2089  0.1222   \n",
       " 732   2992.37  2484.270  2210.9778  1572.4698     1.0204  106.2089  0.1222   \n",
       " 733   3037.92  2361.500  2210.9778  1572.4698     1.0204  106.2089  0.1222   \n",
       " 734   3069.73  2533.450  2218.6333   870.5620     1.3084  106.9089  0.1195   \n",
       " 735   3027.92  2636.810  2189.2333   972.1465     1.3796  103.5789  0.1207   \n",
       " 736   3028.97  2415.870  2136.4111  1172.5767     0.7809  106.6633  0.1245   \n",
       " 737   2970.11  2437.820  2136.4111  1172.5767     0.7809  106.6633  0.1245   \n",
       " 738   2957.59  2441.800  2189.8778  1158.0925     1.5291  107.8200  0.1219   \n",
       " 739   3061.59  2483.440  2151.6889  1197.2643     1.5018  102.6711  0.1239   \n",
       " 740   3090.55  2540.970  2136.4111  1172.5767     0.7809  106.6633  0.1245   \n",
       " 741   3014.54  2539.590  2201.3556  1249.8445     1.4699  103.9978  0.1228   \n",
       " 742   2988.93  2512.380  2189.8778  1158.0925     1.5291  107.8200  0.1219   \n",
       " 743   2996.90  2448.070  2162.7556  1041.1557     0.8479  107.2622  0.1221   \n",
       " 744   2957.41  2469.130  2203.4556  1441.1445     0.8264  104.6767  0.1232   \n",
       " 745   2953.41  2531.050  2136.4111  1172.5767     0.7809  106.6633  0.1245   \n",
       " 746   3102.49  2502.440  2189.8778  1158.0925     1.5291  107.8200  0.1219   \n",
       " 747   3057.65  2540.350  2203.4556  1441.1445     0.8264  104.6767  0.1232   \n",
       " 748   3083.62  2728.280  2203.4556  1441.1445     0.8264  104.6767  0.1232   \n",
       " 749   3056.58  2505.620  2219.3445  1734.8435     0.9800  104.3867  0.1221   \n",
       " 750   2971.93  2557.580  2189.8778  1158.0925     1.5291  107.8200  0.1219   \n",
       " 751   3080.89  2590.450  2162.7556  1006.7789     0.8736  106.8744  0.1220   \n",
       " 752   2951.59  2474.600  2251.8222  1104.9132     1.3881  104.9456  0.1206   \n",
       " 753   3108.23  2423.570  2223.2778  1222.0834     1.5160  102.9600  0.1218   \n",
       " 754   3059.82  2727.560  2146.9111  1063.8762     1.5377  104.3711  0.1232   \n",
       " 755   2940.26  2525.290  2203.4556  1441.1445     0.8264  104.6767  0.1232   \n",
       " 756   2959.36  2504.860  2210.9778  1572.4698     1.0204  106.2089  0.1222   \n",
       " 757   3011.27  2477.470  2210.9778  1572.4698     1.0204  106.2089  0.1222   \n",
       " 758   3046.97  2517.220  2151.6889  1197.2643     1.5018  102.6711  0.1239   \n",
       " 759   3017.65  2542.640  2162.7556  1006.7789     0.8736  106.8744  0.1220   \n",
       " 760   3001.36  2491.230  2155.3111   918.2161     1.2753  105.0478  0.1227   \n",
       " 761   2981.92  2530.670  2151.6889  1197.2643     1.5018  102.6711  0.1239   \n",
       " 762   2931.70  2487.010  2155.3111   918.2161     1.2753  105.0478  0.1227   \n",
       " 763   3032.26  2455.050  2145.4555  1017.0577     1.4534  103.7878  0.1229   \n",
       " 764   2922.05  2511.370  2155.3111   918.2161     1.2753  105.0478  0.1227   \n",
       " 765   2939.40  2504.440  2210.9778  1572.4698     1.0204  106.2089  0.1222   \n",
       " 766   3097.14  2463.060  2155.3111   918.2161     1.2753  105.0478  0.1227   \n",
       " 767   2976.91  2583.600  2117.5889   894.0996     1.4330  106.4944  0.1253   \n",
       " 768   3003.96  2560.610  2155.3111   918.2161     1.2753  105.0478  0.1227   \n",
       " 769   2961.73  2368.340  2146.9111  1063.8762     1.5377  104.3711  0.1232   \n",
       " 770   3005.04  2431.660  2136.4111  1172.5767     0.7809  106.6633  0.1245   \n",
       " 771   3028.21  2529.080  2245.3667  1315.8241     1.5424  102.4089  0.1210   \n",
       " 772   3086.08  2415.750  2190.1555  1298.8207     1.3947  105.7411  0.1242   \n",
       " 773   2962.28  2452.280  2127.9555  1011.1924     1.0863  107.1522  0.1261   \n",
       " 774   3070.63  2377.970  2130.0556   874.9165     1.2545  105.9011  0.1253   \n",
       " 775   3055.13  2523.650  2181.0444   939.8456     1.3968  104.0133  0.1214   \n",
       " 776   2899.68  2492.240  2127.9555  1011.1924     1.0863  107.1522  0.1261   \n",
       " 777   2957.04  2525.160  2190.1555  1298.8207     1.3947  105.7411  0.1242   \n",
       " 778   3015.41  2474.940  2174.7555  1206.3506     1.4202  104.2622  0.1246   \n",
       " 779   3051.05  2582.080  2190.1555  1298.8207     1.3947  105.7411  0.1242   \n",
       " 780   3055.66  2625.200  2211.7222  1335.4837     1.3922  100.9256  0.1232   \n",
       " 781   3043.14  2517.250  2190.1555  1298.8207     1.3947  105.7411  0.1242   \n",
       " 782   2982.59  2466.860  2117.5889   894.0996     1.4330  106.4944  0.1253   \n",
       " 783   3068.42  2570.500  2190.1555  1298.8207     1.3947  105.7411  0.1242   \n",
       " 784   3068.45  2575.330  2130.0556   874.9165     1.2545  105.9011  0.1253   \n",
       " 785   2978.45  2504.940  2181.0444   939.8456     1.3968  104.0133  0.1214   \n",
       " 786   2928.55  2579.840  2181.0444   939.8456     1.3968  104.0133  0.1214   \n",
       " 787   3049.58  2432.090  2169.1111  1431.8740     1.0321  100.1744  0.1252   \n",
       " 788   2999.24  2435.470  2175.9667  1539.5734     1.4139   98.5822  0.1254   \n",
       " 789   3018.82  2470.210  2175.9667  1539.5734     1.4139   98.5822  0.1254   \n",
       " 790   2982.16  2574.450  2181.0444   939.8456     1.3968  104.0133  0.1214   \n",
       " 791   2958.83  2488.500  2197.5222  1373.0077     1.1369  106.0733  0.1240   \n",
       " 792   2975.57  2566.790  2197.5222  1373.0077     1.1369  106.0733  0.1240   \n",
       " 793   2925.62  2501.910  2197.5222  1373.0077     1.1369  106.0733  0.1240   \n",
       " 794   3043.70  2535.710  2127.9555  1011.1924     1.0863  107.1522  0.1261   \n",
       " 796   3090.47  2314.520  2169.1111  1431.8740     1.0321  100.1744  0.1252   \n",
       " 798   3064.76  2510.720  2194.5889  1614.2776     1.2686   97.7622  0.1247   \n",
       " 799   2977.39  2535.070  2194.5889  1614.2776     1.2686   97.7622  0.1247   \n",
       " 800   3010.26  2508.800  2170.0666  1364.5157     1.5447   96.7700  0.1230   \n",
       " 801   2947.39  2308.450  2218.6333   870.5620     1.3084  106.9089  0.1195   \n",
       " 802   3036.64  2483.930  2194.5889  1614.2776     1.2686   97.7622  0.1247   \n",
       " 803   3045.07  2618.250  2175.9667  1539.5734     1.4139   98.5822  0.1254   \n",
       " 804   3085.93  2518.470  2204.5445  2076.6730     1.0961   95.9222  0.1251   \n",
       " 805   2952.31  2453.530  2188.7222  1325.5230     1.1721  105.1700  0.1250   \n",
       " 806   3047.06  2531.160  2204.5445  2076.6730     1.0961   95.9222  0.1251   \n",
       " 807   2991.25  2519.260  2194.8000  1456.0760     1.5130   99.0344  0.1233   \n",
       " 808   3045.33  2472.230  2194.5889  1614.2776     1.2686   97.7622  0.1247   \n",
       " 809   3208.47  2294.010  2194.8000  1456.0760     1.5130   99.0344  0.1233   \n",
       " 810   2939.93  2595.820  2204.5445  2076.6730     1.0961   95.9222  0.1251   \n",
       " 811   3042.85  2403.210  2204.5445  2076.6730     1.0961   95.9222  0.1251   \n",
       " 812   3039.04  2598.940  2187.8111  2228.7377     1.4990   90.4933  0.1240   \n",
       " 813   3050.36  2846.440  2220.0445  2253.2847     1.7112   88.0444  0.1222   \n",
       " 814   3012.09  2551.080  2216.7333  1748.0885     1.1127   97.5822  0.1242   \n",
       " 815   2924.39  2470.950  2188.7222  1325.5230     1.1721  105.1700  0.1250   \n",
       " 816   2988.81  2461.820  2187.8111  2228.7377     1.4990   90.4933  0.1240   \n",
       " 817   2958.04  2502.890  2134.7445  1244.2899     1.1827   99.5133  0.1249   \n",
       " 818   3087.39  2392.000  2188.6000  2115.2244     1.3174   93.1656  0.1255   \n",
       " 819   3064.71  2478.510  2216.7333  1748.0885     1.1127   97.5822  0.1242   \n",
       " 820   2946.62  2470.260  2187.8111  2228.7377     1.4990   90.4933  0.1240   \n",
       " 821   3009.71  2565.530  2224.6778  1308.6479     1.3907  101.1333  0.1208   \n",
       " 822   3105.30  2435.900  2194.5889  1614.2776     1.2686   97.7622  0.1247   \n",
       " 823   3021.77  2544.100  2204.5445  2076.6730     1.0961   95.9222  0.1251   \n",
       " 824   2927.54  2563.080  2188.7111  1115.4922     1.5156  102.0011  0.1225   \n",
       " 825   2951.83  2484.440  2218.9445  1491.3268     1.4420   96.4389  0.1210   \n",
       " 827   3073.73  2504.010  2218.8778  1188.2630     1.2557  100.9444  0.1212   \n",
       " 828   3103.85  2256.120  2194.5889  1614.2776     1.2686   97.7622  0.1247   \n",
       " 829   3002.22  2462.060  2202.1222  1833.3772     1.8005   95.8500  0.1242   \n",
       " 830   2968.33  2476.580  2216.7333  1748.0885     1.1127   97.5822  0.1242   \n",
       " 832   3072.03  2500.680  2205.7445  1363.1048     1.0518  101.8644  0.1220   \n",
       " 833   3021.83  2419.830  2205.7445  1363.1048     1.0518  101.8644  0.1220   \n",
       " 834   3006.95  2435.340  2189.8111  1084.6502     1.1993  104.8856  0.1234   \n",
       " 835   3003.72  2537.660  2210.7778  2008.9216     1.1351   91.1078  0.1240   \n",
       " 836   2953.59  2504.860  2224.6778  1308.6479     1.3907  101.1333  0.1208   \n",
       " 837   3086.52  2360.040  2204.2333  2110.8288     1.6392   89.0356  0.1245   \n",
       " 838   3048.76  2545.680  2224.6778  1308.6479     1.3907  101.1333  0.1208   \n",
       " 839   2984.06  2619.600  2225.0222  1730.8480     1.5333   98.5978  0.1232   \n",
       " 840   2947.87  2460.050  2204.2333  2110.8288     1.6392   89.0356  0.1245   \n",
       " 841   3008.28  2504.210  2202.2556  1914.0689     1.6013   94.6922  0.1242   \n",
       " 842   3084.81  2445.200  2224.6778  1308.6479     1.3907  101.1333  0.1208   \n",
       " 843   3034.50  2431.350  2220.0445  2253.2847     1.7112   88.0444  0.1222   \n",
       " 844   3001.26  2519.920  2224.6778  1308.6479     1.3907  101.1333  0.1208   \n",
       " 845   3017.39  2544.640  2246.5778  1963.8016     1.1665   96.7089  0.1209   \n",
       " 846   3018.80  2440.930  2195.6667  1333.7299     1.0772   98.9844  0.1223   \n",
       " 847   2931.05  2528.740  2205.7000  1072.8058     1.2856  100.8511  0.1216   \n",
       " 848   3004.08  2514.670  2200.2333  1173.8377     1.3281  101.6111  0.1211   \n",
       " 849   2893.07  2596.630  2214.2889   988.2071     1.2513  101.7044  0.1209   \n",
       " 850   3215.87  2453.970  2212.8667  1066.9539     0.8161  101.6156  0.1203   \n",
       " 851   3004.39  2468.560  2200.9556  1126.8678     0.7860  100.3700  0.1215   \n",
       " 852   2929.93  2517.500  2218.8778  1188.2630     1.2557  100.9444  0.1212   \n",
       " 853   3089.10  2414.290  2200.9556  1126.8678     0.7860  100.3700  0.1215   \n",
       " 854   2993.19  2577.230  2204.2333  2110.8288     1.6392   89.0356  0.1245   \n",
       " 855   3048.39  2489.150  2205.7445  1363.1048     1.0518  101.8644  0.1220   \n",
       " 856   2951.85  2525.000  2189.5777  1320.3197     1.3459  100.7744  0.1234   \n",
       " 857   2889.67  2254.990  2199.6334  1242.8420     1.4083   99.2178  0.1221   \n",
       " 858   2938.02  2499.680  2114.6667  1549.4874     1.3393   98.7844  0.1262   \n",
       " 859   2919.71  2420.320  2134.7445  1244.2899     1.1827   99.5133  0.1249   \n",
       " 860   3081.34  2453.780  2175.3445  1147.3421     1.1419  107.1422  0.1251   \n",
       " 861   3069.72  2372.790  2127.9555  1011.1924     1.0863  107.1522  0.1261   \n",
       " 862   2982.62  2474.880  2114.6667  1549.4874     1.3393   98.7844  0.1262   \n",
       " 863   2997.94  2471.970  2214.1667  1705.2046     0.9113  100.4633  0.1231   \n",
       " 864   2904.21  2530.630  2188.6000  2115.2244     1.3174   93.1656  0.1255   \n",
       " 865   2949.75  2528.500  2246.5778  1963.8016     1.1665   96.7089  0.1209   \n",
       " 866   2979.77  2493.030  2208.6556  1728.7576     1.5137   97.0367  0.1240   \n",
       " 867   3006.33  2424.130  2189.3556  2363.6412     2.1415   83.4233  0.1246   \n",
       " 868   3043.78  2522.760  2192.0000  2085.6871     1.1723   94.9300  0.1238   \n",
       " 869   3019.38  2499.750  2200.9556  1126.8678     0.7860  100.3700  0.1215   \n",
       " 870   3085.10  2500.130  2199.6334  1242.8420     1.4083   99.2178  0.1221   \n",
       " 872   2982.07  2447.060  2199.6334  1242.8420     1.4083   99.2178  0.1221   \n",
       " 873   3116.38  2404.130  2212.8667  1066.9539     0.8161  101.6156  0.1203   \n",
       " 874   3123.02  2488.960  2240.8666  2048.2566     1.1620   86.3822  0.1233   \n",
       " 875   3078.60  2441.680  2213.5111  1183.4356     0.7531  103.0911  0.1199   \n",
       " 876   2921.73  2515.910  2189.3556  2363.6412     2.1415   83.4233  0.1246   \n",
       " 877   3060.41  2453.750  2196.0000  1230.0293     0.7684   97.5578  0.1219   \n",
       " 878   3030.23  2568.320  2196.0000  1230.0293     0.7684   97.5578  0.1219   \n",
       " 879   2998.89  2532.660  2189.3556  2363.6412     2.1415   83.4233  0.1246   \n",
       " 880   3073.25  2440.080  2240.8666  2048.2566     1.1620   86.3822  0.1233   \n",
       " 881   3078.77  2533.040  2187.4111  1942.3069     1.1864   88.0911  0.1245   \n",
       " 882   3007.47  2571.180  2187.4111  1942.3069     1.1864   88.0911  0.1245   \n",
       " 883   3019.98  2515.630  2196.0000  1230.0293     0.7684   97.5578  0.1219   \n",
       " 884   3056.28  2464.400  2211.7778  1177.4224     1.3377   98.9989  0.1200   \n",
       " 885   3011.49  2532.450  2191.1333  2197.6570     1.1569   89.7222  0.1251   \n",
       " 886   2993.79  2510.590  2211.7778  1177.4224     1.3377   98.9989  0.1200   \n",
       " 887   3007.75  2535.140  2216.5000  1111.5436     0.8373   99.9867  0.1205   \n",
       " 888   3083.14  2456.750  2216.5000  1111.5436     0.8373   99.9867  0.1205   \n",
       " 889   3094.08  2664.520  2202.5556  1081.8043     1.2913  101.8922  0.1205   \n",
       " 890   3071.05  2642.150  2200.9889  1054.5240     1.3830  100.1800  0.1201   \n",
       " 891   3074.96  2448.820  2315.2667  2360.1325     1.1259   90.1144  0.1160   \n",
       " 892   3001.95  2598.220  2213.2111  2070.7147     1.9705   87.7411  0.1232   \n",
       " 893   3197.43  2405.620  2171.5000  1028.4440     0.7899  101.5122  0.1224   \n",
       " 894   3001.60  2500.260  2171.5000  1028.4440     0.7899  101.5122  0.1224   \n",
       " 895   2956.77  2518.500  2187.4111  1942.3069     1.1864   88.0911  0.1245   \n",
       " 896   3013.86  2527.200  2195.6667  1333.7299     1.0772   98.9844  0.1223   \n",
       " 897   2950.92  2439.590  2213.5111  1183.4356     0.7531  103.0911  0.1199   \n",
       " 898   2948.09  2480.050  2200.9889  1054.5240     1.3830  100.1800  0.1201   \n",
       " 899   3058.08  2524.600  2192.3778  1110.5453     0.8147   99.2922  0.1226   \n",
       " 900   3066.38  2473.100  2171.5000  1028.4440     0.7899  101.5122  0.1224   \n",
       " 901   3043.18  2545.530  2192.3778  1110.5453     0.8147   99.2922  0.1226   \n",
       " 902   2978.57  2559.800  2180.5556  1165.1351     0.7892  101.4578  0.1226   \n",
       " 903   2825.67  2286.090  2210.2778  2120.5760     1.0700   95.1089  0.1230   \n",
       " 904   2993.98  2456.250  2191.1333  2197.6570     1.1569   89.7222  0.1251   \n",
       " 905   2948.57  2526.190  2216.5000  1111.5436     0.8373   99.9867  0.1205   \n",
       " 906   2966.39  2507.220  2171.5000  1028.4440     0.7899  101.5122  0.1224   \n",
       " 907   3086.05  2417.340  2216.5000  1111.5436     0.8373   99.9867  0.1205   \n",
       " 908   2972.52  2454.800  2180.5556  1165.1351     0.7892  101.4578  0.1226   \n",
       " 909   2954.92  2412.760  2171.5000  1028.4440     0.7899  101.5122  0.1224   \n",
       " 910   2986.28  2483.710  2200.9889  1054.5240     1.3830  100.1800  0.1201   \n",
       " 911   3111.64  2387.230  2206.5777   978.7832     1.3316  100.6189  0.1203   \n",
       " 912   3093.57  2492.270  2210.2778  2120.5760     1.0700   95.1089  0.1230   \n",
       " 913   2980.66  2437.210  2200.9889  1054.5240     1.3830  100.1800  0.1201   \n",
       " 915   3184.83  2400.400  2189.9667  1046.6212     0.8662  102.3622  0.1208   \n",
       " 916   2998.01  2479.770  2189.9667  1046.6212     0.8662  102.3622  0.1208   \n",
       " 917   2973.56  2536.680  2189.9667  1046.6212     0.8662  102.3622  0.1208   \n",
       " 918   3221.21  2391.200  2189.9667  1046.6212     0.8662  102.3622  0.1208   \n",
       " 919   3000.38  2593.090  2191.1333  2197.6570     1.1569   89.7222  0.1251   \n",
       " 920   2964.63  2494.750  2201.0667  1285.2144     1.3168  101.5122  0.1224   \n",
       " 921   2990.58  2506.560  2194.3778  1265.1715     0.7588  100.1833  0.1204   \n",
       " 922   3007.00  2572.620  2213.2111  2070.7147     1.9705   87.7411  0.1232   \n",
       " 923   2982.10  2572.780  2201.0667  1285.2144     1.3168  101.5122  0.1224   \n",
       " 925   3013.66  2526.440  2185.2111  1141.6306     0.8447  100.5978  0.1217   \n",
       " 927   3084.82  2387.420  2171.5000  1028.4440     0.7899  101.5122  0.1224   \n",
       " 928   2955.87  2541.890  2201.0667  1285.2144     1.3168  101.5122  0.1224   \n",
       " 930   3037.62  2431.930  2194.9556  2341.7833     2.3917   86.8100  0.1231   \n",
       " 931   3097.63  2461.580  2194.9556  2341.7833     2.3917   86.8100  0.1231   \n",
       " 932   3084.23  2433.670  2194.9556  2341.7833     2.3917   86.8100  0.1231   \n",
       " 933   2919.84  2484.900  2213.2111  2070.7147     1.9705   87.7411  0.1232   \n",
       " 934   3154.13  2368.120  2189.9667  1046.6212     0.8662  102.3622  0.1208   \n",
       " 935   3054.81  2430.600  2060.6600  1410.3599     3.8894   96.6256  0.1181   \n",
       " 936   2913.40  2496.010  2227.2444  1129.8739     1.2803  102.7956  0.1188   \n",
       " 937   3150.13  2531.810  2211.4334  2130.9862     1.1351   86.7956  0.1237   \n",
       " 938   3059.09  2555.690  2060.6600  1410.3599     3.8894   96.6256  0.1181   \n",
       " 939   3076.81  2521.250  2195.5000  1027.5153     1.3548  105.8378  0.1203   \n",
       " 940   3015.77  2440.940  2194.3778  1265.1715     0.7588  100.1833  0.1204   \n",
       " 941   2958.22  2467.500  2227.2444  1129.8739     1.2803  102.7956  0.1188   \n",
       " 942   2945.90  2539.580  2227.2444  1129.8739     1.2803  102.7956  0.1188   \n",
       " 943   3087.24  2470.280  2060.6600  1410.3599     3.8894   96.6256  0.1181   \n",
       " 944   2964.71  2495.700  2202.3556  1020.9264     0.8676  102.7833  0.1200   \n",
       " 945   2964.00  2475.500  2227.2444  1129.8739     1.2803  102.7956  0.1188   \n",
       " 946   2980.29  2508.520  2060.6600  1410.3599     3.8894   96.6256  0.1181   \n",
       " 947   3048.93  2451.820  2197.3222  1586.9476     2.0983   97.6267  0.1249   \n",
       " 948   3055.29  2705.000  2201.0667  1285.2144     1.3168  101.5122  0.1224   \n",
       " 949   3001.53  2540.070  2060.6600  1410.3599     3.8894   96.6256  0.1181   \n",
       " 950   3077.04  2469.100  2060.6600  1410.3599     3.8894   96.6256  0.1181   \n",
       " 951   3062.47  2442.490  2168.7333   999.5929     1.3739  104.8544  0.1217   \n",
       " 952   3071.15  2501.000  2228.7222  2941.8341     1.7383   94.6056  0.1248   \n",
       " 953   3075.67  2516.470  2213.9111  1973.9690     1.2541   96.9456  0.1238   \n",
       " 954   2971.86  2528.730  2204.5000  1768.1986     1.3398   91.8511  0.1257   \n",
       " 955   2892.86  2538.740  2197.3222  1586.9476     2.0983   97.6267  0.1249   \n",
       " 956   2996.69  2461.170  2211.4334  2130.9862     1.1351   86.7956  0.1237   \n",
       " 957   2999.04  2438.560  2184.7222  1009.0221     1.3187  101.9178  0.1219   \n",
       " 958   3037.98  2370.440  2168.7333   999.5929     1.3739  104.8544  0.1217   \n",
       " 959   3024.67  2620.960  2206.1444  1876.9899     2.0607   95.9511  0.1246   \n",
       " 960   3034.43  2586.860  2197.3222  1586.9476     2.0983   97.6267  0.1249   \n",
       " 961   3044.39  2526.850  2211.4334  2130.9862     1.1351   86.7956  0.1237   \n",
       " 962   3040.48  2512.030  2206.1444  1876.9899     2.0607   95.9511  0.1246   \n",
       " 963   3082.05  2447.430  2210.2778  2120.5760     1.0700   95.1089  0.1230   \n",
       " 964   3101.48  2577.440  2168.7333   999.5929     1.3739  104.8544  0.1217   \n",
       " 965   2947.91  2456.680  2195.5000  1027.5153     1.3548  105.8378  0.1203   \n",
       " 966   2928.24  2524.910  2227.2444  1129.8739     1.2803  102.7956  0.1188   \n",
       " 967   3074.24  2426.860  2213.9111  1973.9690     1.2541   96.9456  0.1238   \n",
       " 968   2941.85  2462.750  2172.3111  1202.5347     1.2955  101.3556  0.1212   \n",
       " 969   2945.05  2469.590  2204.5000  1768.1986     1.3398   91.8511  0.1257   \n",
       " 970   3066.18  2539.010  2180.5556  1165.1351     0.7892  101.4578  0.1226   \n",
       " 971   3087.64  2403.240  2197.3222  1586.9476     2.0983   97.6267  0.1249   \n",
       " 972   3043.12  2473.670  2180.5556  1165.1351     0.7892  101.4578  0.1226   \n",
       " 973   2971.54  2457.640  2180.5556  1165.1351     0.7892  101.4578  0.1226   \n",
       " 974   2931.17  2467.400  2206.1444  1876.9899     2.0607   95.9511  0.1246   \n",
       " 975   3055.54  2508.880  2168.7333   999.5929     1.3739  104.8544  0.1217   \n",
       " 976   2949.99  2498.670  2204.1555  1276.2565     0.7880  100.6922  0.1198   \n",
       " 977   2927.94  2481.520  2206.5777   978.7832     1.3316  100.6189  0.1203   \n",
       " 978   3041.48  2494.580  2213.9111  1973.9690     1.2541   96.9456  0.1238   \n",
       " 979   3188.90  2445.410  2213.9111  1973.9690     1.2541   96.9456  0.1238   \n",
       " 980   3083.31  2377.310  2172.3111  1202.5347     1.2955  101.3556  0.1212   \n",
       " 981   3075.10  2614.510  2168.7333   999.5929     1.3739  104.8544  0.1217   \n",
       " 982   3012.68  2420.350  2172.3111  1202.5347     1.2955  101.3556  0.1212   \n",
       " 983   2981.70  2415.790  2201.8666  1185.4396     1.3996   99.9211  0.1199   \n",
       " 984   3000.77  2428.200  2201.8666  1185.4396     1.3996   99.9211  0.1199   \n",
       " 985   3058.91  2498.770  2060.6600  1410.3599     3.8894   96.6256  0.1181   \n",
       " 986   2999.96  2510.240  2206.1444  1876.9899     2.0607   95.9511  0.1246   \n",
       " 987   3079.44  2395.390  2209.0889  1459.4739     2.1612   98.9011  0.1242   \n",
       " 988   3234.24  2347.020  2202.3556  1854.7633     1.3593  103.2378  0.1239   \n",
       " 989   2994.15  2522.160  2211.6000  2122.2580     1.4470   94.2089  0.1242   \n",
       " 990   3043.23  2411.350  2172.3111  1202.5347     1.2955  101.3556  0.1212   \n",
       " 991   2940.53  2517.420  2206.1444  1876.9899     2.0607   95.9511  0.1246   \n",
       " 992   3040.06  2520.630  2201.8666  1185.4396     1.3996   99.9211  0.1199   \n",
       " 993   2966.26  2524.570  2213.9111  1973.9690     1.2541   96.9456  0.1238   \n",
       " 994   3086.92  2547.790  2215.5111  2039.3196     1.2150   99.4189  0.1238   \n",
       " 995   2982.40  2528.210  2201.0667  1285.2144     1.3168  101.5122  0.1224   \n",
       " 996   2901.62  2569.450  2223.9000  1745.3724     1.9974   96.7567  0.1241   \n",
       " 997   3053.64  2351.940  2206.1444  1876.9899     2.0607   95.9511  0.1246   \n",
       " 998   2938.84  2453.250  2188.5444  1251.2688     0.7980  103.5944  0.1199   \n",
       " 999   3045.11  2444.870  2215.1778  2192.1867     1.8829   85.6589  0.1237   \n",
       " 1000  3033.11  2435.140  2172.3111  1202.5347     1.2955  101.3556  0.1212   \n",
       " 1001  3022.60  2533.920  2184.7222  1009.0221     1.3187  101.9178  0.1219   \n",
       " 1002  2976.58  2495.160  2215.5111  2039.3196     1.2150   99.4189  0.1238   \n",
       " 1003  2917.47  2481.560  2247.3666  1039.7410     1.3594  104.3600  0.1171   \n",
       " 1004  3071.03  2483.660  2215.5111  2039.3196     1.2150   99.4189  0.1238   \n",
       " 1005  2945.22  2570.440  2194.9556  2341.7833     2.3917   86.8100  0.1231   \n",
       " 1006  3072.86  2434.600  2206.1444  1876.9899     2.0607   95.9511  0.1246   \n",
       " 1007  3075.12  2476.800  2189.3556  2363.6412     2.1415   83.4233  0.1246   \n",
       " 1008  3004.57  2460.590  2166.2111   894.8612     1.4073  105.6256  0.1219   \n",
       " 1009  3055.47  2423.380  2226.3111  2252.1538     1.2295   92.6178  0.1239   \n",
       " 1010  2965.50  2500.250  2206.1444  1876.9899     2.0607   95.9511  0.1246   \n",
       " 1011  2957.70  2509.620  2226.3111  2252.1538     1.2295   92.6178  0.1239   \n",
       " 1012  2992.40  2590.800  2223.9000  1745.3724     1.9974   96.7567  0.1241   \n",
       " 1013  2933.15  2497.760  2218.6223  1416.6631     1.9010   99.6067  0.1260   \n",
       " 1014  3099.58  2397.990  2153.9778  1192.6994     1.3522  100.9367  0.1225   \n",
       " 1015  2990.68  2449.840  2172.3111  1202.5347     1.2955  101.3556  0.1212   \n",
       " 1016  3061.73  2463.720  2153.9778  1192.6994     1.3522  100.9367  0.1225   \n",
       " 1017  3053.34  2353.230  2211.6000  2122.2580     1.4470   94.2089  0.1242   \n",
       " 1018  2972.60  2622.600  2169.8334  1301.9348     0.8012  101.9733  0.1216   \n",
       " 1019  3083.80  2494.570  2201.8666  1185.4396     1.3996   99.9211  0.1199   \n",
       " 1020  2994.92  2516.350  2146.7778  3715.0417     0.9069   84.9267  0.1286   \n",
       " 1021  3026.09  2553.900  2215.5111  2039.3196     1.2150   99.4189  0.1238   \n",
       " 1022  2994.06  2548.910  2247.3666  1039.7410     1.3594  104.3600  0.1171   \n",
       " 1023  2978.33  2405.530  2203.4445  1915.7558     1.2616  101.1833  0.1261   \n",
       " 1024  3044.47  2361.710  2192.5556  1908.5530     1.3766   96.1400  0.1258   \n",
       " 1025  2975.13  2557.370  2194.9555  1108.2246     1.2476  102.2822  0.1202   \n",
       " 1026  3014.07  2554.210  2226.0667  2073.6381     1.2717   95.7489  0.1238   \n",
       " 1027  3064.09  2488.150  2231.6111  2005.8966     1.2969   93.7522  0.1234   \n",
       " 1028  3070.16  2561.210  2247.3666  1039.7410     1.3594  104.3600  0.1171   \n",
       " 1030  3073.48  2467.180  2200.2000  1121.1875     1.3171  103.8978  0.1191   \n",
       " 1031  3027.61  2430.030  2219.7667  2086.4710     1.3381   98.8900  0.1234   \n",
       " 1032  2950.97  2533.950  2249.2556  2065.0624     2.1216   95.7967  0.1222   \n",
       " 1033  3093.12  2500.900  2219.7667  2086.4710     1.3381   98.8900  0.1234   \n",
       " 1034  2988.76  2497.580  2153.9778  1192.6994     1.3522  100.9367  0.1225   \n",
       " 1035  2990.69  2428.850  2249.2556  2065.0624     2.1216   95.7967  0.1222   \n",
       " 1036  2978.77  2441.170  2254.7111  1981.2999     2.1046   90.9167  0.1224   \n",
       " 1037  3036.99  2448.380  2194.9555  1108.2246     1.2476  102.2822  0.1202   \n",
       " 1038  3006.75  2517.090  2254.7111  1981.2999     2.1046   90.9167  0.1224   \n",
       " 1039  2960.63  2570.460  2206.1444  1876.9899     2.0607   95.9511  0.1246   \n",
       " 1040  3021.61  2440.290  2227.2222  2043.5876     1.1892   95.5267  0.1253   \n",
       " 1041  2960.56  2465.420  2213.9111  1973.9690     1.2541   96.9456  0.1238   \n",
       " 1042  2978.15  2506.000  2153.9778  1192.6994     1.3522  100.9367  0.1225   \n",
       " 1043  3053.48  2674.540  2231.6111  2005.8966     1.2969   93.7522  0.1234   \n",
       " 1044  3031.22  2538.560  2211.5222  1079.7336     0.8126  105.1278  0.1190   \n",
       " 1045  2935.34  2551.270  2276.4556  2148.5397     1.2317   93.6778  0.1199   \n",
       " 1046  2972.34  2465.620  2231.6111  2005.8966     1.2969   93.7522  0.1234   \n",
       " 1047  3051.68  2505.270  2219.7667  2086.4710     1.3381   98.8900  0.1234   \n",
       " 1048  3056.49  2488.900  2192.5556  1908.5530     1.3766   96.1400  0.1258   \n",
       " 1049  3034.13  2481.640  2219.7667  2086.4710     1.3381   98.8900  0.1234   \n",
       " 1050  3090.72  2408.280  2226.0667  2073.6381     1.2717   95.7489  0.1238   \n",
       " 1051  3012.45  2422.480  2208.5222  1838.7054     1.1571   95.2056  0.1249   \n",
       " 1052  2990.29  2522.350  2200.2000  1121.1875     1.3171  103.8978  0.1191   \n",
       " 1053  2997.67  2446.470  2147.1111  1081.8758     1.3550  103.2778  0.1226   \n",
       " 1054  3040.28  2577.440  2233.6334  1962.0026     1.2120   99.0133  0.1222   \n",
       " 1055  2995.73  2515.830  2231.6111  2005.8966     1.2969   93.7522  0.1234   \n",
       " 1056  2908.22  2481.320  2168.7333   999.5929     1.3739  104.8544  0.1217   \n",
       " 1057  3005.31  2409.490  2200.2000  1121.1875     1.3171  103.8978  0.1191   \n",
       " 1058  3005.52  2444.920  2227.2222  2043.5876     1.1892   95.5267  0.1253   \n",
       " 1059  3208.56  2398.660  2147.1111  1081.8758     1.3550  103.2778  0.1226   \n",
       " 1060  3063.64  2519.770  2231.6111  2005.8966     1.2969   93.7522  0.1234   \n",
       " 1061  3150.47  2621.330  2231.6111  2005.8966     1.2969   93.7522  0.1234   \n",
       " 1063  2940.74  2596.710  2200.2000  1121.1875     1.3171  103.8978  0.1191   \n",
       " 1064  2981.49  2536.820  2147.1111  1081.8758     1.3550  103.2778  0.1226   \n",
       " 1065  3026.53  2558.280  2147.1111  1081.8758     1.3550  103.2778  0.1226   \n",
       " 1066  3054.95  2531.780  2178.5889  1135.3544     0.8614  103.7022  0.1205   \n",
       " 1067  3060.97  2579.880  2203.4445  1915.7558     1.2616  101.1833  0.1261   \n",
       " 1068  2996.61  2496.090  2178.4667   993.1907     0.7649  104.6267  0.1215   \n",
       " 1069  2994.22  2541.270  2222.0111  1229.4964     0.7647  105.2711  0.1183   \n",
       " 1070  2945.66  2572.150  2185.1000  1201.0491     0.7821  105.8489  0.1208   \n",
       " 1071  3160.57  2569.690  2244.9778  2208.4483     1.9074   87.2789  0.1217   \n",
       " 1072  3180.09  2327.290  2200.2000  1121.1875     1.3171  103.8978  0.1191   \n",
       " 1073  3069.30  2501.130  2200.2000  1121.1875     1.3171  103.8978  0.1191   \n",
       " 1074  3136.15  2504.960  2178.4667   993.1907     0.7649  104.6267  0.1215   \n",
       " 1075  2988.21  2490.260  2219.7667  2086.4710     1.3381   98.8900  0.1234   \n",
       " 1076  3068.67  2517.040  2258.2778  2073.0764     1.2329   96.9478  0.1208   \n",
       " 1077  3225.61  2493.130  2266.8333  2040.1937     1.6558   94.8489  0.1216   \n",
       " 1078  2936.51  2483.290  2266.8333  2040.1937     1.6558   94.8489  0.1216   \n",
       " 1079  3170.24  2485.640  2186.9000   936.4327     1.3493  104.3367  0.1206   \n",
       " 1080  3039.40  2473.480  2178.4667   993.1907     0.7649  104.6267  0.1215   \n",
       " 1081  3078.20  2573.390  2187.6667   991.7358     1.2727  105.0156  0.1212   \n",
       " 1082  3010.30  2421.700  2249.2556  2065.0624     2.1216   95.7967  0.1222   \n",
       " 1083  3016.34  2538.440  2266.8333  2040.1937     1.6558   94.8489  0.1216   \n",
       " 1084  2978.96  2499.405  2250.3667  1847.0925     2.0218   91.0589  0.1214   \n",
       " 1085  2911.84  2507.040  2176.4667  1024.0477     1.2782  105.7178  0.1203   \n",
       " 1086  2888.58  2481.370  2200.2000  1121.1875     1.3171  103.8978  0.1191   \n",
       " 1087  3043.77  2588.520  2219.7667  2086.4710     1.3381   98.8900  0.1234   \n",
       " 1088  2888.90  2553.540  2187.6667   991.7358     1.2727  105.0156  0.1212   \n",
       " 1089  3059.68  2463.230  2227.9778  3619.7397     1.6656   87.2200  0.1239   \n",
       " 1090  3009.94  2524.620  2200.2000  1121.1875     1.3171  103.8978  0.1191   \n",
       " 1091  2966.24  2540.030  2169.9444  1095.3519     1.3425  104.0144  0.1208   \n",
       " 1092  3186.16  2316.010  2169.9444  1095.3519     1.3425  104.0144  0.1208   \n",
       " 1093  3162.14  2266.530  2169.9444  1095.3519     1.3425  104.0144  0.1208   \n",
       " 1094  3018.33  2438.580  2169.9444  1095.3519     1.3425  104.0144  0.1208   \n",
       " 1095  3104.27  2448.680  2169.9444  1095.3519     1.3425  104.0144  0.1208   \n",
       " 1096  3062.78  2491.310  2266.8333  2040.1937     1.6558   94.8489  0.1216   \n",
       " 1097  3070.78  2592.560  2222.0111  1229.4964     0.7647  105.2711  0.1183   \n",
       " 1098  3072.44  2458.090  2179.1111  1305.8262     0.8117  103.5033  0.1203   \n",
       " 1099  3160.54  2304.830  2233.6334  1962.0026     1.2120   99.0133  0.1222   \n",
       " 1100  2880.58  2489.760  2256.6000  2365.4787     1.1384   85.4278  0.1205   \n",
       " 1101  3099.37  2450.290  2178.5889  1135.3544     0.8614  103.7022  0.1205   \n",
       " 1102  2998.44  2509.540  2249.2556  2065.0624     2.1216   95.7967  0.1222   \n",
       " 1103  3065.12  2627.290  2249.2556  2065.0624     2.1216   95.7967  0.1222   \n",
       " 1104  3088.41  2443.660  2249.2556  2065.0624     2.1216   95.7967  0.1222   \n",
       " 1105  3018.57  2500.010  2137.6111   950.5720     0.9199  105.1889  0.1251   \n",
       " 1106  3034.01  2545.830  2256.6000  2365.4787     1.1384   85.4278  0.1205   \n",
       " 1107  2991.47  2493.200  2276.4556  2148.5397     1.2317   93.6778  0.1199   \n",
       " 1108  3052.93  2563.530  2260.7333  1969.7867     1.2109   91.3367  0.1207   \n",
       " 1109  2885.80  2607.010  2231.9555  1943.0435     1.2758   96.8789  0.1224   \n",
       " 1110  3151.80  2571.060  2244.9778  2208.4483     1.9074   87.2789  0.1217   \n",
       " 1111  3010.83  2501.260  2158.7000   902.9779     1.3177  106.3611  0.1228   \n",
       " 1112  3135.88  2578.760  2227.9778  3619.7397     1.6656   87.2200  0.1239   \n",
       " 1113  2916.93  2424.520  2231.9555  1943.0435     1.2758   96.8789  0.1224   \n",
       " 1114  2910.99  2533.390  2186.9000   936.4327     1.3493  104.3367  0.1206   \n",
       " 1115  3182.35  2520.400  2187.7888  1350.3395     0.7945  101.9267  0.1200   \n",
       " 1116  3046.14  2475.590  2231.9555  1943.0435     1.2758   96.8789  0.1224   \n",
       " 1117  2983.56  2595.740  2206.1444  1876.9899     2.0607   95.9511  0.1246   \n",
       " 1118  3052.22  2496.490  2219.7667  2086.4710     1.3381   98.8900  0.1234   \n",
       " 1119  3059.62  2376.270  2266.8333  2040.1937     1.6558   94.8489  0.1216   \n",
       " 1120  3002.82  2450.490  2269.2556  2023.4334     1.1215   91.6400  0.1194   \n",
       " 1121  3038.68  2512.100  2182.6222  1209.5092     0.8975  104.6689  0.1226   \n",
       " 1122  2935.99  2577.580  2276.4556  2148.5397     1.2317   93.6778  0.1199   \n",
       " 1123  3030.88  2398.400  2211.4334  2130.9862     1.1351   86.7956  0.1237   \n",
       " 1124  3033.32  2562.200  2221.9778  2434.5273     1.2596   86.1700  0.1219   \n",
       " 1125  3180.23  2517.210  2147.1111  1081.8758     1.3550  103.2778  0.1226   \n",
       " 1126  3038.74  2432.830  2222.0111  1229.4964     0.7647  105.2711  0.1183   \n",
       " 1127  3230.20  2476.100  2276.4556  2148.5397     1.2317   93.6778  0.1199   \n",
       " 1128  3041.09  2630.420  2193.3555  1087.7379     1.2169  104.4778  0.1195   \n",
       " 1129  3078.17  2450.310  2260.7333  1969.7867     1.2109   91.3367  0.1207   \n",
       " 1130  2995.26  2447.220  2185.1000  1201.0491     0.7821  105.8489  0.1208   \n",
       " 1131  2939.59  2567.480  2165.0333   926.8103     1.3455  102.1067  0.1239   \n",
       " 1132  2938.90  2532.400  2221.9778  2434.5273     1.2596   86.1700  0.1219   \n",
       " 1133  3217.96  2485.800  2137.6111   950.5720     0.9199  105.1889  0.1251   \n",
       " 1134  3056.65  2427.200  2266.8333  2040.1937     1.6558   94.8489  0.1216   \n",
       " 1135  3013.15  2573.370  2222.0111  1229.4964     0.7647  105.2711  0.1183   \n",
       " 1136  3074.58  2452.150  2221.9778  2434.5273     1.2596   86.1700  0.1219   \n",
       " 1137  2895.09  2552.960  2260.7333  1969.7867     1.2109   91.3367  0.1207   \n",
       " 1138  2939.80  2605.970  2231.6111  2005.8966     1.2969   93.7522  0.1234   \n",
       " 1139  3052.44  2628.680  2231.6111  2005.8966     1.2969   93.7522  0.1234   \n",
       " 1140  2972.00  2555.270  2201.0667  1285.2144     1.3168  101.5122  0.1224   \n",
       " 1141  2952.57  2552.650  2243.8556  2209.0871     2.1422   89.4944  0.1216   \n",
       " 1142  3031.01  2431.120  2168.7000   895.5849     0.8484  108.0122  0.1241   \n",
       " 1143  3037.20  2456.060  2178.5889  1135.3544     0.8614  103.7022  0.1205   \n",
       " 1145  3063.69  2382.180  2244.5000  1830.2771     1.2287   90.0089  0.1207   \n",
       " 1146  2996.48  2495.580  2158.7000   902.9779     1.3177  106.3611  0.1228   \n",
       " 1147  2983.51  2522.770  2244.5000  1830.2771     1.2287   90.0089  0.1207   \n",
       " 1148  3232.64  2512.140  2165.0333   926.8103     1.3455  102.1067  0.1239   \n",
       " 1149  3054.52  2499.040  2165.0333   926.8103     1.3455  102.1067  0.1239   \n",
       " 1150  3035.18  2438.110  2185.1000  1201.0491     0.7821  105.8489  0.1208   \n",
       " 1152  3111.43  2528.730  2260.7333  1969.7867     1.2109   91.3367  0.1207   \n",
       " 1153  3005.05  2510.260  2168.7000   895.5849     0.8484  108.0122  0.1241   \n",
       " 1154  3171.54  2435.850  2168.7000   895.5849     0.8484  108.0122  0.1241   \n",
       " 1155  2990.36  2563.280  2209.9555  2325.1727     2.1685   85.7689  0.1222   \n",
       " 1156  3023.37  2574.380  2179.1111  1305.8262     0.8117  103.5033  0.1203   \n",
       " 1157  3254.32  2397.740  2175.1222  1004.5029     1.2964  105.9689  0.1249   \n",
       " 1158  3026.07  2449.840  2170.7444  1128.0791     1.3753  104.2989  0.1246   \n",
       " 1159  2932.10  2365.170  2228.9445  1502.7821     1.2895  100.6867  0.1235   \n",
       " 1160  3035.89  2412.640  2243.8556  2209.0871     2.1422   89.4944  0.1216   \n",
       " 1161  3098.02  2401.500  2244.5000  1830.2771     1.2287   90.0089  0.1207   \n",
       " 1162  2982.89  2558.880  2182.0222   939.3851     1.3267  103.1978  0.1207   \n",
       " 1163  3094.57  2492.720  2188.5111   940.2228     0.8673  105.8678  0.1232   \n",
       " 1164  3023.95  2462.090  2175.1222  1004.5029     1.2964  105.9689  0.1249   \n",
       " 1165  3065.48  2539.610  2175.1222  1004.5029     1.2964  105.9689  0.1249   \n",
       " 1166  3020.29  2452.100  2241.1778  1521.1987     1.3349   98.5900  0.1217   \n",
       " 1167  3010.75  2523.210  2175.1222  1004.5029     1.2964  105.9689  0.1249   \n",
       " 1168  3070.94  2502.420  2170.7444  1128.0791     1.3753  104.2989  0.1246   \n",
       " 1169  3003.05  2508.940  2228.9445  1502.7821     1.2895  100.6867  0.1235   \n",
       " 1170  2829.84  2595.700  2168.7000   895.5849     0.8484  108.0122  0.1241   \n",
       " 1171  3169.93  2618.120  2183.0556  1257.1118     0.9762  102.9956  0.1231   \n",
       " 1172  3044.34  2585.920  2241.1778  1521.1987     1.3349   98.5900  0.1217   \n",
       " 1173  3025.07  2391.870  2182.6222  1209.5092     0.8975  104.6689  0.1226   \n",
       " 1174  3195.57  2464.450  2215.0222  1548.9419     1.2557   96.7900  0.1225   \n",
       " 1175  2939.73  2546.740  2175.2556  1022.1660     1.2833  100.6222  0.1250   \n",
       " 1176  3097.21  2418.410  2213.3444  1731.1527     2.1197   96.1589  0.1220   \n",
       " 1177  2960.58  2453.340  2175.2556  1022.1660     1.2833  100.6222  0.1250   \n",
       " 1178  2923.36  2503.520  2186.9000   936.4327     1.3493  104.3367  0.1206   \n",
       " 1179  2953.29  2496.320  2215.0222  1548.9419     1.2557   96.7900  0.1225   \n",
       " 1180  3093.46  2479.170  2221.3667  2490.6947     1.2667   88.6578  0.1220   \n",
       " 1181  2997.33  2472.600  2221.5667  1593.3485     1.9170  100.7456  0.1227   \n",
       " 1182  3093.16  2631.920  2260.2222  1602.2369     1.2100   97.1467  0.1196   \n",
       " 1183  2958.48  2611.650  2188.5111   940.2228     0.8673  105.8678  0.1232   \n",
       " 1184  3299.40  2490.630  2226.5444  1299.0694     1.9946   98.5244  0.1217   \n",
       " 1186  3049.97  2441.120  2263.3222  1383.8334     2.3986   85.1778  0.1196   \n",
       " 1187  2935.84  2549.210  2241.1778  1521.1987     1.3349   98.5900  0.1217   \n",
       " 1188  3045.88  2603.920  2243.8556  2209.0871     2.1422   89.4944  0.1216   \n",
       " 1190  3216.05  2257.950  2228.9445  1502.7821     1.2895  100.6867  0.1235   \n",
       " 1191  3105.06  2266.000  2241.1778  1521.1987     1.3349   98.5900  0.1217   \n",
       " 1192  2890.83  2474.900  2172.1111   999.5716     1.2689  103.3633  0.1250   \n",
       " 1193  2973.08  2514.520  2222.0111  1229.4964     0.7647  105.2711  0.1183   \n",
       " 1194  2952.23  2485.730  2260.2222  1602.2369     1.2100   97.1467  0.1196   \n",
       " 1195  3036.32  2449.590  2232.0778  1606.3548     2.1498   98.0044  0.1214   \n",
       " 1196  2942.64  2567.330  2231.9555  1943.0435     1.2758   96.8789  0.1224   \n",
       " 1197  3050.71  2512.440  2170.7444  1128.0791     1.3753  104.2989  0.1246   \n",
       " 1198  3075.32  2491.070  2185.1000  1201.0491     0.7821  105.8489  0.1208   \n",
       " 1199  3030.56  2530.990  2183.0556  1257.1118     0.9762  102.9956  0.1231   \n",
       " 1200  2972.06  2491.010  2218.7778  2080.6085     2.1254   91.0211  0.1229   \n",
       " 1201  3095.02  2501.260  2188.5111   940.2228     0.8673  105.8678  0.1232   \n",
       " 1202  2943.49  2462.850  2260.2222  1602.2369     1.2100   97.1467  0.1196   \n",
       " 1203  3002.15  2486.940  2218.7778  2080.6085     2.1254   91.0211  0.1229   \n",
       " 1204  3051.88  2501.910  2239.7222  1604.7488     1.1871   96.9100  0.1222   \n",
       " 1205  2979.97  2454.380  2227.9778  3619.7397     1.6656   87.2200  0.1239   \n",
       " 1206  3024.75  2587.370  2269.2556  2023.4334     1.1215   91.6400  0.1194   \n",
       " 1207  3025.09  2548.760  2209.9555  2325.1727     2.1685   85.7689  0.1222   \n",
       " 1208  3217.18  2572.850  2178.5889  1135.3544     0.8614  103.7022  0.1205   \n",
       " 1209  3093.05  2522.030  2170.7444  1128.0791     1.3753  104.2989  0.1246   \n",
       " 1210  3021.25  2473.650  2221.3667  2490.6947     1.2667   88.6578  0.1220   \n",
       " 1212  3070.43  2446.380  2255.5222  1763.0739     1.2226  101.5878  0.1200   \n",
       " 1213  3201.05  2233.290  2172.8111   969.3436     1.2736  102.7367  0.1243   \n",
       " 1214  2924.32  2629.960  2172.8111   969.3436     1.2736  102.7367  0.1243   \n",
       " 1215  3051.02  2565.730  2165.5333   994.3737     0.7992  102.9378  0.1247   \n",
       " 1216  3031.85  2439.420  2172.8111   969.3436     1.2736  102.7367  0.1243   \n",
       " 1217  3001.83  2545.770  2246.0666  1477.0365     1.8881  103.6100  0.1207   \n",
       " 1218  3000.22  2499.405  2172.8111   969.3436     1.2736  102.7367  0.1243   \n",
       " 1219  2952.88  2521.930  2243.8556  2209.0871     2.1422   89.4944  0.1216   \n",
       " 1220  2973.99  2574.340  2226.4667  1591.2235     2.1717   90.3111  0.1219   \n",
       " 1221  2965.67  2553.640  2172.8111   969.3436     1.2736  102.7367  0.1243   \n",
       " 1222  2947.50  2499.405  2173.4889  1145.7970     0.9402  104.0556  0.1247   \n",
       " 1223  3075.05  2572.480  2173.4889  1145.7970     0.9402  104.0556  0.1247   \n",
       " 1224  2975.80  2427.360  2191.8445  1093.0818     0.8725  101.4244  0.1229   \n",
       " 1225  3071.13  2487.120  2199.6556  1140.3983     1.3369  103.0967  0.1227   \n",
       " 1226  2957.77  2555.410  2199.6556  1140.3983     1.3369  103.0967  0.1227   \n",
       " 1228  2908.28  2502.090  2198.8556  1031.1917     1.3204  102.7622  0.1235   \n",
       " 1229  2979.32  2493.340  2226.4667  1591.2235     2.1717   90.3111  0.1219   \n",
       " 1230  3045.98  2544.850  2253.3444  1873.8678     1.1691   94.9722  0.1210   \n",
       " 1231  3151.98  2563.750  2175.2556  1022.1660     1.2833  100.6222  0.1250   \n",
       " 1232  3071.18  2489.860  2195.3000  1151.8233     0.9220  103.5467  0.1232   \n",
       " 1233  2958.46  2523.780  2171.8556  1156.6018     1.4025  100.1367  0.1243   \n",
       " 1234  3196.21  2413.390  2255.5222  1763.0739     1.2226  101.5878  0.1200   \n",
       " 1235  3005.64  2522.400  2173.4889  1145.7970     0.9402  104.0556  0.1247   \n",
       " 1236  3016.93  2547.200  2173.4889  1145.7970     0.9402  104.0556  0.1247   \n",
       " 1237  3013.73  2567.760  2172.8111   969.3436     1.2736  102.7367  0.1243   \n",
       " 1239  2937.77  2532.300  2198.8556  1031.1917     1.3204  102.7622  0.1235   \n",
       " 1240  2976.56  2506.210  2255.5222  1763.0739     1.2226  101.5878  0.1200   \n",
       " 1243  3028.50  2486.530  2236.1111  1546.5931     2.0300   90.4233  0.1224   \n",
       " 1244  3011.82  2446.490  2164.3666   958.7313     1.3409  104.1344  0.1248   \n",
       " 1245  3022.33  2357.090  2221.3667  2490.6947     1.2667   88.6578  0.1220   \n",
       " 1246  2929.54  2501.540  2173.2778  1116.2950     0.8525  103.8200  0.1237   \n",
       " 1247  3118.54  2519.920  2201.6333  1554.3362     2.1917   97.3978  0.1233   \n",
       " 1248  2916.18  2514.380  2167.0889  1253.2140     1.3679  101.6667  0.1243   \n",
       " 1249  2995.33  2405.190  2199.6556  1140.3983     1.3369  103.0967  0.1227   \n",
       " 1250  3185.69  2537.680  2173.2778  1116.2950     0.8525  103.8200  0.1237   \n",
       " 1251  2971.64  2525.840  2236.1111  1546.5931     2.0300   90.4233  0.1224   \n",
       " 1252  2958.99  2459.560  2191.8445  1093.0818     0.8725  101.4244  0.1229   \n",
       " 1253  3028.08  2525.560  2173.2778  1116.2950     0.8525  103.8200  0.1237   \n",
       " 1255  2970.86  2510.190  2236.1111  1546.5931     2.0300   90.4233  0.1224   \n",
       " 1256  2914.04  2487.100  2238.1444  1580.6951     1.0062   91.0489  0.1230   \n",
       " 1257  2889.78  2434.420  2211.4000  1511.7842     1.3004   97.4700  0.1237   \n",
       " 1258  3133.75  2673.380  2173.2778  1116.2950     0.8525  103.8200  0.1237   \n",
       " 1259  3190.78  2424.110  2191.2111  1437.5003     2.2073   97.6444  0.1235   \n",
       " 1260  2953.63  2384.040  2191.2111  1437.5003     2.2073   97.6444  0.1235   \n",
       " 1261  3182.87  2467.440  2162.1333   998.9095     0.8826  104.9722  0.1246   \n",
       " 1262  2982.67  2541.550  2173.4889  1145.7970     0.9402  104.0556  0.1247   \n",
       " 1263  2983.73  2459.870  2162.1333   998.9095     0.8826  104.9722  0.1246   \n",
       " 1264  3110.08  2507.570  2208.9778  1285.2144     2.4210   97.8400  0.1238   \n",
       " 1265  3028.40  2485.820  2173.4889  1145.7970     0.9402  104.0556  0.1247   \n",
       " 1266  2987.39  2548.790  2173.2778  1116.2950     0.8525  103.8200  0.1237   \n",
       " 1267  3225.54  2500.380  2211.4000  1511.7842     1.3004   97.4700  0.1237   \n",
       " 1268  3033.78  2412.910  2226.4667  1591.2235     2.1717   90.3111  0.1219   \n",
       " 1269  3036.15  2477.250  2208.9778  1285.2144     2.4210   97.8400  0.1238   \n",
       " 1270  3062.36  2396.640  2173.2778  1116.2950     0.8525  103.8200  0.1237   \n",
       " 1271  3024.55  2423.980  2255.5222  1763.0739     1.2226  101.5878  0.1200   \n",
       " 1272  2831.18  2579.490  2200.1222  1478.2288     2.2079   97.7778  0.1233   \n",
       " 1273  3043.08  2484.430  2209.8000  1175.5508     1.1464  112.1367  0.1227   \n",
       " 1274  3021.37  2523.940  2209.1000  1244.9641     1.3724  107.3156  0.1238   \n",
       " 1275  3040.17  2582.120  2209.8000  1175.5508     1.1464  112.1367  0.1227   \n",
       " 1276  2999.28  2517.110  2209.1000  1244.9641     1.3724  107.3156  0.1238   \n",
       " 1277  3092.74  2524.880  2209.8000  1175.5508     1.1464  112.1367  0.1227   \n",
       " 1278  3083.67  2465.320  2191.8445  1093.0818     0.8725  101.4244  0.1229   \n",
       " 1279  2983.17  2488.940  2208.9778  1285.2144     2.4210   97.8400  0.1238   \n",
       " 1280  3057.69  2464.860  2164.2667   711.0258     1.2884  129.2522  0.1252   \n",
       " 1281  2961.37  2497.330  2203.1667  1312.9527     0.9448   96.3311  0.1227   \n",
       " 1282  2927.71  2525.390  2199.6556  1140.3983     1.3369  103.0967  0.1227   \n",
       " 1283  2865.00  2538.760  2197.2001  1710.2780     2.2328   96.6233  0.1229   \n",
       " 1284  3050.96  2576.980  2206.7999  1415.2378     0.9778   97.3911  0.1220   \n",
       " 1285  2999.19  2429.470  2200.1222  1478.2288     2.2079   97.7778  0.1233   \n",
       " 1286  3040.48  2486.230  2191.6445  1215.8490     1.6211   96.4644  0.1239   \n",
       " 1287  3007.39  2504.010  2209.8000  1175.5508     1.1464  112.1367  0.1227   \n",
       " 1288  3018.78  2520.880  2202.4556  1199.9759     2.5874  110.9933  0.1229   \n",
       " 1289  2939.94  2485.470  2203.1667  1312.9527     0.9448   96.3311  0.1227   \n",
       " 1290  2971.13  2493.560  2202.4556  1199.9759     2.5874  110.9933  0.1229   \n",
       " 1291  2889.70  2583.050  2202.4556  1199.9759     2.5874  110.9933  0.1229   \n",
       " 1292  3202.90  2551.060  2203.1667  1312.9527     0.9448   96.3311  0.1227   \n",
       " 1293  3154.54  2450.890  2203.1667  1312.9527     0.9448   96.3311  0.1227   \n",
       " 1294  3020.70  2563.190  2164.2667   711.0258     1.2884  129.2522  0.1252   \n",
       " 1295  3089.26  2414.630  2209.8000  1175.5508     1.1464  112.1367  0.1227   \n",
       " 1296  2951.06  2503.180  2228.4778  1721.1108     1.4301   93.6222  0.1221   \n",
       " 1297  3006.75  2415.790  2206.7999  1415.2378     0.9778   97.3911  0.1220   \n",
       " 1298  3054.01  2537.390  2209.1000  1244.9641     1.3724  107.3156  0.1238   \n",
       " 1299  2982.27  2449.210  2164.2667   711.0258     1.2884  129.2522  0.1252   \n",
       " 1300  2851.68  2503.480  2162.1333   998.9095     0.8826  104.9722  0.1246   \n",
       " 1301  2870.13  2563.640  2209.1000  1244.9641     1.3724  107.3156  0.1238   \n",
       " 1304  3014.42  2440.560  2224.1889   997.7953  1112.1600   86.1644  0.1224   \n",
       " 1305  3011.77  2462.340  2211.4000  1511.7842     1.3004   97.4700  0.1237   \n",
       " 1306  2941.26  2532.780  2209.1000  1244.9641     1.3724  107.3156  0.1238   \n",
       " 1307  3045.07  2414.430  2203.1667  1312.9527     0.9448   96.3311  0.1227   \n",
       " 1308  3282.87  2419.550  2189.6778  1295.2883     2.1394  107.4811  0.1238   \n",
       " 1309  3014.85  2453.450  2189.6778  1295.2883     2.1394  107.4811  0.1238   \n",
       " 1310  2918.56  2568.550  2189.6778  1295.2883     2.1394  107.4811  0.1238   \n",
       " 1311  2908.11  2535.930  2208.9000   934.7558     1.9469  119.3544  0.1222   \n",
       " 1312  2986.79  2527.320  2189.6778  1295.2883     2.1394  107.4811  0.1238   \n",
       " 1313  3339.93  2839.460  2189.6778  1295.2883     2.1394  107.4811  0.1238   \n",
       " 1314  3038.53  2474.550  2205.2555  1412.7131     0.9785   95.2556  0.1216   \n",
       " 1315  2942.21  2459.020  2210.8556  1665.0062     2.4475   97.1056  0.1227   \n",
       " 1316  2974.65  2545.400  2210.8556  1665.0062     2.4475   97.1056  0.1227   \n",
       " 1317  2992.34  2512.630  2236.1111  1546.5931     2.0300   90.4233  0.1224   \n",
       " 1318  3212.70  2472.920  2210.8556  1665.0062     2.4475   97.1056  0.1227   \n",
       " 1319  2912.24  2438.170  2203.1667  1312.9527     0.9448   96.3311  0.1227   \n",
       " 1320  2990.66  2535.430  2213.2889  1346.8352  1112.0769   85.4500  0.1224   \n",
       " 1321  2907.65  2516.340  2208.9000   934.7558     1.9469  119.3544  0.1222   \n",
       " 1322  2965.41  2577.390  2210.8556  1665.0062     2.4475   97.1056  0.1227   \n",
       " 1323  3023.56  2555.880  2205.2555  1412.7131     0.9785   95.2556  0.1216   \n",
       " 1326  3033.52  2461.080  2203.1667  1312.9527     0.9448   96.3311  0.1227   \n",
       " 1330  2996.01  2541.130  2205.2555  1412.7131     0.9785   95.2556  0.1216   \n",
       " 1331  3001.56  2506.620  2223.0444  1194.5986     1.2016  112.5811  0.1229   \n",
       " 1332  3045.48  2408.850  2223.0444  1194.5986     1.2016  112.5811  0.1229   \n",
       " 1333  2971.75  2566.300  2206.4000   982.5452     1.1853  116.8167  0.1228   \n",
       " 1334  2983.98  2744.540  2212.3000  1807.5841     0.9816   92.3700  0.1212   \n",
       " 1335  3039.64  2529.790  2223.0444  1194.5986     1.2016  112.5811  0.1229   \n",
       " 1336  2908.47  2551.780  2223.0444  1194.5986     1.2016  112.5811  0.1229   \n",
       " 1337  2915.54  2496.520  2210.8556  1665.0062     2.4475   97.1056  0.1227   \n",
       " 1338  2997.45  2555.010  2206.4000   982.5452     1.1853  116.8167  0.1228   \n",
       " 1339  2990.85  2472.740  2223.0444  1194.5986     1.2016  112.5811  0.1229   \n",
       " 1340  3029.98  2532.340  2206.4000   982.5452     1.1853  116.8167  0.1228   \n",
       " 1341  3033.51  2556.350  2207.9555  1283.4368     1.8467   95.4022  0.1216   \n",
       " 1344  2964.69  2474.970  2212.3000  1807.5841     0.9816   92.3700  0.1212   \n",
       " 1345  2909.43  2534.490  2223.0444  1194.5986     1.2016  112.5811  0.1229   \n",
       " 1346  3014.42  2442.510  2212.3000  1807.5841     0.9816   92.3700  0.1212   \n",
       " 1347  2997.84  2449.850  2223.0444  1194.5986     1.2016  112.5811  0.1229   \n",
       " 1348  2873.83  2540.170  2223.0444  1194.5986     1.2016  112.5811  0.1229   \n",
       " 1349  3001.74  2446.550  2207.9555  1283.4368     1.8467   95.4022  0.1216   \n",
       " 1350  3057.49  2584.520  2207.9555  1283.4368     1.8467   95.4022  0.1216   \n",
       " 1351  2910.77  2529.900  2212.3000  1807.5841     0.9816   92.3700  0.1212   \n",
       " 1352  2985.31  2584.240  2209.6667  1586.6088     1.6782   89.7222  0.1213   \n",
       " 1353  3020.62  2483.200  2206.4000   982.5452     1.1853  116.8167  0.1228   \n",
       " 1354  3019.44  2494.330  2231.0555  1303.5386     0.9751   95.7878  0.1198   \n",
       " 1355  3185.63  2386.260  2209.6667  1586.6088     1.6782   89.7222  0.1213   \n",
       " 1356  3029.94  2517.440  2206.4000   982.5452     1.1853  116.8167  0.1228   \n",
       " 1357  3117.30  2527.680  2212.3000  1807.5841     0.9816   92.3700  0.1212   \n",
       " 1358  3084.17  2451.850  2231.0555  1303.5386     0.9751   95.7878  0.1198   \n",
       " 1359  3005.90  2534.680  2231.0555  1303.5386     0.9751   95.7878  0.1198   \n",
       " 1360  3024.48  2538.130  2207.9555  1283.4368     1.8467   95.4022  0.1216   \n",
       " 1361  2981.72  2580.440  2207.9555  1283.4368     1.8467   95.4022  0.1216   \n",
       " 1362  3001.90  2465.510  2223.0444  1194.5986     1.2016  112.5811  0.1229   \n",
       " 1366  3013.32  2538.520  2231.0555  1303.5386     0.9751   95.7878  0.1198   \n",
       " 1367  2886.03  2499.460  2231.0555  1303.5386     0.9751   95.7878  0.1198   \n",
       " 1368  2916.12  2806.910  2231.0555  1303.5386     0.9751   95.7878  0.1198   \n",
       " 1369  3103.67  2424.950  2209.4667  1556.3930     1.4884   95.1156  0.1206   \n",
       " 1370  3222.84  2508.030  2207.9555  1283.4368     1.8467   95.4022  0.1216   \n",
       " 1371  2979.93  2525.740  2206.9667  1314.6277     1.5512   97.1700  0.1216   \n",
       " 1372  2881.10  2493.030  2209.6667  1586.6088     1.6782   89.7222  0.1213   \n",
       " 1373  2945.78  2457.480  2209.4667  1556.3930     1.4884   95.1156  0.1206   \n",
       " 1374  2996.28  2581.640  2212.3000  1807.5841     0.9816   92.3700  0.1212   \n",
       " 1375  3062.34  2594.520  2209.4667  1556.3930     1.4884   95.1156  0.1206   \n",
       " 1376  3028.38  2494.430  2209.6667  1586.6088     1.6782   89.7222  0.1213   \n",
       " 1377  3017.35  2580.250  2223.0444  1194.5986     1.2016  112.5811  0.1229   \n",
       " 1378  3013.79  2480.710  2206.4000   982.5452     1.1853  116.8167  0.1228   \n",
       " 1379  3043.32  2494.820  2206.4000   982.5452     1.1853  116.8167  0.1228   \n",
       " 1380  3093.31  2456.820  2207.9555  1283.4368     1.8467   95.4022  0.1216   \n",
       " 1381  2862.69  2593.670  2206.9667  1314.6277     1.5512   97.1700  0.1216   \n",
       " 1382  3097.89  2583.010  2218.7333  1190.9639     1.7047   97.4300  0.1202   \n",
       " 1383  2875.39  2499.150  2216.8333  1468.5974     1.7074   95.9856  0.1203   \n",
       " 1384  3060.16  2465.710  2207.1444  1109.3369     1.5555   97.5678  0.1216   \n",
       " 1385  2975.87  2514.440  2206.4000   982.5452     1.1853  116.8167  0.1228   \n",
       " 1386  2955.82  2541.600  2231.0555  1303.5386     0.9751   95.7878  0.1198   \n",
       " 1387  2994.64  2484.710  2200.2111  1048.3108     1.6485   98.3222  0.1214   \n",
       " 1388  3053.36  2538.380  2192.1889  1435.9611     2.3870  107.3989  0.1229   \n",
       " 1389  2871.59  2556.950  2217.0000  1436.8313     1.6604   93.6611  0.1199   \n",
       " 1390  3001.71  2504.200  2210.7333  1368.7060     2.2449   99.4200  0.1228   \n",
       " 1391  3042.43  2442.600  2217.0000  1436.8313     1.6604   93.6611  0.1199   \n",
       " 1392  3001.81  2424.390  2217.0000  1436.8313     1.6604   93.6611  0.1199   \n",
       " 1393  2915.71  2560.380  2196.1111  1472.6400     1.5599   94.6522  0.1212   \n",
       " 1394  3056.37  2477.750  2200.2111  1048.3108     1.6485   98.3222  0.1214   \n",
       " 1395  3015.36  2427.140  2217.5778  1283.3701     2.5361  104.9022  0.1222   \n",
       " 1396  2981.04  2475.900  2215.8111  1389.3065     2.3183   98.4500  0.1214   \n",
       " 1397  2984.41  2550.880  2192.1889  1435.9611     2.3870  107.3989  0.1229   \n",
       " 1398  2852.18  2573.940  2216.8333  1468.5974     1.7074   95.9856  0.1203   \n",
       " 1399  2864.77  2521.490  2192.1889  1435.9611     2.3870  107.3989  0.1229   \n",
       " 1401  2870.76  2583.560  2192.1889  1435.9611     2.3870  107.3989  0.1229   \n",
       " 1402  3014.77  2493.110  2210.7333  1368.7060     2.2449   99.4200  0.1228   \n",
       " 1403  3027.72  2430.700  2192.1889  1435.9611     2.3870  107.3989  0.1229   \n",
       " 1404  2893.19  2498.090  2200.2111  1048.3108     1.6485   98.3222  0.1214   \n",
       " 1405  3006.50  2516.640  2217.5778  1283.3701     2.5361  104.9022  0.1222   \n",
       " 1406  3094.77  2454.330  2215.8111  1389.3065     2.3183   98.4500  0.1214   \n",
       " 1407  2985.78  2460.970  2200.2111  1048.3108     1.6485   98.3222  0.1214   \n",
       " 1408  3171.42  2634.430  2217.0000  1436.8313     1.6604   93.6611  0.1199   \n",
       " 1409  3068.47  2489.070  2200.2111  1048.3108     1.6485   98.3222  0.1214   \n",
       " 1410  3012.98  2403.890  2215.8111  1389.3065     2.3183   98.4500  0.1214   \n",
       " 1411  3060.55  2420.030  2217.0000  1436.8313     1.6604   93.6611  0.1199   \n",
       " 1412  3025.46  2516.060  2195.9778  1388.2869     1.5605  103.2400  0.1234   \n",
       " 1413  2973.39  2528.670  2195.9778  1388.2869     1.5605  103.2400  0.1234   \n",
       " 1414  3014.16  2384.100  2211.0667  1107.1227     1.5788   97.0089  0.1213   \n",
       " 1415  3037.71  2441.770  2150.0556  1215.2183     1.4756  100.9744  0.1244   \n",
       " 1416  2999.11  2419.250  2217.0000  1436.8313     1.6604   93.6611  0.1199   \n",
       " 1417  2991.63  2599.930  2150.0556  1215.2183     1.4756  100.9744  0.1244   \n",
       " 1418  2958.30  2574.700  2200.2111  1048.3108     1.6485   98.3222  0.1214   \n",
       " 1419  2998.88  2493.210  2195.9778  1388.2869     1.5605  103.2400  0.1234   \n",
       " 1420  2975.74  2517.350  2162.5556  1041.0369     1.4305  100.4111  0.1238   \n",
       " 1421  2939.57  2564.930  2205.5000  1287.3538     2.3842  111.7644  0.1241   \n",
       " 1422  3095.53  2434.790  2211.0667  1107.1227     1.5788   97.0089  0.1213   \n",
       " 1423  3073.26  2470.500  2150.0556  1215.2183     1.4756  100.9744  0.1244   \n",
       " 1424  3077.47  2387.730  2180.8889  1084.7221     0.9085   94.2467  0.1226   \n",
       " 1425  3169.00  2265.600  2187.9889  1096.3790     0.9065   97.6567  0.1221   \n",
       " 1426  2923.19  2516.400  2180.8889  1084.7221     0.9085   94.2467  0.1226   \n",
       " 1427  3003.43  2448.340  2205.5000  1287.3538     2.3842  111.7644  0.1241   \n",
       " 1428  2953.63  2500.050  2195.9778  1388.2869     1.5605  103.2400  0.1234   \n",
       " 1429  3034.74  2458.900  2192.1889  1435.9611     2.3870  107.3989  0.1229   \n",
       " 1430  3038.57  2380.390  2205.5000  1287.3538     2.3842  111.7644  0.1241   \n",
       " 1431  2902.88  2259.490  2205.5000  1287.3538     2.3842  111.7644  0.1241   \n",
       " 1432  3001.45  2481.720  2162.5556  1041.0369     1.4305  100.4111  0.1238   \n",
       " 1433  2975.72  2585.720  2150.0556  1215.2183     1.4756  100.9744  0.1244   \n",
       " 1434  3356.35  2815.310  2217.0000  1436.8313     1.6604   93.6611  0.1199   \n",
       " 1435  3053.48  2524.950  2215.8111  1389.3065     2.3183   98.4500  0.1214   \n",
       " 1436  2816.81  2557.200  2195.9778  1388.2869     1.5605  103.2400  0.1234   \n",
       " 1437  3100.96  2507.020  2205.5000  1287.3538     2.3842  111.7644  0.1241   \n",
       " 1439  3004.07  2458.880  2180.8889  1084.7221     0.9085   94.2467  0.1226   \n",
       " 1440  2974.97  2479.730  2195.9778  1388.2869     1.5605  103.2400  0.1234   \n",
       " 1441  2987.80  2489.530  2216.8333  1468.5974     1.7074   95.9856  0.1203   \n",
       " 1442  2955.33  2521.870  2187.9889  1096.3790     0.9065   97.6567  0.1221   \n",
       " 1444  2990.57  2534.660  2162.5556  1041.0369     1.4305  100.4111  0.1238   \n",
       " 1445  3219.75  2373.310  2239.4111  1080.4985     1.2079  115.6522  0.1203   \n",
       " 1446  2962.45  2496.670  2281.5667  1161.3004  1114.5366  108.3144  0.0000   \n",
       " 1447  2949.12  2553.240  2176.8000  1461.4374     0.8864   96.2367  0.1232   \n",
       " 1448  3047.38  2597.000  2201.0667  1285.2144     1.3168  101.5122  0.0000   \n",
       " 1449  3214.14  2445.560  2239.4111  1080.4985     1.2079  115.6522  0.1203   \n",
       " 1450  3043.18  2574.430  2201.0667  1285.2144     1.3168  101.5122  0.1207   \n",
       " 1451  2976.92  2510.090  2187.9889  1096.3790     0.9065   97.6567  0.1221   \n",
       " 1452  3059.44  2423.750  2239.4111  1080.4985     1.2079  115.6522  0.1203   \n",
       " 1453  2995.84  2504.480  2167.6888  1096.0264     1.5066   99.5989  0.1234   \n",
       " 1454  2913.84  2393.970  2218.8111  1021.2003     1.8970  115.3800  0.1230   \n",
       " 1455  3022.41  2507.660  2239.4111  1080.4985     1.2079  115.6522  0.1203   \n",
       " 1456  3026.86  2497.620  2306.1000     0.0000  1112.4728   88.4722  0.0000   \n",
       " 1457  3048.58  2531.020  2218.8111  1021.2003     1.8970  115.3800  0.1230   \n",
       " 1458  2928.89  2504.810  2236.1111  1546.5931     2.0300   90.4233  0.1224   \n",
       " 1459  3006.80  2536.870  2201.0667  1285.2144     1.3168  101.5122  0.0000   \n",
       " 1460  3051.80  2527.700  2201.0667  1285.2144     1.3168  101.5122  0.0000   \n",
       " 1461  2900.29  2451.500  2201.0667  1285.2144     1.3168  101.5122  0.0000   \n",
       " 1462  3051.10  2509.140  2195.1000  1526.4440     0.8279   96.3100  0.1203   \n",
       " 1463  2982.03  2507.740  2195.1000  1526.4440     0.8279   96.3100  0.1203   \n",
       " 1464  3088.81  2590.620  2207.1000  1298.4309     1.0251   96.9478  0.1203   \n",
       " 1465  2969.65  2449.490  2184.4222  1017.9147     1.3713   99.4367  0.1213   \n",
       " 1466  2963.26  2530.970  2199.3000  1106.9703     0.8938  113.3544  0.1249   \n",
       " 1467  2936.59  2444.690  2201.0667  1285.2144     1.3168  101.5122  0.0000   \n",
       " 1468  2968.78  2451.530  2207.1000  1298.4309     1.0251   96.9478  0.1203   \n",
       " 1469  2898.17  2547.650  2201.0667  1285.2144     1.3168  101.5122  0.0000   \n",
       " 1470  3045.87  2493.720  2168.5111  1171.6442     0.8752  101.6567  0.1235   \n",
       " 1471  2926.38  2383.760  2207.1000  1298.4309     1.0251   96.9478  0.1203   \n",
       " 1472  3033.46  2424.390  2204.6667  1078.7860     0.9152  103.9611  0.1196   \n",
       " 1473  3064.53  2349.850  2182.5555  1261.0898     1.2110  112.2922  0.1252   \n",
       " 1474  2959.51  2538.410  2195.1000  1526.4440     0.8279   96.3100  0.1203   \n",
       " 1475  3068.72  2472.410  2195.1000  1526.4440     0.8279   96.3100  0.1203   \n",
       " 1476  3000.24  2477.370  2207.1000  1298.4309     1.0251   96.9478  0.1203   \n",
       " 1477  3284.82  2417.920  2201.2889  1015.4370     2.1366  119.9011  0.1251   \n",
       " 1478  2938.51  2515.150  2207.1000  1298.4309     1.0251   96.9478  0.1203   \n",
       " 1479  3126.61  2426.760  2204.2889  2637.9989     1.5549   86.1089  0.1234   \n",
       " 1480  3016.68  2482.880  2204.6667  1078.7860     0.9152  103.9611  0.1196   \n",
       " 1481  3025.04  2590.880  2182.4111  3355.2007     1.4529   82.1311  0.1248   \n",
       " 1482  2983.64  2542.560  2182.4111  3355.2007     1.4529   82.1311  0.1248   \n",
       " 1483  3036.24  2548.460  2204.6667  1078.7860     0.9152  103.9611  0.1196   \n",
       " 1484  2990.46  2507.010  2204.6667  1078.7860     0.9152  103.9611  0.1196   \n",
       " 1485  2911.22  2556.640  2204.2889  2637.9989     1.5549   86.1089  0.1234   \n",
       " 1486  2984.93  2432.380  2177.4333  2945.8855     1.3321   83.1700  0.1253   \n",
       " 1487  2987.19  2512.750  2191.5000  1147.9162     0.9431   97.7200  0.1212   \n",
       " 1488  2970.42  2494.390  2191.5000  1147.9162     0.9431   97.7200  0.1212   \n",
       " 1489  3190.97  2480.560  2181.1889  1338.8895     2.1195  108.1400  0.1263   \n",
       " 1490  2977.92  2538.370  2192.7556   867.3027     1.7393  123.4244  0.1251   \n",
       " 1491  3042.12  2444.670  2195.4444  2914.1792     1.5978   85.1011  0.1235   \n",
       " 1492  3070.74  2499.350  2181.1889  1338.8895     2.1195  108.1400  0.1263   \n",
       " 1493  3002.54  2549.850  2182.5555  1261.0898     1.2110  112.2922  0.1252   \n",
       " 1494  3068.64  2498.020  2192.7556   867.3027     1.7393  123.4244  0.1251   \n",
       " 1495  2983.24  2459.970  2187.9889  1096.3790     0.9065   97.6567  0.1221   \n",
       " 1496  3073.57  2528.590  2217.4111  1032.2836     1.4802  101.3511  0.1195   \n",
       " 1497  2962.43  2543.100  2201.2889  1015.4370     2.1366  119.9011  0.1251   \n",
       " 1498  2918.48  2505.290  2204.6667  1078.7860     0.9152  103.9611  0.1196   \n",
       " 1499  2924.54  2451.310  2190.7666  3530.2362     0.8017   83.8767  0.1249   \n",
       " 1500  3107.42  2404.670  2177.4333  2945.8855     1.3321   83.1700  0.1253   \n",
       " 1501  3033.22  2546.240  2187.3444  2882.8558     1.5876   85.4189  0.1235   \n",
       " 1502  2989.59  2506.000  2204.2889  2637.9989     1.5549   86.1089  0.1234   \n",
       " 1503  2921.78  2483.770  2177.4333  2945.8855     1.3321   83.1700  0.1253   \n",
       " 1504  2905.07  2493.470  2192.7556   867.3027     1.7393  123.4244  0.1251   \n",
       " 1505  3026.67  2529.820  2192.7889  1268.5852     1.9935  104.5867  0.1268   \n",
       " 1506  2883.88  2515.460  2177.4333  2945.8855     1.3321   83.1700  0.1253   \n",
       " 1507  2996.65  2512.020  2191.3556  1111.4764     1.4549   97.1556  0.1210   \n",
       " 1508  2992.07  2354.500  2191.3556  1111.4764     1.4549   97.1556  0.1210   \n",
       " 1509  2931.11  2485.990  2208.5889  1116.3316     0.8396  107.6300  0.1207   \n",
       " 1510  2966.20  2588.360  2192.7889  1268.5852     1.9935  104.5867  0.1268   \n",
       " 1511  2984.99  2501.050  2190.7666  3530.2362     0.8017   83.8767  0.1249   \n",
       " 1512  3031.93  2580.670  2205.5000  1287.3538     2.3842  111.7644  0.1241   \n",
       " 1513  3067.02  2608.210  2190.7666  3530.2362     0.8017   83.8767  0.1249   \n",
       " 1514  2949.90  2447.260  2264.7000  1211.1291     0.8785   99.6356  0.1173   \n",
       " 1515  3012.34  2416.090  2192.7889  1268.5852     1.9935  104.5867  0.1268   \n",
       " 1516  3018.70  2430.370  2192.7889  1268.5852     1.9935  104.5867  0.1268   \n",
       " 1517  2935.39  2575.410  2179.7333  3085.3781     1.4843   82.2467  0.1248   \n",
       " 1518  3273.46  2276.930  2223.5333  1352.1869     0.8714   97.2189  0.1196   \n",
       " 1520  2998.06  2465.430  2223.5333  1352.1869     0.8714   97.2189  0.1196   \n",
       " 1521  2943.76  2481.990  2190.7666  3530.2362     0.8017   83.8767  0.1249   \n",
       " 1522  3015.71  2531.500  2204.6667  1078.7860     0.9152  103.9611  0.1196   \n",
       " 1523  2930.66  2538.330  2187.3444  2882.8558     1.5876   85.4189  0.1235   \n",
       " 1524  3142.21  2389.910  2223.5333  1352.1869     0.8714   97.2189  0.1196   \n",
       " 1525  2977.74  2611.500  2192.7889  1268.5852     1.9935  104.5867  0.1268   \n",
       " 1526  3042.45  2530.960  2191.3556  1111.4764     1.4549   97.1556  0.1210   \n",
       " 1527  3051.20  2401.590  2190.7666  3530.2362     0.8017   83.8767  0.1249   \n",
       " 1528  3028.34  2499.720  2179.7333  3085.3781     1.4843   82.2467  0.1248   \n",
       " 1529  3068.78  2452.520  2217.4111  1032.2836     1.4802  101.3511  0.1195   \n",
       " 1530  3072.29  2354.240  2177.0333  1183.7287     1.5726   98.7978  0.1213   \n",
       " 1531  2959.86  2437.760  2192.7889  1268.5852     1.9935  104.5867  0.1268   \n",
       " 1532  3169.64  2563.950  2167.4222  2837.8788     1.4892   83.8222  0.1255   \n",
       " 1533  3011.32  2417.430  2184.9889  1124.2096     2.1987  114.5856  0.1265   \n",
       " 1534  2969.29  2558.560  2155.8556   812.1294     1.0047  123.4100  0.1263   \n",
       " 1535  3028.64  2532.710  2192.7889  1268.5852     1.9935  104.5867  0.1268   \n",
       " 1536  3059.59  2465.540  2191.3556  1111.4764     1.4549   97.1556  0.1210   \n",
       " 1537  3006.22  2525.200  2192.7889  1268.5852     1.9935  104.5867  0.1268   \n",
       " 1538  3128.11  2367.160  2223.5333  1352.1869     0.8714   97.2189  0.1196   \n",
       " 1539  2908.94  2560.990  2187.3444  2882.8558     1.5876   85.4189  0.1235   \n",
       " 1540  2996.04  2555.920  2190.7666  3530.2362     0.8017   83.8767  0.1249   \n",
       " 1541  3246.31  2499.790  2216.8111  1190.4067     2.5148  114.5533  0.1230   \n",
       " 1542  2965.57  2487.910  2210.3556   910.7177     1.6941  119.8822  0.1268   \n",
       " 1543  3109.18  2447.970  2210.3556   910.7177     1.6941  119.8822  0.1268   \n",
       " 1544  3108.98  2537.730  2210.3556   910.7177     1.6941  119.8822  0.1268   \n",
       " 1545  3100.19  2490.600  2212.8445  1068.5644     1.7835  113.8833  0.1249   \n",
       " 1546  3093.24  2488.180  2212.8445  1068.5644     1.7835  113.8833  0.1249   \n",
       " 1547  3008.77  2542.360  2167.4222  2837.8788     1.4892   83.8222  0.1255   \n",
       " 1548  3027.01  2464.980  2212.6334  1081.5662     1.0096  113.4278  0.1253   \n",
       " 1549  3183.63  2498.000  2195.4444  2914.1792     1.5978   85.1011  0.1235   \n",
       " 1550  3072.20  2406.470  2195.4444  2914.1792     1.5978   85.1011  0.1235   \n",
       " 1551  2958.43  2489.060  2192.7556   867.3027     1.7393  123.4244  0.1251   \n",
       " 1552  2939.35  2521.980  2195.1000  1526.4440     0.8279   96.3100  0.1203   \n",
       " 1553  3020.79  2500.190  2210.3556   910.7177     1.6941  119.8822  0.1268   \n",
       " 1554  3031.78  2528.550  2182.5555  1261.0898     1.2110  112.2922  0.1252   \n",
       " 1555  2902.96  2515.030  2181.1889  1338.8895     2.1195  108.1400  0.1263   \n",
       " 1556  3025.21  2503.300  2179.7333  3085.3781     1.4843   82.2467  0.1248   \n",
       " 1557  3072.10  2534.870  2177.4333  2945.8855     1.3321   83.1700  0.1253   \n",
       " 1558  3012.30  2466.840  2217.4111  1032.2836     1.4802  101.3511  0.1195   \n",
       " 1559  3076.33  2456.130  2217.4111  1032.2836     1.4802  101.3511  0.1195   \n",
       " 1560  2770.40  2549.420  2204.2889  2637.9989     1.5549   86.1089  0.1234   \n",
       " 1561  2951.14  2326.590  2212.6334  1081.5662     1.0096  113.4278  0.1253   \n",
       " 1562  2899.41  2464.360  2179.7333  3085.3781     1.4843   82.2467  0.1248   \n",
       " 1563  3052.31  2522.550  2198.5667  1124.6595     0.8763   98.4689  0.1205   \n",
       " 1564  2978.81  2379.780  2206.3000  1110.4967     0.8236   99.4122  0.1208   \n",
       " 1565  2894.92  2532.010  2177.0333  1183.7287     1.5726   98.7978  0.1213   \n",
       " 1566  2944.92  2450.760  2195.4444  2914.1792     1.5978   85.1011  0.1235   \n",
       " \n",
       "            8       9      10  ...      565       570     571       572  \\\n",
       " 0     1.5005  0.0162 -0.0034  ...  0.11955  533.8500  2.1113    8.9500   \n",
       " 1     1.4966 -0.0005 -0.0148  ...  0.11955  535.0164  2.4335    5.9200   \n",
       " 3     1.4882 -0.0124 -0.0033  ...  0.16300  530.5682  2.0253    9.3300   \n",
       " 4     1.5031 -0.0031 -0.0072  ...  0.11955  532.0155  2.0275    8.8300   \n",
       " 5     1.5287  0.0167  0.0055  ...  0.19050  534.2091  2.3236    8.9100   \n",
       " 6     1.5816 -0.0270  0.0105  ...  0.11955  541.9036  2.4229    6.4800   \n",
       " 7     1.5153  0.0157  0.0007  ...  0.13340  493.0054  2.2008  278.1900   \n",
       " 8     1.5358  0.0111 -0.0066  ...  0.11450  535.1818  2.2170    7.0900   \n",
       " 9     1.5381  0.0159  0.0049  ...  0.03610  533.4200  2.2598    3.5400   \n",
       " 12    1.5310 -0.0259  0.0216  ...  0.11955  530.1800  1.9690    9.9500   \n",
       " 13    1.5236 -0.0209 -0.0031  ...  0.19380  533.2464  2.2354    9.9300   \n",
       " 15    1.5465  0.0250 -0.0024  ...  0.11955  536.1118  2.3120    8.5300   \n",
       " 16    1.4368  0.0150 -0.0037  ...  0.11650  537.8145  2.1113    8.4300   \n",
       " 17    1.5537  0.0220 -0.0027  ...  0.10890  531.8418  2.1849   10.2600   \n",
       " 18    1.5481 -0.0367  0.0014  ...  0.09100  532.2673  2.2820    6.0000   \n",
       " 19    1.5362 -0.0259 -0.0179  ...  0.11955  534.6455  2.0642    7.8100   \n",
       " 20    1.6343 -0.0263  0.0116  ...  0.06060  531.2491  2.2134    6.8600   \n",
       " 21    1.5559  0.0002 -0.0044  ...  0.13760  530.1291  2.0645    8.1000   \n",
       " 22    1.5465  0.0195 -0.0114  ...  0.10800  528.1864  2.3338    7.4900   \n",
       " 24    1.4227  0.0194  0.0073  ...  0.11955  535.7491  2.2233    7.1700   \n",
       " 25    1.5136  0.0018  0.0058  ...  0.18320  532.6318  2.2530    9.2500   \n",
       " 26    1.4860 -0.0019 -0.0056  ...  0.22330  530.4545  2.3075    5.4000   \n",
       " 27    1.5582 -0.0101  0.0204  ...  0.20080  528.4100  2.3265    9.3700   \n",
       " 28    1.5438  0.0065  0.0032  ...  0.09720  521.2055  2.2750    7.4100   \n",
       " 29    1.4200 -0.0016  0.0138  ...  0.09370  534.3927  2.2461    8.7800   \n",
       " 30    1.4481 -0.0134  0.0177  ...  0.11955  530.4073  2.1823    7.7300   \n",
       " 31    1.5602  0.0041 -0.0056  ...  0.13930  533.2154  2.2671    7.9500   \n",
       " 32    1.5837 -0.0266 -0.0029  ...  0.17010  536.5709  2.1983   10.8600   \n",
       " 33    1.5047 -0.0216  0.0064  ...  0.23650  529.3927  2.1345   11.7900   \n",
       " 34    1.5374 -0.0177  0.0170  ...  0.10710  529.5609  2.2805    8.0900   \n",
       " 35    1.3475 -0.0152  0.0115  ...  0.25150  531.5427  2.2580    5.2000   \n",
       " 36    1.5316 -0.0194  0.0088  ...  0.11955  532.7664  2.4052   10.9000   \n",
       " 37    1.4400 -0.0163  0.0211  ...  0.11670  533.7700  2.2632   10.3500   \n",
       " 39    1.5335  0.0098 -0.0013  ...  0.11955  532.1118  2.2194    6.2300   \n",
       " 41    1.4652 -0.0008  0.0137  ...  0.19000  531.3755  2.4747    6.5900   \n",
       " 42    1.5042  0.0082  0.0092  ...  0.12840  532.8618  2.3145   10.4300   \n",
       " 43    1.5337 -0.0207  0.0067  ...  0.11955  527.8355  2.1599   12.1900   \n",
       " 44    1.5764 -0.0219 -0.0080  ...  0.11955  535.6282  2.2383   11.6200   \n",
       " 46    1.4919 -0.0030  0.0034  ...  0.11955  529.9682  2.1756   12.3000   \n",
       " 47    1.5179  0.0114  0.0110  ...  0.11955  533.6809  2.1400   10.3100   \n",
       " 51    1.5277 -0.0187 -0.0076  ...  0.07680  534.2509  2.5108    9.2900   \n",
       " 52    1.4710  0.0442  0.0146  ...  0.11955  530.8764  2.1917    7.0000   \n",
       " 53    1.5321 -0.0008  0.0047  ...  0.06050  532.5454  2.1461    8.7400   \n",
       " 54    1.6486 -0.0239  0.0031  ...  0.14450  535.2400  2.3483    8.2800   \n",
       " 55    1.5512 -0.0019  0.0033  ...  0.11955  535.5864  2.3608    8.0699   \n",
       " 56    1.4873  0.0362  0.0029  ...  0.09620  536.4636  2.0117    9.2700   \n",
       " 59    1.5632 -0.0156 -0.0010  ...  0.19930  529.9773  2.3048    7.2400   \n",
       " 60    1.5928  0.0163 -0.0042  ...  0.20780  531.7764  2.2132    8.5800   \n",
       " 61    1.5644  0.0201  0.0044  ...  0.21870  535.7673  2.0382   10.6600   \n",
       " 63    1.5068  0.0126 -0.0023  ...  0.09650  536.7873  2.2478    6.2400   \n",
       " 65    1.4970 -0.0077  0.0068  ...  0.09540  533.8600  2.3365    6.1400   \n",
       " 66    1.4727 -0.0044  0.0136  ...  0.08510  537.2882  2.0190    6.9800   \n",
       " 67    1.5079 -0.0086  0.0070  ...  0.12860  535.0427  2.0348   10.3000   \n",
       " 68    1.4440  0.0007  0.0090  ...  0.08340  534.6255  2.4592    7.9100   \n",
       " 69    1.5569  0.0189  0.0077  ...  0.06620  536.5182  2.0018   12.5400   \n",
       " 70    1.5693  0.0064 -0.0058  ...  0.16090  532.4236  2.5469    6.4900   \n",
       " 71    1.5769 -0.0359  0.0076  ...  0.16600  535.4264  2.0184    7.5500   \n",
       " 72    1.4395  0.0012  0.0011  ...  0.11955  534.5155  2.4013   10.5700   \n",
       " 73    1.4829 -0.0176  0.0064  ...  0.16070  523.0227  2.3358    5.3900   \n",
       " 74    1.4981  0.0234 -0.0042  ...  0.29290  537.5900  1.7633    6.3900   \n",
       " 75    1.4656 -0.0103 -0.0029  ...  0.02450  536.3945  2.0371    9.6000   \n",
       " 76    1.4408  0.0247  0.0022  ...  0.08340  534.8927  2.4332   10.7500   \n",
       " 77    1.5371 -0.0160  0.0022  ...  0.14170  537.1373  2.1412    7.9300   \n",
       " 78    1.4615 -0.0034 -0.0042  ...  0.10890  537.6418  2.2678   10.2200   \n",
       " 79    1.5264  0.0170 -0.0099  ...  0.11955  529.3318  2.3418   10.5000   \n",
       " 80    1.4912 -0.0004 -0.0016  ...  0.07620  533.6146  2.0713   11.0900   \n",
       " 81    1.5218 -0.0283 -0.0010  ...  0.09810  535.7664  1.9407    6.1500   \n",
       " 83    1.5884 -0.0108  0.0165  ...  0.18200  531.1209  2.2509    9.5300   \n",
       " 84    1.6026 -0.0252  0.0033  ...  0.11955  539.2336  2.1559    7.5400   \n",
       " 85    1.4880  0.0252 -0.0044  ...  0.11955  510.9145  1.7483  439.0500   \n",
       " 86    1.5553  0.0245 -0.0111  ...  0.07030  510.9145  1.7483  439.0500   \n",
       " 87    1.3296 -0.0011  0.0007  ...  0.14170  533.1773  2.3063    9.9300   \n",
       " 88    1.3167  0.0428  0.0230  ...  0.11955  533.8209  2.2711   10.2700   \n",
       " 89    1.4989 -0.0042 -0.0009  ...  0.16670  510.9145  1.7483  439.0500   \n",
       " 90    1.4527 -0.0077 -0.0098  ...  0.16890  533.3118  2.3907    7.0100   \n",
       " 91    1.5867  0.0086  0.0349  ...  0.18170  529.3927  2.1345   11.7900   \n",
       " 92    1.5414 -0.0095 -0.0167  ...  0.12970  529.3927  2.1345   11.7900   \n",
       " 93    1.5024 -0.0039  0.0008  ...  0.20480  535.1736  2.4707    9.8400   \n",
       " 94    1.5409 -0.0097  0.0043  ...  0.06710  510.9145  1.7483  439.0500   \n",
       " 95    1.5048 -0.0108  0.0070  ...  0.11380  534.9591  1.8002    8.2900   \n",
       " 97    1.5310  0.0006  0.0028  ...  0.11955  531.6718  2.3956   10.2600   \n",
       " 98    1.6227  0.0062  0.0194  ...  0.11955  529.0600  2.3468    8.1900   \n",
       " 99    1.5115 -0.0100  0.0053  ...  0.11955  530.9282  2.1959   11.7800   \n",
       " 100   1.5035  0.0046  0.0041  ...  0.11955  529.8373  2.3027   12.5000   \n",
       " 101   1.3609 -0.0003 -0.0046  ...  0.10090  530.9864  2.3854    7.9300   \n",
       " 102   1.4581  0.0002  0.0004  ...  0.32570  550.5855  2.1543  420.2400   \n",
       " 103   1.5748 -0.0062 -0.0024  ...  0.11955  529.8309  2.3190    9.6000   \n",
       " 104   1.4051 -0.0276 -0.0093  ...  0.13610  535.1718  2.4874    8.1400   \n",
       " 105   1.4039  0.0088  0.0169  ...  0.11160  534.1436  1.6744    8.3000   \n",
       " 106   1.3985 -0.0092  0.0044  ...  0.11820  533.7727  2.3508    9.7000   \n",
       " 107   1.4876  0.0018  0.0094  ...  0.14380  536.7136  2.4272    7.0900   \n",
       " 108   1.4321 -0.0114  0.0026  ...  0.07600  537.0518  1.8363    8.1000   \n",
       " 109   1.4639 -0.0091 -0.0063  ...  0.11955  536.3864  2.4386    7.5700   \n",
       " 110   1.5242  0.0221  0.0153  ...  0.23820  530.7191  2.5798    6.1000   \n",
       " 111   1.5847  0.0283  0.0118  ...  0.10730  533.3509  2.1114    8.0900   \n",
       " 112   1.5497 -0.0019  0.0029  ...  0.06370  534.8827  2.5700    8.4200   \n",
       " 113   1.4453 -0.0126  0.0152  ...  0.11955  532.7255  1.8964    9.6000   \n",
       " 114   1.4634 -0.0094  0.0006  ...  0.11955  529.1618  2.3860    8.2600   \n",
       " 116   1.4443  0.0008 -0.0059  ...  0.27170  536.0627  2.3428    9.7200   \n",
       " 117   1.5166 -0.0141  0.0090  ...  0.11740  535.7082  2.1497    7.7700   \n",
       " 118   1.4740  0.0024  0.0069  ...  0.09290  532.4836  2.2079   11.5000   \n",
       " 119   1.4046 -0.0080  0.0040  ...  0.56310  533.3209  2.4315    5.9500   \n",
       " 120   1.4649  0.0309 -0.0068  ...  0.06820  528.6918  2.3251   10.2401   \n",
       " 121   1.5385  0.0329 -0.0010  ...  0.16640  533.2491  2.0020    8.9800   \n",
       " 122   1.5370 -0.0013  0.0006  ...  0.16320  534.6418  2.1732   10.8900   \n",
       " 123   1.5374 -0.0131 -0.0007  ...  0.10240  536.4745  2.1084    9.6100   \n",
       " 124   1.4974 -0.0046 -0.0002  ...  0.14940  531.9991  2.3672    8.4600   \n",
       " 125   1.3642 -0.0167  0.0033  ...  0.54510  533.7818  2.4911   10.2100   \n",
       " 126   1.4125 -0.0003  0.0140  ...  0.06470  534.8573  2.4009    9.0500   \n",
       " 127   1.5430  0.0059 -0.0057  ...  0.13430  532.3064  2.5503    8.6801   \n",
       " 128   1.4306  0.0383 -0.0045  ...  0.11955  526.4382  2.4025    8.5200   \n",
       " 129   1.4402 -0.0180 -0.0013  ...  0.19010  534.0064  2.4764    9.5700   \n",
       " 130   1.4700 -0.0092  0.0083  ...  0.19100  531.6346  2.3149    8.3000   \n",
       " 132   1.3872 -0.0089  0.0028  ...  0.11955  532.6609  2.1807   11.3900   \n",
       " 133   1.4712 -0.0150  0.0056  ...  0.13600  523.4991  1.8204    6.4300   \n",
       " 134   1.4845 -0.0079 -0.0004  ...  0.23170  466.5409  2.4894  283.0300   \n",
       " 135   1.3325  0.0067  0.0098  ...  0.09400  535.6018  2.0446   11.3200   \n",
       " 136   1.4704  0.0024  0.0124  ...  0.08580  531.3545  2.4380    6.2900   \n",
       " 137   1.3714  0.0037  0.0070  ...  0.13730  531.6273  2.2843   11.7300   \n",
       " 138   1.4523 -0.0032  0.0093  ...  0.12640  531.8255  2.0893    7.9500   \n",
       " 139   1.3809 -0.0025 -0.0149  ...  0.18980  535.1736  2.4707    9.8400   \n",
       " 140   1.5668 -0.0074 -0.0074  ...  0.14520  534.5045  2.4213   10.1300   \n",
       " 141   1.4491  0.0015 -0.0036  ...  0.18640  534.5045  2.4213   10.1300   \n",
       " 142   1.4921 -0.0113  0.0136  ...  0.25080  539.0418  2.5479    8.8400   \n",
       " 143   1.4149 -0.0041  0.0036  ...  0.20970  531.3073  2.3028   11.9400   \n",
       " 144   1.4786  0.0017 -0.0080  ...  0.08600  531.4036  2.2910   12.2100   \n",
       " 145   1.4785 -0.0205 -0.0057  ...  0.13530  536.2418  2.4342    8.7400   \n",
       " 146   1.4599  0.0032  0.0067  ...  0.14680  533.2282  2.3215    7.1100   \n",
       " 147   1.5572  0.0003 -0.0031  ...  0.12390  537.3973  2.0402    8.2700   \n",
       " 148   1.5637 -0.0137 -0.0048  ...  0.23690  531.2291  2.4177   11.4700   \n",
       " 149   1.4416 -0.0037 -0.0035  ...  0.09520  534.9591  1.8002    8.2900   \n",
       " 150   1.5708 -0.0139 -0.0027  ...  0.11240  534.5045  2.4213   10.1300   \n",
       " 151   1.5796  0.0103 -0.0029  ...  0.11955  530.1700  2.1279   11.1800   \n",
       " 152   1.4635  0.0001  0.0047  ...  0.12700  532.5973  2.0612    8.7000   \n",
       " 153   1.4313  0.0022  0.0085  ...  0.10010  534.2527  1.9665    9.0800   \n",
       " 155   1.4891 -0.0007 -0.0088  ...  0.09180  531.0300  2.2599    9.0800   \n",
       " 156   1.5568 -0.0214 -0.0043  ...  0.12870  534.5873  2.1965    6.9900   \n",
       " 159   1.5859 -0.0139 -0.0085  ...  0.29800  530.5818  2.2833   11.5800   \n",
       " 160   1.4549 -0.0125 -0.0196  ...  0.16320  535.0645  2.2855    5.7000   \n",
       " 161   1.4519 -0.0206  0.0026  ...  0.09840  537.2082  2.4297    8.0100   \n",
       " 162   1.4580  0.0063 -0.0133  ...  0.16070  535.1564  2.2877    5.7200   \n",
       " 163   1.4059 -0.0083  0.0059  ...  0.11955  510.9145  1.7483  439.0500   \n",
       " 164   1.5554 -0.0127  0.0041  ...  0.07750  534.5045  2.4213   10.1300   \n",
       " 165   1.4704 -0.0159  0.0082  ...  0.23940  533.2418  2.4157    6.5800   \n",
       " 166   1.5129 -0.0152 -0.0038  ...  0.13580  534.9627  2.3420    6.0100   \n",
       " 168   1.4377 -0.0126 -0.0008  ...  0.18790  534.6009  2.2614    5.6500   \n",
       " 170   1.4435 -0.0119 -0.0013  ...  0.15660  534.3564  2.5551    9.5300   \n",
       " 171   1.5186 -0.0048 -0.0042  ...  0.11955  536.1964  2.0946   12.5500   \n",
       " 172   1.4767 -0.0073  0.0146  ...  0.18100  534.5045  2.4213   10.1300   \n",
       " 173   1.5792  0.0016  0.0177  ...  0.29840  532.2736  2.1116    9.9800   \n",
       " 174   1.5884  0.0097  0.0031  ...  0.14670  537.6355  2.1349    7.9800   \n",
       " 175   1.6138  0.0146  0.0080  ...  0.24910  535.1318  2.3810    9.6900   \n",
       " 176   1.3868 -0.0023  0.0035  ...  0.08770  535.3736  2.3921    8.9700   \n",
       " 177   1.5442 -0.0108  0.0060  ...  0.11955  535.3736  2.3921    8.9700   \n",
       " 178   1.5622  0.0167 -0.0117  ...  0.08770  536.0109  2.2047    6.1100   \n",
       " 179   1.5496  0.0106  0.0001  ...  0.08770  532.6700  2.2124    8.8000   \n",
       " 181   1.4779  0.0052 -0.0013  ...  0.19720  367.4400  2.4357  142.5000   \n",
       " 183   1.4554 -0.0267  0.0027  ...  0.11955  528.6918  2.3251   10.2401   \n",
       " 184   1.6081  0.0143 -0.0146  ...  0.19080  535.3736  2.3921    8.9700   \n",
       " 185   1.4976  0.0056 -0.0011  ...  0.11955  530.5091  2.3854   11.1800   \n",
       " 187   1.4616 -0.0018  0.0097  ...  0.11955  534.5918  2.3970    6.9800   \n",
       " 190   1.4957 -0.0119  0.0129  ...  0.09230  533.8218  2.0970    8.1400   \n",
       " 191   1.6030 -0.0158  0.0097  ...  0.08950  535.7864  2.1227    7.2300   \n",
       " 192   1.3840 -0.0150  0.0095  ...  0.11955  531.0818  2.3445    9.0000   \n",
       " 193   1.6035  0.0023  0.0114  ...  0.08600  532.5227  2.0369    9.6200   \n",
       " 194   1.4845 -0.0326  0.0097  ...  0.09670  534.9427  1.9155    6.6000   \n",
       " 195   1.5150  0.0031  0.0097  ...  0.11955  534.9591  1.8002    8.2900   \n",
       " 196   1.5334 -0.0185  0.0126  ...  0.19540  531.9955  2.2473    9.0300   \n",
       " 197   1.4833 -0.0101  0.0130  ...  0.07880  534.5045  2.4213   10.1300   \n",
       " 198   1.5679 -0.0197  0.0100  ...  0.11955  535.3736  2.3921    8.9700   \n",
       " 199   1.3479  0.0306 -0.0106  ...  0.11955  537.9418  1.9847    9.9100   \n",
       " 200   1.4281  0.0055 -0.0119  ...  0.09750  533.9245  2.1340    7.8700   \n",
       " 201   1.4005  0.0056 -0.0192  ...  0.11955  538.5064  2.5244    8.1100   \n",
       " 202   1.5362 -0.0088  0.0039  ...  0.11955  529.2545  1.7725    9.0900   \n",
       " 203   1.4334  0.0056  0.0012  ...  0.11955  534.8400  1.4942   10.1800   \n",
       " 204   1.4310  0.0090  0.0033  ...  0.14670  535.2100  2.1136    7.9700   \n",
       " 205   1.5101  0.0061  0.0093  ...  0.13630  532.7000  2.3601   10.4600   \n",
       " 206   1.3933 -0.0229 -0.0137  ...  0.12850  532.7482  2.2585    5.8300   \n",
       " 207   1.4969 -0.0171  0.0030  ...  0.11955  534.9527  2.2757    6.8700   \n",
       " 208   1.4659  0.0055 -0.0135  ...  0.08450  537.2691  2.0028    7.0700   \n",
       " 209   1.4845 -0.0178  0.0084  ...  0.11110  531.9700  2.2792    7.8700   \n",
       " 210   1.3780 -0.0219 -0.0019  ...  0.11955  531.8709  2.2561    9.7500   \n",
       " 211   1.5094 -0.0027 -0.0015  ...  0.13160  508.2764  2.0687  272.4100   \n",
       " 212   1.5573  0.0019  0.0117  ...  0.11955  534.8200  1.8025    7.7200   \n",
       " 213   1.4281  0.0049  0.0092  ...  0.11955  533.3809  1.5992    9.3300   \n",
       " 214   1.4510 -0.0129 -0.0106  ...  0.11955  533.8691  2.0721    7.4800   \n",
       " 215   1.4538  0.0161  0.0071  ...  0.11955  535.9764  2.2302    7.2200   \n",
       " 216   1.3580 -0.0024 -0.0143  ...  0.28120  531.5982  1.5656    8.5500   \n",
       " 217   1.5337 -0.0010 -0.0018  ...  0.14470  529.6709  2.4583   11.3600   \n",
       " 219   1.4029  0.0174 -0.0023  ...  0.11930  535.9227  1.9110    8.6500   \n",
       " 220   1.5210  0.0075 -0.0127  ...  0.11600  532.7000  2.3601   10.4600   \n",
       " 221   1.5356  0.0043 -0.0008  ...  0.17970  537.2809  2.0931    6.8000   \n",
       " 223   1.4355  0.0135 -0.0063  ...  0.11955  529.2827  2.2933    9.7400   \n",
       " 224   1.5059  0.0218  0.0018  ...  0.17960  530.9864  2.4457   11.1300   \n",
       " 225   1.4785 -0.0004 -0.0113  ...  0.20430  529.5091  2.5085   11.3700   \n",
       " 226   1.4703  0.0275 -0.0061  ...  0.14420  530.0736  2.5630    9.1000   \n",
       " 227   1.4406  0.0156 -0.0068  ...  0.11955  529.4427  2.5058    6.1400   \n",
       " 228   1.4685  0.0016  0.0004  ...  0.16750  528.8918  2.2461    9.1100   \n",
       " 229   1.4557 -0.0085 -0.0119  ...  0.11955  531.6709  2.2584    7.7900   \n",
       " 230   1.3731  0.0056 -0.0015  ...  0.11955  535.7309  1.7132    9.7700   \n",
       " 232   1.3906 -0.0018 -0.0008  ...  0.09570  535.4118  1.5550    7.7700   \n",
       " 233   1.4881 -0.0054 -0.0037  ...  0.08770  534.7509  1.5983    6.6800   \n",
       " 234   1.4476 -0.0234 -0.0001  ...  0.12400  529.1691  2.1248    8.0700   \n",
       " 237   1.5221 -0.0175  0.0086  ...  0.04090  534.7382  1.4832    9.3200   \n",
       " 239   1.3595  0.0063 -0.0034  ...  0.22810  528.7264  2.4341    6.3400   \n",
       " 242   1.4737  0.0046 -0.0030  ...  0.13770  534.0745  1.4351    8.0700   \n",
       " 245   1.4190 -0.0296  0.0073  ...  0.11955  532.5527  2.1698   10.2400   \n",
       " 246   1.4245 -0.0056 -0.0089  ...  0.22030  533.4982  2.3658    8.4200   \n",
       " 247   1.4925 -0.0074 -0.0043  ...  0.11955  532.8891  2.2839    9.1500   \n",
       " 248   1.4493  0.0176 -0.0025  ...  0.13540  391.5764  2.2395  177.9300   \n",
       " 249   1.4808 -0.0107 -0.0050  ...  0.11955  401.4482  2.4312  282.4700   \n",
       " 250   1.5145  0.0345 -0.0064  ...  0.11955  531.5309  1.8496    7.5400   \n",
       " 251   1.5198  0.0064 -0.0012  ...  0.22110  532.6864  2.3328    7.6200   \n",
       " 252   1.4777  0.0324 -0.0037  ...  0.11955  533.2682  1.8703    8.7700   \n",
       " 253   1.4360 -0.0057  0.0101  ...  0.12870  364.1436  2.2891  280.7900   \n",
       " 254   1.4025 -0.0365  0.0008  ...  0.16310  532.8455  1.6887    8.7100   \n",
       " 255   1.4840  0.0045 -0.0019  ...  0.11350  535.7282  2.1451    7.4900   \n",
       " 256   1.4596  0.0034  0.0057  ...  0.11955  534.7900  1.6127    9.2300   \n",
       " 257   1.4400 -0.0281 -0.0044  ...  0.13200  531.0027  2.4523    9.9800   \n",
       " 258   1.4378 -0.0282  0.0012  ...  0.20130  532.3182  2.2750   10.1900   \n",
       " 259   1.5193 -0.0212 -0.0044  ...  0.17140  534.9600  1.8846   10.0300   \n",
       " 260   1.5183 -0.0161  0.0004  ...  0.11955  535.2782  1.8070    8.9200   \n",
       " 261   1.4420  0.0183  0.0023  ...  0.11955  538.1409  1.7150    5.9300   \n",
       " 262   1.4067 -0.0234  0.0029  ...  0.11955  529.1055  2.3400   11.1000   \n",
       " 263   1.4896  0.0064 -0.0013  ...  0.11660  531.0218  2.4000    8.9400   \n",
       " 264   1.4929 -0.0252  0.0112  ...  0.11955  526.3709  2.3553    7.8200   \n",
       " 265   1.5032 -0.0042  0.0021  ...  0.11955  533.1318  2.5132    8.6200   \n",
       " 266   1.4701 -0.0003  0.0026  ...  0.11955  534.4582  2.3295    6.9900   \n",
       " 267   1.5155 -0.0045  0.0027  ...  0.12490  531.1618  1.7323    6.9700   \n",
       " 268   1.3993  0.0042 -0.0103  ...  0.11955  531.4909  2.3698   10.5000   \n",
       " 269   1.4193  0.0015  0.0015  ...  0.11955  529.9355  2.1598    7.4300   \n",
       " 270   1.4305  0.0001 -0.0054  ...  0.11955  534.1545  1.6889    9.8800   \n",
       " 271   1.3669 -0.0005  0.0017  ...  0.11955  530.0118  1.7527   10.3200   \n",
       " 272   1.4150 -0.0062 -0.0032  ...  0.11955  532.8855  1.8536    8.8500   \n",
       " 274   1.3989  0.0127  0.0069  ...  0.11955  532.7391  2.4119    8.8800   \n",
       " 275   1.4984 -0.0068  0.0010  ...  0.11955  513.3955  2.3483  274.8700   \n",
       " 276   1.3971  0.0300  0.0118  ...  0.08770  532.9491  2.2987   10.4100   \n",
       " 278   1.4049  0.0389 -0.0147  ...  0.04810  532.9782  2.1360    6.5000   \n",
       " 279   1.4547  0.0285 -0.0017  ...  0.18900  532.0100  2.3061    7.5100   \n",
       " 280   1.4732 -0.0009  0.0068  ...  0.11955  533.9900  1.7577    6.9600   \n",
       " 281   1.4763 -0.0043  0.0067  ...  0.11955  532.0454  2.1206    9.8100   \n",
       " 283   1.4389  0.0172  0.0306  ...  0.27910  535.5327  1.7027    7.8300   \n",
       " 284   1.4250 -0.0063  0.0076  ...  0.11955  531.0309  2.4452    9.6200   \n",
       " 285   1.4246 -0.0184  0.0070  ...  0.11955  534.1218  1.9309    5.8000   \n",
       " 286   1.4459 -0.0408  0.0113  ...  0.11955  527.8727  2.2436    7.5600   \n",
       " 287   1.4282  0.0005 -0.0113  ...  0.29540  533.3264  1.6541    8.8800   \n",
       " 288   1.4738 -0.0213  0.0022  ...  0.11750  532.9573  2.2141    7.9100   \n",
       " 289   1.5040 -0.0376 -0.0025  ...  0.29550  529.8873  2.3048   11.6800   \n",
       " 290   1.3576  0.0000 -0.0046  ...  0.11955  535.3509  2.0813    7.1800   \n",
       " 292   1.4470 -0.0163  0.0530  ...  0.12550  537.2809  2.0931    6.8000   \n",
       " 293   1.4173 -0.0066 -0.0093  ...  0.15810  531.6236  2.5605    9.2000   \n",
       " 295   1.3812  0.0031 -0.0046  ...  0.11955  526.8464  2.3356    7.0600   \n",
       " 296   1.3743 -0.0005  0.0024  ...  0.11955  530.5527  2.5117   10.4400   \n",
       " 297   1.4610  0.0030 -0.0042  ...  0.08770  525.9545  2.4083    9.5000   \n",
       " 298   1.3526 -0.0040 -0.0105  ...  0.11955  533.2600  2.5306    8.2600   \n",
       " 299   1.4814 -0.0238  0.0396  ...  0.11230  535.9227  1.9110    8.6500   \n",
       " 300   1.4700 -0.0102 -0.0073  ...  0.35880  525.9545  2.4083    9.5000   \n",
       " 301   1.2921 -0.0108 -0.0020  ...  0.21490  532.4264  2.3599    8.2100   \n",
       " 302   1.4715  0.0040 -0.0103  ...  0.06980  331.9618  2.3704  129.1200   \n",
       " 303   1.5386 -0.0090  0.0057  ...  0.11955  530.2800  2.3134   12.4000   \n",
       " 304   1.4339  0.0032 -0.0067  ...  0.16840  533.9673  2.3091    8.4300   \n",
       " 305   1.3800 -0.0218 -0.0145  ...  0.11955  534.5736  2.3528    7.3500   \n",
       " 306   1.4965 -0.0045 -0.0006  ...  0.12190  532.7046  1.6932    9.4800   \n",
       " 307   1.4043 -0.0210 -0.0052  ...  0.11955  534.6436  2.3390    8.3500   \n",
       " 308   1.4280 -0.0169  0.0016  ...  0.11955  533.7055  1.8993   11.7800   \n",
       " 309   1.4542 -0.0362 -0.0025  ...  0.14010  533.6318  2.2147    8.3700   \n",
       " 310   1.4350 -0.0004 -0.0057  ...  0.17420  534.1773  2.2813    9.7800   \n",
       " 311   1.4147  0.0130  0.0050  ...  0.15600  531.1391  2.3129   10.0800   \n",
       " 312   1.4984  0.0020 -0.0076  ...  0.23500  534.1755  2.5320    6.7900   \n",
       " 313   1.4293 -0.0171 -0.0075  ...  0.17630  535.8209  2.0976    7.3000   \n",
       " 314   1.4291 -0.0048 -0.0185  ...  0.08880  531.3882  2.2970    9.6200   \n",
       " 315   1.4204  0.0011 -0.0135  ...  0.14650  531.5864  2.4302    8.3800   \n",
       " 316   1.4446 -0.0318  0.0366  ...  0.07070  534.3564  2.3688    7.5500   \n",
       " 317   1.5305  0.0007 -0.0068  ...  0.08770  535.5691  1.7613    6.3000   \n",
       " 318   1.5357 -0.0337 -0.0220  ...  0.30870  534.8873  2.4691    6.6200   \n",
       " 319   1.3899  0.0054  0.0103  ...  0.10610  535.5691  1.7613    6.3000   \n",
       " 320   1.4609  0.0048 -0.0034  ...  0.07690  480.5291  2.4944  277.1100   \n",
       " 322   1.5695 -0.0013  0.0024  ...  0.11955  531.9045  2.0793   11.2400   \n",
       " 324   1.4010 -0.0001 -0.0054  ...  0.11955  531.9673  2.3808   10.2600   \n",
       " 325   1.5176 -0.0049  0.0084  ...  0.11955  533.5627  2.3471    8.6100   \n",
       " 328   1.5546 -0.0107  0.0072  ...  0.11955  535.2782  1.8070    8.9200   \n",
       " 329   1.5279 -0.0061  0.0031  ...  0.11955  534.3055  2.4677    7.1800   \n",
       " 330   1.5091 -0.0064  0.0044  ...  0.18570  530.9227  2.3869   12.6600   \n",
       " 331   1.5400 -0.0167 -0.0036  ...  0.08980  537.2809  2.0931    6.8000   \n",
       " 332   1.4535  0.0141  0.0026  ...  0.04500  533.4627  2.3152    9.2700   \n",
       " 333   1.4941  0.0037  0.0086  ...  0.13380  532.5527  2.1698   10.2400   \n",
       " 334   1.4783 -0.0129 -0.0051  ...  0.16690  534.7936  2.1494    8.9800   \n",
       " 335   1.4512 -0.0018 -0.0032  ...  0.11955  487.2691  2.4733  280.6600   \n",
       " 337   1.5979 -0.0142  0.0010  ...  0.11955  535.4818  2.4326    4.9200   \n",
       " 338   1.4909 -0.0039  0.0082  ...  0.11955  534.9700  2.3699    8.2100   \n",
       " 339   1.5971 -0.0160 -0.0023  ...  0.11955  534.9600  2.4029    8.4900   \n",
       " 340   1.4400  0.0092 -0.0113  ...  0.16950  535.9227  1.9110    8.6500   \n",
       " 341   1.4975 -0.0130  0.0016  ...  0.20740  379.0045  2.3354   12.8800   \n",
       " 342   1.6103  0.0100  0.0049  ...  0.14260  532.5764  2.5229    7.8400   \n",
       " 343   1.4551  0.0117  0.0057  ...  0.08770  535.9227  1.9110    8.6500   \n",
       " 345   1.4752 -0.0118  0.0105  ...  0.19260  533.5964  2.2693   10.7000   \n",
       " 346   1.5126 -0.0192  0.0107  ...  0.11955  530.6173  2.4608   10.5600   \n",
       " 347   1.3806 -0.0090  0.0117  ...  0.11000  360.7645  2.3897  152.8700   \n",
       " 348   1.4036  0.0035  0.0141  ...  0.08770  534.9600  1.8846   10.0300   \n",
       " 349   1.4042  0.0093  0.0087  ...  0.14840  508.2764  2.0687  272.4100   \n",
       " 350   1.3428  0.0093  0.0052  ...  0.08770  536.0973  2.0779    9.0200   \n",
       " 352   1.5037 -0.0109  0.0069  ...  0.07740  534.4473  2.4356    9.3300   \n",
       " 353   1.5272 -0.0219  0.0083  ...  0.08770  535.9227  1.9110    8.6500   \n",
       " 354   1.4636  0.0009  0.0031  ...  0.19530  531.5445  2.4212    7.4900   \n",
       " 355   1.5852 -0.0077  0.0048  ...  0.11690  535.7282  2.1451    7.4900   \n",
       " 356   1.4629  0.0175  0.0097  ...  0.09160  530.6027  2.3970    7.1100   \n",
       " 357   1.5923  0.0098  0.0039  ...  0.08770  536.0973  2.0779    9.0200   \n",
       " 358   1.3924  0.0036 -0.0017  ...  0.24970  486.4218  2.4624  276.5200   \n",
       " 359   1.4103 -0.0137  0.0004  ...  0.08770  534.0518  2.4314    8.0700   \n",
       " 360   1.4281 -0.0109  0.0089  ...  0.08770  535.7282  2.1451    7.4900   \n",
       " 361   1.5108  0.0255  0.0178  ...  0.08770  532.4355  2.3806    8.2100   \n",
       " 362   1.4428 -0.0110  0.0101  ...  0.11955  529.5727  2.3873   10.3700   \n",
       " 363   1.4414  0.0124  0.0015  ...  0.02240  534.4473  2.4356    9.3300   \n",
       " 364   1.4956  0.0151  0.0107  ...  0.15770  530.8909  2.5326    8.5000   \n",
       " 365   1.5829 -0.0278 -0.0324  ...  0.08770  536.8282  2.1262    6.8800   \n",
       " 366   1.4611 -0.0090  0.0111  ...  0.13170  534.9600  1.8846   10.0300   \n",
       " 367   1.5758 -0.0093  0.0013  ...  0.23140  532.4355  2.3806    8.2100   \n",
       " 369   1.2260  0.0307  0.0087  ...  0.08770  537.2809  2.0931    6.8000   \n",
       " 370   1.5389 -0.0027  0.0095  ...  0.07870  534.3564  2.3688    7.5500   \n",
       " 371   1.5996 -0.0012 -0.0017  ...  0.07540  529.8082  1.5705    9.2700   \n",
       " 372   1.4859  0.0015  0.0041  ...  0.11955  534.8045  1.5655    9.6200   \n",
       " 374   1.4600  0.0109 -0.0021  ...  0.08770  534.9600  1.8846   10.0300   \n",
       " 375   1.3814  0.0013 -0.0046  ...  0.11955  535.2709  2.3676    7.9300   \n",
       " 376   1.4598 -0.0194 -0.0329  ...  0.11955  537.4455  1.5694    7.5000   \n",
       " 377   1.5168  0.0143  0.0072  ...  0.08770  534.3564  2.3688    7.5500   \n",
       " 378   1.4603  0.0039  0.0152  ...  0.11955  532.7618  2.3637    8.7700   \n",
       " 379   1.5362  0.0138  0.0029  ...  0.11955  534.5891  2.4801    6.5100   \n",
       " 380   1.4388  0.0009 -0.0064  ...  0.27660  532.7818  2.4880    7.5800   \n",
       " 381   1.4604 -0.0151 -0.0036  ...  0.13820  534.4727  2.1534    7.6800   \n",
       " 382   1.5608 -0.0004 -0.0052  ...  0.16560  533.4909  1.6300    8.6801   \n",
       " 383   1.5169  0.0136  0.0019  ...  0.11955  511.1427  1.6450  434.0400   \n",
       " 384   1.5400  0.0221  0.0091  ...  0.11955  529.6518  2.5380   10.7300   \n",
       " 385   1.5001 -0.0199 -0.0289  ...  0.20070  533.1600  2.2493    9.7000   \n",
       " 386   1.5016  0.0110 -0.0001  ...  0.14350  534.0436  2.1886   10.7400   \n",
       " 387   1.4313 -0.0077 -0.0013  ...  0.11955  536.1245  2.3189    7.7300   \n",
       " 388   1.4712  0.0267 -0.0046  ...  0.11955  535.9118  1.7915    8.5200   \n",
       " 389   1.5432 -0.0163  0.0048  ...  0.12310  529.1527  2.5264    9.1400   \n",
       " 390   1.5865  0.0080 -0.0216  ...  0.07780  534.0609  2.4197    8.3600   \n",
       " 391   1.4822  0.0105 -0.0170  ...  0.11955  533.9055  2.3606    7.4600   \n",
       " 393   1.3531  0.0069 -0.0019  ...  0.19380  336.6718  2.2705  150.7700   \n",
       " 394   1.1910  0.0101 -0.0181  ...  0.10300  532.8691  2.3668   10.3800   \n",
       " 395   1.5387  0.0126  0.0063  ...  0.13650  533.1491  2.4279    9.6600   \n",
       " 396   1.2476  0.0170  0.0125  ...  0.09510  534.4473  2.4356    9.3300   \n",
       " 397   1.4426  0.0132 -0.0185  ...  0.11955  534.9473  1.6309    7.8500   \n",
       " 398   1.5381 -0.0077  0.0018  ...  0.12530  532.3445  2.4733    8.3800   \n",
       " 399   1.5320  0.0068 -0.0173  ...  0.11955  532.6755  2.3130    8.4700   \n",
       " 400   1.5164  0.0030 -0.0203  ...  0.06490  532.1827  2.2771    7.4700   \n",
       " 401   1.3967  0.0176 -0.0239  ...  0.11955  533.9036  2.3605    9.3500   \n",
       " 402   1.4991 -0.0064 -0.0148  ...  0.18180  534.3564  2.3688    7.5500   \n",
       " 403   1.4319 -0.0067 -0.0146  ...  0.11955  535.4736  2.3436    7.6700   \n",
       " 404   1.5327 -0.0192  0.0023  ...  0.23930  535.2009  1.6920    8.9000   \n",
       " 405   1.4732 -0.0063 -0.0148  ...  0.07270  534.3564  2.3688    7.5500   \n",
       " 407   1.6128  0.0004 -0.0081  ...  0.18500  536.8282  2.1262    6.8800   \n",
       " 408   1.5518  0.0130 -0.0008  ...  0.15630  534.7936  2.2749    6.2100   \n",
       " 409   1.4603  0.0214 -0.0018  ...  0.08770  532.7645  2.4112    9.8300   \n",
       " 410   1.4768  0.0193  0.0009  ...  0.11955  530.3155  2.2485   10.3600   \n",
       " 411   1.5410 -0.0064 -0.0083  ...  0.09740  533.5927  2.2888    9.4800   \n",
       " 412   1.5330 -0.0059  0.0228  ...  0.12080  530.6855  2.2688   11.1800   \n",
       " 413   1.5082 -0.0057 -0.0069  ...  0.09470  539.1145  1.7970    7.9600   \n",
       " 414   1.5611  0.0128 -0.0103  ...  0.11955  536.0091  2.0315    8.7700   \n",
       " 415   1.5440 -0.0163 -0.0238  ...  0.16980  534.5991  2.3354   10.8200   \n",
       " 416   1.5316 -0.0105 -0.0218  ...  0.08770  531.4409  2.3579    7.6900   \n",
       " 417   1.5238  0.0007 -0.0247  ...  0.11955  531.7354  2.3128    7.7900   \n",
       " 418   1.5262  0.0065 -0.0052  ...  0.20620  532.3982  2.3705    8.5699   \n",
       " 419   1.5973  0.0013 -0.0057  ...  0.08770  534.4964  2.4582    9.1400   \n",
       " 420   1.6405 -0.0410 -0.0067  ...  0.16750  532.7645  1.6780    8.5800   \n",
       " 421   1.5164 -0.0013 -0.0059  ...  0.08770  531.6036  2.0159    7.3500   \n",
       " 422   1.5116  0.0130  0.0007  ...  0.10740  532.9282  2.4995    7.8400   \n",
       " 423   1.4458  0.0006 -0.0029  ...  0.13440  531.2591  2.3950    5.9600   \n",
       " 425   1.4794 -0.0198 -0.0004  ...  0.11955  532.4618  2.2846    9.3600   \n",
       " 426   1.5322  0.0205 -0.0213  ...  0.13740  410.2409  2.4691   59.5800   \n",
       " 427   1.4945  0.0247 -0.0049  ...  0.11955  538.3636  1.9631    6.0400   \n",
       " 428   1.5680  0.0009  0.0094  ...  0.11955  534.5545  2.3853    8.2000   \n",
       " 429   1.4920 -0.0095  0.0165  ...  0.26500  532.9282  2.4995    7.8400   \n",
       " 430   1.5383 -0.0268 -0.0010  ...  0.11955  534.9382  2.3874    8.4500   \n",
       " 431   1.4688  0.0147 -0.0100  ...  0.11955  510.2164  2.4690  280.0000   \n",
       " 432   1.4925  0.0203 -0.0109  ...  0.11955  531.4355  2.4873    9.5000   \n",
       " 433   1.5356  0.0278 -0.0019  ...  0.11955  534.8673  2.2965    6.5699   \n",
       " 434   1.4398 -0.0075 -0.0055  ...  0.08710  536.0973  2.0779    9.0200   \n",
       " 435   1.4501  0.0011 -0.0140  ...  0.11955  532.0082  2.2994    6.8900   \n",
       " 436   1.5270  0.0066 -0.0124  ...  0.08770  530.6027  2.3970    7.1100   \n",
       " 437   1.4266  0.0181 -0.0118  ...  0.19850  536.8282  2.1262    6.8800   \n",
       " 438   1.3787  0.0025 -0.0085  ...  0.11955  365.3818  2.5672  137.8500   \n",
       " 439   1.5533  0.0196 -0.0030  ...  0.14320  384.6536  2.5236  287.1900   \n",
       " 440   1.5760  0.0272  0.0009  ...  0.05800  564.5500  2.0868  454.5600   \n",
       " 442   1.4117  0.0089 -0.0093  ...  0.11955  537.0273  1.4573    7.5900   \n",
       " 443   1.4607  0.0155  0.0093  ...  0.17180  335.5982  2.6271  125.6600   \n",
       " 444   1.4164  0.0006  0.0056  ...  0.11955  531.3936  2.3207   10.1900   \n",
       " 445   1.5604  0.0054  0.0034  ...  0.08840  536.1555  1.8940    9.4100   \n",
       " 446   1.4794  0.0083  0.0005  ...  0.08540  529.4873  2.4094    7.7600   \n",
       " 447   1.5130  0.0011  0.0026  ...  0.10740  530.2100  2.2897   10.4000   \n",
       " 449   1.5766 -0.0104  0.0042  ...  0.08770  536.4400  1.4805    8.3300   \n",
       " 450   1.6004 -0.0024  0.0056  ...  0.20880  530.5864  2.4746    9.9600   \n",
       " 451   1.5261  0.0019  0.0034  ...  0.08770  533.0191  2.0856   10.4301   \n",
       " 452   1.4337 -0.0006  0.0082  ...  0.08770  533.0191  2.0856   10.4301   \n",
       " 453   1.4873  0.0000  0.0031  ...  0.08770  533.0191  2.0856   10.4301   \n",
       " 454   1.4396  0.0043  0.0064  ...  0.11955  532.3800  1.9613    7.1400   \n",
       " 455   1.4110 -0.0016  0.0129  ...  0.14040  535.1664  2.0973    7.1900   \n",
       " 456   1.4482 -0.0321  0.0048  ...  0.33990  532.5227  1.9491    7.1800   \n",
       " 457   1.4035  0.0020  0.0027  ...  0.13460  534.3400  2.0440    9.1700   \n",
       " 458   1.5448 -0.0160  0.0016  ...  0.11955  533.3100  2.3610    9.1600   \n",
       " 459   1.5458 -0.0107 -0.0048  ...  0.13010  531.4773  2.3788    8.9500   \n",
       " 460   1.4499  0.0212 -0.0027  ...  0.11955  535.0200  2.4763    7.9900   \n",
       " 461   1.3732  0.0314  0.0041  ...  0.21640  534.6536  1.6344    6.4500   \n",
       " 462   1.5477  0.0145 -0.0032  ...  0.09150  528.1664  2.0671   10.1500   \n",
       " 463   1.4680 -0.0330 -0.0022  ...  0.09990  533.3109  2.2646   10.3700   \n",
       " 464   1.4116 -0.0041  0.0025  ...  0.12380  534.3600  2.3676    8.1000   \n",
       " 465   1.4526  0.0383 -0.0115  ...  0.11955  534.5873  2.3512    9.0200   \n",
       " 466   1.5446 -0.0034  0.0034  ...  0.23580  538.3864  2.2819  234.7800   \n",
       " 467   1.4868 -0.0325  0.0003  ...  0.08770  532.7609  1.9518    7.9901   \n",
       " 468   1.3750  0.0022  0.0193  ...  0.11955  529.3827  1.9833    8.3900   \n",
       " 469   1.3647  0.0106  0.0039  ...  0.21150  538.7818  1.7819    5.3200   \n",
       " 470   1.5545 -0.0370 -0.0001  ...  0.11955  531.4127  2.1005   11.1100   \n",
       " 471   1.5067 -0.0081  0.0037  ...  0.11955  536.5600  1.5473    7.6800   \n",
       " 472   1.4412  0.0051 -0.0004  ...  0.11955  534.1073  2.0960    7.3000   \n",
       " 473   1.4144 -0.0061 -0.0072  ...  0.30600  533.0927  2.4479    8.4600   \n",
       " 474   1.4738 -0.0045  0.0017  ...  0.11955  531.2018  2.0455    5.5500   \n",
       " 475   1.5216 -0.0257  0.0020  ...  0.06880  539.0582  2.0084    5.4100   \n",
       " 476   1.4633  0.0204 -0.0066  ...  0.23320  533.9609  2.2365    7.5700   \n",
       " 477   1.4441  0.0086 -0.0027  ...  0.11955  533.4455  2.2724    9.2300   \n",
       " 478   1.4487 -0.0145  0.0083  ...  0.11955  532.5227  1.9491    7.1800   \n",
       " 479   1.4964  0.0204  0.0133  ...  0.08770  535.3327  2.0886    9.3700   \n",
       " 480   1.5320 -0.0005 -0.0051  ...  0.13020  533.3591  2.3759    9.7500   \n",
       " 481   1.4411  0.0238 -0.0183  ...  0.11955  531.4409  2.3579    7.6900   \n",
       " 482   1.4909 -0.0034 -0.0088  ...  0.11955  530.7018  2.5277   10.1700   \n",
       " 483   1.4102  0.0308 -0.0119  ...  0.11955  534.1400  2.4975    7.8000   \n",
       " 484   1.3539  0.0088 -0.0022  ...  0.11955  537.5418  1.7735    7.9300   \n",
       " 485   1.5448 -0.0088  0.0057  ...  0.11955  539.0582  2.0084    5.4100   \n",
       " 486   1.5713 -0.0125 -0.0065  ...  0.11955  535.5609  2.0970    8.4000   \n",
       " 487   1.5975  0.0104 -0.0095  ...  0.19650  535.1664  2.0973    7.1900   \n",
       " 488   1.4537 -0.0275  0.0002  ...  0.23670  535.2218  2.2168    6.7100   \n",
       " 489   1.4413  0.0136  0.0083  ...  0.12740  533.7936  2.3170    9.1400   \n",
       " 490   1.5171 -0.0154 -0.0165  ...  0.11955  535.6864  2.2420    6.5500   \n",
       " 491   1.3322  0.0256  0.0073  ...  0.08770  535.9227  1.9110    8.6500   \n",
       " 492   1.5150 -0.0114 -0.0075  ...  0.31890  533.1246  2.1672    9.4300   \n",
       " 493   1.3552  0.0024 -0.0150  ...  0.11955  536.1264  1.8394    7.2700   \n",
       " 494   1.3947  0.0018 -0.0125  ...  0.11955  532.7718  2.3951    7.6400   \n",
       " 496   1.4547 -0.0106  0.0088  ...  0.13220  535.3318  2.0870    7.7400   \n",
       " 497   1.3784  0.0166  0.0083  ...  0.11955  328.4664  2.6883  127.5100   \n",
       " 498   1.4699  0.0080  0.0058  ...  0.11955  529.7355  2.0133    8.8200   \n",
       " 499   1.4560  0.0142 -0.0041  ...  0.11955  532.4136  2.3575   10.3700   \n",
       " 500   1.4192  0.0247 -0.0041  ...  0.11955  529.3827  1.9833    8.3900   \n",
       " 501   1.4207  0.0016 -0.0056  ...  0.15390  533.6555  2.2472    8.0800   \n",
       " 502   1.4902 -0.0013  0.0017  ...  0.11955  531.3900  2.1855    9.8200   \n",
       " 503   1.4711 -0.0106  0.0065  ...  0.11955  536.8291  2.2289    7.0500   \n",
       " 504   1.5188 -0.0109 -0.0005  ...  0.23130  535.1664  2.0973    7.1900   \n",
       " 505   1.5634 -0.0059 -0.0029  ...  0.11955  532.3055  2.0377    8.2400   \n",
       " 506   1.4699  0.0112 -0.0112  ...  0.11955  536.1964  2.1583    7.2100   \n",
       " 507   1.4161  0.0209 -0.0095  ...  0.09930  536.8282  2.3113    6.8100   \n",
       " 509   1.5299 -0.0037 -0.0037  ...  0.08770  533.1246  2.1672    9.4300   \n",
       " 510   1.4469  0.0208 -0.0032  ...  0.16440  513.0527  1.8366  426.6900   \n",
       " 511   1.5094  0.0039  0.0059  ...  0.16890  533.5409  2.0568    9.1100   \n",
       " 512   1.5032  0.0266 -0.0081  ...  0.11955  534.6782  2.3952   10.1800   \n",
       " 513   1.4119 -0.0031  0.0117  ...  0.11955  535.5236  2.0048    8.5800   \n",
       " 514   1.3761  0.0108 -0.0013  ...  0.11890  535.3318  2.0870    7.7400   \n",
       " 515   1.4710  0.0154 -0.0039  ...  0.11955  533.0191  2.0856   10.4301   \n",
       " 516   1.5015 -0.0097  0.0133  ...  0.11610  533.8791  1.9980    8.7400   \n",
       " 517   1.4805 -0.0073  0.0025  ...  0.04860  538.8482  2.2732    7.0900   \n",
       " 519   1.3973  0.0348 -0.0170  ...  0.24290  534.7291  2.4644    8.8800   \n",
       " 520   1.5460 -0.0149 -0.0024  ...  0.10790  534.5173  1.5036   10.3000   \n",
       " 521   1.5514 -0.0275 -0.0001  ...  0.08770  539.0582  2.0084    5.4100   \n",
       " 522   1.5700  0.0178 -0.0002  ...  0.11955  535.1664  2.0973    7.1900   \n",
       " 523   1.5877 -0.0046 -0.0013  ...  0.14930  531.3473  2.0767    9.2100   \n",
       " 524   1.4510  0.0041  0.0021  ...  0.20870  570.9627  2.1059  445.3200   \n",
       " 525   1.5755 -0.0009 -0.0027  ...  0.04390  534.3400  2.0440    9.1700   \n",
       " 526   1.5602 -0.0006 -0.0044  ...  0.11955  531.4073  2.0161    8.0500   \n",
       " 527   1.5274  0.0075  0.0044  ...  0.16030  533.0809  2.0118    7.8400   \n",
       " 528   1.5288 -0.0144 -0.0046  ...  0.11955  533.4936  1.9555    8.2000   \n",
       " 529   1.4619  0.0189 -0.0060  ...  0.24550  534.6555  2.0338    6.9300   \n",
       " 530   1.4357  0.0089  0.0052  ...  0.26980  533.2291  2.2587    8.2800   \n",
       " 531   1.5527  0.0119 -0.0082  ...  0.11955  531.9209  2.2497    7.9900   \n",
       " 532   1.4588 -0.0143  0.0017  ...  0.11955  537.4100  2.1437    5.1700   \n",
       " 533   1.5973 -0.0534 -0.0284  ...  0.11955  530.8673  2.3145    9.8500   \n",
       " 534   1.5525 -0.0078 -0.0005  ...  0.11955  535.8409  2.2266    6.3700   \n",
       " 535   1.4509  0.0028 -0.0059  ...  0.15930  535.0791  2.1879    8.1500   \n",
       " 536   1.4810  0.0191 -0.0029  ...  0.16580  535.1218  1.9541    7.2400   \n",
       " 537   1.3906  0.0047 -0.0069  ...  0.11955  586.9145  2.1612  451.6900   \n",
       " 538   1.5301 -0.0063  0.0039  ...  0.11955  533.0000  2.3950    8.4200   \n",
       " 539   1.5495 -0.0180 -0.0218  ...  0.11955  533.3336  2.0324    8.0900   \n",
       " 540   1.5657 -0.0079  0.0028  ...  0.11955  533.2618  2.2691   10.5600   \n",
       " 541   1.4511 -0.0018  0.0013  ...  0.11955  532.7309  1.8891    8.4000   \n",
       " 542   1.5391  0.0086 -0.0063  ...  0.11955  532.7427  2.3382    9.1300   \n",
       " 543   1.5166 -0.0088  0.0049  ...  0.18700  512.4609  1.5211  426.8500   \n",
       " 544   1.5308  0.0055 -0.0062  ...  0.11955  532.5182  2.0621    8.8700   \n",
       " 545   1.5219 -0.0022 -0.0003  ...  0.08920  537.4627  2.1325    7.4800   \n",
       " 546   1.5168 -0.0097  0.0009  ...  0.16530  535.0918  2.0927    6.3600   \n",
       " 547   1.6053 -0.0017 -0.0049  ...  0.16260  533.7682  1.9722    5.6201   \n",
       " 548   1.5190 -0.0053 -0.0124  ...  0.10040  531.0854  2.0685    9.9300   \n",
       " 549   1.5886 -0.0044  0.0092  ...  0.08060  529.4909  2.0437   10.6600   \n",
       " 550   1.4863 -0.0064 -0.0059  ...  0.11410  534.1364  2.3033    9.1400   \n",
       " 551   1.4790 -0.0041  0.0095  ...  0.11955  533.1246  2.1672    9.4300   \n",
       " 552   1.5520  0.0175  0.0027  ...  0.11380  533.7700  2.0384    6.6600   \n",
       " 553   1.4103 -0.0089  0.0061  ...  0.11955  533.5727  2.1470   10.2100   \n",
       " 554   1.4862  0.0014 -0.0056  ...  0.11955  533.7791  1.6068    9.3100   \n",
       " 555   1.5828  0.0091 -0.0045  ...  0.11955  535.1664  2.0973    7.1900   \n",
       " 556   1.5870 -0.0191  0.0064  ...  0.11955  532.1109  2.0349    9.4900   \n",
       " 557   1.5267  0.0228  0.0068  ...  0.11955  589.5082  2.0367  452.5400   \n",
       " 558   1.4616 -0.0100  0.0037  ...  0.11955  536.2118  1.9617    9.6400   \n",
       " 559   1.5006 -0.0321 -0.0087  ...  0.16410  536.1500  1.9941    8.7800   \n",
       " 560   1.5211  0.0206 -0.0159  ...  0.11955  533.0327  2.0001    9.3000   \n",
       " 561   1.5740 -0.0116 -0.0051  ...  0.08770  532.5646  2.7395   13.3500   \n",
       " 562   1.4899  0.0005  0.0055  ...  0.10410  532.5646  2.7395   13.3500   \n",
       " 563   1.5643  0.0263 -0.0021  ...  0.11955  532.6046  2.1100    9.9200   \n",
       " 564   1.3701 -0.0081  0.0013  ...  0.11955  529.8882  1.9955    8.8800   \n",
       " 565   1.4866  0.0018 -0.0018  ...  0.16160  531.4073  2.0161    8.0500   \n",
       " 566   1.5613 -0.0028  0.0008  ...  0.11955  533.5555  2.0417    6.0800   \n",
       " 567   1.4234 -0.0020  0.0027  ...  0.11955  534.7282  1.6257    7.0000   \n",
       " 568   1.5607  0.0138  0.0039  ...  0.08770  532.8245  1.9063   10.0099   \n",
       " 569   1.4330  0.0388  0.0167  ...  0.11955  531.6773  1.9737    7.5800   \n",
       " 570   1.5624 -0.0159  0.0013  ...  0.11955  531.8827  2.0572    8.3100   \n",
       " 571   1.4413 -0.0206  0.0151  ...  0.08770  534.0555  2.0153    8.5800   \n",
       " 572   1.5996 -0.0095 -0.0072  ...  0.09900  534.3355  2.2662    9.0300   \n",
       " 573   1.4922  0.0195  0.0003  ...  0.12940  536.4800  1.9240    8.9800   \n",
       " 574   1.5835 -0.0236  0.0044  ...  0.08770  532.5646  2.7395   13.3500   \n",
       " 575   1.4838  0.0000 -0.0074  ...  0.11955  536.6973  1.5839    8.1400   \n",
       " 577   1.5731  0.0076  0.0040  ...  0.08770  531.2927  1.9943   11.3300   \n",
       " 578   1.4981  0.0022 -0.0020  ...  0.11955  531.1664  2.0013   11.1700   \n",
       " 579   1.3725  0.0335  0.0001  ...  0.13670  531.5855  2.3460    9.0100   \n",
       " 580   1.3976 -0.0022 -0.0064  ...  0.07620  533.3591  2.3759    9.7500   \n",
       " 581   1.4268  0.0127 -0.0005  ...  0.20480  532.4464  2.3405    9.2700   \n",
       " 582   1.4794  0.0042  0.0089  ...  0.15980  533.3945  2.3192    8.1900   \n",
       " 584   1.5415  0.0055 -0.0208  ...  0.09510  538.5400  2.0768    5.3000   \n",
       " 585   1.5859  0.0286 -0.0035  ...  0.08770  532.5646  2.7395   13.3500   \n",
       " 586   1.5071 -0.0085 -0.0068  ...  0.11955  534.2982  1.9700    5.7900   \n",
       " 587   1.4773  0.0028  0.0101  ...  0.09170  534.0555  2.0153    8.5800   \n",
       " 588   1.5470 -0.0021  0.0030  ...  0.12750  534.6764  2.0007    8.0900   \n",
       " 589   1.4661 -0.0057  0.0036  ...  0.11955  534.6764  2.0007    8.0900   \n",
       " 590   1.5157 -0.0158  0.0008  ...  0.15200  532.9764  2.3909   10.9800   \n",
       " 591   1.5055  0.0354 -0.0032  ...  0.09330  531.4073  2.0161    8.0500   \n",
       " 592   1.4527  0.0000 -0.0055  ...  0.13090  536.0000  2.0571    7.9100   \n",
       " 593   1.4713  0.0384  0.0011  ...  0.08770  532.5336  2.2723    8.0000   \n",
       " 594   1.4817  0.0030  0.0049  ...  0.11955  531.4073  2.0161    8.0500   \n",
       " 595   1.5186  0.0171  0.0043  ...  0.07090  532.7527  2.4554    8.6900   \n",
       " 596   1.4309  0.0455 -0.0186  ...  0.16180  536.0573  1.8813    8.4200   \n",
       " 597   1.4043  0.0069 -0.0041  ...  0.11955  531.4073  2.0161    8.0500   \n",
       " 598   1.4708  0.0121 -0.0155  ...  0.16540  533.9782  2.3725    7.7900   \n",
       " 599   1.5477  0.0197  0.0057  ...  0.08770  535.0918  2.0927    6.3600   \n",
       " 600   1.3698  0.0141  0.0040  ...  0.11955  533.5309  1.9940    8.0100   \n",
       " 602   1.4868 -0.0089  0.0142  ...  0.11955  538.0664  2.1359    8.2300   \n",
       " 603   1.4207  0.0101 -0.0269  ...  0.11955  528.7891  2.0351   11.1500   \n",
       " 604   1.3946 -0.0019  0.0094  ...  0.11955  533.1082  2.1742    9.6500   \n",
       " 606   1.4791 -0.0117 -0.0053  ...  0.12380  535.1145  2.1058    8.9000   \n",
       " 607   1.4444 -0.0014  0.0001  ...  0.11955  532.0600  2.1892    7.8800   \n",
       " 608   1.4522 -0.0099 -0.0020  ...  0.15630  532.9918  2.0121    9.2599   \n",
       " 609   1.4209 -0.0262  0.0018  ...  0.08390  532.7609  1.9518    7.9901   \n",
       " 610   1.3843 -0.0010 -0.0004  ...  0.11955  533.0873  2.0606    8.6700   \n",
       " 611   1.4750  0.0248 -0.0097  ...  0.17940  531.0854  2.0685    9.9300   \n",
       " 612   1.5334  0.0098  0.0010  ...  0.14460  531.4291  2.1392    9.3200   \n",
       " 613   1.4373 -0.0027  0.0034  ...  0.13540  530.0355  2.0373    7.8500   \n",
       " 614   1.3949  0.0084 -0.0031  ...  0.11955  533.4727  2.0774   10.6300   \n",
       " 615   1.4635  0.0101 -0.0075  ...  0.11955  529.1382  2.1146    8.8300   \n",
       " 616   1.5227  0.0088 -0.0164  ...  0.15220  534.9636  2.1236    7.5400   \n",
       " 617   1.4402  0.0035 -0.0001  ...  0.11955  530.8864  2.0458    8.4600   \n",
       " 618   1.4701 -0.0181 -0.0010  ...  0.14580  533.5636  2.0273    7.3300   \n",
       " 619   1.4470 -0.0093 -0.0009  ...  0.11955  534.9282  2.0004    8.1700   \n",
       " 620   1.4256 -0.0125  0.0031  ...  0.11955  510.2891  2.0217  435.5700   \n",
       " 621   1.4002  0.0153  0.0065  ...  0.11955  532.8618  1.9972    9.0600   \n",
       " 622   1.4385 -0.0171  0.0070  ...  0.08770  532.2100  2.2089    8.2200   \n",
       " 623   1.3949  0.0065 -0.0018  ...  0.08770  533.4755  2.1902    9.0800   \n",
       " 624   1.5301 -0.0023  0.0020  ...  0.20200  538.5400  2.0768    5.3000   \n",
       " 625   1.5455 -0.0140 -0.0039  ...  0.08770  531.3891  2.0882    8.0800   \n",
       " 626   1.4647 -0.0212  0.0009  ...  0.09160  535.4364  2.0862    6.2300   \n",
       " 627   1.4262 -0.0209  0.0020  ...  0.22620  531.4073  2.0161    8.0500   \n",
       " 628   1.4849 -0.0072 -0.0033  ...  0.17340  533.7309  2.1724    8.3500   \n",
       " 629   1.5549 -0.0130  0.0074  ...  0.11955  531.2355  2.0715    9.7200   \n",
       " 630   1.5097 -0.0023  0.0045  ...  0.09990  539.2554  1.6494    5.2200   \n",
       " 631   1.4867  0.0014  0.0092  ...  0.10640  535.2109  2.0871    7.4500   \n",
       " 632   1.4819 -0.0153  0.0129  ...  0.08770  533.6900  2.0464    8.8900   \n",
       " 633   1.4464 -0.0081 -0.0037  ...  0.11955  532.9591  2.0687    7.1700   \n",
       " 635   1.3720 -0.0005  0.0052  ...  0.11955  534.1891  2.0627    7.7200   \n",
       " 636   1.5782 -0.0092  0.0054  ...  0.11955  535.8736  1.5932    8.8500   \n",
       " 637   1.4872 -0.0065  0.0066  ...  0.20200  536.4400  2.0978    6.7800   \n",
       " 638   1.4449 -0.0099  0.0037  ...  0.17330  536.6582  2.1390    5.9300   \n",
       " 639   1.5516 -0.0179  0.0036  ...  0.11550  533.9782  2.3449    6.0000   \n",
       " 640   1.4266 -0.0146 -0.0023  ...  0.11955  532.5291  2.0436    6.8100   \n",
       " 641   1.5456 -0.0208  0.0168  ...  0.21170  533.1246  2.1672    9.4300   \n",
       " 642   1.3575 -0.0252 -0.0028  ...  0.11955  535.9100  2.0366    7.3800   \n",
       " 643   1.5283 -0.0027  0.0095  ...  0.11320  529.3191  2.0905   11.2500   \n",
       " 644   1.4403 -0.0169  0.0213  ...  0.12280  537.1036  1.9971    7.0000   \n",
       " 645   1.4670 -0.0088 -0.0010  ...  0.21740  533.0691  1.9841    8.4900   \n",
       " 646   1.3984 -0.0108  0.0100  ...  0.22360  534.8727  2.3094    6.1600   \n",
       " 647   1.5166 -0.0118 -0.0060  ...  0.12720  530.5964  2.0289   12.0600   \n",
       " 648   1.5316 -0.0214 -0.0073  ...  0.16040  531.0918  2.0267   10.6300   \n",
       " 649   1.4955 -0.0222  0.0132  ...  0.11955  534.8518  2.1990    8.5200   \n",
       " 650   1.5089 -0.0131  0.0210  ...  0.08770  531.0854  2.0685    9.9300   \n",
       " 651   1.4149 -0.0045  0.0085  ...  0.11955  532.5764  2.0424   11.1000   \n",
       " 652   1.4829 -0.0072  0.0114  ...  0.11070  531.0854  2.0685    9.9300   \n",
       " 653   1.3785 -0.0026  0.0055  ...  0.03060  535.4364  2.0862    6.2300   \n",
       " 654   1.4479  0.0073  0.0083  ...  0.10690  534.1318  2.0039    8.2900   \n",
       " 655   1.4821 -0.0171  0.0073  ...  0.08770  533.0691  1.9841    8.4900   \n",
       " 656   1.3969  0.0116  0.0016  ...  0.11955  534.9473  2.3269    7.3500   \n",
       " 657   1.5869 -0.0344 -0.0019  ...  0.14600  532.0036  2.0531    8.1400   \n",
       " 658   1.4012 -0.0112  0.0029  ...  0.11955  534.6764  2.0007    8.0900   \n",
       " 659   1.4473 -0.0207  0.0196  ...  0.11955  532.2100  2.2089    8.2200   \n",
       " 660   1.4679 -0.0022  0.0098  ...  0.05350  536.5882  2.0221    6.9500   \n",
       " 661   1.4895 -0.0393 -0.0032  ...  0.11955  533.7327  2.2848    6.8700   \n",
       " 662   1.4950 -0.0097 -0.0054  ...  0.11955  534.8336  2.1127    6.3800   \n",
       " 663   1.3857 -0.0027  0.0031  ...  0.11955  536.0336  2.2003    7.3900   \n",
       " 664   1.4786 -0.0179  0.0085  ...  0.09300  538.9091  2.0618    5.4100   \n",
       " 665   1.3877  0.0149  0.0131  ...  0.13890  533.9636  2.0317    8.6500   \n",
       " 666   1.4975 -0.0248  0.0135  ...  0.20340  532.5336  2.2723    8.0000   \n",
       " 667   1.4890 -0.0058 -0.0026  ...  0.09050  538.9091  2.0618    5.4100   \n",
       " 668   1.5095 -0.0272 -0.0044  ...  0.15020  532.4936  2.1201    8.0200   \n",
       " 669   1.4830 -0.0328  0.0048  ...  0.13650  530.7118  2.0223    9.2900   \n",
       " 670   1.5227 -0.0207  0.0192  ...  0.11955  535.4364  2.0862    6.2300   \n",
       " 671   1.4548  0.0086  0.0061  ...  0.11955  532.6964  2.0363    9.3200   \n",
       " 672   1.5277 -0.0239  0.0149  ...  0.08770  532.5646  2.7395   13.3500   \n",
       " 673   1.4611 -0.0088  0.0121  ...  0.21040  536.7600  2.1073    6.8300   \n",
       " 674   1.3898 -0.0229  0.0191  ...  0.08130  537.1036  1.9971    7.0000   \n",
       " 675   1.4429 -0.0293  0.0013  ...  0.02260  535.4364  2.0862    6.2300   \n",
       " 676   1.4252 -0.0221  0.0046  ...  0.11955  533.6900  2.0464    8.8900   \n",
       " 677   1.4805  0.0203 -0.0178  ...  0.11955  534.0618  2.1214    7.9400   \n",
       " 678   1.5092 -0.0133 -0.0014  ...  0.06660  530.9555  2.1422    8.2100   \n",
       " 679   1.3725 -0.0152  0.0169  ...  0.11955  487.6227  2.3371  435.2600   \n",
       " 680   1.4349  0.0062  0.0084  ...  0.11955  532.4227  2.2358    8.1000   \n",
       " 681   1.4671 -0.0140  0.0142  ...  0.07650  539.2554  1.6494    5.2200   \n",
       " 682   1.5182 -0.0163  0.0034  ...  0.11955  535.2718  1.6985    7.3600   \n",
       " 683   1.3700 -0.0234 -0.0043  ...  0.11955  536.1436  2.2328    7.1500   \n",
       " 684   1.4623  0.0030 -0.0017  ...  0.11955  535.4846  2.1608    7.4300   \n",
       " 685   1.4341 -0.0101 -0.0072  ...  0.11955  529.5782  2.0500   11.3000   \n",
       " 686   1.5034  0.0083 -0.0002  ...  0.11955  529.3191  2.0905   11.2500   \n",
       " 687   1.5291 -0.0110 -0.0094  ...  0.11955  534.3236  1.9533    9.1000   \n",
       " 688   1.3636 -0.0236 -0.0121  ...  0.12500  533.7836  2.0600    8.7800   \n",
       " 689   1.4243  0.0076 -0.0058  ...  0.11955  532.1318  2.3355    9.3701   \n",
       " 690   1.4643  0.0047  0.0039  ...  0.08770  532.2100  2.2089    8.2200   \n",
       " 691   1.4853 -0.0186 -0.0069  ...  0.15470  532.7591  2.1800    8.9800   \n",
       " 692   1.4644  0.0049 -0.0020  ...  0.21390  530.7909  2.0698   10.5500   \n",
       " 693   1.4601  0.0047 -0.0072  ...  0.17730  537.1036  1.9971    7.0000   \n",
       " 694   1.5099 -0.0135  0.0035  ...  0.08770  532.5646  2.7395   13.3500   \n",
       " 695   1.4130 -0.0167  0.0224  ...  0.18630  531.4073  2.0161    8.0500   \n",
       " 696   1.4063 -0.0106  0.0145  ...  0.11955  534.6764  2.0007    8.0900   \n",
       " 697   1.4165  0.0148  0.0309  ...  0.11955  532.7591  2.1800    8.9800   \n",
       " 698   1.4194 -0.0027  0.0217  ...  0.14450  533.0691  1.9841    8.4900   \n",
       " 699   1.4020 -0.0170 -0.0026  ...  0.11990  531.3891  2.0882    8.0800   \n",
       " 700   1.4443  0.0005  0.0022  ...  0.18000  530.8818  2.1238    9.1900   \n",
       " 701   1.4721  0.0008  0.0077  ...  0.22300  538.5400  2.0768    5.3000   \n",
       " 702   1.4602 -0.0061 -0.0043  ...  0.21610  533.5555  2.0417    6.0800   \n",
       " 703   1.3807 -0.0119  0.0126  ...  0.67290  538.0000  2.0030    4.9400   \n",
       " 704   1.4416  0.0135  0.0051  ...  0.08770  531.4073  2.0161    8.0500   \n",
       " 705   1.5214 -0.0037 -0.0055  ...  0.08770  538.5400  2.0768    5.3000   \n",
       " 706   1.5126 -0.0133  0.0132  ...  0.09450  537.1036  1.9971    7.0000   \n",
       " 707   1.4667 -0.0472 -0.0077  ...  0.13720  533.0691  1.9841    8.4900   \n",
       " 708   1.4924 -0.0185  0.0057  ...  0.08770  531.4073  2.0161    8.0500   \n",
       " 710   1.4922  0.0045 -0.0087  ...  0.18040  533.0691  1.9841    8.4900   \n",
       " 711   1.3923  0.0061  0.0091  ...  0.14120  533.6900  2.0464    8.8900   \n",
       " 712   1.4112 -0.0156  0.0174  ...  0.14720  531.0854  2.0685    9.9300   \n",
       " 713   1.4333  0.0022 -0.0045  ...  0.35440  530.9718  2.1158   10.7100   \n",
       " 714   1.4063 -0.0101  0.0022  ...  0.34820  535.5664  1.9829    7.0200   \n",
       " 715   1.4125 -0.0199  0.0024  ...  0.28770  530.8818  2.1238    9.1900   \n",
       " 716   1.4918 -0.0068  0.0005  ...  0.31730  534.9391  2.1006   10.4600   \n",
       " 717   1.4674  0.0161  0.0005  ...  0.11955  533.0700  2.0591    7.5800   \n",
       " 718   1.4271 -0.0136  0.0197  ...  0.11955  531.0854  2.0685    9.9300   \n",
       " 719   1.4586 -0.0011  0.0056  ...  0.08770  532.9600  1.9280   10.2300   \n",
       " 720   1.4374 -0.0128 -0.0064  ...  0.11640  538.0000  2.0030    4.9400   \n",
       " 721   1.4115 -0.0130 -0.0022  ...  0.03090  535.5664  1.9829    7.0200   \n",
       " 722   1.4906 -0.0061 -0.0084  ...  0.08770  537.1036  1.9971    7.0000   \n",
       " 723   1.4874 -0.0016 -0.0119  ...  0.08360  530.8818  2.1238    9.1900   \n",
       " 724   1.4401 -0.0214 -0.0023  ...  0.08770  536.0100  2.1350    9.5601   \n",
       " 725   1.4968  0.0063 -0.0024  ...  0.24570  536.6173  1.9323    6.6200   \n",
       " 726   1.4476  0.0169  0.0025  ...  0.08770  533.9782  2.3449    6.0000   \n",
       " 727   1.4436 -0.0053 -0.0014  ...  0.21470  533.9782  2.3449    6.0000   \n",
       " 728   1.4583  0.0144  0.0015  ...  0.20490  532.5646  2.7395   13.3500   \n",
       " 729   1.5453 -0.0101  0.0023  ...  0.08770  530.8818  2.1238    9.1900   \n",
       " 730   1.4285 -0.0222 -0.0006  ...  0.06920  534.2818  2.2407    8.5400   \n",
       " 731   1.5251  0.0061 -0.0002  ...  0.29970  534.9900  2.0051    8.5400   \n",
       " 732   1.5031  0.0145 -0.0005  ...  0.11955  530.8818  2.1238    9.1900   \n",
       " 733   1.5251  0.0468 -0.0010  ...  0.12320  534.5745  1.9791    9.7000   \n",
       " 734   1.4645  0.0148  0.0016  ...  0.21350  535.5664  1.9829    7.0200   \n",
       " 735   1.4621  0.0084 -0.0021  ...  0.11955  533.8954  2.0083    8.7100   \n",
       " 736   1.4231 -0.0196 -0.0041  ...  0.22560  534.1500  2.3534    8.4200   \n",
       " 737   1.5131 -0.0105  0.0057  ...  0.08770  534.1500  2.3534    8.4200   \n",
       " 738   1.4903 -0.0064 -0.0028  ...  0.08770  534.1500  2.3534    8.4200   \n",
       " 739   1.5271 -0.0070  0.0069  ...  0.08770  534.2327  2.2409    5.8400   \n",
       " 740   1.4883  0.0109  0.0157  ...  0.08770  535.8591  2.3048    8.5500   \n",
       " 741   1.4355 -0.0153 -0.0035  ...  0.13290  530.8573  2.2566    9.1100   \n",
       " 742   1.4652 -0.0142  0.0150  ...  0.08770  535.8591  2.3048    8.5500   \n",
       " 743   1.4446 -0.0016 -0.0055  ...  0.09960  532.1391  2.3488    9.4400   \n",
       " 744   1.5059 -0.0084 -0.0024  ...  0.18310  533.2609  2.3318    6.5500   \n",
       " 745   1.5753 -0.0111 -0.0029  ...  0.12530  535.0991  2.1541    9.0500   \n",
       " 746   1.4855 -0.0025  0.0147  ...  0.23080  535.9764  2.6756    8.5300   \n",
       " 747   1.5527 -0.0216  0.0013  ...  0.19240  534.9691  2.3278    9.1000   \n",
       " 748   1.5160 -0.0041  0.0084  ...  0.12170  530.1873  2.2967   11.2800   \n",
       " 749   1.5343 -0.0013  0.0035  ...  0.07360  535.3618  2.3768    9.7000   \n",
       " 750   1.5361  0.0188 -0.0002  ...  0.08770  536.6882  2.1806    6.5800   \n",
       " 751   1.4187  0.0012  0.0004  ...  0.05050  534.5309  2.2417    6.3700   \n",
       " 752   1.5629 -0.0261 -0.0034  ...  0.09210  531.0464  2.3151    5.5500   \n",
       " 753   1.4814  0.0135 -0.0121  ...  0.08770  532.5646  2.7395   13.3500   \n",
       " 754   1.5284 -0.0136 -0.0090  ...  0.10490  531.9818  2.4070    6.3400   \n",
       " 755   1.4763  0.0012 -0.0127  ...  0.08700  532.3936  2.2964   10.0600   \n",
       " 756   1.5165 -0.0228  0.0101  ...  0.08770  536.0100  2.1350    9.5601   \n",
       " 757   1.5079  0.0031 -0.0105  ...  0.06980  530.8818  2.1238    9.1900   \n",
       " 758   1.4955  0.0236 -0.0040  ...  0.08770  531.2564  2.3999   11.9300   \n",
       " 759   1.3835  0.0042 -0.0087  ...  0.15160  537.8600  2.4783    5.7700   \n",
       " 760   1.4206 -0.0052  0.0010  ...  0.10230  532.5973  2.3271    9.0200   \n",
       " 761   1.4898  0.0200 -0.0153  ...  0.05060  535.9764  2.6756    8.5300   \n",
       " 762   1.4405  0.0019  0.0027  ...  0.08770  536.6509  2.4087    5.8600   \n",
       " 763   1.3602 -0.0153  0.0032  ...  0.08770  535.3036  2.3086    8.3000   \n",
       " 764   1.4372 -0.0113 -0.0075  ...  0.08020  533.9909  2.2169    8.9100   \n",
       " 765   1.4754  0.0089 -0.0046  ...  0.08130  536.0100  2.1350    9.5601   \n",
       " 766   1.4714  0.0065  0.0147  ...  0.08770  531.2564  2.3999   11.9300   \n",
       " 767   1.5245 -0.0076 -0.0050  ...  0.08770  536.6882  2.1806    6.5800   \n",
       " 768   1.4434 -0.0183 -0.0101  ...  0.09940  534.1500  2.3534    8.4200   \n",
       " 769   1.4801 -0.0301 -0.0123  ...  0.16960  533.2545  2.3218    7.0699   \n",
       " 770   1.5106  0.0072 -0.0022  ...  0.07000  531.2564  2.3999   11.9300   \n",
       " 771   1.5849 -0.0076 -0.0069  ...  0.17490  535.5664  1.9829    7.0200   \n",
       " 772   1.4346  0.0051  0.0084  ...  0.14950  536.6509  2.4087    5.8600   \n",
       " 773   1.4432  0.0085 -0.0151  ...  0.22330  533.6400  2.3592   10.5200   \n",
       " 774   1.4374 -0.0117 -0.0066  ...  0.15160  535.0509  1.6984    7.7100   \n",
       " 775   1.3619 -0.0228  0.0057  ...  0.08770  533.6536  2.1648   10.3700   \n",
       " 776   1.4723 -0.0093 -0.0039  ...  0.06220  535.7755  2.2816   10.5800   \n",
       " 777   1.4080 -0.0032  0.0049  ...  0.25300  538.4991  2.2135    8.7200   \n",
       " 778   1.4323 -0.0180  0.0090  ...  0.08770  538.4991  2.2135    8.7200   \n",
       " 779   1.4250 -0.0210 -0.0018  ...  0.08770  536.6509  2.4087    5.8600   \n",
       " 780   1.3420  0.0008  0.0022  ...  0.11955  534.0891  2.1926    9.8100   \n",
       " 781   1.4692 -0.0216 -0.0056  ...  0.18780  532.5573  2.1761   13.3500   \n",
       " 782   1.4418 -0.0096  0.0006  ...  0.08770  535.4918  2.2717    6.5600   \n",
       " 783   1.4279 -0.0212 -0.0178  ...  0.08770  536.6509  2.4087    5.8600   \n",
       " 784   1.4169 -0.0103 -0.0171  ...  0.08770  533.3882  2.3461    8.6300   \n",
       " 785   1.3596 -0.0138 -0.0025  ...  0.12540  535.6282  2.2411    9.1100   \n",
       " 786   1.3386 -0.0006  0.0060  ...  0.08770  534.1500  2.3534    8.4200   \n",
       " 787   1.4050 -0.0120  0.0055  ...  0.17150  535.6282  2.2411    9.1100   \n",
       " 788   1.3675  0.0038  0.0007  ...  0.19190  532.8364  1.6423   11.8100   \n",
       " 789   1.4209  0.0035  0.0107  ...  0.12870  534.1482  1.6633   10.9200   \n",
       " 790   1.4122 -0.0058  0.0057  ...  0.08770  537.5200  1.7312   10.4400   \n",
       " 791   1.3886  0.0107  0.0077  ...  0.12640  540.1218  1.7997    6.8700   \n",
       " 792   1.3735 -0.0023  0.0086  ...  0.08770  540.1218  1.7997    6.8700   \n",
       " 793   1.3920 -0.0044 -0.0165  ...  0.08770  534.1500  2.3534    8.4200   \n",
       " 794   1.4221 -0.0098 -0.0166  ...  0.04200  536.6882  2.1806    6.5800   \n",
       " 796   1.3819 -0.0041  0.0036  ...  0.12700  529.9273  2.3342    9.5000   \n",
       " 798   1.5366  0.0091  0.0057  ...  0.18080  533.3382  2.4033    9.4300   \n",
       " 799   1.4588  0.0067  0.0055  ...  0.13750  532.2855  2.2181    6.8600   \n",
       " 800   1.4149 -0.0092 -0.0008  ...  0.18030  535.0509  1.6984    7.7100   \n",
       " 801   1.3794 -0.0052 -0.0035  ...  0.11955  536.4418  1.9814    7.6801   \n",
       " 802   1.5093 -0.0022  0.0126  ...  0.08770  534.1482  1.6633   10.9200   \n",
       " 803   1.4547  0.0007  0.0159  ...  0.17310  534.1482  1.6633   10.9200   \n",
       " 804   1.3500  0.0002  0.0023  ...  0.08770  536.6509  2.4087    5.8600   \n",
       " 805   1.4480 -0.0233  0.0087  ...  0.10690  529.6845  2.3500    9.0300   \n",
       " 806   1.4056  0.0177  0.0153  ...  0.13730  534.1482  1.6633   10.9200   \n",
       " 807   1.4745 -0.0053  0.0080  ...  0.08580  538.4991  2.2135    8.7200   \n",
       " 808   1.4070  0.0053  0.0047  ...  0.14570  538.4991  2.2135    8.7200   \n",
       " 809   1.4700 -0.0283 -0.0086  ...  0.11900  534.1482  1.6633   10.9200   \n",
       " 810   1.3640  0.0035  0.0057  ...  0.09990  533.3382  2.4033    9.4300   \n",
       " 811   1.4005  0.0032  0.0055  ...  0.18630  536.6509  2.4087    5.8600   \n",
       " 812   1.3958  0.0024  0.0053  ...  0.08770  535.6282  2.2411    9.1100   \n",
       " 813   1.3761  0.0023  0.0032  ...  0.22350  534.8373  2.4114    9.9100   \n",
       " 814   1.5136 -0.0090  0.0129  ...  0.11120  533.6364  2.2904   13.0900   \n",
       " 815   1.4339 -0.0124 -0.0078  ...  0.08770  534.6691  2.1795    9.5099   \n",
       " 816   1.3753  0.0079  0.0145  ...  0.08770  536.6509  2.4087    5.8600   \n",
       " 817   1.4353 -0.0001  0.0018  ...  0.13060  532.6945  2.2425   11.1500   \n",
       " 818   1.3355 -0.0115 -0.0209  ...  0.17960  534.1482  1.6633   10.9200   \n",
       " 819   1.4266  0.0126  0.0075  ...  0.22460  534.1482  1.6633   10.9200   \n",
       " 820   1.4379  0.0053  0.0079  ...  0.19050  535.6282  2.2411    9.1100   \n",
       " 821   1.4440 -0.0079 -0.0076  ...  0.16260  532.5954  2.1606   10.5900   \n",
       " 822   1.4875  0.0052  0.0003  ...  0.12040  533.3882  2.3461    8.6300   \n",
       " 823   1.3754  0.0003 -0.0028  ...  0.08770  532.8364  1.6423   11.8100   \n",
       " 824   1.4854 -0.0006  0.0089  ...  0.10210  534.1245  2.1586    9.9700   \n",
       " 825   1.4579 -0.0021  0.0059  ...  0.14090  535.0509  1.6984    7.7100   \n",
       " 827   1.4008 -0.0014  0.0132  ...  0.09680  534.1482  1.6633   10.9200   \n",
       " 828   1.4889 -0.0047 -0.0091  ...  0.19060  532.9891  1.7784    7.5000   \n",
       " 829   1.5173  0.0069  0.0075  ...  0.11890  533.3882  2.3461    8.6300   \n",
       " 830   1.5300 -0.0279 -0.0040  ...  0.22590  534.1482  1.6633   10.9200   \n",
       " 832   1.3896  0.0138  0.0000  ...  0.18320  534.1482  1.6633   10.9200   \n",
       " 833   1.4108 -0.0046 -0.0024  ...  0.08770  534.1482  1.6633   10.9200   \n",
       " 834   1.5094 -0.0046  0.0121  ...  0.08770  530.6746  2.0193   10.8000   \n",
       " 835   1.3940 -0.0073  0.0006  ...  0.27290  529.7436  2.2313    7.1900   \n",
       " 836   1.4517  0.0069  0.0094  ...  0.15730  530.6746  2.0193   10.8000   \n",
       " 837   1.4798  0.0046  0.0181  ...  0.21680  534.6127  2.2455    9.5700   \n",
       " 838   1.4563  0.0075  0.0031  ...  0.08770  529.7436  2.2313    7.1900   \n",
       " 839   1.4696  0.0081  0.0063  ...  0.08770  534.1482  1.6633   10.9200   \n",
       " 840   1.5209  0.0097  0.0106  ...  0.19390  532.2636  2.2625    8.3200   \n",
       " 841   1.4055 -0.0134 -0.0135  ...  0.22160  534.1482  1.6633   10.9200   \n",
       " 842   1.4997  0.0027  0.0056  ...  0.15400  532.0164  2.3658   13.4900   \n",
       " 843   1.4175 -0.0144 -0.0119  ...  0.18940  536.4418  2.3416    6.6600   \n",
       " 844   1.5172 -0.0135  0.0070  ...  0.08040  536.6727  1.7324    7.4600   \n",
       " 845   1.4959  0.0137  0.0050  ...  0.27560  533.3382  2.4033    9.4300   \n",
       " 846   1.6063 -0.0020  0.0066  ...  0.22660  532.9764  2.4327   13.4400   \n",
       " 847   1.5568  0.0107  0.0070  ...  0.19280  530.6746  2.0193   10.8000   \n",
       " 848   1.4914 -0.0023  0.0157  ...  0.28780  530.6746  2.0193   10.8000   \n",
       " 849   1.4612 -0.0052 -0.0130  ...  0.08770  530.6746  2.0193   10.8000   \n",
       " 850   1.3964  0.0065 -0.0080  ...  0.25940  532.5954  2.1606   10.5900   \n",
       " 851   1.4106 -0.0181 -0.0130  ...  0.35120  534.7818  2.0324    8.7000   \n",
       " 852   1.5189 -0.0090  0.0096  ...  0.09530  534.1482  1.6633   10.9200   \n",
       " 853   1.4698 -0.0104  0.0075  ...  0.29130  532.1636  1.7199    8.9800   \n",
       " 854   1.4788  0.0124  0.0147  ...  0.10030  535.0509  1.6984    7.7100   \n",
       " 855   1.4282  0.0154  0.0120  ...  0.10200  534.1482  1.6633   10.9200   \n",
       " 856   1.5590 -0.0032  0.0135  ...  0.10040  531.6791  2.0502    6.5900   \n",
       " 857   1.4302  0.0058 -0.0060  ...  0.08770  532.5954  2.1606   10.5900   \n",
       " 858   1.4446 -0.0109  0.0057  ...  0.16230  533.3709  2.1470    9.3400   \n",
       " 859   1.3331 -0.0126  0.0076  ...  0.07070  532.0164  2.3658   13.4900   \n",
       " 860   1.3997  0.0062 -0.0063  ...  0.08770  532.1736  2.4452   10.8300   \n",
       " 861   1.3740 -0.0021 -0.0017  ...  0.13290  538.5400  2.0768    5.3000   \n",
       " 862   1.3333 -0.0054  0.0045  ...  0.15660  529.9855  2.2416   10.0600   \n",
       " 863   1.4964 -0.0099  0.0061  ...  0.11955  532.8382  2.0005   11.2401   \n",
       " 864   1.3179 -0.0089 -0.0084  ...  0.13470  532.9700  2.2248   10.0900   \n",
       " 865   1.5070  0.0090 -0.0038  ...  0.15040  534.1482  1.6633   10.9200   \n",
       " 866   1.4691 -0.0088  0.0002  ...  0.17460  534.6127  2.2455    9.5700   \n",
       " 867   1.4951 -0.0049 -0.0086  ...  0.09900  526.5336  2.6194    9.6000   \n",
       " 868   1.4463 -0.0031 -0.0050  ...  0.41270  532.5954  2.1606   10.5900   \n",
       " 869   1.4445 -0.0154  0.0125  ...  0.09930  530.5682  2.1417   10.8700   \n",
       " 870   1.4873  0.0121  0.0121  ...  0.13350  538.4991  2.2135    8.7200   \n",
       " 872   1.4542  0.0142 -0.0064  ...  0.26450  534.3127  2.1819    7.9300   \n",
       " 873   1.5084 -0.0118 -0.0023  ...  0.24840  531.8455  2.2544    8.6400   \n",
       " 874   1.5540 -0.0049  0.0129  ...  0.51860  531.5127  2.2373   13.8100   \n",
       " 875   1.5250 -0.0100  0.0010  ...  0.08770  535.9636  2.2026    6.7700   \n",
       " 876   1.5793  0.0021  0.0037  ...  0.30690  532.4645  2.2512    8.2500   \n",
       " 877   1.4080  0.0130  0.0044  ...  0.12200  531.8455  2.2544    8.6400   \n",
       " 878   1.4411  0.0096  0.0090  ...  0.18740  532.0973  2.1448    9.6000   \n",
       " 879   1.4108  0.0095 -0.0026  ...  0.06060  531.8455  2.2544    8.6400   \n",
       " 880   1.5960  0.0023  0.0084  ...  0.21710  535.1345  2.1394    5.7300   \n",
       " 881   1.4500 -0.0053  0.0005  ...  0.08770  531.3346  2.2421    8.4900   \n",
       " 882   1.3746  0.0012  0.0107  ...  0.08770  532.0973  2.1448    9.6000   \n",
       " 883   1.4919 -0.0001 -0.0018  ...  0.14700  528.7918  2.5088   10.3300   \n",
       " 884   1.6411 -0.0068 -0.0033  ...  0.11280  531.8455  2.2544    8.6400   \n",
       " 885   1.5762  0.0028 -0.0066  ...  0.04770  533.3073  2.3490   10.1900   \n",
       " 886   1.4335  0.0046  0.0042  ...  0.08770  534.6682  2.0435    9.4300   \n",
       " 887   1.4503  0.0149  0.0010  ...  0.08770  533.2873  2.3473   10.9300   \n",
       " 888   1.3434  0.0030  0.0000  ...  0.14290  530.5682  2.1417   10.8700   \n",
       " 889   1.4334  0.0082 -0.0027  ...  0.20920  504.9173  2.2656  438.8700   \n",
       " 890   1.4532  0.0049 -0.0048  ...  0.08770  533.2873  2.3473   10.9300   \n",
       " 891   1.4017  0.0003  0.0041  ...  0.12260  532.5027  2.4053    7.5400   \n",
       " 892   1.5475 -0.0087 -0.0093  ...  0.08770  529.2209  1.9102    7.9800   \n",
       " 893   1.3481 -0.0025  0.0150  ...  0.38660  535.9636  2.2026    6.7700   \n",
       " 894   1.4521 -0.0124 -0.0099  ...  0.08770  532.0973  2.1448    9.6000   \n",
       " 895   1.4113  0.0108  0.0062  ...  0.08770  530.5682  2.1417   10.8700   \n",
       " 896   1.5241 -0.0025 -0.0096  ...  0.13210  533.8955  2.3306    9.6400   \n",
       " 897   1.3882  0.0008  0.0050  ...  0.08770  530.6746  2.0193   10.8000   \n",
       " 898   1.4463  0.0073  0.0046  ...  0.08770  531.3346  2.2421    8.4900   \n",
       " 899   1.4958  0.0004  0.0037  ...  0.11370  531.0009  2.0786   11.1500   \n",
       " 900   1.3623 -0.0076  0.0112  ...  0.11550  526.5336  2.6194    9.6000   \n",
       " 901   1.3824 -0.0001 -0.0050  ...  0.38220  526.5336  2.6194    9.6000   \n",
       " 902   1.3907  0.0017 -0.0045  ...  0.17630  528.7918  2.5088   10.3300   \n",
       " 903   1.3947  0.0075 -0.0146  ...  0.20530  533.2873  2.3473   10.9300   \n",
       " 904   1.5196  0.0183 -0.0008  ...  0.08770  530.5682  2.1417   10.8700   \n",
       " 905   1.4684  0.0013  0.0044  ...  0.10440  526.5336  2.6194    9.6000   \n",
       " 906   1.3487  0.0050  0.0006  ...  0.14300  504.9173  2.2656  438.8700   \n",
       " 907   1.4678  0.0024 -0.0017  ...  0.25980  532.0973  2.1448    9.6000   \n",
       " 908   1.5424  0.0197  0.0034  ...  0.14830  504.9173  2.2656  438.8700   \n",
       " 909   1.4418 -0.0002  0.0028  ...  0.10230  532.0973  2.1448    9.6000   \n",
       " 910   1.5794 -0.0034 -0.0127  ...  0.11955  532.4618  2.2425   10.2700   \n",
       " 911   1.4706  0.0152 -0.0055  ...  0.11955  530.9809  2.3259    9.9400   \n",
       " 912   1.3527  0.0198  0.0076  ...  0.08770  530.5682  2.1417   10.8700   \n",
       " 913   1.3713  0.0102  0.0058  ...  0.20820  532.8227  2.3242   11.7600   \n",
       " 915   1.4526 -0.0175 -0.0028  ...  0.08770  526.6064  1.9339   10.0400   \n",
       " 916   1.3952 -0.0198  0.0026  ...  0.24870  528.0491  1.9226    8.6800   \n",
       " 917   1.3885 -0.0096 -0.0038  ...  0.08770  533.2855  2.0362    6.3200   \n",
       " 918   1.4756 -0.0025  0.0025  ...  0.08770  533.2855  2.0362    6.3200   \n",
       " 919   1.5947  0.0143 -0.0064  ...  0.56640  535.1345  2.1394    5.7300   \n",
       " 920   1.4646  0.0021  0.0059  ...  0.28400  536.4418  2.3416    6.6600   \n",
       " 921   1.5865 -0.0199  0.0021  ...  0.08940  529.5955  2.1619    8.0099   \n",
       " 922   1.4446 -0.0050 -0.0007  ...  0.21570  530.5682  2.1417   10.8700   \n",
       " 923   1.4406 -0.0265 -0.0033  ...  0.04930  532.9891  1.7784    7.5000   \n",
       " 925   1.5337  0.0090  0.0058  ...  0.08770  533.9718  2.5640    9.2800   \n",
       " 927   1.3603 -0.0031  0.0086  ...  0.08770  532.0973  2.1448    9.6000   \n",
       " 928   1.4493 -0.0194 -0.0018  ...  0.46980  534.0800  2.1203   10.3700   \n",
       " 930   1.4483 -0.0103  0.0151  ...  0.08770  530.9809  2.3259    9.9400   \n",
       " 931   1.5347 -0.0254  0.0247  ...  0.35230  532.8227  2.3242   11.7600   \n",
       " 932   1.3059  0.0013 -0.0045  ...  0.13290  533.2873  2.3473   10.9300   \n",
       " 933   1.4455 -0.0218 -0.0068  ...  0.08770  533.2873  2.3473   10.9300   \n",
       " 934   1.4478 -0.0017  0.0072  ...  0.22660  533.2855  2.0362    6.3200   \n",
       " 935   1.3813  0.0025  0.0028  ...  0.15000  532.0973  2.1448    9.6000   \n",
       " 936   1.6316 -0.0195 -0.0008  ...  0.20070  504.9173  2.2656  438.8700   \n",
       " 937   1.6383 -0.0154  0.0052  ...  0.07360  533.2873  2.3473   10.9300   \n",
       " 938   1.5569  0.0095  0.0007  ...  0.44580  530.5682  2.1417   10.8700   \n",
       " 939   1.5979 -0.0108 -0.0104  ...  0.18440  533.4773  2.2113    7.2800   \n",
       " 940   1.4802 -0.0038  0.0130  ...  0.08770  531.1709  2.2895   10.4400   \n",
       " 941   1.6157 -0.0112  0.0135  ...  0.08770  533.4773  2.2113    7.2800   \n",
       " 942   1.5401  0.0036 -0.0036  ...  0.05670  530.9809  2.3259    9.9400   \n",
       " 943   1.3824 -0.0070  0.0010  ...  0.33430  531.3346  2.2421    8.4900   \n",
       " 944   1.4226 -0.0130  0.0063  ...  0.08770  530.9809  2.3259    9.9400   \n",
       " 945   1.4910 -0.0159  0.0087  ...  0.14930  530.9809  2.3259    9.9400   \n",
       " 946   1.6057  0.0023  0.0070  ...  0.08770  532.5954  2.1606   10.5900   \n",
       " 947   1.5119 -0.0087  0.0051  ...  0.11340  530.3073  2.4103    8.0500   \n",
       " 948   1.4326  0.0138 -0.0096  ...  0.13670  534.9155  1.7591    9.0800   \n",
       " 949   1.5317 -0.0036 -0.0045  ...  0.08770  531.8455  2.2544    8.6400   \n",
       " 950   1.5410  0.0211 -0.0038  ...  0.24280  530.6746  2.0193   10.8000   \n",
       " 951   1.5498  0.0168  0.0055  ...  0.08770  533.4773  2.2113    7.2800   \n",
       " 952   1.5431 -0.0093  0.0105  ...  0.29460  532.8227  2.3242   11.7600   \n",
       " 953   1.5651 -0.0131 -0.0085  ...  0.14280  531.1709  2.2895   10.4400   \n",
       " 954   1.4570 -0.0068 -0.0070  ...  0.11830  531.7509  2.3526    8.0900   \n",
       " 955   1.5620  0.0069 -0.0021  ...  0.08770  530.3073  2.4103    8.0500   \n",
       " 956   1.4056  0.0013 -0.0020  ...  0.20560  504.9173  2.2656  438.8700   \n",
       " 957   1.4684 -0.0116 -0.0053  ...  0.30590  536.3264  2.2421    7.7000   \n",
       " 958   1.4938  0.0078  0.0010  ...  0.08770  531.8455  2.2544    8.6400   \n",
       " 959   1.5648  0.0153 -0.0165  ...  0.27080  533.3073  2.3490   10.1900   \n",
       " 960   1.5842 -0.0143  0.0049  ...  0.08770  533.4773  2.2113    7.2800   \n",
       " 961   1.3656 -0.0251  0.0018  ...  0.45810  531.3346  2.2421    8.4900   \n",
       " 962   1.6242  0.0327 -0.0061  ...  0.08770  530.9809  2.3259    9.9400   \n",
       " 963   1.5367  0.0285  0.0111  ...  0.27920  526.5336  2.6194    9.6000   \n",
       " 964   1.6045 -0.0086 -0.0037  ...  0.16020  530.9809  2.3259    9.9400   \n",
       " 965   1.4765  0.0103 -0.0049  ...  0.15790  533.2855  2.0362    6.3200   \n",
       " 966   1.6373 -0.0139  0.0062  ...  0.09840  533.7718  2.2619    5.9700   \n",
       " 967   1.6539 -0.0149 -0.0039  ...  0.15580  530.9809  2.3259    9.9400   \n",
       " 968   1.4078  0.0146 -0.0112  ...  0.08770  530.3073  2.4103    8.0500   \n",
       " 969   1.3786  0.0069  0.0023  ...  0.43630  530.3073  2.4103    8.0500   \n",
       " 970   1.4454  0.0177  0.0017  ...  0.08770  531.3346  2.2421    8.4900   \n",
       " 971   1.4314  0.0200  0.0062  ...  0.11955  530.9809  2.3259    9.9400   \n",
       " 972   1.4276  0.0121 -0.0051  ...  0.08800  531.1709  2.2895   10.4400   \n",
       " 973   1.4785  0.0279  0.0092  ...  0.08770  532.7764  2.3109    6.8700   \n",
       " 974   1.5922  0.0105 -0.0011  ...  0.09920  531.0918  2.4015   10.1800   \n",
       " 975   1.4461  0.0196 -0.0125  ...  0.08770  530.9809  2.3259    9.9400   \n",
       " 976   1.5400 -0.0071 -0.0095  ...  0.42880  533.9936  2.2318    8.2700   \n",
       " 977   1.4675 -0.0146  0.0005  ...  0.16650  535.9636  2.2026    6.7700   \n",
       " 978   1.4380 -0.0062 -0.0033  ...  0.12960  533.2855  2.0362    6.3200   \n",
       " 979   1.6564 -0.0135 -0.0002  ...  0.16080  531.2973  1.9279    9.1200   \n",
       " 980   1.6136 -0.0201  0.0005  ...  0.20080  532.7455  1.7255    7.0400   \n",
       " 981   1.5578  0.0110  0.0008  ...  0.08770  504.9173  2.2656  438.8700   \n",
       " 982   1.3885  0.0017 -0.0110  ...  0.08770  504.9173  2.2656  438.8700   \n",
       " 983   1.4947 -0.0003  0.0077  ...  0.08770  531.3346  2.2421    8.4900   \n",
       " 984   1.6202 -0.0158 -0.0065  ...  0.10010  529.8718  1.9408    6.3300   \n",
       " 985   1.5438  0.0014 -0.0057  ...  0.08770  532.1636  1.7199    8.9800   \n",
       " 986   1.4172  0.0285 -0.0078  ...  0.08770  532.8227  2.3242   11.7600   \n",
       " 987   1.5335  0.0158 -0.0095  ...  0.08770  530.9173  1.7153    7.3600   \n",
       " 988   1.3924 -0.0116 -0.0132  ...  0.08770  530.3073  2.4103    8.0500   \n",
       " 989   1.5215 -0.0164  0.0102  ...  0.08770  531.1982  2.2418   10.6300   \n",
       " 990   1.5698  0.0141  0.0111  ...  0.27940  533.3073  2.3490   10.1900   \n",
       " 991   1.4438  0.0120 -0.0083  ...  0.08770  533.3073  2.3490   10.1900   \n",
       " 992   1.6092 -0.0339 -0.0043  ...  0.18630  533.2873  2.3473   10.9300   \n",
       " 993   1.4323  0.0104 -0.0195  ...  0.08770  529.5955  2.1619    8.0099   \n",
       " 994   1.5017  0.0173  0.0111  ...  0.10990  504.9173  2.2656  438.8700   \n",
       " 995   1.4805  0.0123  0.0009  ...  0.08730  533.6364  2.2904   13.0900   \n",
       " 996   1.5950 -0.0163  0.0061  ...  0.08770  531.8455  2.2544    8.6400   \n",
       " 997   1.4587  0.0284 -0.0070  ...  0.17380  533.3073  2.3490   10.1900   \n",
       " 998   1.5136  0.0225 -0.0269  ...  0.08770  532.4564  1.9432   11.6500   \n",
       " 999   1.4193  0.0113 -0.0007  ...  0.08770  530.5682  2.1417   10.8700   \n",
       " 1000  1.4620 -0.0172 -0.0007  ...  0.08770  530.9809  2.3259    9.9400   \n",
       " 1001  1.4772  0.0283 -0.0072  ...  0.08510  532.0973  2.1448    9.6000   \n",
       " 1002  1.6147 -0.0132  0.0009  ...  0.08770  530.5682  2.1417   10.8700   \n",
       " 1003  1.5351  0.0039 -0.0046  ...  0.08770  530.9809  2.3259    9.9400   \n",
       " 1004  1.4726  0.0095 -0.0009  ...  0.16830  529.5955  2.1619    8.0099   \n",
       " 1005  1.4207  0.0086  0.0071  ...  0.13660  504.9173  2.2656  438.8700   \n",
       " 1006  1.5587  0.0031  0.0024  ...  0.15880  533.3073  2.3490   10.1900   \n",
       " 1007  1.5631  0.0084  0.0014  ...  0.17270  533.1418  2.0876   10.5800   \n",
       " 1008  1.3830  0.0305  0.0018  ...  0.13060  533.4773  2.2113    7.2800   \n",
       " 1009  1.5948  0.0077 -0.0078  ...  0.31010  530.3073  2.4103    8.0500   \n",
       " 1010  1.5332  0.0097  0.0014  ...  0.19440  533.3073  2.3490   10.1900   \n",
       " 1011  1.3561  0.0045 -0.0011  ...  0.11955  533.4773  2.2113    7.2800   \n",
       " 1012  1.5519 -0.0273  0.0063  ...  0.31670  531.1982  2.2418   10.6300   \n",
       " 1013  1.5314 -0.0241 -0.0076  ...  0.08770  529.8718  1.9408    6.3300   \n",
       " 1014  1.4823 -0.0012  0.0233  ...  0.08770  531.0518  1.8932    6.8000   \n",
       " 1015  1.4616  0.0106 -0.0046  ...  0.11955  530.9809  2.3259    9.9400   \n",
       " 1016  1.5714 -0.0073  0.0170  ...  0.08770  524.2318  2.0104  436.5200   \n",
       " 1017  1.5416 -0.0257  0.0041  ...  0.08770  533.2855  2.0362    6.3200   \n",
       " 1018  1.5264 -0.0130 -0.0192  ...  0.08770  533.9982  1.8821    7.4500   \n",
       " 1019  1.6255 -0.0172 -0.0049  ...  0.08770  531.1982  2.2418   10.6300   \n",
       " 1020  1.6187  0.0027  0.0077  ...  0.08770  530.3073  2.4103    8.0500   \n",
       " 1021  1.4328  0.0054  0.0105  ...  0.14510  529.5955  2.1619    8.0099   \n",
       " 1022  1.4459  0.0172  0.0028  ...  0.37320  529.8718  1.9408    6.3300   \n",
       " 1023  1.3376  0.0201 -0.0050  ...  0.08770  532.8227  2.3242   11.7600   \n",
       " 1024  1.3601  0.0031 -0.0080  ...  0.30610  529.5955  2.1619    8.0099   \n",
       " 1025  1.4465  0.0091 -0.0017  ...  0.19520  533.3391  1.9511    5.8500   \n",
       " 1026  1.4422 -0.0013  0.0031  ...  0.08770  529.5955  2.1619    8.0099   \n",
       " 1027  1.3650  0.0034 -0.0068  ...  0.08770  533.3073  2.3490   10.1900   \n",
       " 1028  1.5275  0.0159  0.0168  ...  0.13580  535.9636  2.2026    6.7700   \n",
       " 1030  1.4015  0.0095 -0.0070  ...  0.08770  528.7673  1.9809    6.9500   \n",
       " 1031  1.4090  0.0160  0.0018  ...  0.08770  530.1782  1.9155   10.7800   \n",
       " 1032  1.3418 -0.0112  0.0015  ...  0.08770  531.4064  2.0135    9.9600   \n",
       " 1033  1.4117  0.0087 -0.0108  ...  0.24050  529.8718  1.9408    6.3300   \n",
       " 1034  1.4590 -0.0074 -0.0060  ...  0.08770  530.1782  1.9155   10.7800   \n",
       " 1035  1.3682 -0.0110 -0.0085  ...  0.08770  530.1782  1.9155   10.7800   \n",
       " 1036  1.4726  0.0041  0.0060  ...  0.08770  529.5955  2.1619    8.0099   \n",
       " 1037  1.5149  0.0025 -0.0063  ...  0.20510  524.2318  2.0104  436.5200   \n",
       " 1038  1.5901 -0.0071  0.0127  ...  0.42070  531.0518  1.8932    6.8000   \n",
       " 1039  1.5341  0.0350 -0.0068  ...  0.08770  530.3073  2.4103    8.0500   \n",
       " 1040  1.5335 -0.0083 -0.0095  ...  0.08770  530.3073  2.4103    8.0500   \n",
       " 1041  1.6001  0.0105 -0.0099  ...  0.08770  529.5955  2.1619    8.0099   \n",
       " 1042  1.4035  0.0222 -0.0014  ...  0.36500  528.4045  1.9943    9.0100   \n",
       " 1043  1.4217  0.0029 -0.0119  ...  0.11955  529.8718  1.9408    6.3300   \n",
       " 1044  1.4615 -0.0109  0.0140  ...  0.08770  531.0518  1.8932    6.8000   \n",
       " 1045  1.4132 -0.0310  0.0069  ...  0.23130  532.8000  2.0925    6.2800   \n",
       " 1046  1.3824  0.0045 -0.0068  ...  0.22930  533.9718  1.9524    5.4400   \n",
       " 1047  1.3242  0.0130 -0.0102  ...  0.18670  533.9718  1.9524    5.4400   \n",
       " 1048  1.5745  0.0050  0.0006  ...  0.08770  533.2855  2.0362    6.3200   \n",
       " 1049  1.3910  0.0086 -0.0015  ...  0.19180  531.2973  1.9279    9.1200   \n",
       " 1050  1.4945  0.0195 -0.0120  ...  0.08770  532.4618  2.2425   10.2700   \n",
       " 1051  1.4939  0.0096  0.0071  ...  0.17440  533.9936  2.2318    8.2700   \n",
       " 1052  1.4320  0.0142 -0.0170  ...  0.08770  533.9982  1.8821    7.4500   \n",
       " 1053  1.4440  0.0159 -0.0135  ...  0.11960  529.8718  1.9408    6.3300   \n",
       " 1054  1.4200  0.0150 -0.0019  ...  0.21010  528.7673  1.9809    6.9500   \n",
       " 1055  1.4136  0.0129 -0.0125  ...  0.08770  529.5955  2.1619    8.0099   \n",
       " 1056  1.5293  0.0115 -0.0053  ...  0.11670  504.9173  2.2656  438.8700   \n",
       " 1057  1.4649  0.0077  0.0046  ...  0.08770  528.4045  1.9943    9.0100   \n",
       " 1058  1.5712 -0.0121  0.0051  ...  0.10750  532.7764  2.3109    6.8700   \n",
       " 1059  1.4675  0.0095  0.0052  ...  0.32770  533.9936  2.2318    8.2700   \n",
       " 1060  1.3704  0.0152 -0.0170  ...  0.14480  531.8727  2.1285    8.0800   \n",
       " 1061  1.4202  0.0150  0.0094  ...  0.08770  528.4045  1.9943    9.0100   \n",
       " 1063  1.3836  0.0135  0.0013  ...  0.08770  532.4564  1.9432   11.6500   \n",
       " 1064  1.6075  0.0095  0.0059  ...  0.08770  528.4045  1.9943    9.0100   \n",
       " 1065  1.4286  0.0113 -0.0051  ...  0.24860  530.3073  2.4103    8.0500   \n",
       " 1066  1.3499 -0.0009  0.0044  ...  0.08770  530.7036  2.1207    8.4000   \n",
       " 1067  1.4293  0.0199  0.0076  ...  0.29660  531.0918  2.4015   10.1800   \n",
       " 1068  1.4536  0.0192 -0.0152  ...  0.11955  532.6327  2.1085    7.7700   \n",
       " 1069  1.4243  0.0059 -0.0034  ...  0.19260  533.9718  1.9524    5.4400   \n",
       " 1070  1.3900  0.0302 -0.0112  ...  0.08770  529.6464  2.0320    5.8100   \n",
       " 1071  1.5688  0.0181  0.0180  ...  0.13590  530.7036  2.1207    8.4000   \n",
       " 1072  1.3209 -0.0040 -0.0013  ...  0.25920  530.3836  1.8367   10.3400   \n",
       " 1073  1.3496 -0.0018 -0.0094  ...  0.08770  528.7673  1.9809    6.9500   \n",
       " 1074  1.4234 -0.0179 -0.0066  ...  0.08770  531.0418  2.0433    9.8200   \n",
       " 1075  1.3825  0.0050  0.0018  ...  0.11955  524.2318  2.0104  436.5200   \n",
       " 1076  1.3524  0.0020 -0.0088  ...  0.16680  532.7118  2.1958    8.7000   \n",
       " 1077  1.4162  0.0020 -0.0136  ...  0.08770  533.9718  1.9524    5.4400   \n",
       " 1078  1.4105 -0.0262  0.0064  ...  0.19150  531.4173  2.1444    6.7200   \n",
       " 1079  1.3525  0.0313 -0.0088  ...  0.08770  531.0418  2.0433    9.8200   \n",
       " 1080  1.4286 -0.0143 -0.0175  ...  0.15610  532.0064  1.9363   10.8900   \n",
       " 1081  1.4182 -0.0029  0.0148  ...  0.08770  531.0364  2.1244    8.9700   \n",
       " 1082  1.4818 -0.0088 -0.0064  ...  0.05470  533.9718  1.9524    5.4400   \n",
       " 1083  1.3613  0.0006  0.0100  ...  0.08770  528.7673  1.9809    6.9500   \n",
       " 1084  1.3809  0.0013 -0.0124  ...  0.08770  531.0518  1.8932    6.8000   \n",
       " 1085  1.3573  0.0162 -0.0094  ...  0.28290  531.0364  2.1244    8.9700   \n",
       " 1086  1.4342  0.0168 -0.0010  ...  0.26030  531.9282  2.1096   10.4500   \n",
       " 1087  1.4575  0.0093 -0.0071  ...  0.08770  529.5955  2.1619    8.0099   \n",
       " 1088  1.6040 -0.0226  0.0072  ...  0.38160  533.9718  1.9524    5.4400   \n",
       " 1089  1.4368  0.0162 -0.0064  ...  0.17830  531.0418  2.0433    9.8200   \n",
       " 1090  1.3661  0.0113 -0.0261  ...  0.08770  528.4045  1.9943    9.0100   \n",
       " 1091  1.3825 -0.0062  0.0106  ...  0.23880  531.8136  2.0854    7.0100   \n",
       " 1092  1.4757 -0.0106 -0.0048  ...  0.49370  529.9445  1.9740    7.9100   \n",
       " 1093  1.4635  0.0004  0.0016  ...  0.08770  529.5182  2.1130   10.6500   \n",
       " 1094  1.4587 -0.0028  0.0044  ...  0.08770  529.9445  1.9740    7.9100   \n",
       " 1095  1.4007 -0.0133  0.0028  ...  0.16480  529.5182  2.1130   10.6500   \n",
       " 1096  1.3909  0.0053 -0.0055  ...  0.14300  530.2227  1.9126    9.3900   \n",
       " 1097  1.4145  0.0204 -0.0057  ...  0.16530  529.8718  1.9408    6.3300   \n",
       " 1098  1.3837  0.0024 -0.0026  ...  0.08770  529.6464  2.0320    5.8100   \n",
       " 1099  1.4381  0.0082 -0.0058  ...  0.08770  531.4591  1.9611    9.6600   \n",
       " 1100  1.6114  0.0484  0.0043  ...  0.08770  531.0364  2.1244    8.9700   \n",
       " 1101  1.5323  0.0171  0.0081  ...  0.12420  531.4591  1.9611    9.6600   \n",
       " 1102  1.3096  0.0185 -0.0057  ...  0.26850  530.9364  1.9355   12.1000   \n",
       " 1103  1.3728  0.0151 -0.0123  ...  0.24520  533.9936  2.2318    8.2700   \n",
       " 1104  1.4021 -0.0085 -0.0078  ...  0.30990  529.8718  1.9408    6.3300   \n",
       " 1105  1.3384  0.0336 -0.0136  ...  0.11160  524.4955  1.0912   11.1200   \n",
       " 1106  1.4004  0.0228  0.0141  ...  0.08770  524.2318  2.0104  436.5200   \n",
       " 1107  1.3917 -0.0103 -0.0018  ...  0.23140  531.0418  2.0433    9.8200   \n",
       " 1108  1.3782  0.0118  0.0067  ...  0.08770  530.2409  1.9614   11.5400   \n",
       " 1109  1.4912  0.0001 -0.0062  ...  0.17560  528.4045  1.9943    9.0100   \n",
       " 1110  1.4500  0.0063  0.0099  ...  0.19180  531.4591  1.9611    9.6600   \n",
       " 1111  1.3273 -0.0190  0.0009  ...  0.27440  530.3745  2.1745    7.9300   \n",
       " 1112  1.3849 -0.0060 -0.0005  ...  0.12140  531.4591  1.9611    9.6600   \n",
       " 1113  1.5826 -0.0089  0.0035  ...  0.08770  530.3836  1.8367   10.3400   \n",
       " 1114  1.3954  0.0246 -0.0117  ...  0.03050  530.2409  1.9614   11.5400   \n",
       " 1115  1.4629 -0.0164 -0.0114  ...  0.08770  528.7673  1.9809    6.9500   \n",
       " 1116  1.4511 -0.0017  0.0030  ...  0.08770  524.2318  2.0104  436.5200   \n",
       " 1117  1.4808 -0.0060 -0.0033  ...  0.12340  532.5954  2.1606   10.5900   \n",
       " 1118  1.3088  0.0105 -0.0059  ...  0.06690  531.4064  2.0135    9.9600   \n",
       " 1119  1.4303  0.0048 -0.0040  ...  0.08770  530.2409  1.9614   11.5400   \n",
       " 1120  1.4048  0.0266  0.0006  ...  0.08770  531.4591  1.9611    9.6600   \n",
       " 1121  1.4782  0.0069  0.0024  ...  0.08770  529.9445  1.9740    7.9100   \n",
       " 1122  1.4040 -0.0025 -0.0007  ...  0.25890  529.0200  1.9969    9.8200   \n",
       " 1123  1.4233 -0.0049  0.0021  ...  0.13150  504.9173  2.2656  438.8700   \n",
       " 1124  1.4078  0.0052  0.0004  ...  0.21970  532.0064  1.9363   10.8900   \n",
       " 1125  1.3813  0.0137 -0.0096  ...  0.16670  531.4064  2.0135    9.9600   \n",
       " 1126  1.4796  0.0004 -0.0042  ...  0.12440  528.4045  1.9943    9.0100   \n",
       " 1127  1.5456 -0.0285 -0.0166  ...  0.15570  531.4591  1.9611    9.6600   \n",
       " 1128  1.4854  0.0173 -0.0095  ...  0.19350  531.4064  2.0135    9.9600   \n",
       " 1129  1.4022  0.0209 -0.0027  ...  0.20170  532.6327  2.1085    7.7700   \n",
       " 1130  1.3608  0.0250 -0.0060  ...  0.08770  529.5955  2.1619    8.0099   \n",
       " 1131  1.4693  0.0022  0.0047  ...  0.08770  530.7036  2.1207    8.4000   \n",
       " 1132  1.4658 -0.0167 -0.0010  ...  0.16360  531.0009  1.8299   13.2800   \n",
       " 1133  1.5947 -0.0112  0.0025  ...  0.08770  529.9445  1.9740    7.9100   \n",
       " 1134  1.4669 -0.0080 -0.0173  ...  0.07600  530.2418  2.4302    9.7100   \n",
       " 1135  1.4048  0.0018  0.0049  ...  0.31960  528.7673  1.9809    6.9500   \n",
       " 1136  1.5927 -0.0227  0.0059  ...  0.08770  529.6464  2.0320    5.8100   \n",
       " 1137  1.4135  0.0283 -0.0065  ...  0.12000  531.0364  2.1244    8.9700   \n",
       " 1138  1.4259  0.0085 -0.0228  ...  0.30030  528.4045  1.9943    9.0100   \n",
       " 1139  1.4200  0.0070 -0.0077  ...  0.08770  530.5682  2.1417   10.8700   \n",
       " 1140  1.5319  0.0184  0.0038  ...  0.19810  530.7318  2.2877    9.6300   \n",
       " 1141  1.4411  0.0002 -0.0038  ...  0.24340  526.7564  2.2090    7.7700   \n",
       " 1142  1.4309  0.0088 -0.0239  ...  0.13770  528.7673  1.9809    6.9500   \n",
       " 1143  1.4224 -0.0042 -0.0021  ...  0.08770  528.7673  1.9809    6.9500   \n",
       " 1145  1.3789  0.0343 -0.0019  ...  0.24160  530.3364  2.0270    9.9200   \n",
       " 1146  1.3395 -0.0261 -0.0040  ...  0.16630  531.4173  2.1444    6.7200   \n",
       " 1147  1.3737 -0.0220  0.0163  ...  0.08770  530.7036  2.1207    8.4000   \n",
       " 1148  1.4205  0.0001  0.0063  ...  0.08770  529.6464  2.0320    5.8100   \n",
       " 1149  1.5263 -0.0028 -0.0001  ...  0.12040  532.0064  1.9363   10.8900   \n",
       " 1150  1.4102  0.0215 -0.0056  ...  0.28670  531.7491  1.9611    7.8700   \n",
       " 1152  1.4144  0.0033 -0.0035  ...  0.17690  530.2409  1.9614   11.5400   \n",
       " 1153  1.5366  0.0176 -0.0073  ...  0.16530  529.9445  1.9740    7.9100   \n",
       " 1154  1.3981 -0.0351 -0.0096  ...  0.08770  530.3364  2.0270    9.9200   \n",
       " 1155  1.4377 -0.0125 -0.0199  ...  0.08770  530.3364  2.0270    9.9200   \n",
       " 1156  1.4868  0.0005  0.0038  ...  0.20850  531.0418  2.0433    9.8200   \n",
       " 1157  1.4400  0.0108  0.0029  ...  0.08770  531.8136  2.0854    7.0100   \n",
       " 1158  1.5905 -0.0142 -0.0100  ...  0.13450  532.6327  2.1085    7.7700   \n",
       " 1159  1.3561  0.0014 -0.0073  ...  0.12580  531.7491  1.9611    7.8700   \n",
       " 1160  1.4033  0.0232 -0.0035  ...  0.08770  530.1782  1.9155   10.7800   \n",
       " 1161  1.3625 -0.0015  0.0163  ...  0.08770  528.7673  1.9809    6.9500   \n",
       " 1162  1.5535 -0.0061 -0.0117  ...  0.23740  530.2409  1.9614   11.5400   \n",
       " 1163  1.4080 -0.0138  0.0043  ...  0.08770  532.0064  1.9363   10.8900   \n",
       " 1164  1.5892 -0.0212  0.0061  ...  0.12970  529.9445  1.9740    7.9100   \n",
       " 1165  1.5110  0.0011 -0.0016  ...  0.25350  530.7036  2.1207    8.4000   \n",
       " 1166  1.3435  0.0071 -0.0019  ...  0.08770  529.5182  2.1130   10.6500   \n",
       " 1167  1.4053 -0.0151  0.0010  ...  0.15930  529.2673  2.0421    8.9400   \n",
       " 1168  1.4812 -0.0155 -0.0058  ...  0.19890  531.0009  1.8299   13.2800   \n",
       " 1169  1.4760 -0.0175 -0.0084  ...  0.40550  529.5182  2.0770    8.9900   \n",
       " 1170  1.3548 -0.0061  0.0015  ...  0.18030  530.3364  2.0270    9.9200   \n",
       " 1171  1.4476  0.0023  0.0014  ...  0.08770  529.9445  1.9740    7.9100   \n",
       " 1172  1.4306 -0.0163 -0.0253  ...  0.08770  529.6464  2.0320    5.8100   \n",
       " 1173  1.3644  0.0237 -0.0174  ...  0.10200  531.0364  2.1244    8.9700   \n",
       " 1174  1.5459 -0.0368 -0.0002  ...  0.19820  524.4955  1.0912   11.1200   \n",
       " 1175  1.5106 -0.0165  0.0063  ...  0.08770  524.4955  1.0912   11.1200   \n",
       " 1176  1.5289 -0.0273 -0.0003  ...  0.08770  528.0491  2.1169    8.9900   \n",
       " 1177  1.5712 -0.0119  0.0060  ...  0.14490  531.8136  2.0854    7.0100   \n",
       " 1178  1.4055  0.0244 -0.0074  ...  0.21150  532.3273  2.3967   10.5700   \n",
       " 1179  1.4382 -0.0182  0.0032  ...  0.08770  529.9445  1.9740    7.9100   \n",
       " 1180  1.3537  0.0134 -0.0039  ...  0.08770  529.9445  1.9740    7.9100   \n",
       " 1181  1.3929 -0.0004  0.0059  ...  0.20390  529.9445  1.9740    7.9100   \n",
       " 1182  1.4146 -0.0260 -0.0004  ...  0.08770  531.3036  1.9250    9.4500   \n",
       " 1183  1.4925  0.0483 -0.0020  ...  0.22650  531.5200  1.9170    9.7500   \n",
       " 1184  1.4634 -0.0005 -0.0077  ...  0.08770  528.5973  2.2200    6.2400   \n",
       " 1186  1.3792  0.0054  0.0039  ...  0.08770  531.8455  2.2544    8.6400   \n",
       " 1187  1.3406  0.0006 -0.0039  ...  0.11710  529.9445  1.9740    7.9100   \n",
       " 1188  1.3928  0.0361 -0.0141  ...  0.08770  532.0109  2.0679   11.3500   \n",
       " 1190  1.3791 -0.0304 -0.0077  ...  0.27740  531.5200  1.9170    9.7500   \n",
       " 1191  1.5016 -0.0011 -0.0014  ...  0.08770  530.3364  2.0270    9.9200   \n",
       " 1192  1.3253 -0.0167 -0.0085  ...  0.08770  532.1636  1.7199    8.9800   \n",
       " 1193  1.4462 -0.0354 -0.0029  ...  0.28100  533.9718  1.9524    5.4400   \n",
       " 1194  1.4359 -0.0068  0.0058  ...  0.08770  531.0364  2.1244    8.9700   \n",
       " 1195  1.5517 -0.0230 -0.0026  ...  0.08770  529.6464  2.0320    5.8100   \n",
       " 1196  1.5755 -0.0072  0.0087  ...  0.18770  531.0182  1.8610    9.2100   \n",
       " 1197  1.5323  0.0133  0.0066  ...  0.24780  530.5109  1.9739   12.0700   \n",
       " 1198  1.4002 -0.0151 -0.0006  ...  0.08770  529.6464  2.0320    5.8100   \n",
       " 1199  1.3892  0.0023  0.0042  ...  0.35950  531.4591  1.9611    9.6600   \n",
       " 1200  1.3979 -0.0118 -0.0013  ...  0.15580  531.0009  1.8299   13.2800   \n",
       " 1201  1.5299 -0.0100 -0.0007  ...  0.08770  530.5109  1.9739   12.0700   \n",
       " 1202  1.5022 -0.0034  0.0118  ...  0.13570  531.3036  1.9250    9.4500   \n",
       " 1203  1.4093 -0.0010 -0.0045  ...  0.08770  531.4591  1.9611    9.6600   \n",
       " 1204  1.3993  0.0018 -0.0116  ...  0.08770  531.8827  1.9109    6.9000   \n",
       " 1205  1.3523 -0.0015 -0.0012  ...  0.08770  528.4045  1.9943    9.0100   \n",
       " 1206  1.4088  0.0235  0.0120  ...  0.08770  531.5418  2.2320    7.5700   \n",
       " 1207  1.4573  0.0294 -0.0042  ...  0.08770  530.7036  2.1207    8.4000   \n",
       " 1208  1.3846  0.0078  0.0097  ...  0.08770  531.7491  1.9611    7.8700   \n",
       " 1209  1.3694 -0.0054 -0.0074  ...  0.11960  530.7036  2.1207    8.4000   \n",
       " 1210  1.3728  0.0162  0.0033  ...  0.12340  529.9445  1.9740    7.9100   \n",
       " 1212  1.6122 -0.0171  0.0078  ...  0.08770  531.5636  2.0805    9.0300   \n",
       " 1213  1.4741 -0.0142 -0.0060  ...  0.21410  531.8100  1.9298    7.9900   \n",
       " 1214  1.4989 -0.0041  0.0021  ...  0.29660  528.5973  2.2200    6.2400   \n",
       " 1215  1.5391  0.0090 -0.0074  ...  0.10320  531.1982  2.2418   10.6300   \n",
       " 1216  1.4711  0.0033 -0.0020  ...  0.08770  530.7091  1.8681    5.6400   \n",
       " 1217  1.4031 -0.0050 -0.0012  ...  0.08770  528.5973  2.2200    6.2400   \n",
       " 1218  1.4670 -0.0134 -0.0103  ...  0.08770  531.3445  2.0359    5.7100   \n",
       " 1219  1.3896  0.0021 -0.0012  ...  0.27860  531.4591  1.9611    9.6600   \n",
       " 1220  1.4844 -0.0288 -0.0029  ...  0.16130  528.7673  1.9809    6.9500   \n",
       " 1221  1.4708 -0.0085 -0.0010  ...  0.19750  527.6991  2.0119    8.4600   \n",
       " 1222  1.4192  0.0119  0.0016  ...  0.08770  528.5973  2.2200    6.2400   \n",
       " 1223  1.5380 -0.0097  0.0012  ...  0.20680  530.3364  2.0270    9.9200   \n",
       " 1224  1.4100  0.0274 -0.0040  ...  0.08770  528.5973  2.2200    6.2400   \n",
       " 1225  1.4419  0.0029 -0.0062  ...  0.08770  529.9445  1.9740    7.9100   \n",
       " 1226  1.4052  0.0079 -0.0173  ...  0.08770  531.3445  2.0359    5.7100   \n",
       " 1228  1.5361  0.0060 -0.0001  ...  0.08770  529.8991  2.0142    7.5600   \n",
       " 1229  1.3401  0.0182 -0.0085  ...  0.15960  531.1754  2.0396    8.0900   \n",
       " 1230  1.4752  0.0084  0.0009  ...  0.13990  524.4955  1.0912   11.1200   \n",
       " 1231  1.4536 -0.0110 -0.0035  ...  0.17310  530.7036  2.1207    8.4000   \n",
       " 1232  1.4826  0.0136  0.0009  ...  0.17070  524.4955  1.0912   11.1200   \n",
       " 1233  1.4645  0.0001  0.0067  ...  0.08770  524.4955  1.0912   11.1200   \n",
       " 1234  1.4845 -0.0037  0.0049  ...  0.23520  495.3682  1.7909  415.0300   \n",
       " 1235  1.4441 -0.0021  0.0095  ...  0.08770  532.0973  2.0850    7.7400   \n",
       " 1236  1.4827 -0.0238  0.0059  ...  0.28340  532.2427  1.9313    8.1700   \n",
       " 1237  1.4889 -0.0076 -0.0017  ...  0.06390  534.3145  1.0024  430.3700   \n",
       " 1239  1.4294  0.0138  0.0052  ...  0.02890  529.8991  2.0142    7.5600   \n",
       " 1240  1.4956 -0.0060  0.0038  ...  0.14200  531.0418  2.0433    9.8200   \n",
       " 1243  1.4051 -0.0204  0.0007  ...  0.27220  531.3445  2.0359    5.7100   \n",
       " 1244  1.4018 -0.0187  0.0009  ...  0.20260  458.8464  1.0936  271.7300   \n",
       " 1245  1.3439  0.0055  0.0000  ...  0.12160  531.5200  1.9170    9.7500   \n",
       " 1246  1.4115 -0.0318  0.0000  ...  0.08770  529.9445  1.9740    7.9100   \n",
       " 1247  1.5492  0.0102 -0.0056  ...  0.21960  513.8473  1.3216  445.8000   \n",
       " 1248  1.4630 -0.0293 -0.0005  ...  0.08770  528.5973  2.2200    6.2400   \n",
       " 1249  1.3823  0.0079  0.0102  ...  0.08770  532.6327  2.1085    7.7700   \n",
       " 1250  1.4653 -0.0204  0.0038  ...  0.08770  533.1809  1.0744    5.6500   \n",
       " 1251  1.3453 -0.0072 -0.0082  ...  0.19950  532.0973  2.0850    7.7400   \n",
       " 1252  1.4346  0.0062 -0.0107  ...  0.08770  524.4955  1.0912   11.1200   \n",
       " 1253  1.3697 -0.0085 -0.0114  ...  0.24550  532.0973  2.0850    7.7400   \n",
       " 1255  1.3582 -0.0364  0.0032  ...  0.29900  532.0973  2.0850    7.7400   \n",
       " 1256  1.4778  0.0095  0.0016  ...  0.08770  527.2555  0.9802    7.4900   \n",
       " 1257  1.4596 -0.0016  0.0284  ...  0.23870  532.0945  2.0071    5.8600   \n",
       " 1258  1.3956 -0.0075  0.0079  ...  0.05340  532.0973  2.0850    7.7400   \n",
       " 1259  1.3873 -0.0135 -0.0086  ...  0.25890  539.0691  1.3198  427.4200   \n",
       " 1260  1.3584 -0.0024 -0.0048  ...  0.07430  513.8473  1.3216  445.8000   \n",
       " 1261  1.4248  0.0101 -0.0009  ...  0.08770  532.6345  2.0657   10.1600   \n",
       " 1262  1.3762 -0.0206 -0.0104  ...  0.08770  524.4955  1.0912   11.1200   \n",
       " 1263  1.3897 -0.0091 -0.0024  ...  0.10250  531.4591  1.9611    9.6600   \n",
       " 1264  1.3857 -0.0088 -0.0032  ...  0.08770  530.9445  1.9567    8.0800   \n",
       " 1265  1.3089  0.0016 -0.0123  ...  0.06150  531.2836  1.8915    7.6200   \n",
       " 1266  1.4352 -0.0113 -0.0282  ...  0.09030  531.1754  2.0396    8.0900   \n",
       " 1267  1.3671 -0.0197  0.0056  ...  0.08770  529.5182  2.1130   10.6500   \n",
       " 1268  1.5132 -0.0217  0.0080  ...  0.20710  528.5973  2.2200    6.2400   \n",
       " 1269  1.4953 -0.0270  0.0033  ...  0.08770  530.0846  1.9812    9.0100   \n",
       " 1270  1.4028 -0.0108 -0.0033  ...  0.14530  528.5973  2.2200    6.2400   \n",
       " 1271  1.5666 -0.0115  0.0084  ...  0.08770  529.2564  1.8034   14.1200   \n",
       " 1272  1.5030 -0.0061  0.0009  ...  0.08770  531.3445  2.0359    5.7100   \n",
       " 1273  1.4639 -0.0123 -0.0173  ...  0.08770  532.3427  2.0841    8.1800   \n",
       " 1274  1.4794 -0.0019 -0.0014  ...  0.08770  514.4364  1.8660  431.7100   \n",
       " 1275  1.4732 -0.0175 -0.0096  ...  0.20890  532.1382  2.0329    9.0200   \n",
       " 1276  1.4711  0.0013 -0.0020  ...  0.20290  532.1382  2.0329    9.0200   \n",
       " 1277  1.4204 -0.0059 -0.0125  ...  0.11170  532.1382  2.0329    9.0200   \n",
       " 1278  1.5456 -0.0108  0.0094  ...  0.08770  531.7491  1.9611    7.8700   \n",
       " 1279  1.4617  0.0043  0.0098  ...  0.20130  530.6345  1.9733    7.6000   \n",
       " 1280  1.3578 -0.0065  0.0004  ...  0.08770  533.1809  1.0744    5.6500   \n",
       " 1281  1.5958  0.0112  0.0024  ...  0.24260  531.1754  2.0396    8.0900   \n",
       " 1282  1.4052 -0.0157 -0.0095  ...  0.19950  531.3445  2.0359    5.7100   \n",
       " 1283  1.3943  0.0076  0.0259  ...  0.34900  532.2427  1.9313    8.1700   \n",
       " 1284  1.4544  0.0071  0.0030  ...  0.39250  528.5973  2.2200    6.2400   \n",
       " 1285  1.4915 -0.0176  0.0054  ...  0.21400  528.7618  1.9743   10.0700   \n",
       " 1286  1.4133  0.0137 -0.0132  ...  0.68920  531.3091  1.9363   10.8500   \n",
       " 1287  1.5456 -0.0051  0.0062  ...  0.08770  532.3427  2.0841    8.1800   \n",
       " 1288  1.5240 -0.0091  0.0064  ...  0.08770  532.3427  2.0841    8.1800   \n",
       " 1289  1.5036  0.0035 -0.0028  ...  0.13320  532.0945  2.0071    5.8600   \n",
       " 1290  1.4711 -0.0133  0.0001  ...  0.08770  532.1382  2.0329    9.0200   \n",
       " 1291  1.6147 -0.0057  0.0084  ...  0.16770  514.4364  1.8660  431.7100   \n",
       " 1292  1.5305  0.0141  0.0187  ...  0.08770  531.1754  2.0396    8.0900   \n",
       " 1293  1.3979  0.0187  0.0196  ...  0.14840  531.1754  2.0396    8.0900   \n",
       " 1294  1.3571 -0.0165 -0.0007  ...  0.08770  532.1455  2.0581    9.8900   \n",
       " 1295  1.3557  0.0024 -0.0050  ...  0.11950  529.8991  2.0142    7.5600   \n",
       " 1296  1.3841 -0.0264  0.0064  ...  0.15730  532.0973  2.0850    7.7400   \n",
       " 1297  1.4477  0.0093  0.0079  ...  0.08770  528.0127  1.1073    9.0400   \n",
       " 1298  1.5418  0.0020  0.0020  ...  0.21740  532.2427  1.9313    8.1700   \n",
       " 1299  1.3642 -0.0032 -0.0035  ...  0.08770  529.8991  2.0142    7.5600   \n",
       " 1300  1.5095  0.0097 -0.0053  ...  0.08770  530.7036  2.1207    8.4000   \n",
       " 1301  1.5761 -0.0022 -0.0069  ...  0.08770  530.0846  1.9812    9.0100   \n",
       " 1304  1.4347  0.0006  0.0084  ...  0.08770  533.1809  1.0744    5.6500   \n",
       " 1305  1.3984 -0.0028 -0.0059  ...  0.08770  528.5973  2.2200    6.2400   \n",
       " 1306  1.4745 -0.0132  0.0054  ...  0.27780  530.2591  0.9847    8.2800   \n",
       " 1307  1.4317  0.0082  0.0168  ...  0.24170  531.3091  1.9363   10.8500   \n",
       " 1308  1.5675 -0.0010  0.0099  ...  0.14580  513.8473  1.3216  445.8000   \n",
       " 1309  1.4214  0.0070  0.0192  ...  0.20050  529.5182  2.1130   10.6500   \n",
       " 1310  1.5290  0.0014  0.0014  ...  0.10440  539.0691  1.3198  427.4200   \n",
       " 1311  1.3636  0.0191  0.0040  ...  0.15720  532.0945  2.0071    5.8600   \n",
       " 1312  1.4418  0.0076  0.0123  ...  0.13070  514.4364  1.8660  431.7100   \n",
       " 1313  1.5029 -0.0036  0.0052  ...  0.08770  532.0973  2.0850    7.7400   \n",
       " 1314  1.4245  0.0021  0.0018  ...  0.22260  531.3091  1.9363   10.8500   \n",
       " 1315  1.5257 -0.0009  0.0039  ...  0.08770  532.1073  1.8126    7.8300   \n",
       " 1316  1.5986 -0.0048 -0.0027  ...  0.10860  530.0846  1.9812    9.0100   \n",
       " 1317  1.4564 -0.0182 -0.0095  ...  0.08770  528.5973  2.2200    6.2400   \n",
       " 1318  1.5757 -0.0037  0.0039  ...  0.37940  530.0846  1.9812    9.0100   \n",
       " 1319  1.4656  0.0040 -0.0064  ...  0.18330  531.3091  1.9363   10.8500   \n",
       " 1320  1.4959  0.0007 -0.0135  ...  0.30090  532.2427  1.9313    8.1700   \n",
       " 1321  1.6193 -0.0127 -0.0164  ...  0.08770  529.2673  2.0421    8.9400   \n",
       " 1322  1.5480  0.0007  0.0039  ...  0.13660  530.2591  0.9847    8.2800   \n",
       " 1323  1.5407 -0.0073  0.0108  ...  0.10990  530.2591  0.9847    8.2800   \n",
       " 1326  1.4323  0.0145 -0.0137  ...  0.08770  530.2591  0.9847    8.2800   \n",
       " 1330  1.5507 -0.0077  0.0040  ...  0.08770  531.3091  1.9363   10.8500   \n",
       " 1331  1.4004 -0.0079 -0.0075  ...  0.14400  530.2591  0.9847    8.2800   \n",
       " 1332  1.3838 -0.0125  0.0073  ...  0.27960  531.1754  2.0396    8.0900   \n",
       " 1333  1.5840 -0.0058  0.0065  ...  0.08770  534.2564  1.3737   10.8700   \n",
       " 1334  1.3909 -0.0132  0.0099  ...  0.05370  528.5973  2.2200    6.2400   \n",
       " 1335  1.5576 -0.0098  0.0105  ...  0.14740  514.4364  1.8660  431.7100   \n",
       " 1336  1.5714  0.0076  0.0196  ...  0.17740  532.1382  2.0329    9.0200   \n",
       " 1337  1.5656 -0.0027  0.0092  ...  0.08770  513.8473  1.3216  445.8000   \n",
       " 1338  1.4319  0.0043  0.0123  ...  0.15300  514.4364  1.8660  431.7100   \n",
       " 1339  1.3533  0.0013  0.0064  ...  0.23010  532.1073  1.8126    7.8300   \n",
       " 1340  1.5139 -0.0127  0.0007  ...  0.17650  530.0846  1.9812    9.0100   \n",
       " 1341  1.5120 -0.0124  0.0090  ...  0.08770  532.8227  2.1206    7.9300   \n",
       " 1344  1.4424 -0.0242  0.0050  ...  0.08770  532.1073  1.8126    7.8300   \n",
       " 1345  1.3702 -0.0118  0.0067  ...  0.08770  532.3427  2.0841    8.1800   \n",
       " 1346  1.4159 -0.0068 -0.0073  ...  0.11290  531.1754  2.0396    8.0900   \n",
       " 1347  1.4931 -0.0106  0.0015  ...  0.08770  539.0691  1.3198  427.4200   \n",
       " 1348  1.5267 -0.0067 -0.0034  ...  0.08770  532.1073  1.8126    7.8300   \n",
       " 1349  1.3997  0.0008  0.0043  ...  0.15370  514.4364  1.8660  431.7100   \n",
       " 1350  1.3220  0.0145  0.0141  ...  0.08770  531.9873  2.0680   10.2500   \n",
       " 1351  1.5021 -0.0108 -0.0032  ...  0.13110  514.4364  1.8660  431.7100   \n",
       " 1352  1.4753 -0.0038  0.0090  ...  0.24770  531.9873  2.0680   10.2500   \n",
       " 1353  1.3985 -0.0297  0.0005  ...  0.06800  514.4364  1.8660  431.7100   \n",
       " 1354  1.4001 -0.0156  0.0048  ...  0.10500  532.8227  2.1206    7.9300   \n",
       " 1355  1.3601 -0.0077  0.0170  ...  0.21930  530.5455  1.9626    9.4200   \n",
       " 1356  1.4148 -0.0120  0.0022  ...  0.08770  530.2591  0.9847    8.2800   \n",
       " 1357  1.3914 -0.0072  0.0000  ...  0.08770  531.3091  1.9363   10.8500   \n",
       " 1358  1.5337 -0.0281  0.0075  ...  0.26180  531.1754  2.0396    8.0900   \n",
       " 1359  1.5664 -0.0350  0.0170  ...  0.23060  532.1382  2.0329    9.0200   \n",
       " 1360  1.4288 -0.0063  0.0012  ...  0.08770  532.8227  2.1206    7.9300   \n",
       " 1361  1.4458 -0.0108  0.0128  ...  0.20110  531.9873  2.0680   10.2500   \n",
       " 1362  1.4201 -0.0182  0.0055  ...  0.19310  514.4364  1.8660  431.7100   \n",
       " 1366  1.3717  0.0059  0.0011  ...  0.08770  531.6964  2.0737    8.2100   \n",
       " 1367  1.4685  0.0057 -0.0007  ...  0.08770  529.6464  2.0320    5.8100   \n",
       " 1368  1.4888 -0.0128 -0.0141  ...  0.08770  535.9491  1.9753    9.8800   \n",
       " 1369  1.4022 -0.0058 -0.0039  ...  0.08770  530.0846  1.9812    9.0100   \n",
       " 1370  1.5111 -0.0106  0.0101  ...  0.12570  514.4364  1.8660  431.7100   \n",
       " 1371  1.4441 -0.0129  0.0002  ...  0.26640  531.9873  2.0680   10.2500   \n",
       " 1372  1.3663 -0.0091 -0.0020  ...  0.20250  530.5455  1.9626    9.4200   \n",
       " 1373  1.4386 -0.0125 -0.0015  ...  0.08770  530.0846  1.9812    9.0100   \n",
       " 1374  1.3058 -0.0097  0.0073  ...  0.33600  530.2591  0.9847    8.2800   \n",
       " 1375  1.4252 -0.0267 -0.0216  ...  0.23880  531.9873  2.0680   10.2500   \n",
       " 1376  1.4509  0.0015 -0.0004  ...  0.08770  532.1382  2.0329    9.0200   \n",
       " 1377  1.4036 -0.0014  0.0015  ...  0.14960  530.0846  1.9812    9.0100   \n",
       " 1378  1.4226 -0.0076  0.0293  ...  0.25010  531.1754  2.0396    8.0900   \n",
       " 1379  1.4454  0.0071  0.0073  ...  0.20660  531.3091  1.9363   10.8500   \n",
       " 1380  1.4198 -0.0156 -0.0043  ...  0.08770  529.8827  1.9602   11.0500   \n",
       " 1381  1.5915 -0.0120  0.0085  ...  0.18970  539.0691  1.3198  427.4200   \n",
       " 1382  1.2877  0.0243  0.0142  ...  0.09100  531.9873  2.0680   10.2500   \n",
       " 1383  1.5369 -0.0287 -0.0216  ...  0.19710  532.8227  2.1206    7.9300   \n",
       " 1384  1.5616 -0.0141 -0.0014  ...  0.16400  532.4700  2.1429    9.1400   \n",
       " 1385  1.3657  0.0273  0.0063  ...  0.08770  531.3091  1.9363   10.8500   \n",
       " 1386  1.3859 -0.0007 -0.0058  ...  0.08770  530.2591  0.9847    8.2800   \n",
       " 1387  1.5552 -0.0031  0.0046  ...  0.14720  531.7464  2.2909    9.1299   \n",
       " 1388  1.4487  0.0014 -0.0063  ...  0.14090  531.9873  2.0680   10.2500   \n",
       " 1389  1.5903 -0.0062 -0.0010  ...  0.10470  532.4700  2.1429    9.1400   \n",
       " 1390  1.5002 -0.0122 -0.0110  ...  0.17930  533.1364  2.0133    7.2400   \n",
       " 1391  1.4906 -0.0051 -0.0164  ...  0.08770  529.4391  2.0075   10.0700   \n",
       " 1392  1.5521 -0.0085 -0.0152  ...  0.10050  533.1364  2.0133    7.2400   \n",
       " 1393  1.5083  0.0029  0.0058  ...  0.08770  532.8227  2.1206    7.9300   \n",
       " 1394  1.3692 -0.0105 -0.0109  ...  0.08770  531.9873  2.0680   10.2500   \n",
       " 1395  1.4895 -0.0099 -0.0046  ...  0.18460  539.0691  1.3198  427.4200   \n",
       " 1396  1.5033  0.0192  0.0027  ...  0.08770  532.8227  2.1206    7.9300   \n",
       " 1397  1.5613  0.0005  0.0023  ...  0.08770  531.9873  2.0680   10.2500   \n",
       " 1398  1.4832 -0.0191  0.0016  ...  0.08770  531.9873  2.0680   10.2500   \n",
       " 1399  1.5464 -0.0084  0.0048  ...  0.24190  531.9873  2.0680   10.2500   \n",
       " 1401  1.4719 -0.0003 -0.0006  ...  0.21820  532.8145  2.1082    6.8800   \n",
       " 1402  1.6010  0.0075 -0.0032  ...  0.24530  532.1382  2.0329    9.0200   \n",
       " 1403  1.5697 -0.0122 -0.0050  ...  0.08770  531.7464  2.2909    9.1299   \n",
       " 1404  1.5471  0.0133  0.0095  ...  0.31170  514.4364  1.8660  431.7100   \n",
       " 1405  1.5715 -0.0159 -0.0037  ...  0.08770  532.2664  2.0223    9.0300   \n",
       " 1406  1.5084  0.0112 -0.0010  ...  0.20700  531.9873  2.0680   10.2500   \n",
       " 1407  1.5242 -0.0006  0.0053  ...  0.20320  529.4391  2.0075   10.0700   \n",
       " 1408  1.4193  0.0033 -0.0057  ...  0.18570  531.3091  1.9363   10.8500   \n",
       " 1409  1.4409 -0.0112 -0.0017  ...  0.08770  535.2245  2.0034    8.7400   \n",
       " 1410  1.5680  0.0165 -0.0113  ...  0.12370  530.5455  1.9626    9.4200   \n",
       " 1411  1.6162 -0.0063 -0.0108  ...  0.08770  533.1364  2.0133    7.2400   \n",
       " 1412  1.4298  0.0122  0.0176  ...  0.08770  531.7464  2.2909    9.1299   \n",
       " 1413  1.3622  0.0387 -0.0202  ...  0.29910  530.5455  1.9626    9.4200   \n",
       " 1414  1.4425 -0.0291 -0.0025  ...  0.18760  530.5455  1.9626    9.4200   \n",
       " 1415  1.3987  0.0313 -0.0182  ...  0.08770  531.7464  2.2909    9.1299   \n",
       " 1416  1.4781  0.0002 -0.0062  ...  0.22890  529.4391  2.0075   10.0700   \n",
       " 1417  1.5010  0.0259 -0.0228  ...  0.23250  532.4700  2.1429    9.1400   \n",
       " 1418  1.4440  0.0168 -0.0001  ...  0.25580  532.8227  2.1206    7.9300   \n",
       " 1419  1.4066 -0.0094 -0.0280  ...  0.17350  532.8145  2.1082    6.8800   \n",
       " 1420  1.4968 -0.0201 -0.0060  ...  0.08770  532.4527  1.9857    5.8300   \n",
       " 1421  1.5823  0.0113 -0.0022  ...  0.08770  532.8145  2.1082    6.8800   \n",
       " 1422  1.4075  0.0280 -0.0349  ...  0.08770  529.8827  1.9602   11.0500   \n",
       " 1423  1.3188  0.0422 -0.0189  ...  0.07650  531.7464  2.2909    9.1299   \n",
       " 1424  1.3145  0.0410 -0.0209  ...  0.08770  536.4482  1.4106    7.0500   \n",
       " 1425  1.4109 -0.0239 -0.0121  ...  0.11150  569.9964  1.7961  441.9200   \n",
       " 1426  1.3137  0.0345 -0.0137  ...  0.08770  536.4482  1.4106    7.0500   \n",
       " 1427  1.4339  0.0020 -0.0036  ...  0.10170  531.7464  2.2909    9.1299   \n",
       " 1428  1.5177  0.0183 -0.0137  ...  0.08770  532.4700  2.1429    9.1400   \n",
       " 1429  1.5447  0.0036 -0.0125  ...  0.08770  533.1364  2.0133    7.2400   \n",
       " 1430  1.4855 -0.0034 -0.0103  ...  0.08770  531.7464  2.2909    9.1299   \n",
       " 1431  1.4089  0.0156 -0.0142  ...  0.08770  528.9927  1.9172    8.9100   \n",
       " 1432  1.6217 -0.0088  0.0064  ...  0.23520  528.7918  2.0831    6.8100   \n",
       " 1433  1.4413  0.0086 -0.0173  ...  0.08770  529.4391  2.0075   10.0700   \n",
       " 1434  1.5547 -0.0048  0.0020  ...  0.09880  532.0782  2.0447    8.3900   \n",
       " 1435  1.5718  0.0211 -0.0006  ...  0.30400  532.1382  2.0329    9.0200   \n",
       " 1436  1.4151  0.0213 -0.0179  ...  0.08770  532.1073  1.8126    7.8300   \n",
       " 1437  1.5172  0.0074 -0.0077  ...  0.09930  529.4391  2.0075   10.0700   \n",
       " 1439  1.2957  0.0186  0.0070  ...  0.08770  532.8145  2.1082    6.8800   \n",
       " 1440  1.4411 -0.0039 -0.0065  ...  0.21270  531.7464  2.2909    9.1299   \n",
       " 1441  1.3134  0.0158  0.0102  ...  0.16220  530.5455  1.9626    9.4200   \n",
       " 1442  1.3606  0.0030  0.0107  ...  0.08770  532.4527  1.9857    5.8300   \n",
       " 1444  1.6123 -0.0166  0.0017  ...  0.15520  528.8655  2.0891    6.3600   \n",
       " 1445  1.3326  0.0314  0.0021  ...  0.12960  531.7464  2.2909    9.1299   \n",
       " 1446  1.3216  0.0489  0.0035  ...  0.17410  532.1073  1.8126    7.8300   \n",
       " 1447  1.5576 -0.0205  0.0095  ...  0.08770  531.7464  2.2909    9.1299   \n",
       " 1448  1.3860  0.0225  0.0018  ...  0.08770  530.0846  1.9812    9.0100   \n",
       " 1449  1.4857  0.0160  0.0020  ...  0.08770  536.4482  1.4106    7.0500   \n",
       " 1450  1.3571  0.0324 -0.0007  ...  0.23490  532.0973  2.0850    7.7400   \n",
       " 1451  1.4471 -0.0130 -0.0070  ...  0.13440  526.2355  2.1114    8.1500   \n",
       " 1452  1.4120  0.0174  0.0067  ...  0.08770  569.9964  1.7961  441.9200   \n",
       " 1453  1.4133 -0.0078 -0.0076  ...  0.08770  529.8827  1.9602   11.0500   \n",
       " 1454  1.5302  0.0133 -0.0051  ...  0.12740  569.9964  1.7961  441.9200   \n",
       " 1455  1.5124  0.0229  0.0091  ...  0.22160  531.7464  2.2909    9.1299   \n",
       " 1456  1.4460  0.0107 -0.0026  ...  0.25430  532.1073  1.8126    7.8300   \n",
       " 1457  1.5169  0.0093  0.0123  ...  0.19920  536.4482  1.4106    7.0500   \n",
       " 1458  1.3663 -0.0123 -0.0050  ...  0.08770  532.6327  2.1085    7.7700   \n",
       " 1459  1.4814  0.0180 -0.0001  ...  0.20260  530.9445  1.9567    8.0800   \n",
       " 1460  1.4714  0.0049 -0.0034  ...  0.08770  532.1073  1.8126    7.8300   \n",
       " 1461  1.3988  0.0185 -0.0047  ...  0.28150  531.1754  2.0396    8.0900   \n",
       " 1462  1.4718  0.0100 -0.0122  ...  0.08770  530.0073  2.5161    6.9100   \n",
       " 1463  1.3068  0.0042 -0.0153  ...  0.08770  530.0073  2.5161    6.9100   \n",
       " 1464  1.2374  0.0160 -0.0044  ...  0.11640  530.0073  2.5161    6.9100   \n",
       " 1465  1.2005  0.0081 -0.0110  ...  0.08770  530.0073  2.5161    6.9100   \n",
       " 1466  1.4498  0.0097  0.0150  ...  0.20400  569.9964  1.7961  441.9200   \n",
       " 1467  1.4838  0.0033  0.0050  ...  0.19970  530.2591  0.9847    8.2800   \n",
       " 1468  1.3985 -0.0105 -0.0140  ...  0.08770  532.5155  2.3188    7.3000   \n",
       " 1469  1.4425  0.0222  0.0037  ...  0.08200  531.1754  2.0396    8.0900   \n",
       " 1470  1.5442 -0.0135  0.0031  ...  0.20880  536.4482  1.4106    7.0500   \n",
       " 1471  1.3561  0.0021 -0.0012  ...  0.11860  532.5155  2.3188    7.3000   \n",
       " 1472  1.2898  0.0187  0.0017  ...  0.14310  536.5927  2.1566    6.8200   \n",
       " 1473  1.3708  0.0061  0.0033  ...  0.11780  537.9264  2.1814    5.4800   \n",
       " 1474  1.4120  0.0093 -0.0138  ...  0.08770  536.5927  2.1566    6.8200   \n",
       " 1475  1.2462  0.0125 -0.0213  ...  0.08770  537.9264  2.1814    5.4800   \n",
       " 1476  1.4642  0.0749  0.0118  ...  0.13020  534.3264  1.5887    5.4000   \n",
       " 1477  1.4279 -0.0047 -0.0068  ...  0.17630  531.9600  1.8347   12.7000   \n",
       " 1478  1.2532  0.0089 -0.0058  ...  0.11180  536.0709  2.1472    8.3701   \n",
       " 1479  1.4005  0.0486  0.0080  ...  0.11840  532.1618  2.1401    7.1100   \n",
       " 1480  1.3930  0.0495  0.0078  ...  0.08770  537.9264  2.1814    5.4800   \n",
       " 1481  1.3639  0.0609  0.0040  ...  0.14000  532.4527  1.9857    5.8300   \n",
       " 1482  1.5203  0.0424  0.0150  ...  0.19190  498.1891  1.2925  439.2900   \n",
       " 1483  1.3062 -0.0069 -0.0089  ...  0.22290  534.3264  1.5887    5.4000   \n",
       " 1484  1.4058  0.0031 -0.0194  ...  0.08770  534.3264  1.5887    5.4000   \n",
       " 1485  1.4439 -0.0094 -0.0042  ...  0.25190  536.5927  2.1566    6.8200   \n",
       " 1486  1.3471 -0.0089  0.0135  ...  0.13060  537.1318  2.2109    8.4400   \n",
       " 1487  1.4197  0.0026 -0.0023  ...  0.16300  534.3936  1.9098    9.1300   \n",
       " 1488  1.3264 -0.0018  0.0067  ...  0.08770  536.9836  1.8675    7.4700   \n",
       " 1489  1.3976  0.0102  0.0043  ...  0.17320  525.6236  2.0340   10.7600   \n",
       " 1490  1.3699 -0.0021  0.0082  ...  0.08770  532.1618  2.1401    7.1100   \n",
       " 1491  1.3829 -0.0019  0.0017  ...  0.08770  534.3936  1.9098    9.1300   \n",
       " 1492  1.3508  0.0046 -0.0066  ...  0.37120  526.2355  2.1114    8.1500   \n",
       " 1493  1.2634  0.0052 -0.0040  ...  0.15170  531.9873  2.0680   10.2500   \n",
       " 1494  1.4404 -0.0050  0.0046  ...  0.09360  536.5927  2.1566    6.8200   \n",
       " 1495  1.3573 -0.0060  0.0085  ...  0.18890  526.2355  2.1114    8.1500   \n",
       " 1496  1.4234 -0.0045  0.0030  ...  0.08770  526.2355  2.1114    8.1500   \n",
       " 1497  1.4448  0.0042  0.0017  ...  0.08770  526.2355  2.1114    8.1500   \n",
       " 1498  1.4174  0.0384  0.0080  ...  0.08770  536.5927  2.1566    6.8200   \n",
       " 1499  1.3207 -0.0116 -0.0128  ...  0.08770  528.7918  2.0831    6.8100   \n",
       " 1500  1.4119 -0.0053  0.0056  ...  0.08770  537.1318  2.2109    8.4400   \n",
       " 1501  1.3160 -0.0140  0.0007  ...  0.08770  536.4482  1.4106    7.0500   \n",
       " 1502  1.4089  0.0447  0.0027  ...  0.08770  538.1373  2.1186    8.4399   \n",
       " 1503  1.2769 -0.0074  0.0086  ...  0.14790  532.1618  2.1401    7.1100   \n",
       " 1504  1.4443 -0.0025 -0.0025  ...  0.08920  538.1373  2.1186    8.4399   \n",
       " 1505  1.4241  0.0008 -0.0031  ...  0.13860  536.2964  2.0719    8.3900   \n",
       " 1506  1.3881 -0.0146  0.0048  ...  0.08770  536.4482  1.4106    7.0500   \n",
       " 1507  1.2956  0.0005  0.0046  ...  0.08640  526.2355  2.1114    8.1500   \n",
       " 1508  1.3636 -0.0103 -0.0056  ...  0.08770  532.3782  2.0882    9.6300   \n",
       " 1509  1.3488 -0.0018  0.0035  ...  0.08770  537.9264  2.1814    5.4800   \n",
       " 1510  1.4379 -0.0144 -0.0068  ...  0.13450  532.5155  2.3188    7.3000   \n",
       " 1511  1.3108 -0.0077 -0.0120  ...  0.17800  531.9600  1.8347   12.7000   \n",
       " 1512  1.6024 -0.0043  0.0032  ...  0.08770  529.4391  2.0075   10.0700   \n",
       " 1513  1.3059  0.0076  0.0021  ...  0.08770  536.9836  1.8675    7.4700   \n",
       " 1514  1.3380 -0.0018 -0.0063  ...  0.08770  535.9345  2.2444    6.3500   \n",
       " 1515  1.3010 -0.0060  0.0006  ...  0.08590  537.1318  2.2109    8.4400   \n",
       " 1516  1.3517 -0.0029 -0.0065  ...  0.08770  537.9264  2.1814    5.4800   \n",
       " 1517  1.3076  0.0046  0.0044  ...  0.08770  536.9836  1.8675    7.4700   \n",
       " 1518  1.3511  0.0120 -0.0049  ...  0.27590  534.3264  1.5887    5.4000   \n",
       " 1520  1.3454 -0.0032  0.0028  ...  0.11990  529.3100  2.2860    7.6200   \n",
       " 1521  1.3214  0.0009 -0.0074  ...  0.24580  536.0709  2.1472    8.3701   \n",
       " 1522  1.3868  0.0184  0.0035  ...  0.08770  532.1618  2.1401    7.1100   \n",
       " 1523  1.2872 -0.0117  0.0364  ...  0.08770  536.9836  1.8675    7.4700   \n",
       " 1524  1.3465 -0.0054 -0.0170  ...  0.23100  526.2355  2.1114    8.1500   \n",
       " 1525  1.4241 -0.0043 -0.0012  ...  0.08770  534.3264  1.5887    5.4000   \n",
       " 1526  1.3345  0.0085  0.0097  ...  0.06670  528.7918  2.0831    6.8100   \n",
       " 1527  1.3226  0.0025  0.0085  ...  0.08770  528.7918  2.0831    6.8100   \n",
       " 1528  1.3307 -0.0077  0.0046  ...  0.26600  532.1618  2.1401    7.1100   \n",
       " 1529  1.4994  0.0000 -0.0060  ...  0.24060  532.7627  2.2678    7.5000   \n",
       " 1530  1.2963 -0.0059 -0.0077  ...  0.12070  532.1618  2.1401    7.1100   \n",
       " 1531  1.4599 -0.0075 -0.0109  ...  0.08770  528.7918  2.0831    6.8100   \n",
       " 1532  1.4378 -0.0007  0.0043  ...  0.08770  537.9264  2.1814    5.4800   \n",
       " 1533  1.3834  0.0002 -0.0026  ...  0.13350  538.5645  2.3008    4.8400   \n",
       " 1534  1.3297  0.0020  0.0016  ...  0.08770  532.9918  2.0745    8.5200   \n",
       " 1535  1.3532  0.0006  0.0051  ...  0.37210  534.0200  2.1718    7.3500   \n",
       " 1536  1.4663 -0.0123 -0.0019  ...  0.14420  525.2427  2.1327   10.1900   \n",
       " 1537  1.4522 -0.0039 -0.0075  ...  0.14200  533.7318  2.0706    8.1300   \n",
       " 1538  1.3907  0.0074  0.0078  ...  0.08770  528.7918  2.0831    6.8100   \n",
       " 1539  1.4167  0.0041 -0.0056  ...  0.21570  569.9964  1.7961  441.9200   \n",
       " 1540  1.4158 -0.0029  0.0000  ...  0.11850  525.6127  2.0059   10.2500   \n",
       " 1541  1.3966 -0.0057 -0.0169  ...  0.08770  526.2355  2.1114    8.1500   \n",
       " 1542  1.3109  0.0174  0.0038  ...  0.14430  532.2627  2.1154   10.1500   \n",
       " 1543  1.3502  0.0201  0.0037  ...  0.08520  533.0909  2.3195   10.7200   \n",
       " 1544  1.2901  0.0202  0.0031  ...  0.04880  532.2627  2.1154   10.1500   \n",
       " 1545  1.4294 -0.0014  0.0135  ...  0.27410  529.3845  2.3706    8.7200   \n",
       " 1546  1.3482  0.0060  0.0139  ...  0.13740  530.3709  2.3435    6.4900   \n",
       " 1547  1.2895  0.0023 -0.0024  ...  0.08770  532.1618  2.1401    7.1100   \n",
       " 1548  1.4410  0.0035  0.0142  ...  0.08770  527.8955  2.3099    9.6600   \n",
       " 1549  1.4129 -0.0081 -0.0005  ...  0.07330  534.5864  1.8987    7.4600   \n",
       " 1550  1.3148 -0.0024  0.0039  ...  0.08770  532.1618  2.1401    7.1100   \n",
       " 1551  1.4386 -0.0179 -0.0048  ...  0.08770  532.1618  2.1401    7.1100   \n",
       " 1552  1.4366  0.0083 -0.0029  ...  0.05840  532.4527  1.9857    5.8300   \n",
       " 1553  1.3405  0.0201  0.0018  ...  0.12040  532.2627  2.1154   10.1500   \n",
       " 1554  1.3485  0.0151  0.0003  ...  0.08770  537.9264  2.1814    5.4800   \n",
       " 1555  1.3255 -0.0052 -0.0016  ...  0.08770  537.0846  2.1754    7.2800   \n",
       " 1556  1.3687 -0.0070 -0.0033  ...  0.08770  531.9600  1.8347   12.7000   \n",
       " 1557  1.4359 -0.0114  0.0096  ...  0.08770  534.3264  1.5887    5.4000   \n",
       " 1558  1.3832  0.0042  0.0023  ...  0.02640  532.1700  2.1510    9.1600   \n",
       " 1559  1.3120 -0.0043 -0.0063  ...  0.34200  528.7918  2.0831    6.8100   \n",
       " 1560  1.2811  0.0037  0.0017  ...  0.08770  536.5927  2.1566    6.8200   \n",
       " 1561  1.4492 -0.0134  0.0127  ...  0.11955  527.9364  2.3919    9.6900   \n",
       " 1562  1.3424 -0.0045 -0.0057  ...  0.08770  536.3418  2.0153    7.9800   \n",
       " 1563  1.4333 -0.0061 -0.0093  ...  0.13080  537.9264  2.1814    5.4800   \n",
       " 1564  1.4616 -0.0013  0.0004  ...  0.23880  530.3709  2.3435    6.4900   \n",
       " 1565  1.4622 -0.0072  0.0032  ...  0.08770  534.3936  1.9098    9.1300   \n",
       " 1566  1.4616 -0.0013  0.0004  ...  0.13070  528.7918  2.0831    6.8100   \n",
       " \n",
       "          582     583     586     587       589  Pass/Fail  \n",
       " 0     0.5005  0.0118  0.0205  0.0148   71.9005         -1  \n",
       " 1     0.5019  0.0223  0.0096  0.0201  208.2045         -1  \n",
       " 3     0.4990  0.0103  0.0202  0.0149   73.8432         -1  \n",
       " 4     0.4800  0.4766  0.0202  0.0149   73.8432         -1  \n",
       " 5     0.4949  0.0189  0.0342  0.0151   44.0077         -1  \n",
       " 6     0.5010  0.0143  0.0342  0.0151   44.0077         -1  \n",
       " 7     0.4984  0.0106  0.0204  0.0194   95.0310         -1  \n",
       " 8     0.4993  0.0172  0.0111  0.0124  111.6525         -1  \n",
       " 9     0.4967  0.0152  0.0212  0.0191   90.2294         -1  \n",
       " 12    0.4950  0.0153  0.0188  0.0098   52.2039         -1  \n",
       " 13    0.5034  0.0151  0.0188  0.0098   52.2039         -1  \n",
       " 15    0.5058  0.0078  0.0174  0.0174  100.2745         -1  \n",
       " 16    0.5005  0.0108  0.0184  0.0151   82.0989         -1  \n",
       " 17    0.5015  0.0105  0.0184  0.0151   82.0989         -1  \n",
       " 18    0.4948  0.0117  0.0184  0.0151   82.0989         -1  \n",
       " 19    0.5036  0.0169  0.0229  0.0108   47.1586         -1  \n",
       " 20    0.5011  0.0117  0.0229  0.0108   47.1586         -1  \n",
       " 21    0.4947  0.0137  0.0175  0.0060   34.4153         -1  \n",
       " 22    0.4977  0.0114  0.0250  0.0286  114.5979         -1  \n",
       " 24    0.5032  0.0159  0.0288  0.0361  125.0600         -1  \n",
       " 25    0.5012  0.0336  0.0288  0.0361  125.0600         -1  \n",
       " 26    0.5006  0.0083  0.0288  0.0361  125.0600         -1  \n",
       " 27    0.5069  0.0158  0.0183  0.0397  216.9552         -1  \n",
       " 28    0.5036  0.0137  0.0130  0.0165  127.5067         -1  \n",
       " 29    0.5019  0.0139  0.0121  0.0178  146.8715         -1  \n",
       " 30    0.4981  0.0180  0.0209  0.0171   81.5459         -1  \n",
       " 31    0.5033  0.0115  0.0082  0.0162  197.9951         -1  \n",
       " 32    0.4967  0.0113  0.0082  0.0162  197.9951         -1  \n",
       " 33    0.4992  0.0158  0.0082  0.0162  197.9951         -1  \n",
       " 34    0.5037  0.0175  0.0268  0.0199   74.1555         -1  \n",
       " 35    0.5013  0.0138  0.0268  0.0199   74.1555         -1  \n",
       " 36    0.5032  0.0127  0.0290  0.0165   56.8221         -1  \n",
       " 37    0.5073  0.0181  0.0104  0.0159  152.6823         -1  \n",
       " 39    0.5012  0.0099  0.0150  0.0151  100.7279         -1  \n",
       " 41    0.4976  0.0111  0.0219  0.0162   73.8817         -1  \n",
       " 42    0.5014  0.0121  0.0173  0.0298  171.8524         -1  \n",
       " 43    0.4947  0.0127  0.0365  0.0303   83.0563         -1  \n",
       " 44    0.5009  0.0155  0.0149  0.0158  106.1812         -1  \n",
       " 46    0.5035  0.0189  0.0227  0.0232  102.2673         -1  \n",
       " 47    0.4976  0.0127  0.0149  0.0164  110.5454         -1  \n",
       " 51    0.4943  0.0181  0.0245  0.0157   63.9698         -1  \n",
       " 52    0.4987  0.0106  0.0358  0.0182   50.8713         -1  \n",
       " 53    0.5036  0.0150  0.0358  0.0182   50.8713         -1  \n",
       " 54    0.4967  0.0152  0.0247  0.0054   21.9917         -1  \n",
       " 55    0.5000  0.0173  0.0097  0.0111  115.1005         -1  \n",
       " 56    0.5030  0.0118  0.0120  0.0190  157.8567         -1  \n",
       " 59    0.4997  0.0086  0.0285  0.0132   46.4594         -1  \n",
       " 60    0.5039  0.0116  0.0201  0.0216  107.8331         -1  \n",
       " 61    0.5070  0.0186  0.0183  0.0183  100.1756         -1  \n",
       " 63    0.4984  0.0146  0.0296  0.0062   20.8909         -1  \n",
       " 65    0.4970  0.0093  0.0343  0.0115   33.4515         -1  \n",
       " 66    0.5053  0.0121  0.0139  0.0187  134.2014         -1  \n",
       " 67    0.5001  0.0119  0.0139  0.0187  134.2014         -1  \n",
       " 68    0.5001  0.0144  0.0110  0.0134  121.5476         -1  \n",
       " 69    0.4953  0.0148  0.0192  0.0125   65.2312         -1  \n",
       " 70    0.4969  0.0197  0.0108  0.0106   98.1735         -1  \n",
       " 71    0.5022  0.0182  0.0108  0.0106   98.1735         -1  \n",
       " 72    0.5001  0.0129  0.0354  0.0286   80.7240         -1  \n",
       " 73    0.4980  0.0163  0.0266  0.0086   32.3304         -1  \n",
       " 74    0.4993  0.0169  0.0266  0.0086   32.3304         -1  \n",
       " 75    0.4958  0.0120  0.0059  0.0101  169.6401         -1  \n",
       " 76    0.5032  0.0134  0.0059  0.0101  169.6401         -1  \n",
       " 77    0.4977  0.0116  0.0461  0.0542  117.6159         -1  \n",
       " 78    0.4961  0.0124  0.0155  0.0177  114.4127         -1  \n",
       " 79    0.4959  0.0191  0.0165  0.0116   70.1798         -1  \n",
       " 80    0.5011  0.0122  0.0218  0.0152   69.4220         -1  \n",
       " 81    0.4984  0.0137  0.0180  0.0106   59.2072         -1  \n",
       " 83    0.5003  0.0234  0.0278  0.0042   15.2909         -1  \n",
       " 84    0.4986  0.0176  0.0298  0.0146   48.9072         -1  \n",
       " 85    0.5043  0.0131  0.0298  0.0146   48.9072         -1  \n",
       " 86    0.5032  0.0135  0.0298  0.0146   48.9072         -1  \n",
       " 87    0.4944  0.0122  0.0200  0.0237  118.3047         -1  \n",
       " 88    0.4981  0.0086  0.0372  0.0358   96.3971         -1  \n",
       " 89    0.5008  0.0134  0.0201  0.0083   41.3784         -1  \n",
       " 90    0.4962  0.0129  0.0201  0.0083   41.3784         -1  \n",
       " 91    0.5003  0.0136  0.0277  0.0318  114.7497         -1  \n",
       " 92    0.5007  0.0105  0.0277  0.0318  114.7497         -1  \n",
       " 93    0.5025  0.0118  0.0277  0.0318  114.7497         -1  \n",
       " 94    0.5031  0.0146  0.0277  0.0318  114.7497         -1  \n",
       " 95    0.5002  0.0098  0.0277  0.0318  114.7497         -1  \n",
       " 97    0.4998  0.0174  0.0095  0.0184  192.2985         -1  \n",
       " 98    0.4955  0.0110  0.0095  0.0184  192.2985         -1  \n",
       " 99    0.5012  0.0094  0.0199  0.0107   53.6369         -1  \n",
       " 100   0.4924  0.0142  0.0199  0.0107   53.6369         -1  \n",
       " 101   0.4988  0.0126  0.0199  0.0107   53.6369         -1  \n",
       " 102   0.5026  0.0176  0.0189  0.0137   72.3776         -1  \n",
       " 103   0.4943  0.0168  0.0313  0.0208   66.4172         -1  \n",
       " 104   0.4977  0.0138  0.0313  0.0208   66.4172         -1  \n",
       " 105   0.4984  0.0180  0.0194  0.0223  115.1055         -1  \n",
       " 106   0.5033  0.0144  0.0120  0.0082   68.6662         -1  \n",
       " 107   0.5024  0.0120  0.0322  0.0132   40.8703         -1  \n",
       " 108   0.5021  0.0117  0.0325  0.0073   22.4197         -1  \n",
       " 109   0.5045  0.0261  0.0349  0.0146   41.7833         -1  \n",
       " 110   0.4971  0.0151  0.0139  0.0139   99.4503         -1  \n",
       " 111   0.4987  0.0184  0.0139  0.0139   99.4503         -1  \n",
       " 112   0.5045  0.0090  0.0139  0.0139   99.4503         -1  \n",
       " 113   0.5016  0.0160  0.0049  0.0144  293.2614         -1  \n",
       " 114   0.5010  0.0123  0.0049  0.0144  293.2614         -1  \n",
       " 116   0.5030  0.0167  0.0364  0.0166   45.6835         -1  \n",
       " 117   0.5018  0.0106  0.0231  0.0102   44.1220         -1  \n",
       " 118   0.5001  0.0191  0.0231  0.0102   44.1220         -1  \n",
       " 119   0.5025  0.0094  0.0254  0.0145   56.8579         -1  \n",
       " 120   0.5005  0.0128  0.0216  0.0115   53.3157         -1  \n",
       " 121   0.5040  0.0176  0.0220  0.0547  248.3173         -1  \n",
       " 122   0.5000  0.0202  0.0241  0.0086   35.5550         -1  \n",
       " 123   0.4965  0.0113  0.0241  0.0086   35.5550         -1  \n",
       " 124   0.4950  0.0149  0.0241  0.0086   35.5550         -1  \n",
       " 125   0.4966  0.0085  0.0201  0.0379  188.2987         -1  \n",
       " 126   0.5019  0.0130  0.0201  0.0379  188.2987         -1  \n",
       " 127   0.4998  0.0162  0.0201  0.0379  188.2987         -1  \n",
       " 128   0.5029  0.0082  0.0153  0.0134   87.6631         -1  \n",
       " 129   0.5009  0.0153  0.0338  0.0119   35.2148         -1  \n",
       " 130   0.5059  0.0082  0.0208  0.0149   71.5753         -1  \n",
       " 132   0.5001  0.0115  0.0058  0.0169  289.9234         -1  \n",
       " 133   0.5009  0.0100  0.0058  0.0169  289.9234         -1  \n",
       " 134   0.5002  0.0138  0.0058  0.0169  289.9234         -1  \n",
       " 135   0.4955  0.0255  0.0058  0.0169  289.9234         -1  \n",
       " 136   0.5060  0.0264  0.0058  0.0169  289.9234         -1  \n",
       " 137   0.5037  0.0183  0.0058  0.0169  289.9234         -1  \n",
       " 138   0.5025  0.0141  0.0058  0.0169  289.9234         -1  \n",
       " 139   0.5068  0.0160  0.0058  0.0169  289.9234         -1  \n",
       " 140   0.5027  0.0165  0.0058  0.0169  289.9234         -1  \n",
       " 141   0.5005  0.0160  0.0058  0.0169  289.9234         -1  \n",
       " 142   0.5064  0.0117  0.0348  0.0109   31.2736         -1  \n",
       " 143   0.4980  0.0104  0.0135  0.0153  112.8617         -1  \n",
       " 144   0.5000  0.0101  0.0055  0.0218  397.5003         -1  \n",
       " 145   0.4971  0.0132  0.0055  0.0218  397.5003         -1  \n",
       " 146   0.4973  0.0181  0.0055  0.0218  397.5003         -1  \n",
       " 147   0.5047  0.0162  0.0055  0.0218  397.5003         -1  \n",
       " 148   0.5026  0.0089  0.0055  0.0218  397.5003         -1  \n",
       " 149   0.4992  0.0122  0.0182  0.0187  102.8940         -1  \n",
       " 150   0.4985  0.0118  0.0182  0.0187  102.8940         -1  \n",
       " 151   0.4946  0.0141  0.0182  0.0187  102.8940         -1  \n",
       " 152   0.4957  0.0166  0.0336  0.0153   45.3701         -1  \n",
       " 153   0.5057  0.0147  0.0199  0.0117   58.5665         -1  \n",
       " 155   0.5003  0.0150  0.0228  0.0299  131.2543         -1  \n",
       " 156   0.4939  0.0136  0.0228  0.0299  131.2543         -1  \n",
       " 159   0.5065  0.0188  0.0082  0.0184  223.6518         -1  \n",
       " 160   0.5067  0.0126  0.0195  0.0121   62.1248         -1  \n",
       " 161   0.5028  0.0125  0.0075  0.0140  185.5679         -1  \n",
       " 162   0.5067  0.0096  0.0215  0.0101   47.0849         -1  \n",
       " 163   0.5028  0.0228  0.0337  0.0219   64.8236         -1  \n",
       " 164   0.5023  0.0140  0.0078  0.0112  143.3273         -1  \n",
       " 165   0.4995  0.0403  0.0078  0.0112  143.3273         -1  \n",
       " 166   0.5011  0.0100  0.0419  0.0098   23.3511         -1  \n",
       " 168   0.4975  0.0194  0.0419  0.0098   23.3511         -1  \n",
       " 170   0.5045  0.0095  0.0240  0.0115   47.8123         -1  \n",
       " 171   0.5016  0.0454  0.0059  0.0126  213.7714         -1  \n",
       " 172   0.5043  0.0112  0.0254  0.0160   62.9443         -1  \n",
       " 173   0.5031  0.0090  0.0254  0.0160   62.9443         -1  \n",
       " 174   0.5000  0.0115  0.0254  0.0160   62.9443         -1  \n",
       " 175   0.4974  0.0130  0.0254  0.0160   62.9443         -1  \n",
       " 176   0.5032  0.0085  0.0254  0.0160   62.9443         -1  \n",
       " 177   0.5027  0.0245  0.0254  0.0160   62.9443         -1  \n",
       " 178   0.5043  0.0104  0.0171  0.0173  100.9102         -1  \n",
       " 179   0.4992  0.0104 -0.0012  0.0220    0.0000         -1  \n",
       " 181   0.5014  0.0116 -0.0012  0.0220    0.0000         -1  \n",
       " 183   0.5014  0.0109  0.0077  0.0204  264.7525         -1  \n",
       " 184   0.4989  0.0109  0.0077  0.0204  264.7525         -1  \n",
       " 185   0.5003  0.0161  0.0077  0.0204  264.7525         -1  \n",
       " 187   0.5033  0.0127  0.0104  0.0221  211.6182         -1  \n",
       " 190   0.4965  0.0143  0.0112  0.0191  170.4645         -1  \n",
       " 191   0.4929  0.0146  0.0112  0.0191  170.4645         -1  \n",
       " 192   0.4948  0.0149  0.0112  0.0191  170.4645         -1  \n",
       " 193   0.4936  0.0289  0.0112  0.0191  170.4645         -1  \n",
       " 194   0.4996  0.0123  0.0217  0.0244  112.5819         -1  \n",
       " 195   0.4982  0.0221  0.0217  0.0244  112.5819         -1  \n",
       " 196   0.4997  0.0110  0.0123  0.0128  104.0012         -1  \n",
       " 197   0.5000  0.0188  0.0165  0.0129   78.1942         -1  \n",
       " 198   0.4938  0.0205  0.0105  0.0198  188.2466         -1  \n",
       " 199   0.4982  0.0097  0.0009  0.0242    0.0000         -1  \n",
       " 200   0.4952  0.0141  0.0123  0.0094   76.4584         -1  \n",
       " 201   0.4966  0.0226  0.0123  0.0094   76.4584         -1  \n",
       " 202   0.4967  0.0160  0.0123  0.0094   76.4584         -1  \n",
       " 203   0.4965  0.0156  0.0123  0.0094   76.4584         -1  \n",
       " 204   0.5073  0.0157  0.0123  0.0094   76.4584         -1  \n",
       " 205   0.5014  0.0103  0.0123  0.0094   76.4584         -1  \n",
       " 206   0.4991  0.0080  0.0123  0.0094   76.4584         -1  \n",
       " 207   0.4978  0.0129  0.0123  0.0094   76.4584         -1  \n",
       " 208   0.5021  0.0249  0.0123  0.0094   76.4584         -1  \n",
       " 209   0.4970  0.0122  0.0298  0.0116   38.9781         -1  \n",
       " 210   0.5000  0.0178  0.0298  0.0116   38.9781         -1  \n",
       " 211   0.5005  0.0147  0.0298  0.0116   38.9781         -1  \n",
       " 212   0.4919  0.0137  0.0298  0.0116   38.9781         -1  \n",
       " 213   0.4968  0.0128  0.0298  0.0116   38.9781         -1  \n",
       " 214   0.4968  0.0115  0.0298  0.0116   38.9781         -1  \n",
       " 215   0.5025  0.0121  0.0015  0.0207    0.0000         -1  \n",
       " 216   0.4981  0.0143  0.0334  0.0206   61.6014         -1  \n",
       " 217   0.5005  0.0163  0.0279  0.0123   44.1754         -1  \n",
       " 219   0.5040  0.0161  0.0095  0.0162  170.5841         -1  \n",
       " 220   0.5054  0.0127  0.0315  0.0164   51.9199         -1  \n",
       " 221   0.4974  0.0141  0.0440  0.0133   30.2219         -1  \n",
       " 223   0.4958  0.0133  0.0222  0.0048   21.6559         -1  \n",
       " 224   0.5025  0.0144  0.0287  0.0149   51.8065         -1  \n",
       " 225   0.5047  0.0124  0.0265  0.0083   31.4679         -1  \n",
       " 226   0.5047  0.0093  0.0214  0.0127   59.3840         -1  \n",
       " 227   0.4999  0.0102  0.0327  0.0144   44.1766         -1  \n",
       " 228   0.5035  0.0130  0.0131  0.0129   98.1126         -1  \n",
       " 229   0.4978  0.0154  0.0131  0.0129   98.1126         -1  \n",
       " 230   0.4990  0.0130  0.0131  0.0129   98.1126         -1  \n",
       " 232   0.5006  0.0164 -0.0034  0.0093  272.3477         -1  \n",
       " 233   0.5024  0.0151 -0.0034  0.0093  272.3477         -1  \n",
       " 234   0.4977  0.0184 -0.0034  0.0093  272.3477         -1  \n",
       " 237   0.5037  0.0115  0.0261  0.0080   30.7439         -1  \n",
       " 239   0.4915  0.0114  0.0235  0.0184   78.5378         -1  \n",
       " 242   0.5017  0.0127  0.0154  0.0271  176.0329         -1  \n",
       " 245   0.5007  0.0151  0.0154  0.0271  176.0329         -1  \n",
       " 246   0.4989  0.0095  0.0154  0.0271  176.0329         -1  \n",
       " 247   0.5009  0.0120  0.0154  0.0271  176.0329         -1  \n",
       " 248   0.5041  0.0145  0.0123  0.0270  220.0378         -1  \n",
       " 249   0.4964  0.0138  0.0123  0.0270  220.0378         -1  \n",
       " 250   0.4978  0.0137  0.0123  0.0270  220.0378         -1  \n",
       " 251   0.4999  0.0135  0.0123  0.0270  220.0378         -1  \n",
       " 252   0.4924  0.0179  0.0123  0.0270  220.0378         -1  \n",
       " 253   0.4997  0.0186  0.0137  0.0066   48.5023         -1  \n",
       " 254   0.4966  0.0169  0.0376  0.0306   81.5102         -1  \n",
       " 255   0.4988  0.0185  0.0135  0.0114   84.4337         -1  \n",
       " 256   0.5018  0.0105  0.0135  0.0114   84.4337         -1  \n",
       " 257   0.5001  0.0191  0.0135  0.0114   84.4337         -1  \n",
       " 258   0.5016  0.0196  0.0135  0.0114   84.4337         -1  \n",
       " 259   0.5009  0.0096  0.0135  0.0114   84.4337         -1  \n",
       " 260   0.4995  0.0147  0.0135  0.0114   84.4337         -1  \n",
       " 261   0.5077  0.0144  0.0333  0.0125   37.5668         -1  \n",
       " 262   0.5032  0.0140  0.0333  0.0125   37.5668         -1  \n",
       " 263   0.5012  0.0160  0.0181  0.0080   44.2988         -1  \n",
       " 264   0.5000  0.0166  0.0149  0.0076   51.3704         -1  \n",
       " 265   0.5001  0.0124  0.0195  0.0154   78.7890         -1  \n",
       " 266   0.4972  0.0185  0.0195  0.0154   78.7890         -1  \n",
       " 267   0.5022  0.0097  0.0165  0.0286  173.4377         -1  \n",
       " 268   0.5010  0.0199  0.0165  0.0286  173.4377         -1  \n",
       " 269   0.4973  0.0153  0.0165  0.0286  173.4377         -1  \n",
       " 270   0.4999  0.0095  0.0274  0.0142   51.9067         -1  \n",
       " 271   0.5058  0.0305  0.0274  0.0142   51.9067         -1  \n",
       " 272   0.5001  0.0170  0.0274  0.0142   51.9067         -1  \n",
       " 274   0.5033  0.0157  0.0274  0.0142   51.9067         -1  \n",
       " 275   0.5043  0.0128  0.0239  0.0149   62.5617         -1  \n",
       " 276   0.5003  0.0124  0.0313  0.0292   93.1579         -1  \n",
       " 278   0.5032  0.0124  0.0177  0.0184  104.4612         -1  \n",
       " 279   0.5038  0.0158  0.0177  0.0184  104.4612         -1  \n",
       " 280   0.5019  0.0267  0.0121  0.0169  139.8330         -1  \n",
       " 281   0.5015  0.0154  0.0121  0.0169  139.8330         -1  \n",
       " 283   0.4959  0.0105  0.0121  0.0169  139.8330         -1  \n",
       " 284   0.5018  0.0245  0.0121  0.0169  139.8330         -1  \n",
       " 285   0.5044  0.0200  0.0292  0.0197   67.4053         -1  \n",
       " 286   0.4978  0.0179  0.0292  0.0197   67.4053         -1  \n",
       " 287   0.5049  0.0350  0.0064  0.0188  294.6251         -1  \n",
       " 288   0.4976  0.0073  0.0064  0.0188  294.6251         -1  \n",
       " 289   0.5007  0.0140  0.0197  0.0105   53.0253         -1  \n",
       " 290   0.5035  0.0166  0.0037  0.0141  382.6619         -1  \n",
       " 292   0.4968  0.0178  0.0291  0.0237   81.3456         -1  \n",
       " 293   0.5076  0.0167  0.0291  0.0237   81.3456         -1  \n",
       " 295   0.5019  0.0181  0.0291  0.0237   81.3456         -1  \n",
       " 296   0.5025  0.0197  0.0291  0.0237   81.3456         -1  \n",
       " 297   0.5070  0.0151  0.0079  0.0111  140.6419         -1  \n",
       " 298   0.4988  0.0152  0.0277  0.0087   31.4497         -1  \n",
       " 299   0.5017  0.0147  0.0188  0.0133   70.5588         -1  \n",
       " 300   0.4920  0.0082  0.0188  0.0133   70.5588         -1  \n",
       " 301   0.5025  0.0147  0.0188  0.0133   70.5588         -1  \n",
       " 302   0.5007  0.0150  0.0187  0.0108   58.0827         -1  \n",
       " 303   0.5045  0.0125  0.0187  0.0108   58.0827         -1  \n",
       " 304   0.4948  0.0160  0.0187  0.0108   58.0827         -1  \n",
       " 305   0.5051  0.0143  0.0187  0.0108   58.0827         -1  \n",
       " 306   0.5044  0.0192  0.0187  0.0108   58.0827         -1  \n",
       " 307   0.4986  0.0154  0.0187  0.0108   58.0827         -1  \n",
       " 308   0.5005  0.0126  0.0187  0.0108   58.0827         -1  \n",
       " 309   0.5057  0.0116  0.0187  0.0108   58.0827         -1  \n",
       " 310   0.5091  0.0227  0.0187  0.0108   58.0827         -1  \n",
       " 311   0.4999  0.0277  0.0187  0.0108   58.0827         -1  \n",
       " 312   0.5025  0.0104  0.0187  0.0050   26.7738         -1  \n",
       " 313   0.5011  0.0151  0.0187  0.0050   26.7738         -1  \n",
       " 314   0.4967  0.0137  0.0225  0.0053   23.7892         -1  \n",
       " 315   0.5019  0.0109  0.0225  0.0053   23.7892         -1  \n",
       " 316   0.4994  0.0303  0.0032  0.0164  510.0410         -1  \n",
       " 317   0.4992  0.0130  0.0216  0.0183   84.6151         -1  \n",
       " 318   0.4997  0.0152  0.0262  0.0076   29.1606         -1  \n",
       " 319   0.5036  0.0150  0.0262  0.0076   29.1606         -1  \n",
       " 320   0.5003  0.0089  0.0238  0.0174   73.1502         -1  \n",
       " 322   0.5026  0.0126  0.0227  0.0105   46.1077         -1  \n",
       " 324   0.5021  0.0092  0.0107  0.0071   66.2997         -1  \n",
       " 325   0.4932  0.0106  0.0107  0.0071   66.2997         -1  \n",
       " 328   0.4963  0.0208  0.0226  0.0106   46.9253         -1  \n",
       " 329   0.5030  0.0173  0.0226  0.0106   46.9253         -1  \n",
       " 330   0.5098  0.0203  0.0226  0.0106   46.9253         -1  \n",
       " 331   0.5017  0.0134  0.0226  0.0106   46.9253         -1  \n",
       " 332   0.4983  0.0093  0.0226  0.0106   46.9253         -1  \n",
       " 333   0.5031  0.0144  0.0140  0.0085   60.6831         -1  \n",
       " 334   0.4988  0.0196  0.0140  0.0085   60.6831         -1  \n",
       " 335   0.5050  0.0265  0.0416  0.0506  121.6592         -1  \n",
       " 337   0.5068  0.0188  0.0077  0.0098  127.7463         -1  \n",
       " 338   0.5001  0.0087  0.0270  0.0114   42.1428         -1  \n",
       " 339   0.5004  0.0140  0.0270  0.0114   42.1428         -1  \n",
       " 340   0.5014  0.0246 -0.0014  0.0098  706.8240         -1  \n",
       " 341   0.5034  0.0122 -0.0014  0.0098  706.8240         -1  \n",
       " 342   0.5025  0.0134  0.0358  0.0162   45.2582         -1  \n",
       " 343   0.5021  0.0138  0.0358  0.0162   45.2582         -1  \n",
       " 345   0.4967  0.0155  0.0358  0.0162   45.2582         -1  \n",
       " 346   0.5024  0.0162  0.0358  0.0162   45.2582         -1  \n",
       " 347   0.4989  0.0161 -0.0012  0.0151    0.0000         -1  \n",
       " 348   0.4933  0.0166 -0.0012  0.0151    0.0000         -1  \n",
       " 349   0.4934  0.0174 -0.0012  0.0151    0.0000         -1  \n",
       " 350   0.5023  0.0166  0.0275  0.0314  114.1967         -1  \n",
       " 352   0.4973  0.0157  0.0275  0.0314  114.1967         -1  \n",
       " 353   0.5012  0.0130  0.0275  0.0314  114.1967         -1  \n",
       " 354   0.5035  0.0085  0.0484  0.0339   70.0618         -1  \n",
       " 355   0.4940  0.0126  0.0484  0.0339   70.0618         -1  \n",
       " 356   0.4982  0.0144  0.0484  0.0339   70.0618         -1  \n",
       " 357   0.4990  0.0182  0.0484  0.0339   70.0618         -1  \n",
       " 358   0.5013  0.0169  0.0484  0.0339   70.0618         -1  \n",
       " 359   0.5051  0.0136  0.0071  0.0100  141.9849         -1  \n",
       " 360   0.4981  0.0102  0.0094  0.0095  101.3884         -1  \n",
       " 361   0.4987  0.0123  0.0226  0.0221   97.8001         -1  \n",
       " 362   0.5047  0.0101  0.0284  0.0141   49.7490         -1  \n",
       " 363   0.5041  0.0311  0.0284  0.0141   49.7490         -1  \n",
       " 364   0.4987  0.0195  0.0203  0.0226  110.9041         -1  \n",
       " 365   0.4976  0.0148  0.0291  0.0135   46.4165         -1  \n",
       " 366   0.5002  0.0148  0.0046  0.0180  388.9648         -1  \n",
       " 367   0.5088  0.0136  0.0046  0.0180  388.9648         -1  \n",
       " 369   0.5003  0.0154  0.0046  0.0180  388.9648         -1  \n",
       " 370   0.5016  0.0113 -0.0042  0.0138  331.6058         -1  \n",
       " 371   0.5084  0.0282  0.0121  0.0122  100.7960         -1  \n",
       " 372   0.4997  0.0097  0.0075  0.0112  150.3448         -1  \n",
       " 374   0.5017  0.0125  0.0075  0.0112  150.3448         -1  \n",
       " 375   0.4996  0.0302  0.0075  0.0112  150.3448         -1  \n",
       " 376   0.5020  0.0098  0.0075  0.0112  150.3448         -1  \n",
       " 377   0.5026  0.0167  0.0075  0.0112  150.3448         -1  \n",
       " 378   0.4966  0.0207  0.0075  0.0112  150.3448         -1  \n",
       " 379   0.5015  0.0207  0.0159  0.0077   48.6584         -1  \n",
       " 380   0.5077  0.0210  0.0165  0.0318  192.9130         -1  \n",
       " 381   0.4988  0.0139  0.0165  0.0318  192.9130         -1  \n",
       " 382   0.4964  0.0107  0.0165  0.0318  192.9130         -1  \n",
       " 383   0.4996  0.0137  0.0536  0.0607  113.2841         -1  \n",
       " 384   0.5013  0.0160  0.0536  0.0607  113.2841         -1  \n",
       " 385   0.4968  0.0096  0.0056  0.0071  127.2483         -1  \n",
       " 386   0.4992  0.0113  0.0056  0.0071  127.2483         -1  \n",
       " 387   0.5020  0.0114  0.0056  0.0071  127.2483         -1  \n",
       " 388   0.4984  0.0196  0.0056  0.0071  127.2483         -1  \n",
       " 389   0.4994  0.0214  0.0104  0.0083   79.8045         -1  \n",
       " 390   0.4952  0.0136  0.0104  0.0083   79.8045         -1  \n",
       " 391   0.4987  0.0269  0.0104  0.0083   79.8045         -1  \n",
       " 393   0.5036  0.0139  0.0157  0.0191  121.6354         -1  \n",
       " 394   0.5045  0.0155  0.0157  0.0191  121.6354         -1  \n",
       " 395   0.4959  0.0126  0.0157  0.0191  121.6354         -1  \n",
       " 396   0.5043  0.0089  0.0120  0.0104   86.7035         -1  \n",
       " 397   0.4983  0.0185  0.0120  0.0104   86.7035         -1  \n",
       " 398   0.5018  0.0136  0.0120  0.0104   86.7035         -1  \n",
       " 399   0.5022  0.0156  0.0120  0.0104   86.7035         -1  \n",
       " 400   0.5023  0.0119  0.0219  0.0138   62.8422         -1  \n",
       " 401   0.4962  0.0098  0.0061  0.0141  233.2441         -1  \n",
       " 402   0.4998  0.0166  0.0061  0.0141  233.2441         -1  \n",
       " 403   0.4926  0.0124  0.0061  0.0141  233.2441         -1  \n",
       " 404   0.5043  0.0177  0.0061  0.0141  233.2441         -1  \n",
       " 405   0.5010  0.0108  0.0061  0.0141  233.2441         -1  \n",
       " 407   0.5053  0.0116  0.0061  0.0141  233.2441         -1  \n",
       " 408   0.5013  0.0101  0.0061  0.0141  233.2441         -1  \n",
       " 409   0.4998  0.0269  0.0061  0.0141  233.2441         -1  \n",
       " 410   0.4934  0.0179  0.0145  0.0213  146.5131         -1  \n",
       " 411   0.4999  0.0115  0.0145  0.0213  146.5131         -1  \n",
       " 412   0.4976  0.0184  0.0145  0.0213  146.5131         -1  \n",
       " 413   0.5040  0.0193  0.0145  0.0213  146.5131         -1  \n",
       " 414   0.5008  0.0109  0.0433  0.0187   43.1616         -1  \n",
       " 415   0.5076  0.0206  0.0095  0.0173  182.5607         -1  \n",
       " 416   0.4997  0.0099  0.0005  0.0115    0.0000         -1  \n",
       " 417   0.5036  0.0164  0.0005  0.0115    0.0000         -1  \n",
       " 418   0.4969  0.0096  0.0005  0.0115    0.0000         -1  \n",
       " 419   0.4981  0.0118  0.0005  0.0115    0.0000         -1  \n",
       " 420   0.5041  0.0172  0.0005  0.0115    0.0000         -1  \n",
       " 421   0.5008  0.0142  0.0005  0.0115    0.0000         -1  \n",
       " 422   0.5005  0.0097  0.0005  0.0115    0.0000         -1  \n",
       " 423   0.4985  0.0144  0.0005  0.0115    0.0000         -1  \n",
       " 425   0.5080  0.0139  0.0234  0.0073   31.3771         -1  \n",
       " 426   0.5002  0.0138  0.0234  0.0073   31.3771         -1  \n",
       " 427   0.4986  0.0147  0.0142  0.0156  109.5996         -1  \n",
       " 428   0.4981  0.0132  0.0142  0.0156  109.5996         -1  \n",
       " 429   0.4975  0.0140  0.0142  0.0156  109.5996         -1  \n",
       " 430   0.5009  0.0124  0.0142  0.0156  109.5996         -1  \n",
       " 431   0.4981  0.0111  0.0142  0.0156  109.5996         -1  \n",
       " 432   0.5078  0.0159  0.0142  0.0156  109.5996         -1  \n",
       " 433   0.5040  0.0164  0.0116  0.0125  107.1020         -1  \n",
       " 434   0.4998  0.0099  0.0091  0.0111  122.4135         -1  \n",
       " 435   0.5037  0.0148  0.0254  0.0047   18.5390         -1  \n",
       " 436   0.5004  0.0316  0.0329  0.0055   16.6695         -1  \n",
       " 437   0.5033  0.0139  0.0329  0.0055   16.6695         -1  \n",
       " 438   0.5079  0.0157  0.0329  0.0055   16.6695         -1  \n",
       " 439   0.5014  0.0109  0.0329  0.0055   16.6695         -1  \n",
       " 440   0.5002  0.0142  0.0329  0.0055   16.6695         -1  \n",
       " 442   0.4983  0.0125  0.0329  0.0055   16.6695         -1  \n",
       " 443   0.5001  0.0110  0.0282  0.0194   68.7444         -1  \n",
       " 444   0.5036  0.0091  0.0282  0.0194   68.7444         -1  \n",
       " 445   0.5090  0.0168  0.0282  0.0194   68.7444         -1  \n",
       " 446   0.5022  0.0111  0.0283  0.0204   72.2002         -1  \n",
       " 447   0.5037  0.0119  0.0235  0.0355  150.7761         -1  \n",
       " 449   0.4986  0.0110  0.0235  0.0355  150.7761         -1  \n",
       " 450   0.5019  0.0139  0.0235  0.0355  150.7761         -1  \n",
       " 451   0.4960  0.0168  0.0235  0.0355  150.7761         -1  \n",
       " 452   0.4977  0.0152  0.0235  0.0355  150.7761         -1  \n",
       " 453   0.4997  0.0126  0.0235  0.0355  150.7761         -1  \n",
       " 454   0.4941  0.0142  0.0235  0.0355  150.7761         -1  \n",
       " 455   0.4976  0.0254  0.0262  0.0123   47.1425         -1  \n",
       " 456   0.4990  0.0139  0.0050  0.0138  276.8808         -1  \n",
       " 457   0.4954  0.0114  0.0050  0.0138  276.8808         -1  \n",
       " 458   0.5065  0.0141  0.0050  0.0138  276.8808         -1  \n",
       " 459   0.5056  0.0137  0.0232  0.0071   30.4587         -1  \n",
       " 460   0.4995  0.0164  0.0232  0.0071   30.4587         -1  \n",
       " 461   0.5085  0.0216  0.0232  0.0071   30.4587         -1  \n",
       " 462   0.4962  0.0138  0.0155  0.0101   64.8825         -1  \n",
       " 463   0.5037  0.0106  0.0155  0.0101   64.8825         -1  \n",
       " 464   0.5041  0.0108  0.0155  0.0101   64.8825         -1  \n",
       " 465   0.5045  0.0111  0.0193  0.0122   63.0838         -1  \n",
       " 466   0.5023  0.0127  0.0193  0.0122   63.0838         -1  \n",
       " 467   0.4987  0.0138  0.0193  0.0122   63.0838         -1  \n",
       " 468   0.4997  0.0188  0.0193  0.0122   63.0838         -1  \n",
       " 469   0.4993  0.0130  0.0193  0.0122   63.0838         -1  \n",
       " 470   0.4985  0.0150  0.0193  0.0122   63.0838         -1  \n",
       " 471   0.5026  0.0111  0.0189  0.0072   37.8921         -1  \n",
       " 472   0.5015  0.0157  0.0189  0.0072   37.8921         -1  \n",
       " 473   0.5031  0.0127  0.0189  0.0072   37.8921         -1  \n",
       " 474   0.4995  0.0247  0.0189  0.0072   37.8921         -1  \n",
       " 475   0.4959  0.0180  0.0114  0.0220  193.4385         -1  \n",
       " 476   0.5050  0.0081  0.0208  0.0287  138.2861         -1  \n",
       " 477   0.5027  0.0115  0.0208  0.0287  138.2861         -1  \n",
       " 478   0.5015  0.0128  0.0208  0.0287  138.2861         -1  \n",
       " 479   0.4962  0.0114  0.0208  0.0287  138.2861         -1  \n",
       " 480   0.4988  0.0154  0.0208  0.0287  138.2861         -1  \n",
       " 481   0.5024  0.0134  0.0147  0.0100   68.2310         -1  \n",
       " 482   0.5031  0.0122  0.0316  0.0098   31.0788         -1  \n",
       " 483   0.4997  0.0125  0.0316  0.0098   31.0788         -1  \n",
       " 484   0.4989  0.0124  0.0316  0.0098   31.0788         -1  \n",
       " 485   0.4902  0.0097  0.0316  0.0098   31.0788         -1  \n",
       " 486   0.4990  0.0095  0.0316  0.0098   31.0788         -1  \n",
       " 487   0.4952  0.0155  0.0316  0.0098   31.0788         -1  \n",
       " 488   0.5028  0.0151  0.0316  0.0098   31.0788         -1  \n",
       " 489   0.5037  0.0102  0.0279  0.0335  119.9899         -1  \n",
       " 490   0.5038  0.0170  0.0279  0.0335  119.9899         -1  \n",
       " 491   0.5039  0.0113  0.0279  0.0335  119.9899         -1  \n",
       " 492   0.4993  0.0134  0.0279  0.0335  119.9899         -1  \n",
       " 493   0.5055  0.0158  0.0255  0.0260  102.1652         -1  \n",
       " 494   0.5007  0.0108  0.0255  0.0260  102.1652         -1  \n",
       " 496   0.4962  0.0187  0.0255  0.0260  102.1652         -1  \n",
       " 497   0.4987  0.0113  0.0255  0.0260  102.1652         -1  \n",
       " 498   0.4990  0.0236  0.0048  0.0226  474.0812         -1  \n",
       " 499   0.4994  0.0151  0.0048  0.0226  474.0812         -1  \n",
       " 500   0.5038  0.0150  0.0048  0.0226  474.0812         -1  \n",
       " 501   0.5042  0.0099  0.0048  0.0226  474.0812         -1  \n",
       " 502   0.4985  0.0126  0.0048  0.0226  474.0812         -1  \n",
       " 503   0.4982  0.0189  0.0048  0.0226  474.0812         -1  \n",
       " 504   0.4978  0.0193  0.0048  0.0226  474.0812         -1  \n",
       " 505   0.4995  0.0100  0.0048  0.0226  474.0812         -1  \n",
       " 506   0.5002  0.0175  0.0048  0.0226  474.0812         -1  \n",
       " 507   0.4976  0.0138  0.0048  0.0226  474.0812         -1  \n",
       " 509   0.5001  0.0149  0.0048  0.0226  474.0812         -1  \n",
       " 510   0.5011  0.0109  0.0048  0.0226  474.0812         -1  \n",
       " 511   0.4997  0.0140  0.0048  0.0226  474.0812         -1  \n",
       " 512   0.4925  0.0232  0.0279  0.0217   77.7519         -1  \n",
       " 513   0.5052  0.0168  0.0279  0.0217   77.7519         -1  \n",
       " 514   0.4982  0.0162  0.0279  0.0217   77.7519         -1  \n",
       " 515   0.4993  0.0111  0.0279  0.0217   77.7519         -1  \n",
       " 516   0.4999  0.0199  0.0279  0.0217   77.7519         -1  \n",
       " 517   0.4956  0.0104  0.0331  0.0210   63.2615         -1  \n",
       " 519   0.4988  0.0140  0.0331  0.0210   63.2615         -1  \n",
       " 520   0.5036  0.0182  0.0331  0.0210   63.2615         -1  \n",
       " 521   0.4981  0.0184  0.0331  0.0210   63.2615         -1  \n",
       " 522   0.4994  0.0107  0.0331  0.0210   63.2615         -1  \n",
       " 523   0.5020  0.0174  0.0370  0.0178   48.0601         -1  \n",
       " 524   0.5020  0.0107  0.0091  0.0085   93.0476         -1  \n",
       " 525   0.5046  0.0162  0.0217  0.0282  130.1233         -1  \n",
       " 526   0.4972  0.0174  0.0217  0.0282  130.1233         -1  \n",
       " 527   0.4991  0.0161  0.0222  0.0182   81.9472         -1  \n",
       " 528   0.4954  0.0103  0.0222  0.0182   81.9472         -1  \n",
       " 529   0.4998  0.0131  0.0222  0.0182   81.9472         -1  \n",
       " 530   0.5016  0.0152  0.0465  0.0299   64.2405         -1  \n",
       " 531   0.4953  0.0105 -0.0012  0.0252    0.0000         -1  \n",
       " 532   0.4958  0.0111 -0.0012  0.0252    0.0000         -1  \n",
       " 533   0.4962  0.0086 -0.0012  0.0252    0.0000         -1  \n",
       " 534   0.4983  0.0159 -0.0012  0.0252    0.0000         -1  \n",
       " 535   0.5008  0.0115 -0.0012  0.0252    0.0000         -1  \n",
       " 536   0.4967  0.0110 -0.0012  0.0252    0.0000         -1  \n",
       " 537   0.5008  0.0154 -0.0012  0.0252    0.0000         -1  \n",
       " 538   0.5058  0.0104 -0.0012  0.0252    0.0000         -1  \n",
       " 539   0.5011  0.0153 -0.0012  0.0252    0.0000         -1  \n",
       " 540   0.4954  0.0136 -0.0012  0.0252    0.0000         -1  \n",
       " 541   0.4962  0.0343 -0.0012  0.0252    0.0000         -1  \n",
       " 542   0.5056  0.0184  0.0451  0.0087   19.3621         -1  \n",
       " 543   0.4999  0.0133  0.0492  0.0182   36.9980         -1  \n",
       " 544   0.4969  0.0137  0.0334  0.0188   56.2041         -1  \n",
       " 545   0.5003  0.0233  0.0334  0.0188   56.2041         -1  \n",
       " 546   0.5029  0.0206  0.0188  0.0172   91.3965         -1  \n",
       " 547   0.4971  0.0121  0.0188  0.0172   91.3965         -1  \n",
       " 548   0.4952  0.0120  0.0354  0.0146   41.3226         -1  \n",
       " 549   0.4924  0.0111  0.0354  0.0146   41.3226         -1  \n",
       " 550   0.4953  0.0170  0.0335  0.0122   36.2970         -1  \n",
       " 551   0.5011  0.0179  0.0335  0.0122   36.2970         -1  \n",
       " 552   0.4968  0.0178  0.0335  0.0122   36.2970         -1  \n",
       " 553   0.4996  0.0127  0.0335  0.0122   36.2970         -1  \n",
       " 554   0.5045  0.0212  0.0335  0.0122   36.2970         -1  \n",
       " 555   0.5008  0.0127  0.0335  0.0122   36.2970         -1  \n",
       " 556   0.4955  0.0088  0.0335  0.0122   36.2970         -1  \n",
       " 557   0.5019  0.0124  0.0335  0.0122   36.2970         -1  \n",
       " 558   0.5033  0.0132  0.0335  0.0122   36.2970         -1  \n",
       " 559   0.5050  0.0174  0.0335  0.0122   36.2970         -1  \n",
       " 560   0.5010  0.0157  0.0164  0.0093   56.7050         -1  \n",
       " 561   0.5020  0.0191  0.0164  0.0093   56.7050         -1  \n",
       " 562   0.5042  0.0141  0.0267  0.0134   50.0530         -1  \n",
       " 563   0.4985  0.0155  0.0184  0.0127   68.7057         -1  \n",
       " 564   0.5022  0.0105  0.0184  0.0127   68.7057         -1  \n",
       " 565   0.4999  0.0288  0.0184  0.0127   68.7057         -1  \n",
       " 566   0.4979  0.0165  0.0184  0.0127   68.7057         -1  \n",
       " 567   0.5036  0.0136  0.0348  0.0160   45.9641         -1  \n",
       " 568   0.4948  0.0129  0.0348  0.0160   45.9641         -1  \n",
       " 569   0.4969  0.0116  0.0142  0.0102   71.7780         -1  \n",
       " 570   0.5031  0.0220  0.0142  0.0102   71.7780         -1  \n",
       " 571   0.4966  0.0097  0.0142  0.0102   71.7780         -1  \n",
       " 572   0.5029  0.0198  0.0142  0.0102   71.7780         -1  \n",
       " 573   0.4991  0.0121  0.0142  0.0102   71.7780         -1  \n",
       " 574   0.5009  0.0142  0.0277  0.0118   42.7294         -1  \n",
       " 575   0.5039  0.0104  0.0277  0.0118   42.7294         -1  \n",
       " 577   0.5023  0.0156  0.0182  0.0139   76.6094         -1  \n",
       " 578   0.4963  0.0150  0.0182  0.0139   76.6094         -1  \n",
       " 579   0.4970  0.0143  0.0182  0.0139   76.6094         -1  \n",
       " 580   0.4999  0.0100  0.0332  0.0216   65.1043         -1  \n",
       " 581   0.5060  0.0195  0.0332  0.0216   65.1043         -1  \n",
       " 582   0.5006  0.0152  0.0332  0.0216   65.1043         -1  \n",
       " 584   0.4968  0.0096  0.0353  0.0298   84.3454         -1  \n",
       " 585   0.4982  0.0125  0.0353  0.0298   84.3454         -1  \n",
       " 586   0.4954  0.0096  0.0353  0.0298   84.3454         -1  \n",
       " 587   0.4936  0.0223  0.0192  0.0167   86.9681         -1  \n",
       " 588   0.4926  0.0144  0.0256  0.0189   73.8657         -1  \n",
       " 589   0.5003  0.0214  0.0256  0.0189   73.8657         -1  \n",
       " 590   0.5098  0.0173  0.0256  0.0189   73.8657         -1  \n",
       " 591   0.5006  0.0106  0.0256  0.0189   73.8657         -1  \n",
       " 592   0.4985  0.0233  0.0256  0.0189   73.8657         -1  \n",
       " 593   0.5004  0.0179  0.0256  0.0189   73.8657         -1  \n",
       " 594   0.4937  0.0153  0.0256  0.0189   73.8657         -1  \n",
       " 595   0.5004  0.0151  0.0256  0.0189   73.8657         -1  \n",
       " 596   0.5011  0.0157  0.0256  0.0189   73.8657         -1  \n",
       " 597   0.5005  0.0155  0.0256  0.0189   73.8657         -1  \n",
       " 598   0.5041  0.0082  0.0256  0.0189   73.8657         -1  \n",
       " 599   0.5018  0.0101  0.0256  0.0189   73.8657         -1  \n",
       " 600   0.5018  0.0324  0.0225  0.0193   85.7175         -1  \n",
       " 602   0.4936  0.0150  0.0225  0.0193   85.7175         -1  \n",
       " 603   0.5043  0.0128  0.0199  0.0159   79.7752         -1  \n",
       " 604   0.5012  0.0146  0.0199  0.0159   79.7752         -1  \n",
       " 606   0.5029  0.0152  0.0199  0.0159   79.7752         -1  \n",
       " 607   0.5027  0.0243  0.0199  0.0159   79.7752         -1  \n",
       " 608   0.4982  0.0194  0.0252  0.0157   62.3881         -1  \n",
       " 609   0.4995  0.0154  0.0252  0.0157   62.3881         -1  \n",
       " 610   0.5015  0.0137  0.0252  0.0157   62.3881         -1  \n",
       " 611   0.4973  0.0129  0.0252  0.0157   62.3881         -1  \n",
       " 612   0.5097  0.0438  0.0252  0.0157   62.3881         -1  \n",
       " 613   0.5022  0.0193  0.0252  0.0157   62.3881         -1  \n",
       " 614   0.5027  0.0154  0.0252  0.0157   62.3881         -1  \n",
       " 615   0.4983  0.0100  0.0192  0.0077   40.2536         -1  \n",
       " 616   0.4977  0.0136  0.0192  0.0077   40.2536         -1  \n",
       " 617   0.4981  0.0102  0.0192  0.0077   40.2536         -1  \n",
       " 618   0.4959  0.0126  0.0227  0.0149   65.4831         -1  \n",
       " 619   0.4980  0.0118  0.0227  0.0149   65.4831         -1  \n",
       " 620   0.4982  0.0120  0.0227  0.0149   65.4831         -1  \n",
       " 621   0.5051  0.0086  0.0227  0.0149   65.4831         -1  \n",
       " 622   0.5035  0.0462  0.0227  0.0149   65.4831         -1  \n",
       " 623   0.5015  0.0127  0.0227  0.0149   65.4831         -1  \n",
       " 624   0.4933  0.0126  0.0227  0.0149   65.4831         -1  \n",
       " 625   0.5071  0.0123  0.0227  0.0149   65.4831         -1  \n",
       " 626   0.5015  0.0130  0.0227  0.0149   65.4831         -1  \n",
       " 627   0.4973  0.0073  0.0300  0.0326  108.6076         -1  \n",
       " 628   0.5006  0.0068  0.0300  0.0326  108.6076         -1  \n",
       " 629   0.5014  0.0090  0.0300  0.0326  108.6076         -1  \n",
       " 630   0.5033  0.0209  0.0300  0.0326  108.6076         -1  \n",
       " 631   0.4988  0.0406  0.0300  0.0326  108.6076         -1  \n",
       " 632   0.4982  0.0130  0.0300  0.0326  108.6076         -1  \n",
       " 633   0.5071  0.0141  0.0210  0.0107   50.7912         -1  \n",
       " 635   0.4998  0.0097  0.0328  0.0235   71.5333         -1  \n",
       " 636   0.5029  0.0186  0.0328  0.0235   71.5333         -1  \n",
       " 637   0.4980  0.0130  0.0328  0.0235   71.5333         -1  \n",
       " 638   0.4989  0.0091  0.0328  0.0235   71.5333         -1  \n",
       " 639   0.4988  0.0178  0.0111  0.0294  264.1507         -1  \n",
       " 640   0.5007  0.0135  0.0111  0.0294  264.1507         -1  \n",
       " 641   0.4957  0.0151  0.0106  0.0277  262.1302         -1  \n",
       " 642   0.4974  0.0114  0.0106  0.0277  262.1302         -1  \n",
       " 643   0.5043  0.0155  0.0106  0.0277  262.1302         -1  \n",
       " 644   0.5044  0.0139  0.0211  0.0106   50.0650         -1  \n",
       " 645   0.5018  0.0101  0.0211  0.0106   50.0650         -1  \n",
       " 646   0.4983  0.0103  0.0211  0.0106   50.0650         -1  \n",
       " 647   0.5034  0.0130  0.0211  0.0106   50.0650         -1  \n",
       " 648   0.5036  0.0152  0.0211  0.0106   50.0650         -1  \n",
       " 649   0.5005  0.0169  0.0152  0.0115   75.2700         -1  \n",
       " 650   0.4995  0.0215  0.0297  0.0115   38.7106         -1  \n",
       " 651   0.4993  0.0121  0.0297  0.0115   38.7106         -1  \n",
       " 652   0.4941  0.0097  0.0477  0.0443   92.7605         -1  \n",
       " 653   0.4991  0.0134  0.0477  0.0443   92.7605         -1  \n",
       " 654   0.4984  0.0161  0.0140  0.0112   80.4663         -1  \n",
       " 655   0.5000  0.0131  0.0140  0.0112   80.4663         -1  \n",
       " 656   0.5052  0.0123  0.0140  0.0112   80.4663         -1  \n",
       " 657   0.4953  0.0156  0.0140  0.0112   80.4663         -1  \n",
       " 658   0.5060  0.0134  0.0140  0.0112   80.4663         -1  \n",
       " 659   0.5006  0.0149  0.0140  0.0112   80.4663         -1  \n",
       " 660   0.4948  0.0104  0.0140  0.0112   80.4663         -1  \n",
       " 661   0.5014  0.0125  0.0165  0.0114   68.9871         -1  \n",
       " 662   0.5010  0.0147  0.0165  0.0114   68.9871         -1  \n",
       " 663   0.5016  0.0155  0.0263  0.0117   44.3686         -1  \n",
       " 664   0.5040  0.0115  0.0263  0.0117   44.3686         -1  \n",
       " 665   0.5037  0.0165  0.0263  0.0117   44.3686         -1  \n",
       " 666   0.4973  0.0191  0.0263  0.0117   44.3686         -1  \n",
       " 667   0.5006  0.0216  0.0263  0.0117   44.3686         -1  \n",
       " 668   0.4977  0.0137  0.0263  0.0117   44.3686         -1  \n",
       " 669   0.5037  0.0175  0.0263  0.0117   44.3686         -1  \n",
       " 670   0.4992  0.0128  0.0263  0.0117   44.3686         -1  \n",
       " 671   0.4924  0.0124  0.0263  0.0117   44.3686         -1  \n",
       " 672   0.5015  0.0313  0.0106  0.0212  200.1816         -1  \n",
       " 673   0.5010  0.0146  0.0106  0.0212  200.1816         -1  \n",
       " 674   0.5010  0.0142  0.0106  0.0212  200.1816         -1  \n",
       " 675   0.5000  0.0128  0.0106  0.0212  200.1816         -1  \n",
       " 676   0.4979  0.0125  0.0106  0.0212  200.1816         -1  \n",
       " 677   0.5020  0.0167  0.0274  0.0072   26.1430         -1  \n",
       " 678   0.5006  0.0097  0.0274  0.0072   26.1430         -1  \n",
       " 679   0.5028  0.0164  0.0274  0.0072   26.1430         -1  \n",
       " 680   0.4934  0.0126  0.0274  0.0072   26.1430         -1  \n",
       " 681   0.4975  0.0148  0.0274  0.0072   26.1430         -1  \n",
       " 682   0.5008  0.0147  0.0274  0.0072   26.1430         -1  \n",
       " 683   0.5022  0.0104  0.0274  0.0072   26.1430         -1  \n",
       " 684   0.4962  0.0123  0.0274  0.0072   26.1430         -1  \n",
       " 685   0.4982  0.0161  0.0274  0.0072   26.1430         -1  \n",
       " 686   0.5026  0.0121  0.0326  0.0078   23.9629         -1  \n",
       " 687   0.4965  0.0108  0.0326  0.0078   23.9629         -1  \n",
       " 688   0.4994  0.0182  0.0326  0.0078   23.9629         -1  \n",
       " 689   0.4984  0.0116  0.0154  0.0113   73.6411         -1  \n",
       " 690   0.5040  0.0091  0.0154  0.0113   73.6411         -1  \n",
       " 691   0.5013  0.0095  0.0154  0.0113   73.6411         -1  \n",
       " 692   0.5004  0.0119  0.0154  0.0113   73.6411         -1  \n",
       " 693   0.5033  0.0122  0.0123  0.0129  104.7519         -1  \n",
       " 694   0.5043  0.0214  0.0123  0.0129  104.7519         -1  \n",
       " 695   0.5032  0.0112  0.0123  0.0129  104.7519         -1  \n",
       " 696   0.4993  0.0120  0.0123  0.0129  104.7519         -1  \n",
       " 697   0.4927  0.0095  0.0123  0.0129  104.7519         -1  \n",
       " 698   0.5031  0.0133  0.0123  0.0129  104.7519         -1  \n",
       " 699   0.4987  0.0105  0.0123  0.0129  104.7519         -1  \n",
       " 700   0.5005  0.0099  0.0123  0.0129  104.7519         -1  \n",
       " 701   0.4965  0.0097  0.0123  0.0129  104.7519         -1  \n",
       " 702   0.5014  0.0100  0.0123  0.0129  104.7519         -1  \n",
       " 703   0.4982  0.0132  0.0161  0.0350  217.1506         -1  \n",
       " 704   0.5017  0.0112  0.0161  0.0350  217.1506         -1  \n",
       " 705   0.5014  0.0133  0.0161  0.0350  217.1506         -1  \n",
       " 706   0.4980  0.0174  0.0161  0.0350  217.1506         -1  \n",
       " 707   0.5043  0.0115  0.0161  0.0350  217.1506         -1  \n",
       " 708   0.5029  0.0110  0.0161  0.0350  217.1506         -1  \n",
       " 710   0.4954  0.0150  0.0300  0.0076   25.2223         -1  \n",
       " 711   0.5017  0.0162  0.0072  0.0210  291.8040         -1  \n",
       " 712   0.4969  0.0139  0.0072  0.0210  291.8040         -1  \n",
       " 713   0.5028  0.0239  0.0072  0.0210  291.8040         -1  \n",
       " 714   0.4997  0.0109  0.0072  0.0210  291.8040         -1  \n",
       " 715   0.4963  0.0153  0.0072  0.0210  291.8040         -1  \n",
       " 716   0.4978  0.0135  0.0072  0.0210  291.8040         -1  \n",
       " 717   0.5001  0.0190  0.0263  0.0131   49.9454         -1  \n",
       " 718   0.4918  0.0136  0.0121  0.0121  100.3091         -1  \n",
       " 719   0.4995  0.0101  0.0121  0.0121  100.3091         -1  \n",
       " 720   0.4944  0.0184  0.0121  0.0121  100.3091         -1  \n",
       " 721   0.4972  0.0117  0.0121  0.0121  100.3091         -1  \n",
       " 722   0.4997  0.0151  0.0121  0.0121  100.3091         -1  \n",
       " 723   0.4948  0.0182  0.0121  0.0121  100.3091         -1  \n",
       " 724   0.4953  0.0170  0.0472  0.0171   36.1626         -1  \n",
       " 725   0.5003  0.0142  0.0131  0.0112   85.1551         -1  \n",
       " 726   0.5005  0.0118  0.0131  0.0112   85.1551         -1  \n",
       " 727   0.4993  0.0133  0.0131  0.0112   85.1551         -1  \n",
       " 728   0.5030  0.0105  0.0131  0.0112   85.1551         -1  \n",
       " 729   0.5013  0.0187  0.0131  0.0112   85.1551         -1  \n",
       " 730   0.5022  0.0173  0.0335  0.0084   25.1494         -1  \n",
       " 731   0.5038  0.0103  0.0335  0.0084   25.1494         -1  \n",
       " 732   0.4941  0.0106  0.0335  0.0084   25.1494         -1  \n",
       " 733   0.4965  0.0186  0.0335  0.0084   25.1494         -1  \n",
       " 734   0.4929  0.0155  0.0335  0.0084   25.1494         -1  \n",
       " 735   0.5000  0.0148  0.0171  0.0173  101.1981         -1  \n",
       " 736   0.4976  0.0125  0.0270  0.0162   59.9813         -1  \n",
       " 737   0.4936  0.0115  0.0270  0.0162   59.9813         -1  \n",
       " 738   0.4991  0.0146  0.0270  0.0162   59.9813         -1  \n",
       " 739   0.4922  0.0177  0.0270  0.0162   59.9813         -1  \n",
       " 740   0.4977  0.0097  0.0270  0.0162   59.9813         -1  \n",
       " 741   0.5007  0.0116  0.0270  0.0162   59.9813         -1  \n",
       " 742   0.5059  0.0160  0.0270  0.0162   59.9813         -1  \n",
       " 743   0.5039  0.0169  0.0270  0.0162   59.9813         -1  \n",
       " 744   0.4939  0.0132  0.0230  0.0064   27.7705         -1  \n",
       " 745   0.4942  0.0197  0.0419  0.0098   23.3852         -1  \n",
       " 746   0.5021  0.0131  0.0419  0.0098   23.3852         -1  \n",
       " 747   0.5010  0.0144  0.0419  0.0098   23.3852         -1  \n",
       " 748   0.4974  0.0104  0.0419  0.0098   23.3852         -1  \n",
       " 749   0.4958  0.0170  0.0419  0.0098   23.3852         -1  \n",
       " 750   0.4965  0.0217  0.0419  0.0098   23.3852         -1  \n",
       " 751   0.4989  0.0167  0.0119  0.0054   45.7004         -1  \n",
       " 752   0.5070  0.0135  0.0119  0.0054   45.7004         -1  \n",
       " 753   0.4992  0.0163  0.0119  0.0054   45.7004         -1  \n",
       " 754   0.5045  0.0116  0.0136  0.0171  125.5354         -1  \n",
       " 755   0.4995  0.0121  0.0169  0.0276  163.9998         -1  \n",
       " 756   0.4979  0.0159  0.0169  0.0276  163.9998         -1  \n",
       " 757   0.4978  0.0141  0.0169  0.0276  163.9998         -1  \n",
       " 758   0.4941  0.0164  0.0169  0.0276  163.9998         -1  \n",
       " 759   0.4997  0.0090  0.0169  0.0276  163.9998         -1  \n",
       " 760   0.4943  0.0199  0.0169  0.0276  163.9998         -1  \n",
       " 761   0.5015  0.0145  0.0169  0.0276  163.9998         -1  \n",
       " 762   0.4934  0.0109  0.0169  0.0276  163.9998         -1  \n",
       " 763   0.4973  0.0194  0.0169  0.0276  163.9998         -1  \n",
       " 764   0.4988  0.0167  0.0386  0.0074   19.0852         -1  \n",
       " 765   0.5010  0.0086  0.0212  0.0109   51.5472         -1  \n",
       " 766   0.4937  0.0119  0.0212  0.0109   51.5472         -1  \n",
       " 767   0.4954  0.0151  0.0212  0.0109   51.5472         -1  \n",
       " 768   0.4995  0.0176  0.0212  0.0109   51.5472         -1  \n",
       " 769   0.5006  0.0128  0.0212  0.0109   51.5472         -1  \n",
       " 770   0.4970  0.0118  0.0212  0.0109   51.5472         -1  \n",
       " 771   0.5031  0.0146  0.0212  0.0109   51.5472         -1  \n",
       " 772   0.4951  0.0181  0.0262  0.0121   46.1016         -1  \n",
       " 773   0.4989  0.0082  0.0373  0.0079   21.0599         -1  \n",
       " 774   0.5013  0.0111  0.0373  0.0079   21.0599         -1  \n",
       " 775   0.4964  0.0136  0.0373  0.0079   21.0599         -1  \n",
       " 776   0.4974  0.0181  0.0373  0.0079   21.0599         -1  \n",
       " 777   0.4995  0.0144  0.0373  0.0079   21.0599         -1  \n",
       " 778   0.5010  0.0086  0.0373  0.0079   21.0599         -1  \n",
       " 779   0.5002  0.0143  0.0373  0.0079   21.0599         -1  \n",
       " 780   0.5019  0.0222  0.0373  0.0079   21.0599         -1  \n",
       " 781   0.5056  0.0208  0.0373  0.0079   21.0599         -1  \n",
       " 782   0.4986  0.0172  0.0373  0.0079   21.0599         -1  \n",
       " 783   0.4977  0.0158  0.0373  0.0079   21.0599         -1  \n",
       " 784   0.4982  0.0095  0.0373  0.0079   21.0599         -1  \n",
       " 785   0.4999  0.0121  0.0373  0.0079   21.0599         -1  \n",
       " 786   0.5012  0.0125  0.0373  0.0079   21.0599         -1  \n",
       " 787   0.5012  0.0194  0.0373  0.0079   21.0599         -1  \n",
       " 788   0.5034  0.0141  0.0246  0.0064   25.9900         -1  \n",
       " 789   0.4983  0.0153  0.0246  0.0064   25.9900         -1  \n",
       " 790   0.5010  0.0252  0.0246  0.0064   25.9900         -1  \n",
       " 791   0.4975  0.0142  0.0246  0.0064   25.9900         -1  \n",
       " 792   0.5008  0.0115  0.0215  0.0071   33.1090         -1  \n",
       " 793   0.4975  0.0144  0.0215  0.0071   33.1090         -1  \n",
       " 794   0.4948  0.0124  0.0215  0.0071   33.1090         -1  \n",
       " 796   0.4949  0.0097  0.0406  0.0268   66.1687         -1  \n",
       " 798   0.5000  0.0095  0.0122  0.0227  187.0796         -1  \n",
       " 799   0.4992  0.0098  0.0122  0.0227  187.0796         -1  \n",
       " 800   0.4942  0.0135  0.0122  0.0227  187.0796         -1  \n",
       " 801   0.5011  0.0193  0.0122  0.0227  187.0796         -1  \n",
       " 802   0.4959  0.0150  0.0122  0.0227  187.0796         -1  \n",
       " 803   0.4967  0.0110  0.0253  0.0058   22.7661         -1  \n",
       " 804   0.5000  0.0117  0.0253  0.0058   22.7661         -1  \n",
       " 805   0.4949  0.0155  0.0253  0.0058   22.7661         -1  \n",
       " 806   0.4962  0.0124  0.0253  0.0058   22.7661         -1  \n",
       " 807   0.4995  0.0154  0.0253  0.0058   22.7661         -1  \n",
       " 808   0.4941  0.0096  0.0269  0.0160   59.2045         -1  \n",
       " 809   0.4968  0.0114  0.0269  0.0160   59.2045         -1  \n",
       " 810   0.4939  0.0160  0.0269  0.0160   59.2045         -1  \n",
       " 811   0.4953  0.0128  0.0111  0.0069   62.3602         -1  \n",
       " 812   0.4974  0.0119  0.0111  0.0069   62.3602         -1  \n",
       " 813   0.4954  0.0093  0.0111  0.0069   62.3602         -1  \n",
       " 814   0.4994  0.0073  0.0111  0.0069   62.3602         -1  \n",
       " 815   0.4963  0.0129  0.0294  0.0051   17.3775         -1  \n",
       " 816   0.5037  0.0392  0.0294  0.0051   17.3775         -1  \n",
       " 817   0.5002  0.0124  0.0213  0.0076   35.5725         -1  \n",
       " 818   0.4984  0.0169  0.0275  0.0215   78.1199         -1  \n",
       " 819   0.5024  0.0108  0.0275  0.0215   78.1199         -1  \n",
       " 820   0.4970  0.0150  0.0275  0.0215   78.1199         -1  \n",
       " 821   0.4979  0.0078  0.0275  0.0215   78.1199         -1  \n",
       " 822   0.4999  0.0103  0.0275  0.0215   78.1199         -1  \n",
       " 823   0.5021  0.0086  0.0275  0.0215   78.1199         -1  \n",
       " 824   0.5004  0.0135  0.0275  0.0215   78.1199         -1  \n",
       " 825   0.4987  0.0131  0.0275  0.0215   78.1199         -1  \n",
       " 827   0.4961  0.0157  0.0545  0.0184   33.7876         -1  \n",
       " 828   0.4954  0.0122  0.0545  0.0184   33.7876         -1  \n",
       " 829   0.5021  0.0071  0.0545  0.0184   33.7876         -1  \n",
       " 830   0.5004  0.0120  0.0545  0.0184   33.7876         -1  \n",
       " 832   0.4987  0.0118  0.0545  0.0184   33.7876         -1  \n",
       " 833   0.4934  0.0123  0.0545  0.0184   33.7876         -1  \n",
       " 834   0.4987  0.0145  0.0545  0.0184   33.7876         -1  \n",
       " 835   0.5033  0.0154  0.0099  0.0113  114.2878         -1  \n",
       " 836   0.4963  0.0156  0.0099  0.0113  114.2878         -1  \n",
       " 837   0.4925  0.0145  0.0099  0.0113  114.2878         -1  \n",
       " 838   0.5032  0.0129  0.0099  0.0113  114.2878         -1  \n",
       " 839   0.4978  0.0133  0.0128  0.0193  151.1930         -1  \n",
       " 840   0.4985  0.0101  0.0128  0.0193  151.1930         -1  \n",
       " 841   0.4989  0.0125  0.0128  0.0193  151.1930         -1  \n",
       " 842   0.5035  0.0142  0.0128  0.0193  151.1930         -1  \n",
       " 843   0.4944  0.0123  0.0200  0.0095   47.7832         -1  \n",
       " 844   0.4959  0.0142  0.0278  0.0135   48.4818         -1  \n",
       " 845   0.5005  0.0145  0.0278  0.0135   48.4818         -1  \n",
       " 846   0.4994  0.0125  0.0278  0.0135   48.4818         -1  \n",
       " 847   0.5027  0.0155  0.0503  0.0334   66.5448         -1  \n",
       " 848   0.5013  0.0111  0.0221  0.0235  106.1758         -1  \n",
       " 849   0.5025  0.0118  0.0364  0.0172   47.2136         -1  \n",
       " 850   0.4999  0.0122  0.0364  0.0172   47.2136         -1  \n",
       " 851   0.4992  0.0141  0.0364  0.0172   47.2136         -1  \n",
       " 852   0.4999  0.0098  0.0429  0.0401   93.5134         -1  \n",
       " 853   0.5033  0.0146  0.0281  0.0227   80.5639         -1  \n",
       " 854   0.5005  0.0130  0.0281  0.0227   80.5639         -1  \n",
       " 855   0.4956  0.0437  0.0281  0.0227   80.5639         -1  \n",
       " 856   0.5025  0.0178  0.0286  0.0154   53.8577         -1  \n",
       " 857   0.4964  0.0106  0.0286  0.0154   53.8577         -1  \n",
       " 858   0.5014  0.0081  0.0220  0.0143   65.2186         -1  \n",
       " 859   0.4938  0.0106  0.0220  0.0143   65.2186         -1  \n",
       " 860   0.4985  0.0089  0.0220  0.0143   65.2186         -1  \n",
       " 861   0.4975  0.0134  0.0220  0.0143   65.2186         -1  \n",
       " 862   0.4948  0.0142  0.0241  0.0121   50.0888         -1  \n",
       " 863   0.4991  0.0086  0.0140  0.0192  136.9762         -1  \n",
       " 864   0.4988  0.0192  0.0140  0.0192  136.9762         -1  \n",
       " 865   0.4927  0.0147  0.0473  0.0280   59.0825         -1  \n",
       " 866   0.4957  0.0251  0.0473  0.0280   59.0825         -1  \n",
       " 867   0.5024  0.0096  0.0473  0.0280   59.0825         -1  \n",
       " 868   0.5032  0.0149  0.0473  0.0280   59.0825         -1  \n",
       " 869   0.4954  0.0139  0.0117  0.0262  223.1018         -1  \n",
       " 870   0.5006  0.0113  0.0117  0.0262  223.1018         -1  \n",
       " 872   0.4993  0.0151  0.0117  0.0262  223.1018         -1  \n",
       " 873   0.5059  0.0191  0.0117  0.0262  223.1018         -1  \n",
       " 874   0.4948  0.0145  0.0117  0.0262  223.1018         -1  \n",
       " 875   0.5009  0.0126  0.0117  0.0262  223.1018         -1  \n",
       " 876   0.4993  0.0098  0.0117  0.0262  223.1018         -1  \n",
       " 877   0.5001  0.0168  0.0147  0.0095   65.0365         -1  \n",
       " 878   0.4981  0.0136  0.0147  0.0095   65.0365         -1  \n",
       " 879   0.5022  0.0097  0.0147  0.0095   65.0365         -1  \n",
       " 880   0.4995  0.0139  0.0147  0.0095   65.0365         -1  \n",
       " 881   0.4968  0.0202  0.0262  0.0123   46.7092         -1  \n",
       " 882   0.5010  0.0111  0.0262  0.0123   46.7092         -1  \n",
       " 883   0.4968  0.0217  0.0133  0.0139  104.3034         -1  \n",
       " 884   0.4981  0.0148  0.0133  0.0139  104.3034         -1  \n",
       " 885   0.4936  0.0113  0.0133  0.0139  104.3034         -1  \n",
       " 886   0.4962  0.0106  0.0133  0.0139  104.3034         -1  \n",
       " 887   0.4948  0.0099  0.0133  0.0139  104.3034         -1  \n",
       " 888   0.5016  0.0145  0.0309  0.0317  102.4718         -1  \n",
       " 889   0.5016  0.0112  0.0122  0.0131  107.5257         -1  \n",
       " 890   0.4973  0.0139  0.0122  0.0131  107.5257         -1  \n",
       " 891   0.5080  0.0127  0.0226  0.0196   87.0971         -1  \n",
       " 892   0.4997  0.0177  0.0201  0.0067   33.2533         -1  \n",
       " 893   0.5020  0.0127  0.0242  0.0094   38.9642         -1  \n",
       " 894   0.5014  0.0127  0.0242  0.0094   38.9642         -1  \n",
       " 895   0.5003  0.0175  0.0338  0.0069   20.3091         -1  \n",
       " 896   0.4977  0.0104  0.0338  0.0069   20.3091         -1  \n",
       " 897   0.5087  0.0117  0.0338  0.0069   20.3091         -1  \n",
       " 898   0.5015  0.0126  0.0338  0.0069   20.3091         -1  \n",
       " 899   0.4974  0.0171  0.0218  0.0054   24.6547         -1  \n",
       " 900   0.4998  0.0102  0.0218  0.0054   24.6547         -1  \n",
       " 901   0.4989  0.0131  0.0218  0.0054   24.6547         -1  \n",
       " 902   0.5000  0.0176  0.0218  0.0054   24.6547         -1  \n",
       " 903   0.4974  0.0171  0.0218  0.0054   24.6547         -1  \n",
       " 904   0.5024  0.0185  0.0218  0.0054   24.6547         -1  \n",
       " 905   0.5048  0.0118  0.0211  0.0115   54.4761         -1  \n",
       " 906   0.4983  0.0107  0.0211  0.0115   54.4761         -1  \n",
       " 907   0.5007  0.0099  0.0187  0.0190  101.3876         -1  \n",
       " 908   0.4989  0.0159  0.0187  0.0190  101.3876         -1  \n",
       " 909   0.5028  0.0129  0.0118  0.0095   80.9167         -1  \n",
       " 910   0.4969  0.0114  0.0368  0.0146   39.6224         -1  \n",
       " 911   0.4973  0.0081  0.0186  0.0095   51.2287         -1  \n",
       " 912   0.4995  0.0112  0.0134  0.0121   90.4575         -1  \n",
       " 913   0.4993  0.0117  0.0134  0.0121   90.4575         -1  \n",
       " 915   0.4997  0.0114  0.0280  0.0078   27.7601         -1  \n",
       " 916   0.4958  0.0115  0.0280  0.0078   27.7601         -1  \n",
       " 917   0.4981  0.0092  0.0280  0.0078   27.7601         -1  \n",
       " 918   0.4940  0.0123  0.0280  0.0078   27.7601         -1  \n",
       " 919   0.4994  0.0147  0.0280  0.0078   27.7601         -1  \n",
       " 920   0.5019  0.0128  0.0280  0.0078   27.7601         -1  \n",
       " 921   0.4950  0.0123  0.0195  0.0149   76.0035         -1  \n",
       " 922   0.4987  0.0172  0.0195  0.0149   76.0035         -1  \n",
       " 923   0.4939  0.0210  0.0195  0.0149   76.0035         -1  \n",
       " 925   0.5013  0.0076  0.0153  0.0048   31.0176         -1  \n",
       " 927   0.5016  0.0130  0.0153  0.0048   31.0176         -1  \n",
       " 928   0.5023  0.0140  0.0153  0.0048   31.0176         -1  \n",
       " 930   0.5007  0.0242  0.0153  0.0048   31.0176         -1  \n",
       " 931   0.4778  0.4714  0.0299  0.0118   39.5108         -1  \n",
       " 932   0.5013  0.0118  0.0134  0.0156  116.6826         -1  \n",
       " 933   0.4992  0.0150  0.0134  0.0156  116.6826         -1  \n",
       " 934   0.5002  0.0122  0.0134  0.0156  116.6826         -1  \n",
       " 935   0.5056  0.0110  0.0205  0.0127   62.0056         -1  \n",
       " 936   0.4996  0.0095  0.0205  0.0127   62.0056         -1  \n",
       " 937   0.5024  0.0105  0.0205  0.0127   62.0056         -1  \n",
       " 938   0.5039  0.0215  0.0388  0.0345   88.7443         -1  \n",
       " 939   0.5047  0.0396  0.0388  0.0345   88.7443         -1  \n",
       " 940   0.4955  0.0108  0.0388  0.0345   88.7443         -1  \n",
       " 941   0.5015  0.0134  0.0234  0.0102   43.5691         -1  \n",
       " 942   0.5013  0.0086  0.0052  0.0203  390.4146         -1  \n",
       " 943   0.5056  0.0131  0.0052  0.0203  390.4146         -1  \n",
       " 944   0.5016  0.0197  0.0052  0.0203  390.4146         -1  \n",
       " 945   0.4987  0.0186  0.0052  0.0203  390.4146         -1  \n",
       " 946   0.5034  0.0166  0.0052  0.0203  390.4146         -1  \n",
       " 947   0.4964  0.0120  0.0052  0.0203  390.4146         -1  \n",
       " 948   0.4976  0.0397  0.0295  0.0244   82.5604         -1  \n",
       " 949   0.5016  0.0130  0.0295  0.0244   82.5604         -1  \n",
       " 950   0.4968  0.0173  0.0401  0.0219   54.5609         -1  \n",
       " 951   0.5034  0.0109  0.0207  0.0084   40.8368         -1  \n",
       " 952   0.5006  0.0133  0.0220  0.0110   50.1186         -1  \n",
       " 953   0.5022  0.0163  0.0220  0.0110   50.1186         -1  \n",
       " 954   0.4989  0.0146  0.0220  0.0110   50.1186         -1  \n",
       " 955   0.4990  0.0128  0.0208  0.0066   31.8749         -1  \n",
       " 956   0.5013  0.0227  0.0208  0.0066   31.8749         -1  \n",
       " 957   0.5039  0.0102  0.0260  0.0268  103.3520         -1  \n",
       " 958   0.5028  0.0156  0.0260  0.0268  103.3520         -1  \n",
       " 959   0.5050  0.0221  0.0260  0.0268  103.3520         -1  \n",
       " 960   0.5000  0.0117  0.0178  0.0119   66.7668         -1  \n",
       " 961   0.4981  0.0093  0.0178  0.0119   66.7668         -1  \n",
       " 962   0.5026  0.0125  0.0174  0.0272  156.5532         -1  \n",
       " 963   0.4974  0.0116  0.0174  0.0272  156.5532         -1  \n",
       " 964   0.5003  0.0173  0.0219  0.0088   40.4322         -1  \n",
       " 965   0.4983  0.0096  0.0219  0.0088   40.4322         -1  \n",
       " 966   0.4990  0.0118  0.0219  0.0088   40.4322         -1  \n",
       " 967   0.5015  0.0118  0.0219  0.0088   40.4322         -1  \n",
       " 968   0.5024  0.0105  0.0219  0.0088   40.4322         -1  \n",
       " 969   0.5034  0.0106  0.0219  0.0088   40.4322         -1  \n",
       " 970   0.4999  0.0174  0.0223  0.0159   71.0108         -1  \n",
       " 971   0.4987  0.0112  0.0223  0.0159   71.0108         -1  \n",
       " 972   0.4998  0.0166  0.0223  0.0159   71.0108         -1  \n",
       " 973   0.4986  0.0081  0.0223  0.0159   71.0108         -1  \n",
       " 974   0.5024  0.0094  0.0223  0.0159   71.0108         -1  \n",
       " 975   0.5058  0.0112  0.0223  0.0159   71.0108         -1  \n",
       " 976   0.4946  0.0112  0.0223  0.0159   71.0108         -1  \n",
       " 977   0.4940  0.0199  0.0348  0.0303   86.9740         -1  \n",
       " 978   0.4981  0.0126  0.0200  0.0078   38.9976         -1  \n",
       " 979   0.4963  0.0180  0.0237  0.0187   78.6294         -1  \n",
       " 980   0.5033  0.0146  0.0202  0.0069   34.1793         -1  \n",
       " 981   0.4980  0.0077  0.0202  0.0069   34.1793         -1  \n",
       " 982   0.5056  0.0100  0.0202  0.0069   34.1793         -1  \n",
       " 983   0.4995  0.0125  0.0202  0.0069   34.1793         -1  \n",
       " 984   0.4970  0.0093  0.0088  0.0123  139.7237         -1  \n",
       " 985   0.5037  0.0099  0.0162  0.0058   35.6702         -1  \n",
       " 986   0.5015  0.0119  0.0162  0.0058   35.6702         -1  \n",
       " 987   0.5017  0.0184  0.0162  0.0058   35.6702         -1  \n",
       " 988   0.4982  0.0107  0.0162  0.0058   35.6702         -1  \n",
       " 989   0.4992  0.0122  0.0170  0.0150   88.3754         -1  \n",
       " 990   0.5017  0.0153  0.0170  0.0150   88.3754         -1  \n",
       " 991   0.5034  0.0129  0.0200  0.0205  102.5241         -1  \n",
       " 992   0.4986  0.0111  0.0200  0.0205  102.5241         -1  \n",
       " 993   0.4960  0.0071  0.0200  0.0205  102.5241         -1  \n",
       " 994   0.5012  0.0128  0.0200  0.0205  102.5241         -1  \n",
       " 995   0.5013  0.0208  0.0200  0.0205  102.5241         -1  \n",
       " 996   0.5004  0.0174  0.0200  0.0205  102.5241         -1  \n",
       " 997   0.4992  0.0111  0.0260  0.0107   41.2277         -1  \n",
       " 998   0.4984  0.0110  0.0096  0.0191  197.5077         -1  \n",
       " 999   0.5008  0.0190  0.0096  0.0191  197.5077         -1  \n",
       " 1000  0.5006  0.0149  0.0096  0.0191  197.5077         -1  \n",
       " 1001  0.5012  0.0153  0.0096  0.0191  197.5077         -1  \n",
       " 1002  0.5010  0.0123  0.0096  0.0191  197.5077         -1  \n",
       " 1003  0.5035  0.0129  0.0096  0.0191  197.5077         -1  \n",
       " 1004  0.4936  0.0073  0.0552  0.0178   32.2058         -1  \n",
       " 1005  0.4976  0.0164  0.0552  0.0178   32.2058         -1  \n",
       " 1006  0.5025  0.0125  0.0552  0.0178   32.2058         -1  \n",
       " 1007  0.5020  0.0156  0.0552  0.0178   32.2058         -1  \n",
       " 1008  0.4955  0.0126  0.0167  0.0132   79.1086         -1  \n",
       " 1009  0.5014  0.0097  0.0167  0.0132   79.1086         -1  \n",
       " 1010  0.5015  0.0163  0.0167  0.0132   79.1086         -1  \n",
       " 1011  0.5022  0.0092  0.0167  0.0132   79.1086         -1  \n",
       " 1012  0.5039  0.0151  0.0167  0.0132   79.1086         -1  \n",
       " 1013  0.4999  0.0095  0.0167  0.0132   79.1086         -1  \n",
       " 1014  0.5036  0.0164  0.0167  0.0132   79.1086         -1  \n",
       " 1015  0.4987  0.0127  0.0167  0.0132   79.1086         -1  \n",
       " 1016  0.5045  0.0132  0.0167  0.0132   79.1086         -1  \n",
       " 1017  0.4982  0.0116  0.0167  0.0132   79.1086         -1  \n",
       " 1018  0.5036  0.0130  0.0167  0.0132   79.1086         -1  \n",
       " 1019  0.5025  0.0147  0.0352  0.0108   30.6806         -1  \n",
       " 1020  0.5010  0.0136  0.0237  0.0090   38.0467         -1  \n",
       " 1021  0.5012  0.0143  0.0171  0.0071   41.6047         -1  \n",
       " 1022  0.4964  0.0137  0.0171  0.0071   41.6047         -1  \n",
       " 1023  0.5002  0.0075  0.0121  0.0139  114.4153         -1  \n",
       " 1024  0.4965  0.0120  0.0121  0.0139  114.4153         -1  \n",
       " 1025  0.4945  0.0109  0.0119  0.0123  103.5848         -1  \n",
       " 1026  0.4957  0.0146  0.0119  0.0123  103.5848         -1  \n",
       " 1027  0.5028  0.0127  0.0119  0.0123  103.5848         -1  \n",
       " 1028  0.5023  0.0133  0.0267  0.0174   65.1609         -1  \n",
       " 1030  0.5046  0.0065  0.0267  0.0174   65.1609         -1  \n",
       " 1031  0.4984  0.0130  0.0267  0.0174   65.1609         -1  \n",
       " 1032  0.5062  0.0117  0.0267  0.0174   65.1609         -1  \n",
       " 1033  0.5038  0.0133  0.0292  0.0311  106.5582         -1  \n",
       " 1034  0.5001  0.0099  0.0282  0.0217   76.7510         -1  \n",
       " 1035  0.4964  0.0108  0.0229  0.0182   79.6170         -1  \n",
       " 1036  0.4966  0.0078  0.0229  0.0182   79.6170         -1  \n",
       " 1037  0.4942  0.0172  0.0229  0.0182   79.6170         -1  \n",
       " 1038  0.5020  0.0111  0.0506  0.0245   48.3708         -1  \n",
       " 1039  0.4937  0.0146  0.0437  0.0200   45.7019         -1  \n",
       " 1040  0.5091  0.0123  0.0437  0.0200   45.7019         -1  \n",
       " 1041  0.4956  0.0128  0.0437  0.0200   45.7019         -1  \n",
       " 1042  0.5012  0.0123  0.0437  0.0200   45.7019         -1  \n",
       " 1043  0.5068  0.0426  0.0110  0.0096   87.4677         -1  \n",
       " 1044  0.4998  0.0097  0.0257  0.0174   67.6124         -1  \n",
       " 1045  0.4941  0.0178  0.0257  0.0174   67.6124         -1  \n",
       " 1046  0.4964  0.0125  0.0257  0.0174   67.6124         -1  \n",
       " 1047  0.5044  0.0177  0.0111  0.0195  176.2709         -1  \n",
       " 1048  0.4963  0.0123  0.0111  0.0195  176.2709         -1  \n",
       " 1049  0.5021  0.0129  0.0277  0.0116   41.6717         -1  \n",
       " 1050  0.5023  0.0130  0.0084  0.0097  116.1088         -1  \n",
       " 1051  0.4979  0.0120  0.0084  0.0097  116.1088         -1  \n",
       " 1052  0.5049  0.0181  0.0084  0.0097  116.1088         -1  \n",
       " 1053  0.5084  0.0163  0.0084  0.0097  116.1088         -1  \n",
       " 1054  0.4976  0.0130  0.0084  0.0097  116.1088         -1  \n",
       " 1055  0.5011  0.0103  0.0090  0.0166  183.3928         -1  \n",
       " 1056  0.5028  0.0294  0.0090  0.0166  183.3928         -1  \n",
       " 1057  0.4980  0.0118  0.0090  0.0166  183.3928         -1  \n",
       " 1058  0.5054  0.0213  0.0338  0.0058   17.2268         -1  \n",
       " 1059  0.5005  0.0088  0.0189  0.0059   31.0252         -1  \n",
       " 1060  0.4965  0.0159  0.0189  0.0059   31.0252         -1  \n",
       " 1061  0.5027  0.0147  0.0189  0.0059   31.0252         -1  \n",
       " 1063  0.5053  0.0233  0.0189  0.0059   31.0252         -1  \n",
       " 1064  0.4983  0.0093  0.0189  0.0059   31.0252         -1  \n",
       " 1065  0.4978  0.0121  0.0412  0.0176   42.6972         -1  \n",
       " 1066  0.4962  0.0129  0.0412  0.0176   42.6972         -1  \n",
       " 1067  0.4996  0.0111  0.0412  0.0176   42.6972         -1  \n",
       " 1068  0.4997  0.0144  0.0455  0.0072   15.7327         -1  \n",
       " 1069  0.4977  0.0157  0.0455  0.0072   15.7327         -1  \n",
       " 1070  0.4967  0.0187  0.0455  0.0072   15.7327         -1  \n",
       " 1071  0.4953  0.0156  0.0455  0.0072   15.7327         -1  \n",
       " 1072  0.4987  0.0117  0.0455  0.0072   15.7327         -1  \n",
       " 1073  0.5021  0.0168  0.0276  0.0160   57.7824         -1  \n",
       " 1074  0.4965  0.0147  0.0276  0.0160   57.7824         -1  \n",
       " 1075  0.5016  0.0206  0.0276  0.0160   57.7824         -1  \n",
       " 1076  0.5024  0.0210  0.0276  0.0160   57.7824         -1  \n",
       " 1077  0.4991  0.0119  0.0276  0.0160   57.7824         -1  \n",
       " 1078  0.5004  0.0152  0.0156  0.0093   59.5644         -1  \n",
       " 1079  0.5040  0.0148  0.0156  0.0093   59.5644         -1  \n",
       " 1080  0.5013  0.0162  0.0157  0.0081   51.5105         -1  \n",
       " 1081  0.5054  0.0460  0.0119  0.0173  145.3192         -1  \n",
       " 1082  0.4983  0.0167  0.0119  0.0173  145.3192         -1  \n",
       " 1083  0.5022  0.0132  0.0206  0.0185   89.9060         -1  \n",
       " 1084  0.5045  0.0115  0.0389  0.0172   44.2355         -1  \n",
       " 1085  0.4999  0.0088  0.0389  0.0172   44.2355         -1  \n",
       " 1086  0.4980  0.0209  0.0389  0.0172   44.2355         -1  \n",
       " 1087  0.5092  0.0111  0.0162  0.0172  106.5441         -1  \n",
       " 1088  0.4966  0.0179  0.0162  0.0172  106.5441         -1  \n",
       " 1089  0.5066  0.0114  0.0162  0.0172  106.5441         -1  \n",
       " 1090  0.5032  0.0137  0.0162  0.0172  106.5441         -1  \n",
       " 1091  0.4981  0.0117  0.0196  0.0137   70.0306         -1  \n",
       " 1092  0.5023  0.0271  0.0228  0.0132   57.7037         -1  \n",
       " 1093  0.5003  0.0082  0.0186  0.0146   78.0627         -1  \n",
       " 1094  0.4972  0.0162  0.0186  0.0146   78.0627         -1  \n",
       " 1095  0.4969  0.0125  0.0186  0.0146   78.0627         -1  \n",
       " 1096  0.5021  0.0111  0.0179  0.0079   44.1194         -1  \n",
       " 1097  0.5063  0.0124  0.0179  0.0079   44.1194         -1  \n",
       " 1098  0.5025  0.0147  0.0179  0.0079   44.1194         -1  \n",
       " 1099  0.4981  0.0142  0.0179  0.0079   44.1194         -1  \n",
       " 1100  0.5032  0.0117  0.0398  0.0202   50.7949         -1  \n",
       " 1101  0.5015  0.0148  0.0398  0.0202   50.7949         -1  \n",
       " 1102  0.4979  0.0096  0.0798  0.0280   35.1003         -1  \n",
       " 1103  0.5056  0.0115  0.0798  0.0280   35.1003         -1  \n",
       " 1104  0.4990  0.0405  0.0182  0.0200  110.4384         -1  \n",
       " 1105  0.4981  0.0105  0.0368  0.0177   48.2132         -1  \n",
       " 1106  0.5031  0.0133  0.0260  0.0149   57.1964         -1  \n",
       " 1107  0.4994  0.0147  0.0260  0.0149   57.1964         -1  \n",
       " 1108  0.5021  0.0119  0.0631  0.0315   49.9274         -1  \n",
       " 1109  0.4953  0.0131  0.0831  0.0198   23.8407         -1  \n",
       " 1110  0.5063  0.0109  0.0831  0.0198   23.8407         -1  \n",
       " 1111  0.4981  0.0221  0.0831  0.0198   23.8407         -1  \n",
       " 1112  0.5012  0.0081  0.0257  0.0402  156.4628         -1  \n",
       " 1113  0.5042  0.0105  0.0257  0.0402  156.4628         -1  \n",
       " 1114  0.4997  0.0140  0.0169  0.0236  139.6209         -1  \n",
       " 1115  0.5023  0.0173  0.0169  0.0236  139.6209         -1  \n",
       " 1116  0.4965  0.0132  0.0154  0.0123   79.9402         -1  \n",
       " 1117  0.5032  0.0160  0.0154  0.0123   79.9402         -1  \n",
       " 1118  0.4996  0.0118  0.0154  0.0123   79.9402         -1  \n",
       " 1119  0.5024  0.0105  0.0154  0.0123   79.9402         -1  \n",
       " 1120  0.4966  0.0153  0.0088  0.0179  202.5901         -1  \n",
       " 1121  0.4984  0.0153  0.0192  0.0503  262.6164         -1  \n",
       " 1122  0.4963  0.0188  0.0247  0.0087   35.3988         -1  \n",
       " 1123  0.4997  0.0134  0.0247  0.0087   35.3988         -1  \n",
       " 1124  0.5016  0.0148  0.0472  0.0327   69.2994         -1  \n",
       " 1125  0.4979  0.0120  0.0277  0.0047   16.9283         -1  \n",
       " 1126  0.5023  0.0138  0.0258  0.0117   45.3908         -1  \n",
       " 1127  0.5026  0.0142  0.0258  0.0117   45.3908         -1  \n",
       " 1128  0.5023  0.0129  0.0258  0.0117   45.3908         -1  \n",
       " 1129  0.5013  0.0152  0.0272  0.0171   62.7655         -1  \n",
       " 1130  0.5048  0.0132  0.0272  0.0171   62.7655         -1  \n",
       " 1131  0.5014  0.0146  0.0272  0.0171   62.7655         -1  \n",
       " 1132  0.4960  0.0131  0.0256  0.0098   38.1727         -1  \n",
       " 1133  0.4996  0.0140  0.0256  0.0098   38.1727         -1  \n",
       " 1134  0.5029  0.0184  0.0189  0.0060   31.6209         -1  \n",
       " 1135  0.5004  0.0108  0.0167  0.0182  108.9031         -1  \n",
       " 1136  0.4978  0.0100  0.0392  0.0076   19.5115         -1  \n",
       " 1137  0.4996  0.0187  0.0392  0.0076   19.5115         -1  \n",
       " 1138  0.4981  0.0138  0.0392  0.0076   19.5115         -1  \n",
       " 1139  0.5020  0.0109  0.0205  0.0172   83.5851         -1  \n",
       " 1140  0.5055  0.0225  0.0205  0.0172   83.5851         -1  \n",
       " 1141  0.5028  0.0256  0.0289  0.0096   33.3147         -1  \n",
       " 1142  0.5042  0.0094  0.0289  0.0096   33.3147         -1  \n",
       " 1143  0.5010  0.0098  0.0289  0.0096   33.3147         -1  \n",
       " 1145  0.5006  0.0120  0.0275  0.0108   39.1032         -1  \n",
       " 1146  0.5034  0.0180  0.0275  0.0108   39.1032         -1  \n",
       " 1147  0.4999  0.0178  0.0275  0.0108   39.1032         -1  \n",
       " 1148  0.4980  0.0107  0.0275  0.0108   39.1032         -1  \n",
       " 1149  0.5052  0.0116  0.0275  0.0108   39.1032         -1  \n",
       " 1150  0.5024  0.0157  0.0275  0.0108   39.1032         -1  \n",
       " 1152  0.4990  0.0136  0.0354  0.0181   51.0695         -1  \n",
       " 1153  0.4993  0.0133  0.0354  0.0181   51.0695         -1  \n",
       " 1154  0.5041  0.0091  0.0354  0.0181   51.0695         -1  \n",
       " 1155  0.5002  0.0148  0.0354  0.0181   51.0695         -1  \n",
       " 1156  0.5040  0.0146  0.0354  0.0181   51.0695         -1  \n",
       " 1157  0.5007  0.0150  0.0354  0.0181   51.0695         -1  \n",
       " 1158  0.4992  0.0111  0.0354  0.0181   51.0695         -1  \n",
       " 1159  0.5015  0.0137  0.0354  0.0181   51.0695         -1  \n",
       " 1160  0.5004  0.0111  0.0354  0.0181   51.0695         -1  \n",
       " 1161  0.5004  0.0131  0.0119  0.0089   74.3720         -1  \n",
       " 1162  0.5039  0.0146  0.0050  0.0367  737.3048         -1  \n",
       " 1163  0.5035  0.0133  0.0050  0.0367  737.3048         -1  \n",
       " 1164  0.5018  0.0169  0.0123  0.0281  227.6623         -1  \n",
       " 1165  0.5020  0.0095  0.0269  0.0207   77.2047         -1  \n",
       " 1166  0.5029  0.0153  0.0269  0.0207   77.2047         -1  \n",
       " 1167  0.4978  0.0135  0.0269  0.0207   77.2047         -1  \n",
       " 1168  0.5033  0.0122  0.0230  0.0181   78.8778         -1  \n",
       " 1169  0.4983  0.0115  0.0126  0.0115   91.6729         -1  \n",
       " 1170  0.5033  0.0139  0.1028  0.0233   22.6231         -1  \n",
       " 1171  0.5007  0.0192  0.1028  0.0233   22.6231         -1  \n",
       " 1172  0.5067  0.0116  0.1028  0.0233   22.6231         -1  \n",
       " 1173  0.5046  0.0107  0.1028  0.0233   22.6231         -1  \n",
       " 1174  0.5060  0.0084  0.1028  0.0233   22.6231         -1  \n",
       " 1175  0.5041  0.0159  0.0289  0.0032   10.9425         -1  \n",
       " 1176  0.4996  0.0110  0.0289  0.0032   10.9425         -1  \n",
       " 1177  0.5009  0.0151  0.0190  0.0150   78.6525         -1  \n",
       " 1178  0.5027  0.0145  0.0190  0.0150   78.6525         -1  \n",
       " 1179  0.4962  0.0171  0.0190  0.0150   78.6525         -1  \n",
       " 1180  0.5006  0.0148  0.0190  0.0150   78.6525         -1  \n",
       " 1181  0.5002  0.0113  0.0190  0.0150   78.6525         -1  \n",
       " 1182  0.4979  0.0113  0.0233  0.0138   59.1201         -1  \n",
       " 1183  0.4987  0.0106  0.0233  0.0138   59.1201         -1  \n",
       " 1184  0.5028  0.0083  0.0233  0.0138   59.1201         -1  \n",
       " 1186  0.5053  0.0159  0.0262  0.0104   39.5528         -1  \n",
       " 1187  0.4982  0.0088  0.0262  0.0104   39.5528         -1  \n",
       " 1188  0.5029  0.0093  0.0262  0.0104   39.5528         -1  \n",
       " 1190  0.4996  0.0102  0.0262  0.0104   39.5528         -1  \n",
       " 1191  0.5027  0.0106  0.0262  0.0104   39.5528         -1  \n",
       " 1192  0.5069  0.0137  0.0262  0.0104   39.5528         -1  \n",
       " 1193  0.4970  0.0076  0.0262  0.0104   39.5528         -1  \n",
       " 1194  0.4964  0.0239  0.0262  0.0104   39.5528         -1  \n",
       " 1195  0.5045  0.0136  0.0223  0.0105   47.0690         -1  \n",
       " 1196  0.5056  0.0158  0.0223  0.0105   47.0690         -1  \n",
       " 1197  0.4971  0.0105  0.0223  0.0105   47.0690         -1  \n",
       " 1198  0.4974  0.0128  0.0223  0.0105   47.0690         -1  \n",
       " 1199  0.4969  0.0134  0.0076  0.0099  130.0095         -1  \n",
       " 1200  0.5019  0.0178  0.0076  0.0099  130.0095         -1  \n",
       " 1201  0.5008  0.0103  0.0343  0.0140   40.6502         -1  \n",
       " 1202  0.4968  0.0173  0.0194  0.0211  108.5721         -1  \n",
       " 1203  0.5008  0.0112  0.0193  0.0091   47.0754         -1  \n",
       " 1204  0.5014  0.0149  0.0193  0.0091   47.0754         -1  \n",
       " 1205  0.5030  0.0108  0.0106  0.0178  166.8498         -1  \n",
       " 1206  0.4989  0.0167  0.0106  0.0178  166.8498         -1  \n",
       " 1207  0.5035  0.0180  0.0379  0.0155   40.8765         -1  \n",
       " 1208  0.4994  0.0105  0.0264  0.0128   48.4774         -1  \n",
       " 1209  0.4992  0.0158  0.0264  0.0128   48.4774         -1  \n",
       " 1210  0.4999  0.0135  0.0264  0.0128   48.4774         -1  \n",
       " 1212  0.4983  0.0113  0.0212  0.0087   41.0611         -1  \n",
       " 1213  0.5063  0.0097  0.0195  0.0117   59.8462         -1  \n",
       " 1214  0.5071  0.0088  0.0085  0.0064   74.6027         -1  \n",
       " 1215  0.5017  0.0102  0.0085  0.0064   74.6027         -1  \n",
       " 1216  0.5009  0.0167  0.0293  0.0045   15.3444         -1  \n",
       " 1217  0.5077  0.0325  0.0293  0.0045   15.3444         -1  \n",
       " 1218  0.5073  0.0220  0.0293  0.0045   15.3444         -1  \n",
       " 1219  0.4979  0.0129  0.0109  0.0061   56.1631         -1  \n",
       " 1220  0.5030  0.0113  0.0207  0.0112   54.1279         -1  \n",
       " 1221  0.4989  0.0140  0.0207  0.0112   54.1279         -1  \n",
       " 1222  0.5002  0.0133  0.0207  0.0112   54.1279         -1  \n",
       " 1223  0.4949  0.0107  0.0207  0.0112   54.1279         -1  \n",
       " 1224  0.4989  0.0136  0.0207  0.0112   54.1279         -1  \n",
       " 1225  0.4992  0.0100  0.0207  0.0112   54.1279         -1  \n",
       " 1226  0.4981  0.0166  0.0102  0.0133  130.0641         -1  \n",
       " 1228  0.5004  0.0148  0.0102  0.0133  130.0641         -1  \n",
       " 1229  0.4944  0.0181  0.0291  0.0102   35.0279         -1  \n",
       " 1230  0.4948  0.0123  0.0291  0.0102   35.0279         -1  \n",
       " 1231  0.5063  0.0113  0.0291  0.0102   35.0279         -1  \n",
       " 1232  0.5033  0.0112  0.0291  0.0102   35.0279         -1  \n",
       " 1233  0.4954  0.0136  0.0291  0.0102   35.0279         -1  \n",
       " 1234  0.5058  0.0094  0.0053  0.0188  353.8319         -1  \n",
       " 1235  0.5011  0.0133  0.0430  0.0283   65.7689         -1  \n",
       " 1236  0.5005  0.0176  0.0144  0.0080   55.8468         -1  \n",
       " 1237  0.5050  0.0229  0.0144  0.0080   55.8468         -1  \n",
       " 1239  0.5019  0.0146  0.0193  0.0072   37.6251         -1  \n",
       " 1240  0.5071  0.0108  0.0134  0.0104   77.5636         -1  \n",
       " 1243  0.5009  0.0149  0.0283  0.0112   39.4516         -1  \n",
       " 1244  0.5003  0.0128  0.0071  0.0170  239.9809         -1  \n",
       " 1245  0.4980  0.0143  0.0352  0.0251   71.1973         -1  \n",
       " 1246  0.4973  0.0160  0.0112  0.0121  107.7999         -1  \n",
       " 1247  0.4991  0.0124  0.0305  0.0092   30.3421         -1  \n",
       " 1248  0.4997  0.0183  0.0305  0.0092   30.3421         -1  \n",
       " 1249  0.5006  0.0133  0.0230  0.0095   41.2178         -1  \n",
       " 1250  0.5055  0.0104  0.0230  0.0095   41.2178         -1  \n",
       " 1251  0.4940  0.0200  0.0243  0.0100   41.0782         -1  \n",
       " 1252  0.5001  0.0146  0.0243  0.0100   41.0782         -1  \n",
       " 1253  0.5007  0.0154  0.0243  0.0100   41.0782         -1  \n",
       " 1255  0.5001  0.0117  0.0243  0.0100   41.0782         -1  \n",
       " 1256  0.4928  0.0141  0.0223  0.0152   68.2176         -1  \n",
       " 1257  0.5038  0.0145  0.0223  0.0152   68.2176         -1  \n",
       " 1258  0.4991  0.0104  0.0223  0.0152   68.2176         -1  \n",
       " 1259  0.4982  0.0118  0.0137  0.0326  237.4625         -1  \n",
       " 1260  0.4968  0.0188  0.0137  0.0326  237.4625         -1  \n",
       " 1261  0.5006  0.0279  0.0137  0.0326  237.4625         -1  \n",
       " 1262  0.4949  0.0146  0.0137  0.0326  237.4625         -1  \n",
       " 1263  0.5016  0.0240  0.0152  0.0077   50.5947         -1  \n",
       " 1264  0.5019  0.0118 -0.0047  0.0134  286.1303         -1  \n",
       " 1265  0.5044  0.0166 -0.0047  0.0134  286.1303         -1  \n",
       " 1266  0.5026  0.0138 -0.0047  0.0134  286.1303         -1  \n",
       " 1267  0.4985  0.0316 -0.0047  0.0134  286.1303         -1  \n",
       " 1268  0.4999  0.0150  0.0184  0.0148   80.1759         -1  \n",
       " 1269  0.4936  0.0218  0.0184  0.0148   80.1759         -1  \n",
       " 1270  0.5029  0.0161  0.0184  0.0148   80.1759         -1  \n",
       " 1271  0.5010  0.0202  0.0184  0.0148   80.1759         -1  \n",
       " 1272  0.5037  0.0138  0.0184  0.0148   80.1759         -1  \n",
       " 1273  0.4944  0.0142  0.0184  0.0148   80.1759         -1  \n",
       " 1274  0.5029  0.0200  0.0184  0.0148   80.1759         -1  \n",
       " 1275  0.4951  0.0285  0.0184  0.0148   80.1759         -1  \n",
       " 1276  0.4963  0.0178  0.0184  0.0148   80.1759         -1  \n",
       " 1277  0.5017  0.0169  0.0184  0.0148   80.1759         -1  \n",
       " 1278  0.5057  0.0160  0.0184  0.0148   80.1759         -1  \n",
       " 1279  0.5038  0.0107  0.0184  0.0148   80.1759         -1  \n",
       " 1280  0.5050  0.0132  0.0184  0.0148   80.1759         -1  \n",
       " 1281  0.4992  0.0144  0.0184  0.0148   80.1759         -1  \n",
       " 1282  0.5055  0.0195  0.0184  0.0148   80.1759         -1  \n",
       " 1283  0.5041  0.0282  0.0184  0.0148   80.1759         -1  \n",
       " 1284  0.5031  0.0158  0.0184  0.0148   80.1759         -1  \n",
       " 1285  0.4985  0.0190  0.0184  0.0148   80.1759         -1  \n",
       " 1286  0.4979  0.0142  0.0184  0.0148   80.1759         -1  \n",
       " 1287  0.4979  0.0169  0.0184  0.0148   80.1759         -1  \n",
       " 1288  0.4978  0.0114  0.0184  0.0148   80.1759         -1  \n",
       " 1289  0.5040  0.0160  0.0196  0.0049   25.2265         -1  \n",
       " 1290  0.5006  0.0320  0.0196  0.0049   25.2265         -1  \n",
       " 1291  0.4971  0.0093  0.0196  0.0049   25.2265         -1  \n",
       " 1292  0.4953  0.0167  0.0332  0.0178   53.5775         -1  \n",
       " 1293  0.5010  0.0079  0.0332  0.0178   53.5775         -1  \n",
       " 1294  0.4933  0.0128  0.0239  0.0066   27.5232         -1  \n",
       " 1295  0.5000  0.0103  0.0308  0.0183   59.3775         -1  \n",
       " 1296  0.4997  0.0131  0.0308  0.0183   59.3775         -1  \n",
       " 1297  0.5023  0.0196  0.0305  0.0286   93.8008         -1  \n",
       " 1298  0.4971  0.0141  0.0305  0.0286   93.8008         -1  \n",
       " 1299  0.5035  0.0116  0.0197  0.0139   70.4993         -1  \n",
       " 1300  0.5018  0.0172  0.0197  0.0139   70.4993         -1  \n",
       " 1301  0.4990  0.0117  0.0197  0.0139   70.4993         -1  \n",
       " 1304  0.5019  0.0189  0.0144  0.0110   76.0785         -1  \n",
       " 1305  0.5000  0.0155  0.0144  0.0110   76.0785         -1  \n",
       " 1306  0.5021  0.0117  0.0144  0.0110   76.0785         -1  \n",
       " 1307  0.5030  0.0102  0.0230  0.0064   27.7025         -1  \n",
       " 1308  0.5005  0.0135  0.0230  0.0064   27.7025         -1  \n",
       " 1309  0.5004  0.0163  0.0230  0.0064   27.7025         -1  \n",
       " 1310  0.4996  0.0137  0.0259  0.0112   43.2861         -1  \n",
       " 1311  0.4998  0.0108  0.0259  0.0112   43.2861         -1  \n",
       " 1312  0.4990  0.0146  0.0259  0.0112   43.2861         -1  \n",
       " 1313  0.5017  0.0254  0.0259  0.0112   43.2861         -1  \n",
       " 1314  0.5002  0.0147  0.0098  0.0071   72.1094         -1  \n",
       " 1315  0.5014  0.0155  0.0098  0.0071   72.1094         -1  \n",
       " 1316  0.5029  0.0181  0.0196  0.0238  121.1691         -1  \n",
       " 1317  0.4991  0.0134  0.0232  0.0129   55.7204         -1  \n",
       " 1318  0.4986  0.0126  0.0382  0.0136   35.5800         -1  \n",
       " 1319  0.4956  0.0134  0.0382  0.0136   35.5800         -1  \n",
       " 1320  0.5052  0.0139  0.0065  0.0077  118.4201         -1  \n",
       " 1321  0.4966  0.0124  0.0230  0.0210   91.4264         -1  \n",
       " 1322  0.4998  0.0150  0.0230  0.0210   91.4264         -1  \n",
       " 1323  0.5018  0.0127  0.0230  0.0210   91.4264         -1  \n",
       " 1326  0.4979  0.0192  0.0230  0.0210   91.4264         -1  \n",
       " 1330  0.4961  0.0143  0.0230  0.0210   91.4264         -1  \n",
       " 1331  0.4967  0.0127  0.0230  0.0210   91.4264         -1  \n",
       " 1332  0.4990  0.0137  0.0230  0.0210   91.4264         -1  \n",
       " 1333  0.4963  0.0160  0.0230  0.0210   91.4264         -1  \n",
       " 1334  0.5016  0.0135  0.0230  0.0210   91.4264         -1  \n",
       " 1335  0.5005  0.0099  0.0230  0.0210   91.4264         -1  \n",
       " 1336  0.4988  0.0185  0.0328  0.0192   58.5087         -1  \n",
       " 1337  0.5005  0.0117  0.0239  0.0203   85.0589         -1  \n",
       " 1338  0.4997  0.0129  0.0239  0.0203   85.0589         -1  \n",
       " 1339  0.4993  0.0198  0.0346  0.0251   72.6469         -1  \n",
       " 1340  0.4968  0.0164  0.0346  0.0251   72.6469         -1  \n",
       " 1341  0.4990  0.0177  0.0346  0.0251   72.6469         -1  \n",
       " 1344  0.5003  0.0153  0.0346  0.0251   72.6469         -1  \n",
       " 1345  0.4969  0.0164  0.0498  0.0799  160.5042         -1  \n",
       " 1346  0.4970  0.0124  0.0498  0.0799  160.5042         -1  \n",
       " 1347  0.4997  0.0122  0.0097  0.0098  101.2092         -1  \n",
       " 1348  0.5004  0.0107  0.0097  0.0098  101.2092         -1  \n",
       " 1349  0.5031  0.0077  0.0097  0.0098  101.2092         -1  \n",
       " 1350  0.5029  0.0133 -0.0060  0.0078  129.6218         -1  \n",
       " 1351  0.4974  0.0116 -0.0060  0.0078  129.6218         -1  \n",
       " 1352  0.4987  0.0114 -0.0060  0.0078  129.6218         -1  \n",
       " 1353  0.5002  0.0106 -0.0060  0.0078  129.6218         -1  \n",
       " 1354  0.5011  0.0185 -0.0060  0.0078  129.6218         -1  \n",
       " 1355  0.4981  0.0198 -0.0060  0.0078  129.6218         -1  \n",
       " 1356  0.4964  0.0139  0.0077  0.0149  193.4633         -1  \n",
       " 1357  0.4968  0.0150  0.0077  0.0149  193.4633         -1  \n",
       " 1358  0.4960  0.0123  0.0077  0.0149  193.4633         -1  \n",
       " 1359  0.5009  0.0102  0.0077  0.0149  193.4633         -1  \n",
       " 1360  0.5022  0.0112  0.0077  0.0149  193.4633         -1  \n",
       " 1361  0.5025  0.0110  0.0077  0.0149  193.4633         -1  \n",
       " 1362  0.5026  0.0085  0.0182  0.0077   42.5048         -1  \n",
       " 1366  0.4996  0.0141 -0.0006  0.0118    0.0000         -1  \n",
       " 1367  0.4985  0.0060  0.0331  0.0469  141.6245         -1  \n",
       " 1368  0.5028  0.0136  0.0331  0.0469  141.6245         -1  \n",
       " 1369  0.4998  0.0143  0.0331  0.0469  141.6245         -1  \n",
       " 1370  0.4990  0.0131  0.0331  0.0469  141.6245         -1  \n",
       " 1371  0.5018  0.0182  0.0236  0.0098   41.4871         -1  \n",
       " 1372  0.4987  0.0212  0.0236  0.0098   41.4871         -1  \n",
       " 1373  0.5002  0.0114  0.0236  0.0098   41.4871         -1  \n",
       " 1374  0.4953  0.0161  0.0236  0.0098   41.4871         -1  \n",
       " 1375  0.5017  0.0122  0.0236  0.0098   41.4871         -1  \n",
       " 1376  0.4983  0.0151  0.0236  0.0098   41.4871         -1  \n",
       " 1377  0.4962  0.0109  0.0261  0.0063   24.0179         -1  \n",
       " 1378  0.4953  0.0148  0.0025  0.0260    0.0000         -1  \n",
       " 1379  0.4994  0.0180  0.0025  0.0260    0.0000         -1  \n",
       " 1380  0.4988  0.0126  0.0203  0.0134   65.9827         -1  \n",
       " 1381  0.4986  0.0134  0.0295  0.0154   52.2049         -1  \n",
       " 1382  0.5006  0.0186  0.0295  0.0154   52.2049         -1  \n",
       " 1383  0.5009  0.0237  0.0295  0.0154   52.2049         -1  \n",
       " 1384  0.5013  0.0146  0.0295  0.0154   52.2049         -1  \n",
       " 1385  0.4982  0.0079  0.0295  0.0154   52.2049         -1  \n",
       " 1386  0.4963  0.0104  0.0295  0.0154   52.2049         -1  \n",
       " 1387  0.5008  0.0319  0.0295  0.0154   52.2049         -1  \n",
       " 1388  0.4981  0.0230  0.0295  0.0154   52.2049         -1  \n",
       " 1389  0.5003  0.0115  0.0295  0.0154   52.2049         -1  \n",
       " 1390  0.4996  0.0163  0.0134  0.0134  100.1445         -1  \n",
       " 1391  0.5000  0.0154  0.0095  0.0114  119.8239         -1  \n",
       " 1392  0.4993  0.0101  0.0042  0.0159  378.7581         -1  \n",
       " 1393  0.4999  0.0142  0.0366  0.0487  132.9913         -1  \n",
       " 1394  0.5023  0.0092  0.0366  0.0487  132.9913         -1  \n",
       " 1395  0.4993  0.0122  0.0106  0.0075   71.0842         -1  \n",
       " 1396  0.4973  0.0146  0.0106  0.0075   71.0842         -1  \n",
       " 1397  0.5012  0.0126  0.0106  0.0075   71.0842         -1  \n",
       " 1398  0.5027  0.0157  0.0211  0.0393  186.4769         -1  \n",
       " 1399  0.5002  0.0108  0.0268  0.0110   41.1335         -1  \n",
       " 1401  0.5010  0.0127  0.0111  0.0215  193.8286         -1  \n",
       " 1402  0.4987  0.0183  0.0458  0.0277   60.5430         -1  \n",
       " 1403  0.5046  0.0125  0.0458  0.0277   60.5430         -1  \n",
       " 1404  0.4954  0.0106  0.0458  0.0277   60.5430         -1  \n",
       " 1405  0.4958  0.0181  0.0458  0.0277   60.5430         -1  \n",
       " 1406  0.5003  0.0159  0.0458  0.0277   60.5430         -1  \n",
       " 1407  0.4963  0.0138  0.0458  0.0277   60.5430         -1  \n",
       " 1408  0.5022  0.0257  0.0299  0.0216   72.0230         -1  \n",
       " 1409  0.4964  0.0166  0.0299  0.0216   72.0230         -1  \n",
       " 1410  0.4968  0.0171  0.0299  0.0216   72.0230         -1  \n",
       " 1411  0.4988  0.0146  0.0156  0.0405  259.2834         -1  \n",
       " 1412  0.4985  0.0169  0.0378  0.0651  171.9936         -1  \n",
       " 1413  0.5019  0.0135  0.0378  0.0651  171.9936         -1  \n",
       " 1414  0.5035  0.0207  0.0122  0.0080   65.4842         -1  \n",
       " 1415  0.4971  0.0097  0.0122  0.0080   65.4842         -1  \n",
       " 1416  0.4959  0.0099  0.0149  0.0160  107.2427         -1  \n",
       " 1417  0.5002  0.0135  0.0149  0.0160  107.2427         -1  \n",
       " 1418  0.4996  0.0129  0.0191  0.0097   50.6198         -1  \n",
       " 1419  0.5019  0.0140  0.0092  0.0393  427.4732         -1  \n",
       " 1420  0.4994  0.0115  0.0299  0.0071   23.6431         -1  \n",
       " 1421  0.4983  0.0157  0.0299  0.0071   23.6431         -1  \n",
       " 1422  0.5008  0.0167  0.0232  0.0119   51.4726         -1  \n",
       " 1423  0.5020  0.0142  0.0232  0.0119   51.4726         -1  \n",
       " 1424  0.4977  0.0151  0.0232  0.0119   51.4726         -1  \n",
       " 1425  0.4912  0.0127  0.0166  0.0122   73.6335         -1  \n",
       " 1426  0.4939  0.0163  0.0166  0.0122   73.6335         -1  \n",
       " 1427  0.4966  0.0084  0.0166  0.0122   73.6335         -1  \n",
       " 1428  0.5009  0.0135  0.0178  0.0305  171.3183         -1  \n",
       " 1429  0.5008  0.0137  0.0048  0.0178  368.8936         -1  \n",
       " 1430  0.5018  0.0162  0.0048  0.0178  368.8936         -1  \n",
       " 1431  0.5026  0.0158  0.0091  0.0180  197.1279         -1  \n",
       " 1432  0.5011  0.0125  0.0157  0.0190  120.6365         -1  \n",
       " 1433  0.4986  0.0120  0.0157  0.0190  120.6365         -1  \n",
       " 1434  0.5014  0.0183  0.0157  0.0190  120.6365         -1  \n",
       " 1435  0.5007  0.0213  0.0157  0.0190  120.6365         -1  \n",
       " 1436  0.4957  0.0156  0.0178  0.0120   67.7994         -1  \n",
       " 1437  0.4967  0.0115  0.0178  0.0120   67.7994         -1  \n",
       " 1439  0.4997  0.0167  0.0059  0.0148  252.0031         -1  \n",
       " 1440  0.5014  0.0198  0.0059  0.0148  252.0031         -1  \n",
       " 1441  0.4968  0.0132  0.0059  0.0148  252.0031         -1  \n",
       " 1442  0.4983  0.0095  0.0209  0.0141   67.2425         -1  \n",
       " 1444  0.5020  0.0117  0.0274  0.0121   44.0961         -1  \n",
       " 1445  0.4958  0.0139  0.0274  0.0121   44.0961         -1  \n",
       " 1446  0.5008  0.0153  0.0250  0.0088   35.1564         -1  \n",
       " 1447  0.5031  0.0111  0.0250  0.0088   35.1564         -1  \n",
       " 1448  0.5048  0.0162  0.0250  0.0088   35.1564         -1  \n",
       " 1449  0.4965  0.0233  0.0250  0.0088   35.1564         -1  \n",
       " 1450  0.5095  0.0104  0.0250  0.0088   35.1564         -1  \n",
       " 1451  0.5008  0.0122  0.0132  0.0233  176.4448         -1  \n",
       " 1452  0.4983  0.0127  0.0152  0.0059   38.9527         -1  \n",
       " 1453  0.4948  0.0177  0.0152  0.0059   38.9527         -1  \n",
       " 1454  0.5003  0.0161  0.0232  0.0134   57.7031         -1  \n",
       " 1455  0.4922  0.0143  0.0251  0.0113   45.0140         -1  \n",
       " 1456  0.5045  0.0108 -0.0031  0.0168  545.6838         -1  \n",
       " 1457  0.5021  0.0139 -0.0031  0.0168  545.6838         -1  \n",
       " 1458  0.4997  0.0142 -0.0031  0.0168  545.6838         -1  \n",
       " 1459  0.4974  0.0111 -0.0031  0.0168  545.6838         -1  \n",
       " 1460  0.4973  0.0112  0.0169  0.0157   93.0007         -1  \n",
       " 1461  0.5000  0.0099  0.0169  0.0157   93.0007         -1  \n",
       " 1462  0.4966  0.0200  0.0195  0.0701  358.7989         -1  \n",
       " 1463  0.4980  0.0163  0.0308  0.0412  133.4494         -1  \n",
       " 1464  0.5004  0.0119  0.0227  0.0284  125.1290         -1  \n",
       " 1465  0.4986  0.0172  0.0117  0.0373  318.0075         -1  \n",
       " 1466  0.5020  0.0155  0.0117  0.0373  318.0075         -1  \n",
       " 1467  0.4993  0.0147  0.0117  0.0373  318.0075         -1  \n",
       " 1468  0.4972  0.0133  0.0167  0.0310  185.3578         -1  \n",
       " 1469  0.5036  0.0161  0.0167  0.0310  185.3578         -1  \n",
       " 1470  0.4998  0.0234  0.0167  0.0310  185.3578         -1  \n",
       " 1471  0.4986  0.0132  0.0172  0.0222  128.6840         -1  \n",
       " 1472  0.5008  0.0140  0.0246  0.0145   58.7670         -1  \n",
       " 1473  0.5030  0.0150  0.0057  0.0109  192.8017         -1  \n",
       " 1474  0.4979  0.0168  0.0057  0.0328  579.1817         -1  \n",
       " 1475  0.5005  0.0171  0.0057  0.0328  579.1817         -1  \n",
       " 1476  0.5024  0.0132  0.0057  0.0328  579.1817         -1  \n",
       " 1477  0.5008  0.0140  0.0057  0.0328  579.1817         -1  \n",
       " 1478  0.4954  0.0199  0.0148  0.0197  133.4114         -1  \n",
       " 1479  0.5051  0.0101  0.0200  0.0169   84.6772         -1  \n",
       " 1480  0.5027  0.0172  0.0196  0.0165   84.1261         -1  \n",
       " 1481  0.4977  0.0266  0.0171  0.0194  113.5506         -1  \n",
       " 1482  0.5033  0.0160  0.0171  0.0194  113.5506         -1  \n",
       " 1483  0.5030  0.0229 -0.0169  0.0148   87.8627         -1  \n",
       " 1484  0.4965  0.0123  0.0354  0.0266   75.1897         -1  \n",
       " 1485  0.4985  0.0153  0.0104  0.0185  178.0405         -1  \n",
       " 1486  0.4994  0.0118  0.0191  0.0123   64.6789         -1  \n",
       " 1487  0.4998  0.0154  0.0055  0.0205  372.2183         -1  \n",
       " 1488  0.4998  0.0161  0.0055  0.0205  372.2183         -1  \n",
       " 1489  0.4963  0.0191  0.0371  0.0125   33.6823         -1  \n",
       " 1490  0.4987  0.0135  0.0371  0.0125   33.6823         -1  \n",
       " 1491  0.4991  0.0125  0.0239  0.0104   43.5641         -1  \n",
       " 1492  0.5007  0.0148  0.0049  0.0123  248.6235         -1  \n",
       " 1493  0.5045  0.0181  0.0049  0.0123  248.6235         -1  \n",
       " 1494  0.5029  0.0111  0.0068  0.0280  414.4256         -1  \n",
       " 1495  0.5045  0.0249  0.0138  0.0162  117.7603         -1  \n",
       " 1496  0.4950  0.0115  0.0138  0.0162  117.7603         -1  \n",
       " 1497  0.4985  0.0132  0.0138  0.0162  117.7603         -1  \n",
       " 1498  0.5016  0.0112  0.0437  0.0596  136.2608         -1  \n",
       " 1499  0.4985  0.0138  0.0148  0.0091   61.4005         -1  \n",
       " 1500  0.4985  0.0096  0.0141  0.0250  176.9394         -1  \n",
       " 1501  0.5015  0.0241  0.0236  0.0113   47.8577         -1  \n",
       " 1502  0.5041  0.0243  0.0244  0.0319  130.7654         -1  \n",
       " 1503  0.5028  0.0112  0.0243  0.0249  102.4834         -1  \n",
       " 1504  0.5006  0.0110  0.0215  0.0175   81.2810         -1  \n",
       " 1505  0.4995  0.0178  0.0104  0.0127  122.5713         -1  \n",
       " 1506  0.4997  0.0136  0.0200  0.0164   82.0875         -1  \n",
       " 1507  0.5037  0.0145  0.0201  0.0106   52.6841         -1  \n",
       " 1508  0.5067  0.0471  0.0311  0.0317  101.9627         -1  \n",
       " 1509  0.5051  0.0137  0.0311  0.0317  101.9627         -1  \n",
       " 1510  0.4976  0.0160  0.0350  0.0117   33.4695         -1  \n",
       " 1511  0.5019  0.0126  0.0156  0.0101   64.4686         -1  \n",
       " 1512  0.4965  0.0173  0.0138  0.0173  125.3032         -1  \n",
       " 1513  0.5012  0.0166  0.0197  0.0068   34.6093         -1  \n",
       " 1514  0.4966  0.0134  0.0295  0.0180   61.0329         -1  \n",
       " 1515  0.5029  0.0185  0.0263  0.0070   26.5137         -1  \n",
       " 1516  0.4992  0.0117  0.0146  0.0090   61.5936         -1  \n",
       " 1517  0.4992  0.0177  0.0260  0.0136   52.1827         -1  \n",
       " 1518  0.4998  0.0243  0.0298  0.0208   69.9837         -1  \n",
       " 1520  0.5011  0.0151  0.0115  0.0142  122.9917         -1  \n",
       " 1521  0.5007  0.0135  0.0115  0.0142  122.9917         -1  \n",
       " 1522  0.5006  0.0196  0.0128  0.0113   88.1365         -1  \n",
       " 1523  0.4995  0.0127  0.0091  0.0169  185.8369         -1  \n",
       " 1524  0.5037  0.0252  0.0091  0.0169  185.8369         -1  \n",
       " 1525  0.5003  0.0165  0.0363  0.0135   37.2277         -1  \n",
       " 1526  0.4982  0.0143  0.0363  0.0135   37.2277         -1  \n",
       " 1527  0.4991  0.0115  0.0153  0.0230  149.8127         -1  \n",
       " 1528  0.4968  0.0189  0.0139  0.0097   70.2425         -1  \n",
       " 1529  0.4964  0.0193  0.0139  0.0097   70.2425         -1  \n",
       " 1530  0.5008  0.0108  0.0101  0.0140  138.3371         -1  \n",
       " 1531  0.5000  0.0169  0.0168  0.0349  207.2955         -1  \n",
       " 1532  0.5007  0.0247  0.0168  0.0349  207.2955         -1  \n",
       " 1533  0.4995  0.0195  0.0168  0.0349  207.2955         -1  \n",
       " 1534  0.5013  0.0158  0.0080  0.0272  337.9120         -1  \n",
       " 1535  0.4972  0.0145  0.0199  0.0097   48.7045         -1  \n",
       " 1536  0.5023  0.0148  0.0199  0.0097   48.7045         -1  \n",
       " 1537  0.4942  0.0175  0.0199  0.0097   48.7045         -1  \n",
       " 1538  0.4977  0.0144  0.0237  0.0112   47.3376         -1  \n",
       " 1539  0.4987  0.0118  0.0237  0.0112   47.3376         -1  \n",
       " 1540  0.5011  0.0163  0.0181  0.0123   67.6676         -1  \n",
       " 1541  0.5021  0.0103  0.0266  0.0063   23.5979         -1  \n",
       " 1542  0.4992  0.0136  0.0216  0.0263  121.9426         -1  \n",
       " 1543  0.4975  0.0109  0.0273  0.0139   50.8827         -1  \n",
       " 1544  0.4986  0.0192  0.0172  0.0126   72.9676         -1  \n",
       " 1545  0.4951  0.0165  0.0063  0.0252  402.6874         -1  \n",
       " 1546  0.5022  0.0249  0.0134  0.0142  105.7142         -1  \n",
       " 1547  0.5012  0.0160  0.0222  0.0080   36.2975         -1  \n",
       " 1548  0.4944  0.0171  0.0253  0.0224   88.5812         -1  \n",
       " 1549  0.5037  0.0117  0.0253  0.0224   88.5812         -1  \n",
       " 1550  0.5034  0.0178  0.0236  0.0065   27.5514         -1  \n",
       " 1551  0.4972  0.0157  0.0400  0.0123   30.7574         -1  \n",
       " 1552  0.5031  0.0111  0.0085  0.0212  247.6285         -1  \n",
       " 1553  0.4985  0.0126  0.0174  0.0234  134.3983         -1  \n",
       " 1554  0.5025  0.0138  0.0235  0.0275  117.0945         -1  \n",
       " 1555  0.5002  0.0130  0.0086  0.0160  184.8703         -1  \n",
       " 1556  0.5019  0.0158  0.0120  0.0055   46.1076         -1  \n",
       " 1557  0.5010  0.0132  0.0277  0.0074   26.7330         -1  \n",
       " 1558  0.4993  0.0130  0.0097  0.0172  176.6783         -1  \n",
       " 1559  0.4960  0.0157  0.0150  0.0176  117.4564         -1  \n",
       " 1560  0.5009  0.0155  0.0105  0.0133  127.3154         -1  \n",
       " 1561  0.4965  0.0118  0.0320  0.0148   46.4573         -1  \n",
       " 1562  0.4988  0.0143  0.0068  0.0138  203.1720         -1  \n",
       " 1563  0.4975  0.0131  0.0068  0.0138  203.1720         -1  \n",
       " 1564  0.4987  0.0153  0.0197  0.0086   43.5231         -1  \n",
       " 1565  0.5004  0.0178  0.0262  0.0245   93.4941         -1  \n",
       " 1566  0.4987  0.0181  0.0117  0.0162  137.7844         -1  \n",
       " \n",
       " [1463 rows x 202 columns],\n",
       "             0         1          2          3       4         6       7  \\\n",
       " 2     2932.61  2559.940  2186.4111  1698.0172  1.5102   95.4878  0.1241   \n",
       " 10    2994.05  2548.210  2195.1222  1046.1468  1.3204  103.3400  0.1223   \n",
       " 11    2928.84  2479.400  2196.2111  1605.7578  0.9959   97.9156  0.1257   \n",
       " 14    2963.97  2629.480  2224.6222   947.7739  1.2924  104.8489  0.1197   \n",
       " 23    2884.74  2514.540  2160.3667   899.9488  1.4022  105.4978  0.1240   \n",
       " 38    2958.09  2542.240  2222.6778  1547.6125  1.4431  110.5644  0.1211   \n",
       " 40    2962.14  2545.710  2221.5778  1503.6230  1.1878  111.3444  0.1211   \n",
       " 45    2912.87  2446.250  2166.5222   907.0746  1.0647  104.5211  0.1221   \n",
       " 48    2993.59  2345.950  2169.4667  1185.4449  1.2412  100.8444  0.1221   \n",
       " 49    2946.86  2533.910  2174.8666  1039.2291  1.0455  103.6000  0.1221   \n",
       " 50    2942.31  2446.740  2172.9778  1222.6067  1.3658  101.8400  0.1220   \n",
       " 57    2935.94  2586.050  2164.4111  1206.6031  0.9799  100.5189  0.1220   \n",
       " 58    3004.09  2388.740  2223.8000  1503.1248  1.1705  110.0600  0.1211   \n",
       " 62    2990.22  2483.660  2206.5112  1244.1552  1.2691  101.6667  0.1229   \n",
       " 64    2980.84  2628.760  2187.5222  1268.6598  1.4503  102.4622  0.1233   \n",
       " 82    2971.99  2502.620  2239.3000  1192.7495  1.2479  100.1189  0.1187   \n",
       " 96    2992.40  2467.070  2191.6667  1107.4330  1.3529  103.4233  0.1206   \n",
       " 115   3002.85  2502.050  2232.5889  1717.2750  1.6700  104.1067  0.1223   \n",
       " 131   3003.13  2714.300  2171.9000  1811.8799  1.3811   99.2200  0.1276   \n",
       " 154   2973.86  2359.010  2196.6555  1066.1908  1.2188  101.8900  0.1211   \n",
       " 157   2959.86  2491.190  2208.0000  1835.9832  1.5714  100.2478  0.1238   \n",
       " 158   2964.77  2524.440  2181.5111  1177.0830  1.3012  100.9333  0.1215   \n",
       " 167   2889.26  2529.160  2184.8778   960.8486  1.0160  102.5333  0.1214   \n",
       " 169   3041.56  2508.560  2184.8778   960.8486  1.0160  102.5333  0.1214   \n",
       " 180   3058.89  2504.380  2221.9444  1551.6947  1.5296   99.2678  0.1222   \n",
       " 182   2928.03  2497.030  2221.9444  1551.6947  1.5296   99.2678  0.1222   \n",
       " 186   2936.64  2509.650  2221.9444  1551.6947  1.5296   99.2678  0.1222   \n",
       " 188   3074.69  2391.710  2221.9444  1551.6947  1.5296   99.2678  0.1222   \n",
       " 189   2938.03  2480.900  2138.8778  1046.6043  1.2559  103.3400  0.1240   \n",
       " 218   3038.21  2521.840  2273.7556  1549.8407  1.4105  105.0467  0.1171   \n",
       " 222   3037.49  2463.110  2205.2889  1630.3112  1.2733   98.8056  0.1218   \n",
       " 231   2940.65  2499.405  2214.0556  1150.7775  1.3772  102.9389  0.1205   \n",
       " 235   3045.71  2490.250  2197.6444  1247.0334  0.7865   99.9211  0.1203   \n",
       " 236   3035.98  2458.150  2230.0333  1668.6804  1.5739   99.0522  0.1204   \n",
       " 238   2984.45  2444.090  2230.0333  1668.6804  1.5739   99.0522  0.1204   \n",
       " 240   2900.69  2483.060  2191.1111  1564.6023  0.9366  102.5100  0.1234   \n",
       " 241   2916.02  2467.490  2185.9333  1659.6962  1.6290   98.6822  0.1227   \n",
       " 243   2963.75  2593.630  2204.9223  1787.6757  1.5138  100.4322  0.1216   \n",
       " 244   3107.79  2470.810  2280.8222  1125.7334  0.6815  101.9111  0.1221   \n",
       " 273   3075.78  2575.680  2243.7778  1502.9221  1.8160  102.0978  0.1195   \n",
       " 277   3054.18  2408.460  2178.7333  1039.3641  0.7367  101.4922  0.1219   \n",
       " 282   3008.84  2522.900  2177.3222  1089.3655  1.3101  101.1478  0.1216   \n",
       " 291   2983.31  2672.940  2244.1111  1676.7316  0.9197  100.8067  0.1204   \n",
       " 294   2997.92  2509.240  2201.5777   976.4791  0.7679   99.9956  0.1202   \n",
       " 321   2936.59  2526.220  2196.6889  1593.1220  1.5925   99.1133  0.1226   \n",
       " 323   2975.92  2286.250  2238.5444  1659.1424  0.9010   99.3100  0.1204   \n",
       " 326   2973.53  2499.720  2177.3222  1089.3655  1.3101  101.1478  0.1216   \n",
       " 327   2954.36  2559.270  2238.5444  1659.1424  0.9010   99.3100  0.1204   \n",
       " 336   2887.33  2551.170  2201.5777   976.4791  0.7679   99.9956  0.1202   \n",
       " 344   3098.45  2439.820  2197.6778  1056.7817  1.3168  102.9611  0.1203   \n",
       " 351   2950.18  2517.340  2196.6889  1593.1220  1.5925   99.1133  0.1226   \n",
       " 368   2936.04  2574.320  2186.9889   949.2201  1.2981  103.3322  0.1219   \n",
       " 373   3118.63  2478.880  2170.5444   921.0605  1.4390  104.5300  0.1226   \n",
       " 392   3013.98  2488.760  2167.5000  1068.4521  1.3719  102.1733  0.1223   \n",
       " 406   2917.09  2518.960  2213.7556  1113.5599  0.7217  104.1667  0.1211   \n",
       " 424   2977.43  2297.300  2218.0555  1517.4371  0.8579  105.8133  0.1206   \n",
       " 441   3102.59  2514.950  2196.0889  1277.8592  1.8246   95.6322  0.1224   \n",
       " 448   2942.41  2523.710  2207.0444  1269.6078  1.7571   97.0189  0.1221   \n",
       " 495   2973.55  2536.480  2235.0556  1302.6607  1.6347  109.9856  0.1230   \n",
       " 508   3003.82  2527.250  2163.5889  1448.3869  1.7014  104.8333  0.1256   \n",
       " 518   3061.22  2384.190  2170.9667  1600.3858  1.0430  104.9756  0.1249   \n",
       " 576   2990.72  2425.460  2155.6333  1070.0439  0.8024  101.4333  0.1241   \n",
       " 583   2949.82  2497.560  2173.4556  1433.6732  1.0304  110.5422  0.1245   \n",
       " 601   3040.10  2369.950  2201.8222  1288.0857  1.6769   95.9789  0.1209   \n",
       " 605   2943.90  2436.650  2173.4556  1433.6732  1.0304  110.5422  0.1245   \n",
       " 634   2990.76  2449.250  2172.9667  1058.2061  0.8433  104.7189  0.1232   \n",
       " 709   3026.32  2485.810  2205.7222   906.9522  1.3443  105.6600  0.1200   \n",
       " 795   3069.31  2448.370  2174.7555  1206.3506  1.4202  104.2622  0.1246   \n",
       " 797   2896.84  2512.610  2190.1555  1298.8207  1.3947  105.7411  0.1242   \n",
       " 826   3045.05  2443.100  2170.0666  1364.5157  1.5447   96.7700  0.1230   \n",
       " 831   2961.04  2506.430  2170.0666  1364.5157  1.5447   96.7700  0.1230   \n",
       " 871   3212.46  2522.410  2200.2333  1173.8377  1.3281  101.6111  0.1211   \n",
       " 914   2977.29  2471.500  2214.7889  1687.4606  2.2073   97.3378  0.1226   \n",
       " 924   3091.76  2391.560  2315.2667  2360.1325  1.1259   90.1144  0.1160   \n",
       " 926   2982.87  2477.010  2315.2667  2360.1325  1.1259   90.1144  0.1160   \n",
       " 929   2914.86  2465.110  2210.2778  2120.5760  1.0700   95.1089  0.1230   \n",
       " 1029  3017.86  2584.790  2169.8334  1301.9348  0.8012  101.9733  0.1216   \n",
       " 1062  3244.74  2422.000  2208.5222  1838.7054  1.1571   95.2056  0.1249   \n",
       " 1144  2882.22  2518.890  2254.8667  2347.9092  1.2986   87.8044  0.1205   \n",
       " 1151  2965.73  2539.500  2228.9445  1502.7821  1.2895  100.6867  0.1235   \n",
       " 1185  2904.26  2500.850  2231.9555  1943.0435  1.2758   96.8789  0.1224   \n",
       " 1189  3200.74  2534.160  2244.9778  2208.4483  1.9074   87.2789  0.1217   \n",
       " 1211  3037.63  2524.130  2194.9555  1108.2246  1.2476  102.2822  0.1202   \n",
       " 1227  3057.45  2457.420  2173.4889  1145.7970  0.9402  104.0556  0.1247   \n",
       " 1238  3060.00  2571.410  2199.6556  1140.3983  1.3369  103.0967  0.1227   \n",
       " 1241  2871.62  2530.110  2175.2556  1022.1660  1.2833  100.6222  0.1250   \n",
       " 1242  3181.59  2512.770  2171.8556  1156.6018  1.4025  100.1367  0.1243   \n",
       " 1254  2988.41  2598.770  2173.2778  1116.2950  0.8525  103.8200  0.1237   \n",
       " 1302  3054.34  2473.140  2162.1333   998.9095  0.8826  104.9722  0.1246   \n",
       " 1303  2848.46  2588.740  2164.3666   958.7313  1.3409  104.1344  0.1248   \n",
       " 1324  3224.10  2410.530  2208.9778  1285.2144  2.4210   97.8400  0.1238   \n",
       " 1325  3193.53  2587.860  2162.1333   998.9095  0.8826  104.9722  0.1246   \n",
       " 1327  3188.42  2565.930  2208.9000   934.7558  1.9469  119.3544  0.1222   \n",
       " 1328  3177.96  2369.840  2162.1333   998.9095  0.8826  104.9722  0.1246   \n",
       " 1329  3266.55  2425.300  2211.4000  1511.7842  1.3004   97.4700  0.1237   \n",
       " 1342  3034.50  2406.110  2183.5889  1108.6491  1.3963  104.0856  0.1233   \n",
       " 1343  3058.46  2467.730  2228.4778  1721.1108  1.4301   93.6222  0.1221   \n",
       " 1363  3016.46  2559.420  2167.0889  1253.2140  1.3679  101.6667  0.1243   \n",
       " 1364  3163.86  2470.600  2211.4000  1511.7842  1.3004   97.4700  0.1237   \n",
       " 1365  2988.39  2493.720  2206.4000   982.5452  1.1853  116.8167  0.1228   \n",
       " 1400  3052.98  2515.510  2172.8111   969.3436  1.2736  102.7367  0.1243   \n",
       " 1438  2951.84  2477.130  2192.1889  1435.9611  2.3870  107.3989  0.1229   \n",
       " 1443  3173.18  2428.640  2209.4667  1556.3930  1.4884   95.1156  0.1206   \n",
       " 1519  2903.34  2585.480  2196.1111  1472.6400  1.5599   94.6522  0.1212   \n",
       " \n",
       "            8       9      10  ...      565       570     571       572  \\\n",
       " 2     1.4436  0.0041  0.0013  ...  0.62190  535.0245  2.0293   11.2100   \n",
       " 10    1.5144 -0.0190  0.0013  ...  0.12240  532.1764  1.8715    9.5699   \n",
       " 11    1.4690  0.0170 -0.0154  ...  0.08560  533.7464  2.1865    7.7400   \n",
       " 14    1.4474  0.0144 -0.0119  ...  0.11955  532.6446  2.2808   11.4200   \n",
       " 23    1.5585 -0.0317 -0.0138  ...  0.11955  536.0054  1.9902    8.7600   \n",
       " 38    1.4780 -0.0131  0.0079  ...  0.11955  533.9509  1.8946    7.0300   \n",
       " 40    1.5424 -0.0177  0.0140  ...  0.17180  537.6273  2.2025   12.2100   \n",
       " 45    1.5336 -0.0283  0.0050  ...  0.12830  528.4727  2.1628    8.6700   \n",
       " 48    1.5533  0.0173  0.0067  ...  0.13140  530.9973  2.1898    7.2300   \n",
       " 49    1.5516 -0.0343  0.0065  ...  0.11955  533.3209  2.0680    6.4300   \n",
       " 50    1.5717 -0.0105  0.0052  ...  0.11955  533.1055  2.3497    6.5900   \n",
       " 57    1.4786  0.0049  0.0116  ...  0.07640  535.2509  2.2864    9.3200   \n",
       " 58    1.4441 -0.0177  0.0263  ...  0.15630  533.7473  2.2437    8.2200   \n",
       " 62    1.4729  0.0206  0.0030  ...  0.13870  535.8027  1.9736    7.3900   \n",
       " 64    1.4672 -0.0088  0.0071  ...  0.06420  532.2164  2.3626    9.7599   \n",
       " 82    1.5824 -0.0006  0.0064  ...  0.21770  531.8100  2.2115   13.7600   \n",
       " 96    1.4993 -0.0091  0.0006  ...  0.04890  533.9136  2.4527    9.2700   \n",
       " 115   1.4518  0.0066  0.0151  ...  0.11955  532.1818  2.2788   11.6000   \n",
       " 131   1.4240 -0.0080  0.0074  ...  0.11450  527.1709  2.3873   11.2100   \n",
       " 154   1.4544 -0.0052 -0.0003  ...  0.05430  535.0655  2.2935    8.1000   \n",
       " 157   1.4680  0.0033  0.0018  ...  0.25000  535.4327  2.3964   10.4500   \n",
       " 158   1.4047 -0.0080  0.0038  ...  0.06910  536.0109  2.2047    6.1100   \n",
       " 167   1.5285 -0.0073  0.0003  ...  0.14530  531.7309  2.3944    9.8400   \n",
       " 169   1.5473 -0.0128  0.0032  ...  0.11955  530.9354  2.5619    7.6900   \n",
       " 180   1.4068  0.0057  0.0033  ...  0.15310  536.3600  2.2707   10.1200   \n",
       " 182   1.4494 -0.0013  0.0042  ...  0.18120  536.0336  2.2646    7.3701   \n",
       " 186   1.5100 -0.0174  0.0080  ...  0.22840  532.0545  2.5380    9.3800   \n",
       " 188   1.4746 -0.0110  0.0285  ...  0.16740  532.4191  2.3255   12.4100   \n",
       " 189   1.4701 -0.0056  0.0107  ...  0.18320  535.6864  1.9080   10.2500   \n",
       " 218   1.3666 -0.0154  0.0047  ...  0.08360  531.9191  1.7570    8.4500   \n",
       " 222   1.3605  0.0125 -0.0034  ...  0.11955  533.8327  1.5604    9.2800   \n",
       " 231   1.4978  0.0221 -0.0055  ...  0.11900  529.0654  2.4848    6.8900   \n",
       " 235   1.4257 -0.0343 -0.0016  ...  0.11955  530.7709  2.1968    5.0300   \n",
       " 236   1.3720  0.0005 -0.0181  ...  0.22310  538.0918  2.1799    6.2700   \n",
       " 238   1.3560 -0.0055 -0.0225  ...  0.19590  536.3273  2.2994    5.8400   \n",
       " 240   1.5192 -0.0168  0.0104  ...  0.16760  535.9227  1.9110    8.6500   \n",
       " 241   1.4114 -0.0005 -0.0088  ...  0.16770  535.7882  2.2067    7.6500   \n",
       " 243   1.5105 -0.0009  0.0134  ...  0.22500  532.2127  1.5093   10.5300   \n",
       " 244   1.4454 -0.0152 -0.0087  ...  0.10150  533.1445  1.7220    8.1600   \n",
       " 273   1.4362  0.0062  0.0073  ...  0.11955  531.3791  2.4080    8.0200   \n",
       " 277   1.4500 -0.0089  0.0004  ...  0.19610  531.8655  2.3718    9.7900   \n",
       " 282   1.4833  0.0138  0.0012  ...  0.11955  531.2245  2.4840    7.0700   \n",
       " 291   1.5312  0.0133 -0.0037  ...  0.11955  533.7727  1.7872    6.9800   \n",
       " 294   1.4932 -0.0012  0.0028  ...  0.11955  535.8300  1.6951    5.3200   \n",
       " 321   1.4934 -0.0074 -0.0151  ...  0.11955  317.1964  2.5047  286.8200   \n",
       " 323   1.4610  0.0185  0.0058  ...  0.11955  531.2809  2.3875   10.3200   \n",
       " 326   1.3955 -0.0259  0.0003  ...  0.11955  534.1736  1.7757    9.3000   \n",
       " 327   1.4086 -0.0087 -0.0014  ...  0.11955  531.9845  1.8554   10.8900   \n",
       " 336   1.3901 -0.0010  0.0060  ...  0.21410  533.8264  2.3352    8.4200   \n",
       " 344   1.5057 -0.0213  0.0039  ...  0.11955  531.1509  2.3937   10.4400   \n",
       " 351   1.4454 -0.0125 -0.0162  ...  0.15340  529.5736  2.3868   11.5400   \n",
       " 368   1.5044  0.0038  0.0137  ...  0.35190  532.2545  2.4656   12.0500   \n",
       " 373   1.4385 -0.0029  0.0099  ...  0.08770  508.2764  2.0687  272.4100   \n",
       " 392   1.5523 -0.0110  0.0096  ...  0.08770  532.7000  2.3601   10.4600   \n",
       " 406   1.4628  0.0163 -0.0264  ...  0.11955  537.6618  2.2127    8.3700   \n",
       " 424   1.4849 -0.0020  0.0082  ...  0.08770  530.6027  2.3970    7.1100   \n",
       " 441   1.4727 -0.0005 -0.0076  ...  0.11955  532.1854  2.2439    9.1100   \n",
       " 448   1.5272  0.0181  0.0022  ...  0.23110  534.8673  2.1953    8.7300   \n",
       " 495   1.4969 -0.0098 -0.0134  ...  0.13710  534.4918  2.3933    8.1800   \n",
       " 508   1.5240 -0.0016  0.0061  ...  0.02520  538.8482  2.2732    7.0900   \n",
       " 518   1.4867 -0.0091  0.0005  ...  0.11955  537.0373  2.0049    9.1400   \n",
       " 576   1.4756  0.0125 -0.0096  ...  0.14100  534.7200  2.3705    7.0400   \n",
       " 583   1.4031  0.0027  0.0017  ...  0.08260  535.8155  2.3326    7.5800   \n",
       " 601   1.4542 -0.0002 -0.0007  ...  0.11955  529.7309  2.2346    7.5200   \n",
       " 605   1.4454  0.0046  0.0101  ...  0.12820  532.2382  1.7897   10.5100   \n",
       " 634   1.3421 -0.0283  0.0084  ...  0.11890  535.2009  1.6920    8.9000   \n",
       " 709   1.4943  0.0279  0.0121  ...  0.11955  529.3191  2.0905   11.2500   \n",
       " 795   1.4372 -0.0047 -0.0148  ...  0.15340  533.3382  2.4033    9.4300   \n",
       " 797   1.4228 -0.0011 -0.0002  ...  0.08770  533.3382  2.4033    9.4300   \n",
       " 826   1.4037  0.0013  0.0025  ...  0.08770  535.2482  1.8813    6.8400   \n",
       " 831   1.3953  0.0084  0.0062  ...  0.08770  535.9527  2.2638    9.6200   \n",
       " 871   1.4650  0.0035  0.0053  ...  0.18990  534.6682  2.0435    9.4300   \n",
       " 914   1.5071  0.0045 -0.0011  ...  0.07730  531.8455  2.2544    8.6400   \n",
       " 924   1.6107  0.0250  0.0125  ...  0.08770  531.8455  2.2544    8.6400   \n",
       " 926   1.4695  0.0071  0.0215  ...  0.08770  531.8455  2.2544    8.6400   \n",
       " 929   1.5817  0.0118  0.0000  ...  0.08770  531.8455  2.2544    8.6400   \n",
       " 1029  1.5374 -0.0343  0.0101  ...  0.08770  531.8455  2.2544    8.6400   \n",
       " 1062  1.5575  0.0049 -0.0207  ...  0.08770  532.0109  2.0679   11.3500   \n",
       " 1144  1.4600  0.0029  0.0133  ...  0.36930  524.4955  1.0912   11.1200   \n",
       " 1151  1.4797 -0.0047 -0.0056  ...  0.08770  530.2409  1.9614   11.5400   \n",
       " 1185  1.6105 -0.0184  0.0023  ...  0.23550  533.9936  2.2318    8.2700   \n",
       " 1189  1.4304  0.0214  0.0105  ...  0.29420  529.6464  2.0320    5.8100   \n",
       " 1211  1.3969 -0.0074 -0.0152  ...  0.11955  533.9936  2.2318    8.2700   \n",
       " 1227  1.4827  0.0047  0.0145  ...  0.10030  533.1809  1.0744    5.6500   \n",
       " 1238  1.4300  0.0129 -0.0088  ...  0.32020  524.4955  1.0912   11.1200   \n",
       " 1241  1.4679 -0.0033 -0.0022  ...  0.08770  529.9445  1.9740    7.9100   \n",
       " 1242  1.4496  0.0022  0.0008  ...  0.47910  533.1809  1.0744    5.6500   \n",
       " 1254  1.3858 -0.0053  0.0054  ...  0.20190  529.5182  2.1130   10.6500   \n",
       " 1302  1.3464  0.0140 -0.0122  ...  0.08990  530.3364  2.0270    9.9200   \n",
       " 1303  1.5559 -0.0096  0.0028  ...  0.08770  529.9445  1.9740    7.9100   \n",
       " 1324  1.4115  0.0062 -0.0007  ...  0.13560  531.1982  2.2418   10.6300   \n",
       " 1325  1.4610 -0.0154 -0.0050  ...  0.08770  531.3445  2.0359    5.7100   \n",
       " 1327  1.4984  0.0163  0.0191  ...  0.08770  531.3445  2.0359    5.7100   \n",
       " 1328  1.3719  0.0048  0.0089  ...  0.08770  534.1600  1.1090  432.9400   \n",
       " 1329  1.4321 -0.0076 -0.0123  ...  0.08770  531.3445  2.0359    5.7100   \n",
       " 1342  1.5171 -0.0292 -0.0071  ...  0.08770  529.5182  2.1130   10.6500   \n",
       " 1343  1.4854  0.0070  0.0019  ...  0.16540  528.0127  1.1073    9.0400   \n",
       " 1363  1.4093 -0.0041  0.0094  ...  0.19810  530.6345  1.9733    7.6000   \n",
       " 1364  1.4362 -0.0034 -0.0070  ...  0.21760  533.1809  1.0744    5.6500   \n",
       " 1365  1.5279 -0.0094  0.0001  ...  0.55280  530.0846  1.9812    9.0100   \n",
       " 1400  1.4065 -0.0037 -0.0058  ...  0.08770  533.1809  1.0744    5.6500   \n",
       " 1438  1.4613 -0.0027 -0.0039  ...  0.22560  531.7464  2.2909    9.1299   \n",
       " 1443  1.6073 -0.0278 -0.0057  ...  0.28110  533.1364  2.0133    7.2400   \n",
       " 1519  1.5134  0.0036 -0.0055  ...  0.19310  530.3364  2.3002   10.0300   \n",
       " \n",
       "          582     583     586     587       589  Pass/Fail  \n",
       " 2     0.4958  0.0157  0.0584  0.0484   82.8602          1  \n",
       " 10    0.4925  0.0158  0.0355  0.0205   57.8122          1  \n",
       " 11    0.4987  0.0427  0.0370  0.0279   75.5077          1  \n",
       " 14    0.5077  0.0094  0.0202  0.0289  142.9080          1  \n",
       " 23    0.4982  0.0099  0.0098  0.0213  216.8869          1  \n",
       " 38    0.5022  0.0105  0.0150  0.0151  100.7279          1  \n",
       " 40    0.5068  0.0201  0.0189  0.0086   45.4264          1  \n",
       " 45    0.5018  0.0116  0.0137  0.0155  113.3721          1  \n",
       " 48    0.4976  0.0087  0.0246  0.0138   56.0357          1  \n",
       " 49    0.4972  0.0151  0.0296  0.0165   55.8324          1  \n",
       " 50    0.5026  0.0098  0.0296  0.0165   55.8324          1  \n",
       " 57    0.5048  0.0138  0.0201  0.0220  109.4273          1  \n",
       " 58    0.5049  0.0121  0.0207  0.0301  145.6339          1  \n",
       " 62    0.5053  0.0099  0.0194  0.0169   87.3034          1  \n",
       " 64    0.5010  0.0289  0.0297  0.0556  187.3554          1  \n",
       " 82    0.4955  0.0094  0.0278  0.0042   15.2909          1  \n",
       " 96    0.4996  0.0326  0.0095  0.0184  192.2985          1  \n",
       " 115   0.4995  0.0093  0.0364  0.0166   45.6835          1  \n",
       " 131   0.4990  0.0119  0.0058  0.0169  289.9234          1  \n",
       " 154   0.5025  0.0128  0.0199  0.0117   58.5665          1  \n",
       " 157   0.4993  0.0138  0.0140  0.0180  128.2819          1  \n",
       " 158   0.4984  0.0155  0.0140  0.0180  128.2819          1  \n",
       " 167   0.5046  0.0159  0.0419  0.0098   23.3511          1  \n",
       " 169   0.5024  0.0389  0.0419  0.0098   23.3511          1  \n",
       " 180   0.5018  0.0460 -0.0012  0.0220    0.0000          1  \n",
       " 182   0.5005  0.0203  0.0077  0.0204  264.7525          1  \n",
       " 186   0.4987  0.0139  0.0158  0.0186  118.2289          1  \n",
       " 188   0.4982  0.0154  0.0104  0.0221  211.6182          1  \n",
       " 189   0.4915  0.0112  0.0104  0.0221  211.6182          1  \n",
       " 218   0.4968  0.0126  0.0279  0.0123   44.1754          1  \n",
       " 222   0.4995  0.0201  0.0440  0.0133   30.2219          1  \n",
       " 231   0.5038  0.0188  0.0118  0.0098   83.1192          1  \n",
       " 235   0.4947  0.0121 -0.0034  0.0093  272.3477          1  \n",
       " 236   0.5042  0.0116 -0.0034  0.0093  272.3477          1  \n",
       " 238   0.5008  0.0138  0.0198  0.0069   34.7486          1  \n",
       " 240   0.5044  0.0115  0.0154  0.0271  176.0329          1  \n",
       " 241   0.5012  0.0096  0.0154  0.0271  176.0329          1  \n",
       " 243   0.5014  0.0099  0.0154  0.0271  176.0329          1  \n",
       " 244   0.4958  0.0112  0.0154  0.0271  176.0329          1  \n",
       " 273   0.5031  0.0102  0.0274  0.0142   51.9067          1  \n",
       " 277   0.5036  0.0153  0.0177  0.0184  104.4612          1  \n",
       " 282   0.5031  0.0153  0.0121  0.0169  139.8330          1  \n",
       " 291   0.5033  0.0216  0.0291  0.0237   81.3456          1  \n",
       " 294   0.4915  0.0116  0.0291  0.0237   81.3456          1  \n",
       " 321   0.5004  0.0174  0.0171  0.0096   56.0858          1  \n",
       " 323   0.4997  0.0174  0.0107  0.0071   66.2997          1  \n",
       " 326   0.5013  0.0155  0.0210  0.0156   74.0589          1  \n",
       " 327   0.4972  0.0260  0.0226  0.0106   46.9253          1  \n",
       " 336   0.5081  0.0169  0.0353  0.0190   53.7460          1  \n",
       " 344   0.5004  0.0123  0.0358  0.0162   45.2582          1  \n",
       " 351   0.5012  0.0110  0.0275  0.0314  114.1967          1  \n",
       " 368   0.4981  0.0184  0.0046  0.0180  388.9648          1  \n",
       " 373   0.4994  0.0160  0.0075  0.0112  150.3448          1  \n",
       " 392   0.5015  0.0129  0.0104  0.0083   79.8045          1  \n",
       " 406   0.5006  0.0184  0.0061  0.0141  233.2441          1  \n",
       " 424   0.4970  0.0174  0.0005  0.0115    0.0000          1  \n",
       " 441   0.5002  0.0162  0.0329  0.0055   16.6695          1  \n",
       " 448   0.5017  0.0161  0.0235  0.0355  150.7761          1  \n",
       " 495   0.5018  0.0103  0.0255  0.0260  102.1652          1  \n",
       " 508   0.5007  0.0169  0.0048  0.0226  474.0812          1  \n",
       " 518   0.4929  0.0145  0.0331  0.0210   63.2615          1  \n",
       " 576   0.5088  0.0275  0.0182  0.0139   76.6094          1  \n",
       " 583   0.4961  0.0185  0.0332  0.0216   65.1043          1  \n",
       " 601   0.5003  0.0101  0.0225  0.0193   85.7175          1  \n",
       " 605   0.5049  0.0145  0.0199  0.0159   79.7752          1  \n",
       " 634   0.4931  0.0364  0.0328  0.0235   71.5333          1  \n",
       " 709   0.5025  0.0131  0.0161  0.0350  217.1506          1  \n",
       " 795   0.4994  0.0099  0.0406  0.0268   66.1687          1  \n",
       " 797   0.5022  0.0346  0.0406  0.0268   66.1687          1  \n",
       " 826   0.5022  0.0169  0.0275  0.0215   78.1199          1  \n",
       " 831   0.4994  0.0115  0.0545  0.0184   33.7876          1  \n",
       " 871   0.4936  0.0131  0.0117  0.0262  223.1018          1  \n",
       " 914   0.5080  0.0168  0.0134  0.0121   90.4575          1  \n",
       " 924   0.5087  0.0116  0.0153  0.0048   31.0176          1  \n",
       " 926   0.5003  0.0106  0.0153  0.0048   31.0176          1  \n",
       " 929   0.5026  0.0121  0.0153  0.0048   31.0176          1  \n",
       " 1029  0.5010  0.0116  0.0267  0.0174   65.1609          1  \n",
       " 1062  0.5027  0.0184  0.0189  0.0059   31.0252          1  \n",
       " 1144  0.4984  0.0155  0.0275  0.0108   39.1032          1  \n",
       " 1151  0.5014  0.0121  0.0275  0.0108   39.1032          1  \n",
       " 1185  0.4968  0.0167  0.0233  0.0138   59.1201          1  \n",
       " 1189  0.5042  0.0108  0.0262  0.0104   39.5528          1  \n",
       " 1211  0.5033  0.0268  0.0284  0.0209   73.5120          1  \n",
       " 1227  0.4926  0.0135  0.0102  0.0133  130.0641          1  \n",
       " 1238  0.5013  0.0095  0.0193  0.0072   37.6251          1  \n",
       " 1241  0.5012  0.0131  0.0283  0.0112   39.4516          1  \n",
       " 1242  0.5046  0.0191  0.0283  0.0112   39.4516          1  \n",
       " 1254  0.4988  0.0102  0.0243  0.0100   41.0782          1  \n",
       " 1302  0.5037  0.0118  0.0246  0.0167   68.1642          1  \n",
       " 1303  0.5019  0.0154  0.0204  0.0159   77.9730          1  \n",
       " 1324  0.4970  0.0089  0.0230  0.0210   91.4264          1  \n",
       " 1325  0.5046  0.0107  0.0230  0.0210   91.4264          1  \n",
       " 1327  0.5042  0.0099  0.0230  0.0210   91.4264          1  \n",
       " 1328  0.5070  0.0144  0.0230  0.0210   91.4264          1  \n",
       " 1329  0.4997  0.0126  0.0230  0.0210   91.4264          1  \n",
       " 1342  0.4966  0.0130  0.0346  0.0251   72.6469          1  \n",
       " 1343  0.5068  0.0189  0.0346  0.0251   72.6469          1  \n",
       " 1363  0.4995  0.0122  0.0182  0.0077   42.5048          1  \n",
       " 1364  0.5028  0.0113  0.0182  0.0077   42.5048          1  \n",
       " 1365  0.4972  0.0154 -0.0006  0.0118    0.0000          1  \n",
       " 1400  0.5081  0.0158  0.0302  0.0159   52.7014          1  \n",
       " 1438  0.4969  0.0118  0.0178  0.0120   67.7994          1  \n",
       " 1443  0.5021  0.0192  0.0281  0.0247   88.1528          1  \n",
       " 1519  0.4979  0.0176  0.0157  0.0134   85.3122          1  \n",
       " \n",
       " [104 rows x 202 columns]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e6db89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABS0AAAUUCAYAAAA6ADcaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAEAAElEQVR4nOzdeXxcZ3n3/+81I2m0S9bmTbblRY6XJM7irISQACEJbQlQGhJ2WkhToIUuT0vb39NSnvI83aAtLWUtDRQChD1AIIRAWApZnN12vMi2JEuyrX3fNffvj3MmGcsjaSSNdI6kz/v1mtdozrnPfa45GmtuX+dezDknAAAAAAAAAAiLSNABAAAAAAAAAEAykpYAAAAAAAAAQoWkJQAAAAAAAIBQIWkJAAAAAAAAIFSygg4AAAAAAAAAwLkef/zxqqysrM9IOl/Lt/NhXNL+8fHxd1x66aWtiY0kLQEAAAAAAIAQysrK+syaNWt2VlZWdkUiERd0PAshHo9bW1vbrtOnT39G0qsS25drhhYAAAAAAABY6s6vrKzsXa4JS0mKRCKusrKyR15v0he2BxQPAAAAAAAAgOlFlnPCMsF/j2flKRkeDgAAAAAAACClaDR6aW1t7dDExIRt27Zt6J577qkvKiqKL/R56WkJAAAAAAAAIKVYLBY/dOjQwaNHjx7Izs52H/7whysX47wkLQEAAAAAAADM6Jprrumvq6uL3X333SUXXnjhjp07d+66+uqrt588eTJLkr73ve8V7tixY9eOHTt27dy5c1dXV1ekoaEhe+/eveft2LFjV21t7e4f/OAHhemci+HhAAAAAAAAQMj9r689veHI6b78TNa5fU3R4D++bs/JdMqOjY3p/vvvL37FK17Re8MNN/TfdttthyKRiD7ykY9UfPCDH1zz6U9/uunDH/7wmo9+9KMNr3jFKwZ6enoi+fn58X/5l3+pfNnLXtbz93//96fHx8fV19eXVidKkpYAAAAAAAAAUhoZGYns2LFjlyRdccUVfe9973vbn3nmmdxXv/rV1W1tbdmjo6ORDRs2jEjSlVde2f8nf/InG2699dbO22+/vWvr1q3xK6+8cuB3f/d3a8bGxiKve93ruq6++uqhdM5rzi37BYgAAAAAAACAJefpp5+u37NnT3uQMeTn5188ODj4ZPK2yy+//Lz3vve9p9/4xjf2fPe73y364Ac/uO7RRx89LEmPPvpo3re//e2Sz3zmM1U/+MEPjlx88cXD9fX12V//+tdLPvnJT1b9wR/8wZn3vOc9HZPP8/TTT1fs2bOnJvGanpYAAAAAAAAA0tbX1xfduHHjmCTddddd5YntBw4ciF1++eVDl19++dAjjzxSsH///tyCgoL45s2bR//4j/+4fWBgIPLEE0/kSzonaTkZSUsAAAAAAAAAafvLv/zLlttvv33r6tWrR/fu3TvQ2NgYk6R/+Id/qPrlL39ZHIlE3Pbt24de97rX9XzmM58p++hHP7omKyvL5efnT3zxi188kc45GB4OAAAAAAAAhFAYhocvlsnDw9NarQcAAAAAAAAAFgtJSwAAAAAAAAChQtISAAAAAAAAQKiQtAQAAAAAAAAQKiQtAQAAAAAAAIQKSUsAAAAAAAAAoULSEgAAAAAAAEBK0Wj00h07duxKPA4fPpwzVdmLL754hyQdPnw4p7a2dvd8zps1n4MBAAAAAAAALF+xWCx+6NChg+mUffLJJw9l6rz0tAQAAAAAAACQlp6enshVV121fdeuXTu3b9++6wtf+EJpYl9+fv7FmToPPS0BAAAAAACAsPvWuzeo9WB+Ruus2jWoV3/s5HRFRkZGIjt27NglSRs2bBi57777jn3ve9+rKysri586dSrriiuu2PGGN7yhOxLJbN9IkpYAAAAAAAAAUpo8PHxkZMTe9773VT/88MOFkUhEra2tOU1NTVkbN24cz+R5SVoCAAAAAAAAYTdDj8jF8slPfrKso6Mj69lnn30uFou59evXXzA0NJTxKSiZ0xIAAAAAAABAWnp6eqIVFRVjsVjMfec73ylqaWmZcjXx+aCnJQAAAAAAAIC0vOMd7+i8+eabt51//vk7d+/ePbh58+bhhTiPOecWol4AAAAAAAAA8/D000/X79mzpz3oOBbD008/XbFnz56axGuGhwMAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUCFpCQAAAAAAAIRTPB6PW9BBLDT/PcaTt5G0BAAAAAAAAMJpf1tbW8lyTlzG43Fra2srkbQ/eXtWQPEAAAAAAAAAmMb4+Pg7Tp8+/ZnTp0+fr+Xb+TAuaf/4+Pg7kjeacy6geAAAAAAAAADgXMs1QwsAAAAAAABgiSJpCQAAAAAAACBUSFoCAAAAAAAACBWSlgAAAAAAAABChaQlAAAAAAAAgFAhaQkAAAAAAAAgVEhaAgAAAAAAAAgVkpYAAAAAAAAAQoWkJQAAAAAAAIBQIWkJAAAAAAAAIFRIWgIAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUCFpCQAAAAAAACBUSFoCAAAAAAAACBWSlgAAAAAAAABChaQlAAAAAAAAgFAhaQkAAAAAAAAgVEhaAgAAAAAAAAgVkpYAAAAAAAAAQoWkJQAAAAAAAIBQIWkJAAAAAAAAIFRIWgIAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUCFpCQAAAAAAACBUSFoCAAAAAAAACBWSlgAAAAAAAABChaQlAAAAAAAAgFAhaQkAAAAAAAAgVEhaAgAAAAAAAAgVkpYAAAAAAAAAQoWkJQAAAAAAAIBQIWkJAAAAAAAAIFRIWgIAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUCFpCQAAAAAAACBUSFoCAAAAAAAACBWSlgAAAAAAAABChaQlAAAAAAAAgFAhaQkAAAAAAAAgVEhaAgAAAAAAAAgVkpYAAAAAAAAAQoWkJQAAAAAAAIBQIWkJAAAAAAAAIFRIWgIAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUCFpCQAAAAAAACBUSFoCAAAAAAAACBWSlgAAAAAAAABChaQlAAAAAAAAgFAhaQkAAAAAAAAgVEhaAgAAAAAAAAgVkpYAAAAAAAAAQoWkJQAAAAAAAIBQIWkJAAAAAAAAIFRIWgIAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUCFpCQAAAAAAACBUSFoCAAAAAAAACBWSlgAAAAAAAABChaQlAAAAAAAAgFAhaQkAAAAAAAAgVEhaAgAAAAAAAAgVkpYAAAAAAAAAQoWkJQAAAAAAAIBQIWkJAAAAAAAAIFRIWgIAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUCFpCQAAAAAAACBUSFoCAAAAAAAACBWSlgAAAAAAAABChaQlAAAAAAAAgFAhaQkAAAAAAAAgVEhaAgAAAAAAAAgVkpYAAAAAAAAAQoWkJQAAAAAAAIBQIWkJAAAAAAAAIFRIWgIAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUCFpCQAAAAAAACBUSFoCAAAAAAAACBWSlgAAAAAAAABChaQlAAAAAAAAgFAhaQkAAAAAAAAgVEhaAgAAAAAAAAgVkpYAAAAAAAAAQoWkJQAAAAAAAIBQIWkJAAAAAAAAIFRIWgIAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUCFpCQAAAAAAACBUSFoCAAAAAAAACBWSlgAAAAAAAABChaQlAAAAAAAAgFAhaQkAAAAAAAAgVEhaAgAAAAAAAAgVkpYAAAAAAAAAQoWkJQAAAAAAAIBQIWkJAAAAAAAAIFRIWgIAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUCFpCQAAAAAAACBUSFoCAAAAAAAACBWSlgAAAAAAAABChaQlAAAAAAAAgFAhaQkAAAAAAAAgVEhaYkUzsxozc2Z2V9CxAAAAIHzM7CEzc0HHAQDASkPSEsuWn4yc7vG2RYqj3szq53jseWb2aTOrM7MhMxswsxNm9kMz+yszW53hcJckM7su6fd6wsxS/m0zs0Iz600qW7PIoWIBcRMCADLDzKJm9k4z+6mZdZrZmJm1mtkzZvYZM3tV0DGGnZndYGbfNLMWMxs1sy4zO2JmXzWzPzAzCzrGMDCzDyS1yz43TbmXJJWrX8QQsQjM7G2L+f8zAEtHVtABAIvgb6bY/pSkZkk7JfUsWjRpMrOXSvqepFxJv5L0A0mDkmokXSTpBkm/lHQmmAhDaVze9Xm5pB+m2H+bpCK/HH//AACYxMyikr4r6SZJ3fLaIk2SyiRtlfQGSTsk3RtQiKFnZn8h6UPy2hs/kHRYUrakzZJeIul1kv7D3w/PuKTfMrP3Oue6U+x/p2i/AcCKwx99LHvOuQ/MUOTQYsQxB5+Ul7B8m3PunDvPZnahpK5FjyrcfiTpenkN21RJy3dKOiWpUdIVixgXAABLxe3yEpZPS3qJc+6sG7tmli++Q6dkZpskfVBSr6RrnHPPTtofkXfjeSKA8MLsu5JeLemNkj6WvMPMVkn6TUnfkfSaRY8MABAYhodjRZtqOKmZ3eVv32Jmv+8Phxoys4f8/WZmbzWzX5pZm5kNm9lJM7vfzF7vl7nOn/9ok6RNk4am36VpmFmVpG2SelIlLCXJOfeMc+5kimOrzezfzey4mY2YWYeZ3Wtml6UomxiSc52Zvc7MHjWzQX8o2JfNbH2KY7aY2aeShqx3mtmzZvYJMyufVDZmZu/3r9+gPzT752Z2a4p6n/9dmNl2M/uKPxQtbmbXTXe9knRI+oakW8ysclL9F0q6XNJ/aZqeDWa2w4/hpH/9zpjZ3WZ2Xoqy283s78xsn/85GDGzBv/6VKcoP+PnJqmsS3zeUtST+HzWJG1L6/qZ2Y1mdp+ZtfvxHjOzfzSz0hTnqfcfhWb2z36sQ2b2lJm92i+TZWZ/YWZH/fdzzMzeM831ncv58/0yjf4xdWb2Z2YvDK0zsw9IOuG/fKulmApiNtcfAFawq/3nuyYnLCXJOTfonPtJqgPN7HYz+4l5Q6GHzew5M/v/zCw2RfkdZvZZ/2/9iP+99XMz+70UZV9mZj8wr90xbN5Q678zs5IUZR/y//4nf0eN+H/z/97McqaI5zYze9z/rms1s/82s3XTXq1zXSEpKuknkxOWkuScizvn7nfOnTNHppldYWZfM7PT5g0pP2lmn0wVw1zeo5m92My+Y2ZNftnTZvawmf11irJrzexj/u9m1P/e/IaZXZqi7PNDe83sJj+2HpvdPKA/kNej950p9r1Z3o38T09XwSzbGNeb1147aF77dMjM9pvZX5tZboryRWb2v/0yvWbW59f/leRrYi9MWfSBKWI8Z9qodK6f/3t+l//76jWvXf2kmb3HJk2LZGe3Cbf6n6kOP+Yfmtn5frlK/xqc8v9NPWZm108R91zPX2Pe/yna/XPsM7Nfn1T+IXntc0n6Lzu7DVczm+sPYPmhpyUwvX+V9GJ5Q6Pu0wt3xT8k6c/lJUnukTe8fK2kyyT9lqSvSKqXNzT9ff4x/5JU71MznLdHXmKt0MzWOudOpROsmV0ir4dhmaT75SXwKuTduf6Fmb3GOXdfikPfJelV8oZ6/VReg/v1kvaY2UXOuRG//rWSHpNULO96fF1eI3KzvAblv8tLHMpvLN8vbxjUIXl3zfPlDYn6il/vX6SIZaukRyQdkfRFSXnyeiuk69Pyeom8VdI/JW1/pyQn6T/9mM5hZjfJu2bZ8u7m10mqlvRaSb9mZtc7555IOuS1ku6U9BN5Q/VHJe2W9A5Jv2Fme51zzUnl0/nczNeU18/M/kreZ7JTXo+GVkkXSvoTSa80s6ucc5OvdbakB+R9pr4tKUfe9f26mb1C3mfnCknflzTiv49/M7M259xZ72ce5/+hpHX+OcblfZ7/Tt5nLzH9w0OSSiW9V17voG8l1fGU/7wY1x8AlroO/3n7bA4ys/+U9NvyEk/fkDe0/EpJ/0fSy8zsBufceFL5X5P0VUkxeQmrL8n7O75H0p9K+nhS2d/1Xw/4x7RKuk7Sn8n7vn3RFEOK75bXjvu+vO/CV/p1V0l6+6T4/1DSR/y4P+8/3yjv+3020wglrt8WM4s659LqUWlmb5fXhhmR1x47KalWL7QprnTONaY4NK336LdxvueXuVfeFEll8qZJepeSplMys82SfiHvu/fH8n43G+R9V/6amf2mc+67KWJ5nbxeut+X9Al5U/aka0LSZyX9ld9+2pe0753y2tU/murgObQx/kzeNAe/1AvTMb1I0gckXWdmL0/87szM5H1Gr5Y3ZdNn5LVHNsj7HP5c0uOzeK9TSXn9zCzRLr1R3lQDd0salje66N/ktcPenKK+Gnltwuck3eW/fo2kh8zsKv899cpr/5TJm0bp+2a2PfmzNo/zb5L0qKTjkv7bP8frJX3bv76Jmx93yfv3dou8tuZTSXV0L+L1BxBGzjkePJblQ16CyslrfEx+vM0vU+OXuWvSsXf525slbU5Rd4e8Rnl+in0Vk17XS6qfQ/xf82M4Jq/BdUWq8yWVz5KXZBuWN5wred86/72ckhRL2v4B/xy9ki6YdMzd/r5bk7b9vr/tvSnOXyApL+n1n/tl75OUlbS9yr8mTtLVSdsTvwsn6f/O8lpd5x/3BUkm6aikQ0n78+QNpX/Af/0Lv3xNUplVfpl2Sbsm1b9bUr+kJyZtX598PZO2v0Je4/vj8/jcOEkPTfF+70oR/7TXT17D0slrnJdO2vc2f98/p/jsOnkN1eTPzYv97Z3yktilSfu2yEvePpnB89836bNVJa9x2y0pO8U1uGvy+5/t9efBgwePlfqQdLH/dzwuL9HwWkmbZjgm8Xf8G8l/r/19H9CktoO8G6o9/nlekqK+6qSfN8lL5PVK2jGp3H/4dX9q0vaH/O2PSypL2l4gr600IWlN0vYa/xydk75bI/Ju0DpJLs3rV5D0/fUzeYnc3ZKi0xyz3b8WdZLWT9r3Uj/eb87zPSbex54U55/cBrnfL/uXk7ZfLS9Z1CGpMMXvPy7ppll+3hKfj3dI2ujH/cmk/VcmYpHX1nWa1K7W3NoYWyRZinj+j1/+9UnbLvC3fTNF+YikVUmvr/PLfmCK91ufIv5pr1/SNfq35M+RvB69/+nvu2XS5znRJpz8O/zfeqEN9wlJkaR9b57iWs3n/H89qa4b/e33TXEN3pbi/ad9/Xnw4LH8HoEHwIPHQj2SvixTPR7yyyS+VO+adOxdmiI55+/vkNdb65yEVYqy5zRO0ox/lbwGZjwp7gl5vcj+VtLqSeVv8cv84xT1vdff/8qkbYlGyN+mKJ9oAP5T0rZE0vKONOI/6se+I8W+3/Hr+WzStsTv4nQ613VSfdf5x37Bf/1n/utr/deJRtit/utUScvE9Xn3FOf4Z3//rjRjekbS8Xl8bp7/nKbYd1eK+Ke9fpK+6e/fPUWdT0pqTfHZdZK2pih/3N/30hT7fiLvPzXRDJ1/W4ryn/P3nZ/iGtw1xTnSvv48ePDgsZIfkm6Vd6Mzue3U4f8t/40U5Z+UNKZJCSN/X1TeDcFHk7b9sV/nv6YRy19q6htyq+QlM4d09s21h/xjXp7imL/x9/16inP8TYryW+S1v9wsrt+F/jVJvn6D8kazvGvy91BSG+PXpqjvm/73atE83mMiabl9htir/XINSroxmLT/v/39b0na9jZNkVRK41p9wD/2Hf7rRI/RAv/1f/rvfZ2mTlrOuo0xTTzlOreNmkia3Z3G8ddp7knLc66fvKRcu7x/j1kp9pfKa2/fk7Stxq/vhCYly+Ulhp28XstFk/ZF5f07/kmGzl8/+fz+/gZJ7VNcg7elKJ/29efBg8fyezA8HMuec85mLjWlR6fY/kV5CbwDZvZVeY3QX7kUcz/NlXOuS9Jv+nO53Chpr7xhrBf6j98zs5ucc4/5h1zlP2+aYh6dWv95p7yea8n26VyJ+TJXJW27V9L/lfQxM7tR3p34/5F00DnnEoXMrEjenJzNzrlUCx392H++OMW+p50/HH0e7pJ3p/yd8no53CGvwfWtaY5JXL89U1y/xDC5nZIOSs8PF3qjvIbWHnnXKpp0zOikOhb8c6Opr99V8hqiv2Vmv5Vif46kSjMrd851JG3vds4dS1G+Rd60AKmG4zTLuw5r/J/nc/4e51xdivKpPp8zWYzrDwBLnnPuHjP7prwbmNfI+76+Rt70HK82s8/LSy448xbm2SPve/Z9ZimbXSPyvj8TrvSfv59GOJf4zz+evMM512VmT0q6Vt5Q36cnFUm3fZM4x09TnOO4mZ2U1+MzLc65ZyRdbGZ75V3DS+W952v9xx3+lDNd/iGJNshLLMUc5PJGGETltUUmf++m+x6/KK/X7CNm9hV5Nxj/xznXNOnYRNvs5865sRR1/1jSm/xyn5+0b6p282x8Wt4Q6dv87+rXS/qec67FzKb6v+us2xhmViDvhvVr5F3XInmjdRKS53U/KG/I8u3mLbT0bXk3wPc55ya39eYj1fXbLi+RelTS/zfFv68hnf3vK+Epd+70BC3+8xHnXF/yDufchJmdkZe4XqjzS97n86oU26eyWNcfQAiRtASmd3qK7X8ob9j2b0t6v/8YN7P7JP3xFEmWOXHO1ctbSfyTkrfQjrzhUL8hr2F3kV80sQhOqsZassIU27pTbEvMO/V8Es4512Bml8u7K36TvMavJJ00s39yzn3Uf13iP081F2die2mKfVNd87Q5586Y2XfkJX3/Q95/tD48Q8Mmcf1STQCfLPn6fUTenKWn5CVwm+U13CQvkTn5PziL8bmZ6vqVy/ub/9czHF+oF+bjkqaex2tckqZI+CU+O9kZOH/3dOfX2UnimSzav1sAWOr8hNUP/YfMLCpvBefPSnqLvN5t35KXGDNJlZr5b3xCqf/cPF0h35zbFC71PJepvj8S5zgzxTlOaxZJy6Tz71NSUtFvQ31OXpL3r/XCvOeJNsj/mqHKc9pw6b5H59w3/AVQ/lje9+Dv+jE9LunPnXMP+EUDbcPJu0F+Wt5w8Wx5w92nXYBHs2xj+HM0/ljeAo375c3p2CYv8Sm/nucXj/KTeS+V9Ffy5p38e39Xn5l9Tt7160/r3U0v1fVLfDZqNf37S9W+T7WQ1rifeJyufTe5/TbX83dPc460FwRexOsPIIRYPRyYnku50bkJ59y/Ouf2SFotrxH/TXmL2fzAplglMyMBeXfEb5PXi2+PvbBid6LxcYtzzqZ5/E3KitM//3POudfLa8TslZf4iUj6VzP7nUmxrJmimrWTyp11ivnEl+RT8uayvMd/PVODNxHLnhmu3+ek51d4/wN5jd3znHNvcs79mXPuA865D8jrVXKWWX5unKa+sVQ6zfuY6vr1SOqa4b2Zc65hmrrnI+jzB/rvFgCWOv9v6D3yhjJL3lyL0gvfn0/O9Dc+qbpu/zm5N9tU5tOmSFfi2NVT7J/q3LPinHtU0nv8ly9N2pU4f8kM1/CcnqCzPP/3nHMvlZdofpm83+VuSd81s12TYgmkDee8xZruktcz9S/lzUU9U4/c2bYxbpGXsPycc+4C59wdzrm/9Ntvn5wiri7n3B865zbohQWSDsn7fX48qWjcf56qDVcyxXYp9fVLXOdvzvDeNk9T73wEfX5Js7r+AJYZkpbAPDnnWp1z33DO3Srvru1WSecnFZnQ7HqDpWNE5w49fth/fnGGz5WSc27cOfe4c+7v5a0mLXnDxuS84SbHJK03s9oUh1/vPz+RYl+mPCBvzpxqST9zzh2eofxsr98WeX9Df+gmDa/xe8Nume7gND43XfJWRTyL39PlojRjTPawpFVmtnsOx2bCYpw/MQRpxn9vaVx/AEBqie88kyTn9XA6IGm3mZWlWUfiO/fmNMo+6T9fN3mHmZXK+04clrdC8lwl2iMvSXGOLUrxfTwPZ10/32K34Qaccz92zv2RvGl/cvTC7yJxva+ZYjj2YrThPiMvgVctb27JmVZgn20bY5v//PUU+875DEzmnKtzzv2nX7ZfXhI0ITHkP1Ubbpumv/GcyiF5Sf4r/R6ii22xzj+bNtx01x/AMkPSEpglM4uZ2cts0qQu/hd5orE+mLSrQ948OnmzOEeBmf1vM5vqjv/75A3DOOhemP/v2/IShe82s1dOUe9V/txTc2Jml08RU2Jb8vv+rLwG+T/6ibZEHRXyVi5MlFkQzrm4vOHrr5E3p+VM/kteo+yv/eFbZzGziJldl7Sp3n++ZtL7K5TXq/Oshv4cPjePStpoZq+YFMr/pzkMUdMLPWM+bWbrJu/0P3NXTt6eQYtx/i55/8nZmKL+2V5/AFiRzOx2M7vBzM75f4KZrdEL06j8LGnXR+Qlvj7rJxInH7fKzC5J2vQ5eYut/J6ZXZuifPKcel+QN2z39/2kT7L/I6lY3kJ885kP+4tJ56hJiiMi6R81i/8z+W2lt6Vq9/nfOX/mv0y+fv/un/+fzWx7iuNyzGxeCU3/OzBVW/SsNpzzRvQ8IG8xlfdNquMKSW+Q9337zfnEMx3nzaV9k7w23EdnKC7Nvo1R7z9fN6ncFr0w9Dh5++YpEqKr5A0jH0radkjeZ/sWf1ROoo48pfdezuL3PP03eT1cPzrF52ptUk/ZjFrE8yf+P5OqDTeb6w9gmWFOS2D28iT9SFK9mT0irzdfrqQb5E1Cfa9zLvlu/4PyFtD5gZn9TF4vyaedc9+Z5hzZkj4oL4H2qLzJp7vkJVdeJG8VvQFJdyYOcM6Nmdlr5c2t+D0z+6V/3KC8u72Xyev9t1ZzT868QV5S9KeS6vyYtsqbX3NE0r8klf0neXftb5H0tD9vYL68OTerJP2Dc+4Xc4wjLc65J5RmTwDnXIeZvU5eI/xhM3tQXs+RuLwG1FXyhsTn+uVPm9mX5Q3Vf8rMfihvyM8N8np8PKWze0TO9nPzT/IWYPq2eRPmd0q6Wt7iNw8pRY+TGd7fg2b2fkn/T9JR//dxQl7ye5O8u9W/kPefhIxbjPM75/r9a/tiM/uipCPy7tzfK6lRs7v+ALBSXSFvgZLTZvYLeX+rJe/759fkfZ99W9LXEgc45z5rZpfKWxn7mJndL+/vbpl/3LXybg7e6ZdvN7M3+HX8xMy+L+kZeQnIC+W1Wzb7ZevN7H2SPibpCTO7R978gy+R9918SC8kAufEP8f7JX1Y0pP+926PvO/hUj+2C9Osbp3/Xv/dv34H5bUL1sr7jlsjrw31waTzHzKz35Z3M/eAmf1A3ndYtrw2yIvlvecd83ibH5ZUY2YPyUvajcpbIOil8r4Tv5xU9k55Cy3+o3/zdJ+838lvyWsXvX3yKJNMc879cBZlZ9vG+I6838EfmdkF8nqXbpT065K+p3MTZ3skfdO8+T/3y1vMplJeGzdbSYlOvz3+r/Ju0D9p3oJWWfLaGy16YSGc2fg/fgx3SvoNM/uxvPlgq+QNlX6RvKH0B+dQd1jO/yt5/z95n3k9thPzy/6bZnH9ASxDLgRLmPPgsRAPeT2u3Axlavxyd03afpe/vSbFMdmS/lTe/DqN8hqibfKGptwpKWdS+QJ5c600yZt4+pzzpThHRF7D6iOSHpH35Twmb0jRM/KSg+fE5h9bJenv5H2pD8obNnFU3n8M3iQpK6nsB/x4rkvn2sj7j8zH5a3O2SnvzmadvMb5+SnqyJX0F34sQ378v5B0e7q/izR/19f5x34hzfK/mOb3WyOvx8NR/3fbK+8/RP8t6dWTyuZL+pB/DYblrYb4MXnJzYeSP3+z/dz4x7xK3n8UhuXdgf6yvMb3OZ/PdK+fvEWJ7vE/U6N+DE/5n7W9k8rWS6qfop6z3t8s/v1k6vwpP7vyhnx9x79ecb/M2+Zy/Xnw4MFjJT7kJafeLe8m3mH/e3BU3gIs98lrS0SmOPbXJX1XUqt/zGl5Iwf+VtKOFOV3y1uButkvf0beCt53pCj7CnmLAnXJu1FaJ+kfJJWmKDvdd9TbEt8NKfbdLu9mZ+I74gvykpBT1peijiK/nv+S12Zrl9f+65T0S3lzgRdNcewF/ndog/8eO+W1oT4p6aXzeY+SbpX0JXntm37/97pfXjumMkUd6+W1+Rr83027vIWXLpvNNU3jen3AP/YdaZTN8svWT7F/Nm2MDfJ62CYWUTwgr52QOMdDSWWr5Q2j/x//Mz2iF+bavDlFHOb/no/5cTT6n9V8pWjbpHP9/DrfLK8zRKdfb7O8Nu1fSNqQVLZG07QJJ7+/SfvOiW8Bzp/ysyvv/z6/8j+fzn/UzPb68+DBY3k9zDknAAAAAAAAAAgL5rQEAAAAAAAAECokLQEAAAAAAACECklLAAAAAAAAAKFC0hIAAAAAAABAqJC0BAAAAAAAABAqWUEHEKSKigpXU1MTdBgAAGABPf744+3Oucqg48ALaIMBALC80f5CJqzopGVNTY327dsXdBgAAGABmVlD0DHgbLTBAABY3mh/IRMYHg4AAAAAAAAgVEhaAgAAAAAAAAgVkpYAAAAAAAAAQoWkJQAAAAAAAIBQIWkJAAAAAAAAIFRIWgIAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUCFpCQAAAAAAACBUSFoCAAAsM2Z2k5kdNrM6M3t/iv1mZh/19z9jZpf42zeY2U/M7DkzO2Bm7006pszMHjCzo/7zqqR9f+7XddjMblycdwkAAIDljKQlAADAMmJmUUkfk3SzpF2SbjezXZOK3Syp1n/cIenj/vZxSX/snNsp6UpJ70469v2SHnTO1Up60H8tf/9tknZLuknSf/gxAAAAAHNG0hIAAGB5uVxSnXPuuHNuVNKXJd0yqcwtkj7vPA9LKjWztc65U865JyTJOdcn6TlJ65OO+Zz/8+ckvTpp+5edcyPOuROS6vwYAAAAgDkjaQkAALC8rJd0Mul1k15IPKZdxsxqJF0s6RF/02rn3ClJ8p+rZnE+AAAAYFZIWgIAACwvlmKbm00ZMyuU9HVJ73PO9WbgfDKzO8xsn5nta2trm6FKAAAArHQkLQEAAJaXJkkbkl5XS2pJt4yZZctLWH7ROfeNpDJnzGytX2atpNZZnE/OuU855/Y65/ZWVlbO+k0BAABgZSFpCQAAsLw8JqnWzDabWY68RXLunVTmXklv8VcRv1JSj3PulJmZpP+U9Jxz7iMpjnmr//NbJX07afttZhYzs83yFvd5NPNvCwAAACtJVtABAAAAIHOcc+Nm9h5J90uKSvqsc+6Amd3p7/+EpPskvVLeojmDkt7uH/4iSW+W9KyZPeVv+wvn3H2S/k7SPWb2O5IaJf2WX98BM7tH0kF5q4+/2zk3sfDvFAAAAMsZSUsAAIBlxk8y3jdp2yeSfnaS3p3iuF8o9RyVcs51SHrZFPs+JOlD8wgZAAAAOAvDwwEAAAAAAACECklLAAAAAAAAAKFC0hIAAAAAAABAqJC0BAAAAAAAABAqJC0BAAAAAAAAhApJSwAAAAAAAAChQtISAAAAAAAAQKiQtAQAAAAAAAAQKiQtAQAAAAAAAIQKSUsAAAAAAAAAoULSEgAAAAAAAECokLQEAAAAAAAAECokLQEAAAAAAACECklLAAAAAAAAAKFC0hIAAAAAAABAqJC0BAAAAAAAABAqJC0BAAAAAAAAhApJSwAAAAAAAAChQtISAAAAAAAAQKiQtAQAAAAAAAAQKiQtAQAAAAAAAIQKSUsAAAAAAAAAoULSEgAAAAAAAECokLQEAAAAAAAAECokLQEAAAAAAACECklLAAAAAAAAAKFC0hIAAAAAAABAqJC0BAAAAAAAABAqJC0BAAAAAAAAhApJSwAAAAAAAAChQtISAAAAAAAAQKiQtAQAAAAAAAAQKiQtAQAAAAAAAIQKSUsAAAAAAAAAoULSEgAAAAAAAECokLQEAAAAAAAAECokLQEAAAAAAACECklLAAAAAAAAAKFC0hIAAAAAAABAqJC0BAAAAAAAABAqJC0BAAAAAAAAhApJSwAAAAAAAAChQtISAAAAAAAAQKiQtAQAAAAAAAAQKllBBwAAi2bff82/jr1vn38dAAAAkCTd/UhjWuXecMXGBY4EABA29LQEAAAAAAAAECokLQEAAAAAAACECklLAAAAAAAAAKFC0hIAAAAAAABAqJC0BAAAAAAAABAqgSQtzewmMztsZnVm9v4U+83MPurvf8bMLpnpWDO7yMweNrOnzGyfmV2+WO8HAAAAAAAAQOYsetLSzKKSPibpZkm7JN1uZrsmFbtZUq3/uEPSx9M49h8k/Y1z7iJJf+W/BgAAWHHmeYP4s2bWamb7Jx3zFf/m8FNmVm9mT/nba8xsKGnfJxb8DQIAAGDZywrgnJdLqnPOHZckM/uypFskHUwqc4ukzzvnnKSHzazUzNZKqpnmWCep2D++RFLLIrwXAACAUEm6yXuDpCZJj5nZvc655LZW8g3iK+TdIL7C33eXpH+X9Pnkep1zr086x4cl9STtPubfOAYAAAAyIoik5XpJJ5NeN+mFRvJ0ZdbPcOz7JN1vZv8krwfp1alObmZ3yOu9qY0bN87pDQAAAITYnG8QO+dOOed+ZmY1U1VuZibpVkkvXbB3AAAAgBUviDktLcU2l2aZ6Y79PUl/6JzbIOkPJf1nqpM75z7lnNvrnNtbWVmZZsgAAABLxlQ3f2dbZiovlnTGOXc0adtmM3vSzH5qZi9OdZCZ3eHPO76vra0tzVMBAABgpQoiadkkaUPS62qdO5R7qjLTHftWSd/wf/6qvF4GAAAAK818bhCn43ZJX0p6fUrSRufcxZL+SNLdZlY8+SBuHAMAAGA2gkhaPiap1sw2m1mOpNsk3TupzL2S3uJPEn+lpB7n3KkZjm2R9BL/55dKOioAAICVZz43iKdlZlmSXivpK4ltzrkR51yH//Pjko5J2j6nyAEAAADfos9p6ZwbN7P3SLpfUlTSZ51zB8zsTn//JyTdJ+mVkuokDUp6+3TH+lW/U9K/+o3pYfnzVgIAAKwwz9/kldQs7ybvGyaVuVfSe/z5Lq/QCzeIZ/JySYecc02JDWZWKanTOTdhZlvkLe5zPAPvAwAAACtYEAvxyDl3n7zEZPK2TyT97CS9O91j/e2/kHRpZiMFAABYWuZzg1iSzOxLkq6TVGFmTZL+2jmXmCv8Np09NFySrpX0QTMblzQh6U7nXOdCvT8AAACsDIEkLQEAALBw5nmD+PZp6n1bim1fl/T1ucYKAAAApBLEnJYAAAAAAAAAMCWSlgAAAAAAAABChaQlAAAAAAAAgFAhaQkAAAAAAAAgVEhaAgAAAAAAAAgVkpYAAAAAAAAAQoWkJQAAAAAAAIBQIWkJAAAAAAAAIFRIWgIAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUCFpCQAAAAAAACBUSFoCAAAAAAAACBWSlgAAAAAAAABChaQlAAAAAAAAgFAhaQkAAAAAAAAgVEhaAgAAAAAAAAgVkpYAAAAAAAAAQoWkJQAAAAAAAIBQIWkJAAAAAAAAIFRIWgIAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUCFpCQAAAAAAACBUSFoCAAAAAAAACBWSlgAAAAAAAABChaQlAAAAAAAAgFAhaQkAAAAAAAAgVEhaAgAAAAAAAAgVkpYAAAAAAAAAQoWkJQAAAAAAAIBQIWkJAAAAAAAAIFRIWgIAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUCFpCQAAAAAAACBUsoIOAAAAAACw/Nz9SGPQIQAAljB6WgIAAAAAAAAIFZKWAAAAAAAAAEKFpCUAAAAAAACAUGFOSwDLWvJcSlsbO8/Zf8XmssUMBwAAAAAApIGelgAAAAAAAABChaQlAAAAAAAAgFAhaQkAAAAAAAAgVEhaAgAAAAAAAAgVkpYAAAAAAAAAQoXVwwEAAAAAS97djzTOWOYNV2xchEgAAJlAT0sAAAAAAAAAoULSEgAAAAAAAECokLQEAAAAAAAAECokLQEAAAAAgYo7p+Pt/fqfunb1j4wHHQ4AIARIWgIAACwzZnaTmR02szoze3+K/WZmH/X3P2NmlyTt+6yZtZrZ/knHfMDMms3sKf/xyqR9f+7XddjMblzYdwdguXnuVK/+8f7D+szPT+h7z57Sh394WA8dbtVE3AUdGgAgQKweDgAAsIyYWVTSxyTdIKlJ0mNmdq9z7mBSsZsl1fqPKyR93H+WpLsk/bukz6eo/p+dc/806Xy7JN0mabekdZJ+ZGbbnXMTGXtTAJat1r5hfeWxkyoryNFNu9eosiimB587ox8ePKPe4XG9as+6oEMEAASEnpYAAADLy+WS6pxzx51zo5K+LOmWSWVukfR553lYUqmZrZUk59zPJHXO4ny3SPqyc27EOXdCUp0fAwBMa3Q8rrsfaVR21PTWq2u0Z0Op1pXm6c1X1ehFW8v18PEOHWzpDTpMAEBASFoCAAAsL+slnUx63eRvm22ZVN7jDyf/rJmtmk1dZnaHme0zs31tbW1pnArAcnff/lNq6xvR6y/bqJK87LP23bh7jdaV5urrTzSpe3A0oAgBAEEiaQkAALC8WIptkyeGS6fMZB+XtFXSRZJOSfrwbOpyzn3KObfXObe3srJyhlMBWO56h8b0eH2XLt9cpm1Vhefsz4pGdNtlGzURd/r+/tMBRAgACBpJSwAAgOWlSdKGpNfVklrmUOYszrkzzrkJ51xc0qf1whDwWdcFAA8f71DcOV2zrWLKMhWFMV21tVz7m3tU19q/iNEBAMKApCUAAMDy8pikWjPbbGY58hbJuXdSmXslvcVfRfxKST3OuVPTVZqY89L3GkmJ1cXvlXSbmcXMbLO8xX0ezcQbAbA8jY7H9ciJTu1cW6zywti0ZV+0rUJZUdN/PFS3SNEBAMKCpCUAAMAy4pwbl/QeSfdLek7SPc65A2Z2p5nd6Re7T9JxeYvmfFrSuxLHm9mXJP1K0nlm1mRmv+Pv+gcze9bMnpF0vaQ/9M93QNI9kg5K+oGkd7NyOIDpPHmyS0NjE9P2skwojGXp8poyffupFjV2DC5CdACAsMgKOgAAAABklnPuPnmJyeRtn0j62Ul69xTH3j7F9jdPc74PSfrQnIIFsKI45/TLug5Vr8rTpvL8tI55cW2lHmvo0sd/ekz/77UXLHCEAICwoKclAAAAAGBRnO4dVlv/iPZuKpNZqnW8zlWcl61XX7RO336qWQMj4wscIQAgLEhaAgAAAAAWxf7mXpmkXeuKZ3XcrXs3aHB0Qvc9O+30uwCAZYSkJQAAAABgURxo6VFNRYEKY7ObqezSTau0paJAX328aYEiAwCEDUlLAAAAAMCCa+sbUWvfiHbPspelJJmZfvPSaj16olMNHQMLEB0AIGxIWgIAAAAAFtyBlh5J0u51JXM6/rWXrFfEpK/R2xIAVgSSlgAAAACABbe/pUcbVuWpJC97TsevLcnTNbWV+sYTzXLOZTg6AEDYkLQEAAAAACyo7sFRtXQPz7mXZcKr9qxTc/eQnmnqyVBkAICwImkJAAAAAFhQda39kqTta4rmVc/Ld1YpGjHdf+B0JsICAIQYSUsAAAAAwIKqa+tXYSxLq4ti86qnND9HV24pI2kJACsASUsAAAAAwIJxzulY24C2VRXKzOZd34271+hY24DqWvsyEB0AIKxIWgIAAAAAFsyZ3hENjIxra2VhRup7xa41kqT7D5zJSH0AgHAiaQkAAAAAWDB1bd58llsrCzJS35qSXO3ZUMoQcQBY5khaAgAAAAAWzLHWflUU5qg0Pydjdd64e7WeaerRqZ6hjNUJAAgXkpYAAAAAgAUxEXc60T6QsaHhCS/dUSVJ+vnR9ozWCwAIj6ygAwAAAAAALAP7/uusl1sbO3WoP0+jE5t0jT2jrY2/nHvd0bIXft77dp23ukiVRTH9/Gi7bt27Ye71AgBCi6QlgCXr7kcagw4BAAAA03iuL1+StLNoMKP1mpleXFuhnxxq1UTcKRqZ/6rkAIBwYXg4AAAAAGBBHOrPU3XuiIqy4hmv+yXbK9U1OKYDLT0ZrxsAEDySlgAAAACAjIs76chAnnYUZraXZcKLtlVIYl5LAFiuSFoCAAAAADKucSimwYmodhQuzArfFYUx7V5XrJ8daVuQ+gEAwSJpCQAAAADIuMP9eZKk8xYoaSlJL66t1BONXeofGV+wcwAAgkHSEgAAAACQcYf681WWPabKnLEFO8e1tRUam3B65HjHgp0DABAMkpYAAAAAgIxyzluE57zCIdkCLux9yaZVyolG9OiJzoU7CQAgECQtAQAAAAAZ1TQYUedYtnYu0CI8CbnZUV1YXaJH60laAsBykxV0AAAwo33/lXLz1kYapwAAAGG0rz1b0sLOZ5lw+eYyfepnx3XLnvXKyaJfDgAsF/xFBwAAAABk1L6ObOVFJrQxb2TBz3XZ5jKNx51Odi1sr04AwOIiaQkAAAAAyKgnOrK1rWBYkQWczzLh0k2rFDHpRPvAwp8MALBoGB4OYEUo7Tui6taHlDPWo3gkR82V16qtdE/QYQEAACw7/WOmwz1Zes3ankU5X3FutnauLVZ9B0lLAFhOSFoCWN6c0/pTD6i662E1aK2etSu0W/Xa0vIdlfU+J9X8nhSJBh0lAABYiaaYtzvj9r59cc7je6YrS3GZagsWfj7LhMs3l+kLDzdoPB5XVoQBhQCwHPDXHMCyVt7+qKq7HtZ/j79cfxD93/po5E26fuBD+nz2b6m0v0567t6gQwQAAFhWnujwFuFZ1KRlTZnGJpxauocX7ZwAgIVFT0sAy1ZOf5NqWn+k+yf2qq36FfqL8tOSpG+fLtNfNb9GVYXtuunET6RVm6V1FwUbLAAAwDLxREe2thWNqzArvmjnvGxzmSSpvn1AG8vyF+28AICFQ09LAMuTi2vdye+pJV6mY+tepReV9z+/65Y1nbp1XZve3f/b6s6tlg59R4qPBxgsAADA8uCc9GRnti4pH1vU81YUxlRekMMK4gCwjJC0BLAsFXc8rdXxM/pyzm/q0opzE5KvWdOh6rwx/d/R10uDHVLjrwKIEgAAYHk50R9V12hk0ZOWklS9Kk9NXYs3JB0AsLBIWgJYdiw+prWtP9MT8W2qqa5OWSZi0q3r2nXP4CVqza+VjtwvjY8scqQAAADLS2I+y0vKgkha5qtnaEy9Q4t/bgBA5pG0BLDsFLc/qVLXo+/mvkqbCkanLHdpSb/2rBrXB4ZulUb7pZYnFzFKAACA5eeJzmwVZce1rXhi0c+9YVWeJNHbEgCWCZKWAJYX51TesU/74rW6YMOqaYuaSX+0e0D3De1ST84ahogDAADM0xMdWbqobEwRW/xzry3NU8SkJua1BIBlIZCkpZndZGaHzazOzN6fYr+Z2Uf9/c+Y2SXpHGtmv+/vO2Bm/7AY7wVAuBT3H1NlvF0/z7lW63JnHhp07epR7SgZ19fc9VJ3g9TbsghRAgAALD+9Y6bDPVnaG8B8lpKUHY1oTXEuPS0BYJlY9KSlmUUlfUzSzZJ2SbrdzHZNKnazpFr/cYekj890rJldL+kWSRc653ZL+qeFfzcAwia/9Qm1uRJlV25Lq7yZ9JqNI/r3vpfIWRa9LQEAAOboqY5sOZkuDShpKXnzWjZ1DyruXGAxAAAyI4ielpdLqnPOHXfOjUr6srxkY7JbJH3eeR6WVGpma2c49vck/Z1zbkSSnHOti/FmAIRHzmi3Ngwf1tfi1+ui0uG0j3vVhmF1q0hH8i+Smh+X4os/BxMAAMBSt68jWxE5XVQ2HlgM1avyNDwWV0f/1POaAwCWhiCSluslnUx63eRvS6fMdMdul/RiM3vEzH5qZpelOrmZ3WFm+8xsX1tb2zzeBoCwKener4icGoouVnYk/bvra/PjuqpyTF8YukoaG5Q6jy9glAAAAMvTEx3Z2lE6rsLs4Ho5VpflS2JeSwBYDoJIWqaaknnyt9pUZaY7NkvSKklXSvpfku4xs3PKO+c+5Zzb65zbW1lZmX7UAEKvqOug9sW3a2dlTtrHPHKiU4+c6NSF+R362uDFGrcsnT7y6PPbHznRuYARAwAALA8TcacnO7MCHRouSVVFMeVEIzrJvJYAsOQFkbRskrQh6XW1pMkrX0xVZrpjmyR9wx9S/qikuKSKDMYNIMTyhs+ocvy0fqLLdV7B7BupV6zq07jlaH90l8p6D0nMgwQAAJC2Q6d7NTAeCWwRnoSImdaV5qmZnpYAsOQFkbR8TFKtmW02sxxJt0m6d1KZeyW9xV9F/EpJPc65UzMc+y1JL5UkM9suKUdS+4K/GwChUNx1QOMuot7SnTq3j/XM8qNxXVLSr6+PXqGc8T4VDLGKOAAAQLqeaOiSJF0ScNJSktaW5upM7wiL8QDAErfoSUvn3Lik90i6X9Jzku5xzh0wszvN7E6/2H2Sjkuqk/RpSe+a7lj/mM9K2mJm++Ut0PNW5/iWAlYE51Tac1D/Ez9f21fNIWPpu6RkQPeOXqa4IlrVdyiDAQIAACxvjzd0qSp3QtX58aBD0bqSXI1OxNXJYjwAsKRlBXFS59x98hKTyds+kfSzk/TudI/1t49KelNmIwWwFOSNtKp0olM/1qt1U0H6q4ZPtqekXx/XWjVk1aiq/7iaVr8sg1ECAAAsX/saunRp+dicRrxk2pqSPElSS8+QKopiAUcDAJirIIaHA0BGlfYeliR1FG5XZB4N5VXZE6rJG9Yv4hcof/iUssaZCwkAAGAmZ3qH1dQ1FPgiPAlVRTFFTDrdM/eb2QCA4JG0BLDk5ffU6cn4Nm0tjc67rotK+vWtoYtlkooHTsw/OAAAgGUuMZ9lWJKW2dGIqopydYqkJQAsaSQtASxp2WN9qhht0oMTl2hP8cC869tTPKCn3FaNWK5K+o9nIEIAAIDlbV9Dl2JZEe1eNR50KM9bW5KrUz1DQYcBAJgHkpYAlrTSviOSpLrc3SrImv/E79sLh5QTkQ5EzlPJwHGJ9bwAAACm9XhDl/ZUlyonRP+7XFOSq97hcfWPhCeRCgCYnRB9rQDA7BX0HtPJeKXKSoozUl+WSRcUD+qHY3sUG+tRbLQrI/UCAAAsR8NjEzrQ0qNLNq0KOpSzrPUX42FeSwBYukhaAliyzE1o1eAJ/TR+ofaUZG7RnIuK+/XDsQslMa8lAADAdJ5p6tHYhNOloUta5koSQ8QBYAnLCjoAAJirwsEm5bgRParzdXveSMbq3V00qE+5LRqwQhUNnsxYvQAAAMvN44lFeDatkp5buPM8cqLz+Z+PTTTOWL4glqXi3CwW4wGAJYyelgCWrJL+4xp3EXXmb1bEMlfv6tiYVmWP62BkG0lLAEuSmd1kZofNrM7M3p9iv5nZR/39z5jZJUn7PmtmrWa2f9Ix/2hmh/zy3zSzUn97jZkNmdlT/uMTC/4GAYTG4w2d2lJRoLKCnKBDOcfakjx6WgLAEkbSEsCSVdB/XE+5bdpUlNl6zaQdhUP6+dhO5Y51ScM9mT0BACwgM4tK+pikmyXtknS7me2aVOxmSbX+4w5JH0/ad5ekm1JU/YCk851zF0o6IunPk/Ydc85d5D/uzMgbARB6zjk93tAVuvksE9aU5Kqtb0QTcRZWBICliKQlgKVpdEAlwy36+cQF2lmUufksE3YWDuqnYzu9F53MawlgSblcUp1z7rhzblTSlyXdMqnMLZI+7zwPSyo1s7WS5Jz7maTOSeXlnPuhcy6xDO/DkqoX7B0AWBKOtfWra3BMe0OatKwqiinupPb+zE0jBABYPCQtASxN7UdlcnpY52tzfubnKtpZNKQDrkZjypY6j2e8fgBYQOslJc9t0eRvm22Z6fy2pO8nvd5sZk+a2U/N7MWpDjCzO8xsn5nta2trm8WpAITVr451SJKu2loecCSprS72FuNp7SNpCQBLEUlLAEtTR50GFdNw3lplZXA+y4Tq3BHFoqa6yGapi6QlgCUl1V/FyWMj0ymTunKzv5Q0LumL/qZTkjY65y6W9EeS7jaz4nMqd+5Tzrm9zrm9lZWV6ZwKQMj96niH1pXkamNZftChpFRZFJNJau1lMR4AWIpIWgJYkibaj+qRiR06r3hh7pxHzBsi/quJHVJPszROYxfAktEkaUPS62pJLXMocw4ze6ukX5f0RueckyTn3IhzrsP/+XFJxyRtn3P0AJaEeNzpV8c6dOXWcpktwB3kDMiORrSqIEdn6GkJAEsSSUsAS89In6IDZ/RwfJd2FC7cipA7iob0s7HzJDmpp2nBzgMAGfaYpFoz22xmOZJuk3TvpDL3SnqLv4r4lZJ6nHOnpqvUzG6S9GeSXuWcG0zaXukv/iMz2yJvcR+6qAPL3OEzfeoaHNNVW8I5NDxhdVGMnpYAsESRtASw9HTUSZIedTtVW7BwScudhYN6Jr7Ve9HduGDnAYBM8hfLeY+k+yU9J+ke59wBM7vTzBIre98nL7FYJ+nTkt6VON7MviTpV5LOM7MmM/sdf9e/SyqS9ICZPWVmn/C3XyvpGTN7WtLXJN3pnDtnIR8Ay0vY57NMqCrOVXs/K4gDwFKUFXQAADBrHXUaVK6Gc1crJ9K8YKfZnD+sfitUV7RCq7obFuw8AJBpzrn75CUmk7d9IulnJ+ndUxx7+xTbt02x/euSvj7nYAEsSb863qENZXmqXhXO+SwTklcQTyzMAwBYGuhpCWDJiS/wfJYJUZO2FAxrv7bS0xIAAMA3EXd65HiHrt5SEXQoM2IFcQBYukhaAlhaRvoUGWjVw/Gd2rmA81km1BYM6ecjtdJQlzTcu+DnAwAACLvnTvWqd3g89EPDJVYQB4CljOHhAJaWrhOSpH3x8/SeRUhabisY1s9at0pZ0uEjB/R4duoel2+4YuOCxwIAABAGPz3SJkm6elv4k5asIA4ASxdJSwBLS+cJjSlLE8XVyo8u/JDtbQVD+rirUVwRFQ62LPj5AAAAwu6hw606f32xqoqmmCNyfETqqtf61gPKG2lX7miXIvERmXMazS7UcE6Fuotq1VO4VfFI9oLHywriALA0kbQEsKTEu+r1bHyzLqlcnPOVZ48rLzuiRlun0qGFW/QHAABgKegZHNPjDV169/VJa3P1nZYaH5YOfFPqPC71NksurvUyDees0khOmcYjZZKZcsZ6Vdb7nKq6n9RYNE8tFS/WmbK9cpGF+69pVXGuDp/pYwVxAFhiSFoCWDomxqXuk3osfqMurxyVRhf+lGbeEPFnhrbo5uF9knPeRgAAgKWu9ZB06DtS/f94icbhHimSLeXkS/kVUkG5/1zhPUez1dDUrd+NHNGbegqku1uk1gMvLFgYyZZKN0rbXi6VbdG+nhLFo7FzTmtuQkUD9VrX/ittOvNDVXY/qSMbbtVIbGGGmydWEO8YYIg4ACwlJC0BLB29JxVx43oivl2vqxhT3SKN1t5WMKR9fZv1KvuZ8kZaNZS7enFODAAAsBC6G6Uf/m/p4Le812sukCp3SHmrpPiENNonDXRI7UelgV9JQ52Si0uSLpR0YbbkDudJZVuk9ZdKV/yetOEKqeUJKanHZLy/M+XpnUXVW7hVvYVbVdp3RFuav63zj39GdRt+Uz2F21IeMx8VhV7itL1vEe54AwAyhqQlgKWj01uEp7Ngi8pjE6pbpNNuKxjW9+ObJEmreg+TtAQAAEtX3YPSV97sJSGv+3PpkrdKxWunPyY+IQ11Kz4+qps/+nOdv2WjPvzGK88td/qZWYfTXbRd+7e8U9tPfkXbG7+iw5veoN6CzbOuZzqVRX7Ssp+elgCwlESCDgAA0uU669XoqlRbmb+o592aP6xDzlsdfFXvoUU9NwAAQMYc/LZ09+u9HpLveUy67v0zJywlKRKVCsq1vy9PhwcKdM2u6oyGNZpTqkOb3qzhnDJtb/yKCoYyO5wmNzuqwliW2khaAsCSQtISwNLgnMY7T+ix+HZdUbG4Q3vyonGV5UXUoiqSlgAAYGlqfkL62u9I6y+R3vZdqXTDrKt48LlWmUnX1mZ+RcTxrHwd2vQmjUfzVHvyq4pODGW0/orCmNr7SFoCwFJC0hLA0jDYoeyxPj0R367LKsYW/fRb84e1P75Jq/pIWgIAgCVmuFf62m9Lhaul278s5ZXOqZr7nj2ly2rKVF547uI6mTCWXaSj1a9T9niftjTf6y2AmCGVRTn0tASAJYakJYClocubz7I5tlXr8uOLfvrN+cN6amKzigZPKnusb9HPDwAAMGff/zNv8Z3X/aeUXzanKo6c6dPR1n79+oVpDCefh4H89TpZ9XKV9R1WZdcTGau3ojCmwdEJdQ+yGA8ALBUkLQEsCa7zhPpcnioqqgI5/+b8YR103mI8pX1HAokBAABg1robpafvll70XmljisVz0vS9Z07JTLrp/DUZDC610+VXqDd/kzaeeVBZ4/0ZqbPS7x16rG0gI/UBABYeSUsAS8JoR72ejG/T5ZUTgZy/Jn9Eh55fQZwh4gAAYAlwTnruO1J+uXTNH86rqsTQ8Kqi3AwFNw0znVj3a4q4UW06/UBGqqzwVxA/3paZJCgAYOGRtAQQfmPDyhk4pccDms9SknIiTjm5eeqyEpKWAABgaWg7JHUcla79Uym3eM7VHF2koeHJhmMVaql4kSp6nlXRQMO861uVn6OIScfb6WkJAEsFSUsA4dddL5NTXfZW1RQG09NSkjYXjOggi/EAAICl4ugPpbwyae9vz6ua7z27eEPDk7VUXKPRrCJtOPPgvBfliUZMZQUxeloCwBJC0hJA6LnOE4rLlFu2SWbBxbElf1jPTGxSSV+dIvFgenwCAACkpafJW8hw87VSVs6cq4nHnb75ZLOu3Fy+OEPDk7hItpoqX6KioaaMzCleWZij48xpCQBLBklLAKE33HFSR+PrtacqK9A4tuQP62B8k6JuXMX9xwONBQAAYFr1v5Ai2VL15fOq5uETHWroGNTrL9uQocBmp23VRRrKKdeG1h9LLj6vuiqKYmroGNREfH69NgEAi4OkJYBwc06RnkY9E9+iyytHAw1lU/7I8yuIM68lAAAIrdFBqflxaf2lUk7+vKr6ymMnVZybtehDw59nETVVXaf8kTat6js8r6oqC2ManYirqWswQ8EBABYSSUsA4dbdqNhEv45ENmt7cXDzWUpSLOI0WLhJw4qRtAQAAOHV9JgUH5NqrplXNd2Do/r+/tN6zcXrlZsdzVBws9dZvFPDOWVa1/Y/85rbsqIwsYI4Q8QBYCkgaQkg3FqekCTFSzYpEuB8lglrVxXqkNtI0hIAAIRXyxNScbVUUj2var71ZLNGx+N6/WUbMxTYHFlELeVXq3C4RcUDJ+ZcTXmhN7dnQwdJSwBYCkhaAgi1gROPasRlae3q1UGHIklaV5qn/RMbVdp7aN6rWAIAAGTcYIfU3SCtu2he1Tjn9KVHT+rC6hLtWlecmdjmob30Qo1mFWpd+//MuY7CWJYKcqKq72B4OAAsBSQtAYTacP1jes5t1N6qcCQI15Xk6YCrUWyiXwVDTUGHAwAAcLZTT3vP6y6eVzU/P9quw2f69KYrN2UgqPlzkSydLrtCJQMnVNJXN6c6zEybygvoaQkASwRJSwDhFZ9QYed+HXRbtLt0POhoJElrSnL1XNxrvJf2HQ04GgAAgElanpRKNkr55fOq5tM/P66qophuuWhdhgKbv7ZVFytuWapt/PKc66ipyFcDPS0BYEkgaQkgvNqPKhYfUl9BjbJD8tcqNzuq9rzNkkhaAgCAkBlol3pOznto+MGWXv38aLve9qIaxbKCW4BnsvGsfLWXnK/Nzfcqe6xvTnVsKi/Qya5BjU/EMxwdACDTQpIGAIBzDdQ/KkkqqJzfJPKZVlK6Sk2qUmn/3IYmAQAALIgzz3rPa/fMq5rP/OK48nOieuPl4RganuxM2WXKnhjSluZvz+n4mvJ8jU04neoZznBkAIBMywo6AACYSufhX8m5XG1bWy5pIuhwnre2JFeH2qp1aS89LQEAQPAeOdEpSdrR8KyyY5V69oxJ6nx+/7GJRknSG66YeRXwpq5B3ftUi9505SaV5GcvSLzzMZi3Vu2lF2pb41d0eNMbJbNZHb+pvECSVN8xoA1l+QsRIgAgQ+hpCSC0oqef0gG3WReVhydhKUlrS/J02FWreLBekfhY0OEAAAAoEh9T0WCDegq3zKuef/9xnSJm+t2XzK+ehVS34XUqGahXZdeTsz625vmkJfNaAkDYkbQEEE7jo6ocOKIzhbuUG56plCR5PS0Pxzcq6iZUNHAi6HAAAABUNFCviJtQT+G2OdfR0DGgrz7epDdcsVFrS/IyGF1mNa65UWPRAm1t+vqsj60qiik3O6KGdlYQB4CwI2kJIJSGmp5Wtsblqi8NOpRzlORlqz6aWEGceS0BAEDwSvuPacKy1Js/93ko/+3HdcqKmN513dYMRpZ541n5alh3kzaefmDWC/JEIqZNZQX0tASAJWDeSUsz+7qZ/ZqZkQAFkDHNB/5HkrR6x9UBR3IuM9Nw8WaNK8oK4gAWFO0sAOkq6T+mvoJNcpG5LVtwvK1f33iiSW++cpOqinMzHF3mHat+rbImhrTp1Pdnfeym8nw1dNDTEgDCLhML8Xxc0tslfdTMvirpLufcoQzUC2AFG65/TB2uSOfvOl/a/1TQ4ZyjsrRIx/vXqoSkJYCFRTsLwIxyRruVN9qhM2V7py139yONU+67Z99JRSOm333JzL0sp6pna2Nnyu0LoaPkAnUXbtPm5ntVt/HWWR1bU1Ggh460KR53ikRmt5APAGDxzPuuvXPuR865N0q6RFK9pAfM7Jdm9nYzC99ycwCWhJKuZ3Ui5zwV5obzz8jakjwdjleriBXEASwg2lkA0lE8UC9J6i3YPKfjW3uH9fTJbl21pVyVRbEMRraAzFS/7tdU2f20CgabZnXopvJ8jY7Hdbp3eIGCAwBkQkaGGplZuaS3SXqHpCcl/au8xvUDmagfwMoyMtirdWONGqy4MOhQprS2NFeH4xtUMtKirHGGFwFYOLSzAMykaLBRY9E8DcUq53T8g4dalZ0V0Ytr53Z8UBrW3ixJsx4i/sIK4rThACDMMjGn5Tck/VxSvqTfcM69yjn3Fefc70sqnG/9AFae4/sfUdScCrdMP8QpSJVFMR3VRkneHFIAsBBoZwFIR/Fgg/ryN0o2+6HOp3uG9Wxzj67eWq6CWCZmD1s8A/nr1brqYtW0fE9yLu3jNpXnS5IaWIwHAEItEz0tP+Oc2+Wc+3/OuVOSZGYxSXLOhTfjACC02o48KknafMGLAo5kalmRiDoKvDmfWIwHwAKinQVgesM9yh3t8pKWc/CTw62KZUV0zbaKDAe2OBrWvlKl/cdU2nck7WPWluQpJxqhpyUAhFwmkpZ/m2LbrzJQL4CV6tTT6rYSrVq9KehIpuVKNmrQxVTSVxd0KACWL9pZAKbXeVyS1Fcw+6Rla9+w9jf36Mot5crPWVq9LBMa17xCcYtq06n70j4mGjFtKMtTQzs9LQEgzOb8zWRmayStl5RnZhdLSoxFKJY3hAkAZm18Iq6q/kNqLT5PpXMY4rSY1pQW6MiZ9aroTf/OPgCkg3YWgLR1HNNEJFsDuWtnfehPD7cpK2p60RLtZSlJI7Eynaq4WjUt39fT298rWXr9cmrKC+hpCQAhN5/baTfKmxS+WtJHkrb3SfqLedQLYAU7dLJN56lJJ9beGHQoM1pbkqsj8Q3a1vdM0KEAWH5oZwFIT+dx9edVp52se/6wgVE93eStGF64xOaynKxh7Su1vu3PVdn1pNrKLk3rmE3lBfrlsQ4552Qhv1EOACvVnL+dnHOfk/Q5M/tN59zXMxgTgBWs7sCjOt8mVLn98qBDmdHakjwddht06/hPpf42qXBprbgJILxoZwFIy9ig1HdKfZXXzvrQnx9tk5ktuRXDU2la/VKNR3JV03Jf2knLmop8DY1NqK1vRFXFuQscIQBgLuYzPPxNzrkvSKoxsz+avN8595EUhwHAtPpP7JMkrdp6WcCRzCwvJ6rm7BrvRetBqfAlgcYDYPmgnQUgLd0nJTn15W+Y1WHDYxN6srFbF1WXqjgve2FiW0TjWflqWn29Np6+X4/ver/ikZnf06byAklSfccgSUsACKn5LMRT4D8XSipK8QCAWXHOKbd9vwYjhVJpuBfhSegtrvV+aH0u2EAALDe0swDMrLtRkjSQt25Whz3R2KXRibiu3FK+EFEFomHtKxUb69Ga9vTWKqsp96YHZl5LAAiv+QwP/6T//DeZCwfASlbX2q/a+HH1le9W/hKZWyh31Tp19hSq+PT+eU0SDADJaGcBSEt3g1RQqYlo+j0FnXN6+HinNqzK0/pVeQsY3OI6VfkijWYVacPpH6qlaubh8utL85QVMTWQtASA0JpPT0tJkpn9g5kVm1m2mT1oZu1m9qZMBAdgZXns2BntsEblbrok6FDStrY0T0fcBo20HAg6FADLEO0sANPqbpz16JRjbQNq7x9ZVr0sJSkeyVZz1UtU3fqQLD42Y/msaETVq/JU3zG4CNEBAOZi3klLSa9wzvVK+nVJTZK2S/pfGagXwApz8vATitm4ijfvDTqUtK0tydOReLWyO49IzgUdDoDlh3YWgNSGuqWRXql0dvNZPny8Q/k5UZ2/vmRh4grQyTUvV2ysR1Wd+9Iqv6m8gJ6WABBimRjNmJjl+JWSvuSc67QlMqwTQHg45zTW9JQkydZepLsfaXx+39bGzoCimllpfrbqbINyxh+QepulkuqgQwKwvNDOApCaP5+lSjdJ3ekdMjg6rsOn+3TV1nJlRzPRfyVcTlVcrfFonjae/pHOVFw1Y/ma8nw90dAl55z42woA4ZOJb6rvmNkhSXslPWhmlZKGM1AvgBXkZOeQNowc0Vg0XyrbEnQ4aYuYqaNgq/eCxXgAZB7tLACpdTdIFpGK16d9yMGWXk04pwurl18vS0maiOappfIaVZ95UOYmZiy/qbxAfSPj6hwYXYToAACzNe+els6595vZ30vqdc5NmNmApFvmHxqA5Sq5F2XCEw1duj1SrzMF5+lnjzUFENXcDZZul05L8TMHFam9IehwACwjtLMATKnnpFS8Topmz1zW90xzj8oKcrS+dPkswDPZydUv18bTD6ii62m1lU0/T3pNRWIF8UGVF8YWIzwAwCxkarHbnZJqzCy5vs9nqG4AK0BDe692WYPqV90adCizVryqSmdOlaqgab8Kgw4GwHJEOwvA2VzcGx6+Lv3FC/tHxnWstV8v2V65rIdCN1deqwnL1oYzD8yYtNxYViBJaugY0KWbVi1GeACAWZh30tLM/lvSVklPSUr0wXeiMQ1gNtqPKs9G1VWyK+hIZm1tSa4Oxzdoz2lWEAeQWbSzAKQ02CmND0sl6S/Cs7+5R07ShdWl05ZLNSJmKRnPLtTpiqu14fSDemLHn0rTJGg3lOXJTKwgDgAhlYmelnsl7XKOZXMBzE3v0Jiqh49IOVJX8c6gw5m11cW5qnPVuqr3x1I8LkWW38T2AAJDOwvAuXqbveeS9OezfKapR5VFMa0uXv7DoE+ueZnWt/1UZb0H1Vmye8pysayo1pXkqb6dFcQBIIwy8T/r/ZLWZKAeACtUfceAzo/UaywSU29BTdDhzFp2NKLOwq3Kjo9I3fVBhwNgeZlTO8vMbjKzw2ZWZ2bvT7HfzOyj/v5nzOySpH2fNbNWM9s/6ZgyM3vAzI76z6uS9v25X9dhM7txtvECmKXeZkkmFaX356F/ZFwNHQO6YH3Jsh4antBUdb3iFtWG0w/MWLamIl8NnfS0BIAwykTSskLSQTO738zuTTwyUC+AFeJE+4AuiNSru+g8uUimptpdXFbl9xBlBXEAmTXrdpaZRSV9TNLNknZJut3MJs+9cbOkWv9xh6SPJ+27S9JNKap+v6QHnXO1kh70X8uv+zZJu/3j/sOPAcBC6W2WCldL0Zy0ite19stJ2rGmaGHjConRnFKdKbtMG07/SJqho3pNeYEaOuhpCQBhlInswAcyUAeAFayxvV+7I/VqLnlV0KHMWfHG86VGabh5v3J3/FrQ4QBYPj4wh2Mul1TnnDsuSWb2ZXkrjh9MKnOLpM/7w84fNrNSM1vrnDvlnPuZmdWkqPcWSdf5P39O0kOS/szf/mXn3IikE2ZW58fwqznEDiAdPc1S2da0i9e19ikvO6p1y3jV8MlOrn65Lj/4tyrpP6aeom1TlqspL1D34Ji6B0dVmp9eEhgAsDjm3dPSOfdTSfWSsv2fH5P0xHzrBbAyDI6OK9bfoAINqbN46jmHwq52wzo1uQr1n3w26FAALCNzbGetl3Qy6XWTv222ZSZb7Zw75cd1SlLVbOoyszvMbJ+Z7Wtra5vhVACmNDogDXenPZ+lc9LR1n5tqypUZAUMDU9ornqJJGl960PTlttUni9JamAxHgAInXknLc3snZK+JumT/qb1kr4133oBrAwNHYO6wE5IkjpLlt4iPAm71hbrSLxakfZDQYcCYBmZYzsrVVZi8vjIdMqkK626nHOfcs7tdc7traysnOOpADy/CE9xeknLk8Mx9Q2Pq7aqcAGDCp+hvDXqKN41Y9KypqJAkjfHOgAgXDIxp+W7Jb1IUq8kOeeO6oU77wAwrfr2AV0QrdeEZau3MP1hTmFTWRRTY9YmFQ/USxNjQYcDYPmYSzurSdKGpNfVklrmUGayM2a2VpL859Z51AVgrnpml7R8usdLytWuXhnzWSZrrrpOFd3PKDbSMWWZjWX0tASAsMpE0nLEOTeaeGFmWZr7nXoAK0xD56AuyW5Ud1Gt4pHsoMOZMzPT8KrzlOXGpM7jQYcDYPmYSzvrMUm1ZrbZzHLkLZIzefGeeyW9xV9F/EpJPYmh39O4V9Jb/Z/fKunbSdtvM7OYmW2Wt7jPozO9MQBz1Nss5ZZIsfR6Tj7dW6CqophK8pZuO2uumquuk8lpfdvPpiyTmx3V2pJceloCQAhlImn5UzP7C0l5ZnaDpK9K+k4G6gWwzI1NxNXcPagd7oS6lvDQ8ITstd7ivBOnDwQcCYBlZNbtLOfcuKT3SLpf0nOS7nHOHTCzO83sTr/YfZKOS6qT9GlJ70ocb2ZfkreIznlm1mRmv+Pv+jtJN5jZUUk3+K/lnDsg6R55C/38QNK7nXMT83/rAFLqbU67l+VI3HSoP2/FDQ1P6CreoYHc1WnNa0lPSwAIn0ysHv5+Sb8j6VlJvyuvEfyZDNQLYJlr6R7SWtemQtenzuJdQYczbxWbL1B8v6m74RmVX/DaoMMBsDzMqZ3lnLvPL5u87RNJPzt5Q89THXv7FNs7JL1sin0fkvShmeICME/xcan/jFSVXrvpSH+exlxE26pW3tBwSZKZmquu05bmexWdGNZENDdlsZryAv3ouTOLHBwAYCbzTlo65+Jm9i1J33LOsRQkgLQ1dAxqt78IT1fx0u9peV51lerdasWa6WkJIDNoZwE4y0Cb5OJS0dq0ih8ZyJP0wryNK1Fz1fXa3vgVre54VC1V16Yss6m8QO39o+obHlNR7sobRg8AYTXn4eH+HEgfMLN2SYckHTazNjP7q8yFB2A5a+gY0GWxk4pbVN1FtUGHM29bKwt1TBuU23U46FAALHG0swCk1Hfaey5OM2nZn6fq3BHl5UQXMKhwO1N2mcai+Vrf+pMpy9SUsxgPAITRfHpavk/eapaXOedOSJKZbZH0cTP7Q+fcP2cgPgBL2b7/Srl5a2OnnJOa2rdpb+yQhrPKVdO89KfCzY5G1J6/RaXDj0tjw1J26iFIAJCG94l2FoDJ+k5JFpEKqmYsGnfS0YE8XbGqbxECC694NEenKl+k9a0/1WMunrLMpnJvhfWGjkGdv75kMcMDAExjPgvxvEXS7YmGtCQ5545LepO/DwCmdGokR33jWdrmGjWQuybocDJmomKHoopLHUeDDgXA0kY7C8C5+k5LBRVSdOYhzKdGcjQwEdX2gqFFCCzcmqquV/5Im8p6n0u5f5Pf05IVxAEgXOaTtMx2zrVP3ujPt8REIACmdbg/T5XqUmG8TwN56Q1xWgryqy+QJPU0Ph1wJACWONpZAM7Vdyr9+Sz7vfksa0laqqXyxYorovVnUg8RL4hlqbIopvp2kpYAECbzSVqOznEfAOhwf54uzTouSRpcRj0t1205X2Muqp76Z4MOBcDSRjsLwNkmRqWB9rSTlkcH8lQQndC6XP5kjOaUqn3VRapufWjKMjXl+cxpCQAhM5+k5R4z603x6JN0QaYCBLA8HR7I07WxOknLK2m5s7pCJ9waxc8cDDoUAEsb7SwAZ+s/I8mlnbQ83J+n2oIhRWxhw1oqmqqu16q+w1J3Y8r9m8oLGB4OACEz56Slcy7qnCtO8ShyzjFsCcCU+sYjahmO6cLICQ3llGkiGgs6pIwpyc9WY9YmFfYypyWAuaOdBeAcfae856KZb/YOTkTUPJzD0PAkzauv8344/IOU+2vK89XaN6LB0fHFCwoAMK359LQEgDk50u9Ndr4pflKDuctnPsuEvuLtqhg7JY1ytx4AAGRI72kpEpXyK2YsenQgV06m7YUkLRP6CmrUU1AjHb4v5f6aihdWEAcAhANJSwCL7nB/nsrUp6KJrmW1cnhCZPVOSdLIKYaIAwCADOk/LRWs9hKXMzg+4C3Cs61geKGjWlKaq66X6n8hDfees6+mPJG05KYzAIQFSUsAi+7wQJ5elndEkjSQt/ySlqWbLpQktR5jBXEAAJAh/WekotVpFa0fiml1zqjyo/EFDmppaa66ToqPSccePGffxnJvJFA9PS0BIDRIWgJYVKNx6dhArq7KOSZJy3J4+KZt52vEZWvg5DNBhwIAAJaDiTFpsFMqqEqreMNgTBvzRxY4qKWnfdUeKa9MOvz9c/YV52arvCCHnpYAECJZQQcAYGXZ35WlMRfR7ki9RrKLNZ6VH3RIGbexokiHtV5Z7YeCDgUAACwHA22SnFQ4c0/L4QnT6ZEcXVP2whDorY1fXcDglg5nUan2BunoA1J84pyh9pvK81XfTk9LAAgLeloCWFSPd3iL3q4fb9LAMuxlKUmRiKktf4tWDRwLOhQAALAc9Ld6z4Uz97RsHIrJybSJnpap1b5CGuqUmvads6umvICelgAQIiQtASyqfe3ZqsnpVcFYuwaX4SI8CWNl56ki3q6Jwa6gQwEAAEvdgJ+0LKicsWjDUK4kqSaPRXhS2vYyyaLS0fvP2bWpvEAtPcMaHpsIIDAAwGQMDweQUXc/0vj8z1sbO8/a55z0cGuZfrPgoGxkeS7Ck5C7/nypWWo58qQ2XPTSoMMBAABLWf8ZKW+VlBWbsWjDYEwF0QlV5IwvQmBLUN4qacMV0pEfSi/7q7N21VR40xad7BxU7eqiIKIDACShpyWARdM2mq2e8Sxdnu0Nm16uw8Mlac22SyRJ7SdYQRwAAMxTf2taQ8MlqX4oV5vyRmS2wDEtZdtvlM48K/U0n7V5U3mBJFYQB4CwIGkJYNHUDXjDlbarQWPRfI1lLd872Ju2bNeAi2ns1IGgQwEAAEuZc17SsmDmRXjizpvTclM+Q8Ontf1G7/noD8/aXFPu9bRkXksACAeSlgAWTd1AnrItrqqxZg3krdVy7gKQlZWl5uxNyu8+EnQoAABgKRvukSZG0uppeXokWyPxiDblsQjPtCp3SCUbz0lalubnqCQvW/UkLQEgFEhaAlg0xwZztT2vT/kjbRpYxovwJPQW12rtaL2cc0GHAgAAlqqB9FcOTyzCQ0/LGZhJ218hHX9IGjv7WtWU56uB4eEAEAqBJC3N7CYzO2xmdWb2/hT7zcw+6u9/xswumcWxf2JmzswqFvp9AEjfhJNODObqmrwTiiiuwWU8n2VCpGqnytWj5qaTQYcCAACWqv5E0nLm4eENgzFF5FSdO7rAQS1ddz/SqLsfadRD8YulsUH95P5vPr/t7kca5STtb+4JOkwAgAJIWppZVNLHJN0saZek281s16RiN0uq9R93SPp4Osea2QZJN0hqFIBQaR6OaSQe0SXR45KW98rhCatqLpIkNR15PNhAAADA0tV/xls1PFY8Y9GTQzGtzR1VToRRHjM5U365xiO5Wtf2s7O2lxfE1D04ptHxeECRAQASguhpebmkOufccefcqKQvS7plUplbJH3eeR6WVGpma9M49p8l/akkvqWBkEkswlOrRo1HYhrJXhVwRAtv3XkXS5L6Tu4POBIAALBk9bdKBVVpzQXePJyj9fSyTMtENFdnyi/XutafeYsd+SoKc+QkNXYyryUABC2IpOV6ScljJZv8bemUmfJYM3uVpGbn3NOZDhjA/B0byFVBdEKVY80azF2zrBfhSYiVrlefFSra/lzQoQAAgKWq/0xaQ8PH49KZkRytz2URnnQ1V16roqEmFQ+ceH5bRWFMknSsjaQlAAQtiKRlqkzF5J6RU5VJud3M8iX9paS/mvHkZneY2T4z29fW1jZjsAAyo24gT9vyBlQwfGZFDA2XJJmpNW+rKvqPshgPAACYvfERabg7rUV4To3kKC6jp+UstFRdK0la1/bz57dVFnlJy+MkLQEgcFkBnLNJ0oak19WSWtIskzPF9q2SNkt62rzeW9WSnjCzy51zp5Mrds59StKnJGnv3r1kEYBFMBo3NQ7F9LuVRxXpG9fACliEJ2G0Ype2NnxDZ3qGtKY0P+hwAADAUjLgd7JIo6dl87CXbFuft/x7Wm5t/GrG6hqMVWnrya9pLPpCO60ka5tOHHpSum5rxs4DAJi9IHpaPiap1sw2m1mOpNsk3TupzL2S3uKvIn6lpB7n3KmpjnXOPeucq3LO1TjnauQlPS+ZnLAEEIz6wZjiMl2S5S3CM5i7QnpaSsrbcKEKbETHDjOvJQAAmKX+M95zwcw9LZuHcyRJ62L0tJyN7qJtKhpoVHRi+Plt63JHdbwvGmBUAAApgKSlc25c0nsk3S/pOUn3OOcOmNmdZnanX+w+Sccl1Un6tKR3TXfsIr8FALNUN5AnSdrmGjVhWRqKVQQc0eJZU7tXktR14omAIwEAAEtOf6skkwpmbjs1D+eoImdMuVEGk81Gd2GtIoqrpP/489u8pGUQgxIBAMkC+UvsnLtPXmIyedsnkn52kt6d7rEpytTMP0oAmXJsMFdl2WMqH23RUO5qyYLo5B2M3HXna0IRudP0tAQAALM00Crll0vR7BmLNg/FWIRnDvryN2g8mqvS/qPqLNklSVqbO6oH2yPqHhxVaX5OwBECwMq1cjIHAAJzbCBP2/IHlT98ekXNZylJyslXe856lfQeDjoSAACw1PSfSWsRnriTWoZzWIRnLiyi7oKtKu2rk/yFE9fFvOQvK4gDQLBIWgJYUP3jEZ0aydFluU3Kio+snJXDk/SX7lTN+Al19NP7AQAApMnFpf62tJKW7aPZGnURelrOUXdRrbInBlQw7K0Pu85P/h5v6w8yLABY8UhaAlhQxwdzJUmXZJ2QJA2soEV4ErLWna+NkTYdamgOOhQAALBUDHVL8TGpYOaVw1sSi/DQ03JOegq3yUkq7TsqSaqKjSnbnI6309MSAIJE0hLAgjrmL8Kz1TUqroiGYjP3FlhuKrZcKklqrXsy4EgAAMCSkVg5PI2elomVw6tJWs7JeFa++vOqvSHikqImbSycoKclAASMpCWABVU3kKt1sRGVjrRoKLdKLrLyVmIs2LhHkjTS/HTAkQAAgCVjoNV7TitpGVNRdFzF2RMLHNTy1V1Uq8LhFmWPeYnKLUUTOs6clgAQKJKWABbUscFcbc0fUsHw6RU5NFySVFKtwUih8jsPBR0JAABYKgbapaxcKadwxqLNwzlan0cvy/noLqyVJJX0e70ttxSOq6FjUBNxF2RYALCikbQEsGA6R7PUNZati/NOK3tiUIMrNWlppq6i7aoePaaeobGgowEAAEvBQJtUUCGZzVj01HCO1sZIWs7HYO5qjWYVaVW/N6/l1qIJjU7E1dQ1GHBkALBykbQEsGDqBvxFeKLHJEkDeeuCDCdYq8/XeXZS+5u6go4EAAAsBQPtUkHljMX6x0w941law3yW82Om7qJalfQfk8UntKXIG2p/jHktASAwJC0BLJhjg7mKymlLvEFOpoHcmVe/XK5Wbb5IBTaiE0cPBB0KAAAIu/iENNQp5VfMWLS+PypJWhNjNMd8dRfWKhofVdFQo2qLxyVJR8+QtASAoJC0BLBgjg3kaWP+iIpHTmkwt0oukh10SIHJ33iRJKm/4alA4wAAAEvAUKfk4t7w8Bk0DCSSlvS0nK+eghrFLaKSvjqV5DhVFcV0tJWkJQAEhaQlgAXhnFP9YEyb84ZUMNSigdwVPDRckip3Kq6IctoPBh0JAAAIu4F27zmN4eH1fSQtMyUejakvf6NK+72pjWpXF5K0BIAAkbQEsCB6hsbUN5GlPbFTyp4Y0kDe2qBDClZOvnrzN2r96HG1948EHQ0AAAizgTbvOZ2k5UBUpVnjyo2yynUmdBduU/5IqzTUpdqqItWd6ZNzXFsACEJW0AEAWJ5auoclSRdFVsYiPHc/0jhjmRurdmtn/6N6pqlbL92xcuf3BAAAMxhol7JiUk7hjEUb+qMswpNBPYXbpDM/kloPqbb6NzUwOqGWnmGtL80LOjQAWHHoaQlgQbT0DMnktNk1Km4RDcaqgg4pcEUb92hjpE3PnWgOOhQAABBmA21SfqVkNmPRE/1RhoZn0FCsUiNZxVLbc6qtKpIkHT3TF3BUALAykbQEsCBOdQ9pXe6oSoZbNBhbLRehY3fOhkskSX31jwccCQAACLXB9rQW4RkYN7UNk7TMKDP1FG2T2o+otjwmiRXEASAoJC0BLIiWnmHVJBbhWeZDw9O2do8kKbv1WeZGAgAAqcUnpMGO9Oaz7PcX4ckdW+ioVpTuwm3S+LBWdT6pisKYjrbS0xIAgkDSEkDGDYyMq2doTJfGmpUVH2ERnoTCKg3GKrV5/Jiau4eCjgYAAITRUJfk4mn1tGzwk5Zr6WmZUb0FmyWLSEcfUG0VK4gDQFBIWgLIuJYeLyF3cfS4JGkgl56WCWOr9+h8O6Fnm3qCDgUAAITRLFYOP+EnLVfH6GmZSRPRmFS2Rar7kWpXF6ruTD+jZAAgACQtAWTcKX/l8C3xesUtqqHcmRvdK0XBpku01Vp0sOF00KEAAIAwGmj3nvPT62lZEZtQXjS+wEGtQJU7pTP7dWHJoPpGxnW6dzjoiABgxSFpCSDjWnqGVJqfrVUjLRrMXSNn0aBDCo2s9Rcrak699U8EHQoAAAijwTYpGpNiRTMWre+PanPhxCIEtQJV7ZQkXTyyTxKL8QBAEEhaAsi4lu4hrS+OqWD4lPpZhOds/mI8sbb9iscZZgQAACYZ8FcON5uxaH1/VJtIWi6MorVS0TpVd/xSknTkDIvxAMBiywo6AADLy8j4hDr6R3Xj6l5Fu0c1kMsiPAl3P9IoOaffiJZq28gx/duP61RZFDurzBuu2BhQdACWEzO7SdK/SopK+oxz7u8m7Td//yslDUp6m3PuiemONbOvSDrPr6JUUrdz7iIzq5H0nKTD/r6HnXN3Lty7A5a5gTapeOabvkPjUuswScsFYyZte5liB+/VmsI367lTJC0BYLHR0xJARp3uGZaTdHG0XpI0QE/Ls5mpvWinzo/Uq6lrMOhoACxDZhaV9DFJN0vaJel2M9s1qdjNkmr9xx2SPj7Tsc651zvnLnLOXSTp65K+kVTfscQ+EpbAPMQnpMHOtOazPDngTb+zsYCk5YKpvUEa6dGvlzXp0OneoKMBgBWHnpYAMqql21s5fPvEEU1YtoZiMze6l5KtjV+ddx3ZWVFVW5POdPVIG1dlICoAOMvlkuqcc8clycy+LOkWSQeTytwi6fPOWw73YTMrNbO1kmpmOtbvpXmrpJcuwnsBVpahLslNpLVyeCJpuaFgQmPk0xbGluukSJZeGn1an29ep7GJuLKj9PsBgMXCX1wAGdXSM6yCnKjW9e/XQN5ayfgzM9lg3lpl24RiHYdnLgwAs7de0smk103+tnTKpHPsiyWdcc4dTdq22cyeNLOfmtmL5xM8sKIN+iuHp5O0HHwhaYkFklsibbhCuwYf0ehEXMfbBoKOCABWFLIJADKqpXtIm0qytKr3sPrzJv8/F5I0mLtGkrR64DlNsBgPgMxLtXrH5D82U5VJ59jbJX0p6fUpSRudcxdL+iNJd5tZ8TlBmd1hZvvMbF9bW9uUwQMr2kAiaTnzSJXG/qjyok4VMdoSC2rby1Tac0iV6tJzp+jSCgCLiaQlgIwZHY+rtXdEl+c1K+rG1J9fHXRIoTSSXarBaJF26YRa+4aDDgfA8tMkaUPS62pJLWmWmfZYM8uS9FpJX0lsc86NOOc6/J8fl3RM0vbJQTnnPuWc2+uc21tZOXMvMmBFGmiTojlS7Jy8/zlODka1oWAinUXGMR/bbpAkvSzrWT3HvJYAsKhIWgLImCNn+jThnC6JHpMkDdDTMjV/MZ7dkXo1dQ0FHQ2A5ecxSbVmttnMciTdJuneSWXulfQW81wpqcc5dyqNY18u6ZBzrimxwcwq/QV8ZGZb5C3uc3yh3hywrA20e4vwpJGJPDkQZWj4YlhzgVS4Rq/MO8AK4gCwyEhaAsiYgy3e3eft44c1GKvSaPbMvQRWqv5Vu7TTGnWqk8YvgMxyzo1Leo+k+yU9J+ke59wBM7vTzBIre98nL7FYJ+nTkt413bFJ1d+ms4eGS9K1kp4xs6clfU3Snc65zgV5c8ByN9iW1nyWzkknByIkLReDmbTt5do78aSOtHQFHQ0ArCisHg4gY/a39CiWFdH6/v1qL70w6HBCrat4l3JsXDmdhyVtCjocAMuMc+4+eYnJ5G2fSPrZSXp3uscm7Xtbim1fl/T1eYQLQJJcXBrskFZfMGPRrlHTwDhJy0VT+3LlP/UFrR86oPb+61RRGAs6IgBYEehpCSBjDrT06rziERUNNamjdOYG90rWWbJTkrR28LDGJuIBRwMAAAI31CXFJ9JbhGeAlcMX1Zbr5Syq66JP6xBDxAFg0ZC0BJARE3Gn50716prcBklSRwlJy+n05W/UcKRA59txnepmXksAAFa851cOn3l4+Ek/abmRpOXiyCvV+Lq9ui7yFCuIA8AiYng4gIyo7xjQ4OiELoocU1wRdZbsVtFAfdBhhZdF1F68W3s6j+nhriFtLC8IOiIAABCkgTbvOf/snpaPnDh3ithfni6TJJ1pa1dPp1vw0CBln3eDLmj+W3315AlJW4IOBwBWBHpaAsiIA88vwnNIPUXbNJ6VH3BE4ddTdqF2RhrV1tUddCgAACBog+1SJFvKLZmxaOtIjoqzxpUbJWG5aLbdIEkqPPnTgAMBgJWDpCWAjDjQ3KNYVFrbf5BFeNLUUXq+sjWh/M6DQYcCAACCNtDmzWdpNmPR1pFsVeWMLUJQeN6aCzWQXa4dA4+qf2Q86GgAYEUgaQkgIw609Or6yh7FxvvUUULSMh3t/nXaPHJIQ6PMSQUAwIo20J7WfJaSl7SsjJG0XFSRiPrWv1gviuzXgaauoKMBgBWBpCWAeXPO6UBLj64vOClJrByepuHcSvVkV+miyDE1dQ8GHQ4AAAiKi3vDw9NYOTzupPbRbK2OjS5CYEiWv/MGlVufWg49EnQoALAikLQEMG+neobVNTimiyJ1GosWqLdwc9AhLRmdpRdojx1TUxcriAMAsGINdUvxCSl/5p6WHaNZmpAxPDwAxbu8eS2zTjwUbCAAsEKQtAQwb/ubeyRJ1YMH1VF6vpxFA45o6eguu1A1kTPq6TgTdCgAACAog+3ecxo9Lc+M5kgSw8ODULRaJ3O2qrrrV0FHAgArAklLAPN2oKVX+Tas/K5DLMIzSx0l3lD68p5nA44EAAAEZqDNe05jTsu2kWxJ0mqSloFoW32Ndo8/p54e5rUEgIWWFXQAAJa+Ay29urm0WTY0rrZVFwcdzpLSWbJLcZm2jx1RzxD/+QAAYEUaaJci2VJu8YxFW0eyZXIqZ3j4gnrkRKeOTTSes308dqkusc/pG9/5qm570x0BRAYAKwc9LQHM28GWHr00/4QkU3vpnqDDWVLGswrUnrdFeyLH1NTFYjwAAKxIA23e0HCb+b9nZ0azVZEzpixbhLhwjon1l2vI5aiq7ZdBhwIAyx5JSwDz0jkwqpaeYV3oDkpVuzSWPXMPAZytp+wCXRSpU1MnSUsAAFakgXYpf+b5LCVveHgVQ8MDE8sv0BO2S7V9jwUdCgAseyQtAczLgZYeRTWhdb3PShuvDDqcJamr9AKVWb8mOk8EHQoAAFhsLu4txJPGIjySdGYkh5XDA3Ywf682xJuk7nOHjwMAMoekJYD/n737Do+jOvs+/j3b1Hu1JRe5914wvdcESIBQUoAUSCGBlOdNTwgpT56E9JCQkJAKoXcIvRtssI1x702yZVWr19097x+zkteyqi1ptdLvc13rlWbOzNxntN6dveeU47LxQA3TTCFufz2MXRbpcKJS2+RF2bUbCQZthKMRERGRQdVUDUF/rybhaQ4aqv0etbSMsOLMkwCo3vBchCMRERnelLQUkeOy8UANZyWEWgiqpeUxqU6cSIuJYWZwO7sr6iMdjoiIiAym9pnDe25p2TZzuJKWkRWTO51im07D5hciHYqIyLCmpKWIHJeN+6s5JWY7JOdD6phIhxOVrMtLWeI05rp2sq6oKtLhiIiIyGBqT1r23NKypC1p6WsZyIikB6PS4ngzOJvUg29BMBDpcEREhi0lLUXkmNU2tbKrvI7prRth7NJIhxPVqjPmMtvsZsPe8kiHIiIiIoOpvhxcHohN6bFoaYsPUEvLSPO4XGyOX0RcoBYOvBfpcEREhi1PpAMQkei1ubiWfFNOYkuZxrM8ThVp85ix55/U7l0NzIt0OCIiIjJY2mYONz23Jylr9hLjCpLiUeu+wTBx34NdrtvgyyTYZAi8+nO8084/vgMtuv74thcRGabU0lJEjtmG/dUsMludXzSe5XEpS50HQFr5e7T4g5ENRkRERAZPQ1kfZg73kuVrxZgBjkl6lJ/sZr0toOngtkiHIiIybClpKSLHbMOBak6J3QExyZA9I9LhRLWm2CwqvKOYy1a2ldRGOhwREREZDDYI9RW9Gs8SoLTFq67hQ8TkxEbeCM4moW4PtDZGOhwRkWFJSUsROWYb9lez1L0NxiwBlzvS4US98rR5LHJtY+2+Q5EORURERAZDUw0EW3vV0tJaKG32ahKeISLZE2BHzExcBKFie6TDEREZlpS0FJFj0tgSoLT0IPmte9Q1vJ9UZy4g21Sxb9fmSIciIiIig6EPM4fXBtw0Bd1qaTmExGSMo97GYku3RDoUEZFhSUlLETkmmw/WMN+E7iqPUdKyP5SnzQfAXfROhCMRERGRQVFf7jzH99zSsrTZC2jm8KFkQZblreAMWks1rqWIyEDQ7OEickw27q9mqWsz1u3D5C2MdDjDQnXSJBpNPKNr1/G35buJ8Rzd5f6apWMjEJmIiIgMiPoyZ4iduNQei7YnLX1KWg4Vy7Ja+EtwNuc0rXES0L2cUElERHpHLS1F5Jhs2F/DqZ5NkL8IfPGRDmdYsMbN/sSZLHJt40BVU6TDERERkYHWUO60sjQ9fy0rbXGSljkxGtNyqBiTEGSbb6bzS5m6iIuI9De1tBSRY7KnaD/T2I0puDLSoUSlifse7HS58SUw1RQS3PEqExsaji7gTj/886LrByg6ERERGRT1Zb1unVfa7CPF4yfWbQc4KOktYyAvK539JVmMLtuCGX9ypEMSERlW1NJSRPqs2R8gvfwdXFgoODXS4QwrLYl5uIzFXX8g0qGIiIjIQAoGQ12Ke56EB6Ck2avxLIegZdl+XgvMJli+A4KBSIcjIjKsKGkpIn227WAdS9hIwB0LeYsiHc6wUh+XTxBDZvPeSIciIiIiA6n2AARbe520LG32kq2u4UPOsuwWXg/OwR1ogipdv4mI9CclLUWkzzYcqOZE10Za8paCxxfpcIaVgDuGEvcopgd3UtN69EQ8IiIiMkxU7HSee5G09Fsob/GSo0l4hpy8+CD7YqcRxGhcSxGRfqakpYj02e49u5nqKiJ28hmRDmVYqoobw3zXDnbVKyEsIiIybFX2PmlZ0eIliFH38CFqdraXdXYSVklLEZF+paSliPSZr3A5AGaCxrMcEEmjSTRN1NdURjoSERERGSgVO8HlhdiUHouWNDszhytpOTQty27h1cBsqCqElvpIhyMiMmwoaSkifdIaCJJfvYomdyLkzo10OMNSc2I+AIkNhRGORERERAZMxU5n5nDT81ey0lDSMkdjWg5JJ2W38EZgNgYL5dsjHY6IyLChpKWI9MnOsjqWsoFDWYvB7Yl0OMNSszeVCpPK2NZdWBvpaERERGRAVO7swyQ8PtzGku71D3BQciyyYi2BlLE0EAvlWyMdjojIsKGkpYj0yc7tWyhwleCZeFqkQxm+jKHYN4EFZhtlzUoMi4iIDDvBABza47S07IWSFi/ZvlZcZmDDkmN3cm6Q5YGZBEu3orvOIiL9Q0lLEemTlh2vAZA+6+wIRzK8NSSOIdtUUV6rcZFERESGnepCCLT0oaWll2x1DR/STs9t4fXgbFxNldBQHulwRESGBSUtRaRP0kreptok486ZGelQhjVPymgAfLUa11JERGTYqej9zOEAJc0+TcIzxM1Lb2WtK3R9XKYu4iIi/UFJSxHptUAgyLTG9yhKWQguvX0MpNbYTKpIJLtpT6RDERERkf5Wuct57kXSss7voj7gJsenpOVQ5nHB2Ox0isnAlm2LdDgiIsOCsg4i0mv7t64i11TSMPaMSIcy/BnDLs9EpgW3E9SwSCIiIsNLxQ7wJkBMco9FS1ucmcPVPXzoOy23lVf9cwiWb3PGLRURkeOipKWI9Frt+mcASJ1zQYQjGRmq4sYzxpRRUdcQ6VBERESkP1XshIwJYHqeWae02QdAjrqHD3mn5bbwZnA27kATVO+LdDgiIlFPSUsR6bXEwpfZZMdTUDAp0qGMDCl5AASriiMciIiIiPSryp2QPrFXRUua21paKmk51OXEBalOnkoQo3EtRUT6gZKWItI7jVXk161nS+IJeNx66xgMcUnp1No4Uhr2RjoUERER6S+BVji0FzJ6l7QsbfaS5PYT7w4OcGDSH5bleVkfLKClRONaiogcL2UeRKRXWre/jJsgtWNOj3QoI4bL5WKLaxLj/DsjHYqIiIj0l6p9YAO9bmlZqpnDo8q5ec28GZyFp3oPtDZFOhwRkajmiXQAIhId6jY8g7EJbPVM5d6VGqNnsByMGc/ipvUcbGnA+OIjHY6IiIgcr4rQzciMiVC6ucfipS1eCuKV/IoWk5IC/ME3A1fwCWfCpdxZkQ5JRCRqqaWliPQsGCR2z8u8HpzD6PSeZ7mU/tOUOBaA5qoDEY5ERERE+kXFDue5Fy0tAxbKmr3kaObwqGEMjMobS4ONoblE41qKiBwPJS1FpGcH3yeupYJ3PYtIi/dGOpoRJSk1g0brI6auKNKhiIiISH+o3AkxKZCQ2WPR4gYXAYy6h0eZs/KDrAxOo1VJSxGR46Lu4SLSs+0vAlCddwrGmAgHM7KkxcA6JpHbtIsSTo90OCIiInK8KnZCxgSnSV4P9tW7AcjxKWk5FK3cXdnp8qCF1czkjJb3eW/rbuZPLRjkyEREhge1tBSRHgW2Pcf7wQlMGK8LrkjY7ZnI2OB+3P7GSIciIiIix6tyZ68n4SkMJS3V0jK6uAzUJTnXzXG1uyMcjYhI9FLSUkS611CJa/8qXgnOY+6Y1EhHMyIdii/AZSy+Wk2AJCIiEtX8zVBd5EzC0wv76t24sWSopWXUKchM4KBNw1TtjXQoIiJRS93DRaR721/AYHk1MI9P5KdSXHUw0hGNOL6UbBpqYnDVFALLIh2OiIiIHKtDe8AGe93Scl+9m8yYVtwanSfqTEls4l07g9Oa1rJyV0W3wwHsDOzjmqVjBzE6EZHooJaWItK9zU9Q5c6gMnUm6Qm+SEczIo1P8LMqOIXMxl2RDkVERESOR9vM4b1taVnn1niWUcploCJ+IsnU46oviXQ4IiJRSUlLEelacx12x4s8Z5ewYFxGpKMZsRI9QTa6ppAbOAjNtZEOR0SigDHmfGPMVmPMDmPMNzpZb4wxvw2tX2eMWdDTtsaYW40x+40xa0OPC8PWfTNUfqsx5ryBr6FIlKrY6TynT+hV8cJ6N9kxLQMYkAykxMw8AOoriiIciYhIdFLSUkS6tuMFjL+JR5sWsnBcWqSjGdEOxoZaZLS10BAR6YIxxg3cAVwAzACuNsbM6FDsAmBy6HED8Mdebvsra+280OOZ0DYzgKuAmcD5wB9C+xGRjip2QFw6xKf3WLS21VDZ4tIkPFEsP9nLNjuG9PqdkQ5FRCQqKWkpIl3b9ATNvnTeCU5jgZKWEeVOyqbWxtFQoqSliPRoCbDDWrvLWtsC3Adc0qHMJcA/rWMFkGqMGdXLbTu6BLjPWttsrd0N7AjtR0Q6Kt8OmVN6VbRt5vAcJS2jljFQFDuZqcGd1DYHIh2OiEjUUdJSRDrX2gTbn2d90snE+bxMzUmKdEQjWkFCC+8Gp2LLt0c6FBEZ+vKAwrDfi0LLelOmp21vCnUnv9sY03Y3qzfHExGA8m2Q1buk5b5656uauodHN2/aGGKMn4NlZZEORUQk6ihpKSKd2/kytNTxRMtC5o1NxePW20UkjY9vZmVwBgnNpdBUHelwRGRo62yKWtvLMt1t+0dgIjAPKAZ+0YfjYYy5wRizyhizqkxf3mUkaqiEhvK+t7TURDxRLSZtNM14iK3eHelQRESijrIQItK5zU9gY1N4oGICC8aqa3ik+VyW3b7Jzi9qbSki3SsCxoT9ng8c6GWZLre11pZYawPW2iBwF4e7gPfmeFhr/2ytXWStXZSVldXnSolEvfJtznMvk5b76t2keIMkeIIDGJQMNOvyUuSdwKzgFoqbvJEOR0QkqihpKSJH87fA1mcoG30mTUG3xrMcImx8JjU2HqvJeESke+8Ck40xBcYYH84kOU90KPME8InQLOInANXW2uLutg2NednmQ8CGsH1dZYyJMcYU4Ezu885AVU4kavU1aVnnZmyixkEcDlqTxzHNVcj75ZGOREQkungiHYCIDEF7Xoemat6JOxmABWOUtBwKJia2sKJmOmeUbUf36UWkK9ZavzHmJuA5wA3cba3daIz5bGj9ncAzwIU4k+Y0ANd3t21o1z8zxszD6fq9B7gxtM1GY8wDwCbAD3zBWqtMi0hH5dvAHQOpY3tVfE+dm1lp/gEOSgZDa8p4qABbtQ+bNwrT2aAaIiJyFCUtReRom54AXyKP105jcnaAlHilyIaCiQlNvB2cwblNq51xsUREumCtfQYnMRm+7M6wny3whd5uG1r+8W6O92Pgx8car8iIULYNMiaBy91j0ZYgFDW4uXhM8yAEJgOtITaXepPInMBmdjQUMDmhKdIhiYhEBXUPF5Ej+Zud8SynnM87hQ0az3IIyY9tZg0znF8qNK6liIhIVOnDzOGF9W4C1lCQpEbLw4Ix1CWN52TXet4oT450NCIiUUNJSxE50vbnofEQhfkXU93YypKC9EhHJCEuA7GpOVSTBBrXUkREJHq0NkHV3l6PZ7m71mmNWZCo7uHDRV3iBLJNNeVV1fhtpKMREYkOSlqKyJHevw8Ssnm51WnRt3SCkpZDyZz0IG8Fp2PLt4PVFa+IiEhUqNwJNtj7pGVdKGmplpbDRk3iBADm202sr0mIcDQiItFBSUsROayhErY9B7OvYMWeGvJS48hPi490VBJmbnorywMzMU1VULkr0uGIiIhIb/Rx5vBdtR7SfUFSfbpBOVy0eJNp8GVyunsdb1Soi7iISG8oaSkih218BIKtBOdcyTt7KjlhQkakI5IO5qa38nYwNK7lnjciG4yIiIj0TnloLOqMSb0qvrvOTUGSuoYPNzWJE1js2sL7VTE0BvRVXESkJ3qnFJHD3r8Psmew3RRQWd+iruFDUH58kCpfLjWuFNitpKWIiEhUKN8GKWPB17seLHvq3IxPVNfw4aY6cQIxtDLH7ODdqsRIhyMiMuRFJGlpjDnfGLPVGLPDGPONTtYbY8xvQ+vXGWMW9LStMebnxpgtofKPGmNSB6k6IsNDxU4oehfmXMnKPZUAnFCglpZDjTEwP93Pu3Y67H5d41qKiIhEg7KtvZ45vN5vONjoZoKSlsNOTfw4grg4z7uWNyvVRVxEpCeDnrQ0xriBO4ALgBnA1caYGR2KXQBMDj1uAP7Yi21fAGZZa+cA24BvDnBVRIaX9+8DDMz5CCt3VTIqJZYx6XGRjko6MT+9ledbZkN9KZRtiXQ4IiIi0p1gECp29Ho8yz2ahGfYCrpjqIvP50zPOtbVJFDV6o50SCIiQ1okWlouAXZYa3dZa1uA+4BLOpS5BPindawAUo0xo7rb1lr7vLW2beCXFUD+YFRGZFgIBmHdfTDhNGzSKFburmBpQTrGmEhHJp2Yn9HKm4FZzi87X4lsMCIiItK9mv3Q2gCZk3tVfHdtKGmZqDEth6PqxAnkB/aTRi3L1dpSRKRbnggcMw8oDPu9CFjaizJ5vdwW4JPA/ccdqchIse8tqNrH22NvZOVL2ymva8FguHflvkhHJp2Yk+anmEwOxY4lbdcrsOzzkQ5JREREulK+1XnuZUvL3aGWlhrTcniqTpjAGF7l0rj3eLVyCRflHIp0SCIiQ1YkWlp21nSr46BsXZXpcVtjzLcBP3BPpwc35gZjzCpjzKqysrJehCsyAqz6G8SksC/3HHaW1gEwISshwkFJVxK9likpflZ75sKe5eBviXRIIiIi0pW2mcMzp/aq+O5aN6PjAsRFonmJDLj6uNH4XbFc5FvLroY4DjT5Ih2SiMiQFYmkZREwJuz3fOBAL8t0u60x5lrgA8BHre18dgpr7Z+ttYustYuysrKOuRIiw0ZdGWx6HOZdTcAdx/bSOtITfGQkxkQ6MunGggw/T9ZNhdZ6KHon0uGIiIhIV8q3QWwqJGT2qviuOo/GsxzOjIuaxAJmBLZgCGpCHhGRbkQiafkuMNkYU2CM8QFXAU90KPME8InQLOInANXW2uLutjXGnA98HbjYWtswWJURiXpr74FgKyy8Hn8wyK7yeiZlJ0Y6KunB/PRWXm6ahjVujWspIiIylJVtc7qG92KscGudlpbjNZ7lsFadMIE4fzXnJ+7ijYpkumhvIyIy4g160jI0Wc5NwHPAZuABa+1GY8xnjTGfDRV7BtgF7ADuAj7f3bahbX4PJAEvGGPWGmPuHKw6iUStYBBW/w3GnQTZ0yisbKTFH2SykpZD3oKMVmqJpyJlFuxS0lJERGTIKt8GWb0bz7Ks2UV1q4tJamk5rFUnTgDgsrg1lLb42FepNjciIp2JyEgp1tpncBKT4cvuDPvZAl/o7bah5ZP6OUyR4W/3q3BoD5z5XQC2l9biMjAhU0nLoa4gMUBKnJf3ffM468C/oPEQxKVFOiwREREJ13gI6kt7PQnPjhpnEp4pyWppOZw1+9Jo8qUxz27CZ4KsLayKdEgiIkNSJLqHi8hQsepuiM+A6R8EYEdpHflp8cT53BEOTHriMjB/bCrP1E8HG4Tdb0Q6JBEREemofRKe3iUtt9U4bUomJ6ul5XBXnTCBtIY9LEmpZv3+aloDwUiHJCIy5GhOOpHhZtXfeleuqRq2PA0Fp8PaeznUbNh/KJPLRpUzcd+agYxQ+snCsWn8dttobk9MwOx6BWZcHOmQREREJFzZFue5l0nL7TVuUrxBsmKVwBruqhMnkHNoNZclrufNqlN5fVsZZ03PiXRYIiJDilpaioxU+1Y4LfTGLQNgeakPi2Fucn2EA5PeWlyQTqv1UJG1RJPxiIiIDEWlm8EbD2kFvSq+vcbD5GR/b+bskShXk1CAxbCIjcT73Dzx/oFIhyQiMuQoaSkyEgX9sHc5ZE2FhCwAXivxkeAOMDGhKcLBSW/NzU/F6zas9c6HQ7ud8UlFRERk6CjdBFnTwNXz1y5rne7h6ho+MgTcsdTFjSa1fhfTc5N5eUspLX61sBURCaekpchIdOA9aK5xuoYDAQsvF8cwL7kOt+7sR404n5tZeSk8VT/VWaDWliIiIkNLySbIntGrouXNhqoWF5M1Cc+IUZ04gcTGAyzMhtomP2/vqoh0SCIiQ4qSliIjjbWw6zVIzHHu/APvVXipaHaxKLUuwsFJXy0en84zxcnYpFGw69VIhyMiIiJt6sudmcOzp/eq+PbQJDyaOXzkqE6YiMGyzL2JBJ+bZzccjHRIIiJDipKWIiNN5U6oKYKC02gbMOmFAz68xjIvReNZRptF49JoCVgqck6C3a9BUF3KREREhoTSzc5zn5OW+iwfKerj8wi4fORXruT0qdm8sKmEQNBGOiwRkSFDSUuRkWbXq+BNgPxF7YteKI7hhKxW4t0aRyfaLByXBsBa7zxoPATF70c2IBEREXG0Jy171z18W42bZM0cPqJY46YmYTy5FW9z7swcyuuaeW/foUiHJSIyZChpKTKS1JdByUYYdyK4fQDsrHWzq9bD2aObIxycHIuMxBgmZiXwdF1oXMtdGtdSRERkSCjdBLGpkJTbq+LbazxM0czhI051wgSSGgo5K7cBn9ulLuIiImGUtBQZSXa/DsYF409uX/TigRgAJS2j2OLx6bxcBDZnpibjERERGSpKNzutLHuRhbTWSVpq5vCRpzpxAgCJRW9w4qQMntt0EGvVRVxEBJS0FBk5WhugcCWMng+xKe2LXzzgY0ZqK3nx6ooUrRaNT6e6sZXK3JOcv3FLQ6RDEhERGdmsDSUtezeeZXmz4ZBmDh+RmnwZ1Mfmws5XOG9mLoWVjWwuro10WCIiQ4KSliIjxZ7lEGiBCae3LzrY6GJVhZfz1Moyqi0tSAdgtXue8zfe82ZkAxIRERnpavZDc3Wvk5abq7wATEtR0nLEMYbizBNh12ucPSUdY+C5jeoiLiICSlqKjAyBFmdm6axpkJLfvvipwhgshg+OUdIymo1Jjyc/LY7HDxWAJw52vBDpkEREREa2gxuc55xZvSq+scqZOXxmqpKWI9GBrFOguZqsqrUsHpeupKWISIiSliIjQeE70FIHE886YvGThbHMSm1lQpLGT4p2yyZk8OaeOmzBqbD9eadbmoiIiERGyXrnOWdmr4pvqPKQHx8gxafP75HoYOYycHlh23OcOzOHLQdr2VtRH+mwREQiTklLkeEuGHBmlE4dBxmT2hfvqXPz/iEvF49pimBw0l9OnJRBdWMrxVknw6E9ULEz0iGJiIiMXAfXQ9p4iE3uVfFNVR5mprYObEwyZPk9CTDuRNj+POfNdGabV2tLERElLUWGv+L3oaECJp11xOyVTxU6s4Z/QF3Dh4VlEzIBeIN5zgJ1ERcREYmcgxsgd3avita2GnbXeZiVpq7hI9qU86BsC2NMKTNGJfPsBiUtRUSUtBQZzqyFnS9CYvZRYyo9URjL4owWRmvW8GEhNyWWgswEnj8QBxmTYbuSliIiIhHRXAeVuyB3Tq+Kb9Z4lgIw+TzneZvT2nLNvipKa9UjSkRGNiUtRYazsi1Qc8AZy9Ic/u++ucrNthoPF49VK8vh5IQJGbyzu5LgpLOdGcRbGiIdkoiIyMhTugmwfZ6EZ5aSliNb5iRInwDbn+OcGTkAvLKlNMJBiYhElpKWIsPZjhcgNgXyFh6x+P49cfhclg/k6+7tcLJsYga1zX52p50EgWbY80akQxIRERl5Dq5znnvZPXxDlYfMmADZcer9MuJNOR92v8H0DBd5qXG8sElJSxEZ2ZS0FBmuKnY4XZMmngUuT/vipgA8sjeW8/KaSYvRDJXDybIJGQC82DAJvPHqIi4iIhIJBzc4N41T8ntVfGOVR13DxTH5XAg0Y/a8wdnTs3lzRxmNLYFIRyUiEjFKWooMV9ufh5gkGHvCEYuf3R9DTauLqwsaIxSYDJSspBim5Sbx6o5qKDjVeQ1YJaZFREQG1cH1zniWYRMgdqUpANtrNAmPhIw7CXyJsO05zp6RQ1NrkDd3lEc6KhGRiFHSUmQ4OrQbyrfBhDPA7Tti1X274xibEOCErNYIBScD6bSpWazaW0lTwVlQtddpcSsiIiKDIxhwxrTs5XiW26o9BKxRS0txeHww4XTY/jxLx6eTFOPhxU0lkY5KRCRilLQUGY62PQ/eBOdubZjdtW5WlPm4sqARV883/yUKnTYli9aA5V1PaBxTdREXEREZPOXbobUBRs3tVfF1h9om4dHNZAmZch7U7MdXsYnTpmbx0pYSgkH1nBGRkUlJS5HhpqoQyjY7d2k9MUes+s/uWNzGcvk4TcAzXC0al068z83z+2MgcypsezbSIYmIiIwcB95znkfP71Xx1RU+MmMCjEnQJDwj3b0r93Hvyn08UjsTgLUvP0C8z0N5XQs/e24r967cF+EIRUQGn5KWIsPN9ufBGwfjTzlicV2r4T+747ggr5kczU45bPk8Lk6cmMmr20qxUy+AvcuhsSrSYYmIiIwMxWud3i6Zk3tVfFWFl0WZrb0Z/lJGiKbYLCpSZpJf8gpTc5JwGdhcXBPpsEREIkJJS5Hh5OAGKFkP408Fb+wRqx7cE0ttq4tPTW6IUHAyWE6bkklhZSPFuWdC0K8u4iIiIoPlwHswag643D0WLa1porDezaIMdQ2XIxXmnEVm9XrSA2WMz0hQ0lJERiwlLUWGkzdud7qEF5x2xOKAhb/tiGdhRgvzMzTQ+3B32pRsAJ6vzoeEbNj6dIQjEhERGQECfmfm8N52Dd97CIAFSlpKB0U5ZwEwpvRlpo9KprS2mYq65ghHJSIy+DyRDkBE+knZVtj4GEw6C3zxR6x64YCPffVuvjm7LjKxSb9Zubuy/eedgc7HNrpm6VgKMhN4dXsF1009HzY8Cv7mo8Y4FRERkX5Uvs2ZhKcPScsYl2VWmm4oy5FqEidQnVBAfsnLTJ95OU+vL2bLwdpIhyUiMujU0lJkuHjjF85YlgWnH7HYWvjLtnjGJAQ4N093aEeKM6Zm89bOCpomnA8ttbDnjUiHJCIiMry1TcIzal6viq/ae4i56a349I1MOlGYezbZlavI9TaQkxzDJnURF5ERSB+RIsPAEy+/SXDdg2zOu5yVB1pYubuy/XH3+iZWVfj49OQG3BrkfcQ4d2YOLf4gr/lngDcetjwT6ZBERESGt+K14EuEjEk9Fm1qDbDxQLW6hkuXinLOwmUD5JW+yvTcZPZW1FPV0BLpsEREBpWSliLDwIxdf8W6PGwuuO6odQ8fyCTN28qVBY2DH5hEzKJxaaTFe3l2azVMPBO2/tdpdisiIiID48B7TitLV89fsdYVVdMasJqER7pUmTyD+thcxpS8xPRRyQQtvLK1NNJhiYgMKiUtRaJd1T4m7H+CHWMuoyk264hVm2rj2FQXzyW5lcT2PImlDCMet4uzpufw0uYS/FMuhNoDh7utiYiISP/yt0DxOhg9r1fFV+11xqhWS0vpkjEU5pzNqPK3GJdkSY718OyGg5GOSkRkUClpKRLt3vw1Fthc8MmjVj1UnEmqx89ZmVWDHpZE3rkzcqhp8rPatxiMC7aqi7iIiMiAOLgOAs0wZkmvir+5vZypOUmkx6gXhHStKOdM3MEW8sqXM2N0Mq9tK6OhRRM3icjIoaSlSDSrOQDv/Ytd+ZfSEJd7xKoNtfFsrE3g4twKfC5dEI9Ep0zOItbr4pmdLTB2mca1FBERGSiF7zjP+T0nLRta/Kzac4hTp2QOcFAS7crSF9DkS2dMyUvMHJ1CU2uQ17aWRTosEZFB44l0ACJyHJb/FoIBNk04spVl0MI9RVlkeFs5J6sKgJW7KyMQoERSnM/NqZOzeH5TCbeedgHm+e9AxU7ImBjp0ERERIaXoncgZQwkj+qx6IpdFbQEgpw6JQuqByE2iVrWuCnKPp2xB59nwiwfafFe/rvhIBfM7vl1JiIyHKilpUi0qiuF1X+DOVdSHz/miFUrDyWxqyGOK/PK1MpyhDtvZi7F1U1sSD3DWbDx0cgGJCIiMhwVvgv5i3tV9PVt5cR6XSwenz7AQclwUJRzFj5/HXmVKzh3Ri4vbyml2R+IdFgiIoNCSUuRaPX27yHQAqd89YjF/iD850AWY+OaOCW9JkLByVBxzswcfB4XD+8wMGapkpYiI4Qx5nxjzFZjzA5jzDc6WW+MMb8NrV9njFnQ07bGmJ8bY7aEyj9qjEkNLR9vjGk0xqwNPe4clEqKDBU1B6CmqA9JyzKWFmQQ69UsidKzg5nLaPYmM674v5w/K5e6Zj/Ld5RHOiwRkUGhpKVINGqohHf+AjM/DJmTjlj1QnkaJc0+rskrw2UiFJ8MGcmxXs6cms1T64oJzvgQlGyAsq2RDktEBpAxxg3cAVwAzACuNsbM6FDsAmBy6HED8MdebPsCMMtaOwfYBnwzbH87rbXzQo/PDkzNRIaooned515MwlNY2cCu8nqna7hILwRdXvblnkt+yUucODaWpFgPT6/TLOIiMjIoaSkSjVb8AVrr4dSvHbG4odnPgwcymZVUz7zk+ggFJ0PNJfNGU17XzKr4UwEDGx6JdEgiMrCWADustbustS3AfcAlHcpcAvzTOlYAqcaYUd1ta6193lrbNm3tCiB/MCojMuQVvgPuGMid02PR17c7k6icpkl4pA/2jL4Ib6CRmB3Pcf7MXJ7beJCmVnURF5HhT0lLkWjTeAhW/gmmXwzZ049Y9cLmEhoDLq4dU4JRK8sR696V+454lNY2E+Nx8X/LqylJX0T1qvvBaqxTkWEsDygM+70otKw3ZXqzLcAngf+G/V5gjHnPGPOaMeaUYw1cJCoVroTR88Dj67Hoa1vLGJ0Sy8SsxIGPS4aNsrQF1MfmwvoHuGReHnXNfl7ZUhrpsEREBpySliLRZsUfobkGTvv6EYsPVjfxzu5KzsmqYmxcS4SCk6HI63Yxc3QyG4ur2ZVzLin1u51u4iIyXHV226rjnYquyvS4rTHm24AfuCe0qBgYa62dD3wFuNcYk3xUUMbcYIxZZYxZVVZW1kMVRKJESz0ceA/Gndhj0bpmP69tK+OcGTkY3V2WvjAu9oy+EHa8xLJcS2ZiDI+vPRDpqEREBpySliLRpPGQk7ScfjHkzmpfbK3lqXUHiPW6+chofRGUo83JT6WpNcjL5gSCxq0u4iLDWxEwJuz3fKDjt9uuynS7rTHmWuADwEetdZpsW2ubrbUVoZ9XAzuBKR2Dstb+2Vq7yFq7KCtL4/nJMFG4EoJ+GHdyj0Vf2lxCsz/IB+aOHoTAZLjZM/oisAHcmx/jg3NH8fLWUqobWyMdlojIgFLSUiSavH1Hp60s3y+qZld5PefOzCHRE4xQcDKUTcxKJCnWwxsHoCRjKWx8RF3ERYavd4HJxpgCY4wPuAp4okOZJ4BPhGYRPwGottYWd7etMeZ84OvAxdbahrYdGWOyQhP4YIyZgDO5z66BraLIELFnORg3jF3aY9En3y8mNzmWhWPTBiEwGW6qk6ZA9kxY53QRb/EHeW6jJuQRkeHNE+kARKSXGiphxZ0w45IjWllWN7byzPpi8tPiWDw+/ciRyERC3C7DgrFpvL6tjC1zz2bU1tvgwBrIWxjp0ESkn1lr/caYm4DnADdwt7V2ozHms6H1dwLPABcCO4AG4Prutg3t+vdADPBCqGvritBM4acCtxlj/EAA+Ky1tnJwaisSYXvfglFzISbpiMX3rtx3xO9NrQFe2VrKCQXp3Peuc7F2zdKxgxamDBNzroAXb2VuwiHGZ8Tz6Jr9fGTRmJ63ExGJUkpaikSLt++AltqjWln+8vmt1Df7ufbE8bg0PpJ0Y9G4NF7bVsbjTQs4w+WF9Q8raSkyTFlrn8FJTIYvuzPsZwt8obfbhpZP6qL8w8DDxxOvSFRqbYT9q2DpjT0W3VRcQyBomZ2fOvBxyfA163J48VbM+oe4fOGl3P78NvaU1zM+MyHSkYmIDAh1DxeJBg2VzozhMy6FnJnti9cXVfOvFXs5YUIGealxkYtPokJGYgwFmQm8XuTHTjkf1j8AAY2FJCIickyKVkGgpVfjWa4vqiY13suYNF2vyXFIHQPjToL1D3DFwnzcLtPecldEZDhS0lIkGrz9e2ipO6KVZSBo+c5j60lPiOGcGTkRDE6iyeLxaVTWt7Al94NQXwY7Xox0SCIiItFp73LAwNgTui1W09jK9tJa5uSlatZwOX5zPgLl28ip2cBZ07J5aHUhLX6NaS8iw5OSliJDXVsry5mXQs6M9sX3vrOP94uq+e4HphPrdUcuPokqM0enEOt1ceeBCZCQBWvviXRIIiIi0WnXq854lnGp3RZ7d28lQevcOBQ5brMuA28CrPk7Vy8ZS3ldCy9uLol0VCIiA0JJS5Gh7q3fQUv9Ea0sy2qb+dmzWzhxYgYXzx0dweAk2njdLhaMTeOZjWU0TLsMtj4L9RWRDktERCS6NFVD4Tsw6axuiwWClnd3VzIlJ5GMxJhBCk6GtZgkmH0ZbHiEU8fFMDol9qiJn0REhgslLUWGsvpyeOfPMPNDkD29ffH/PrOZptYAt10yS92MpM+WTcjAH7Q86D8Ngq3w/n8iHZKIiEh02f0G2ABMPLPbYpuLa6hp8rO0IGOQApMRYcF10NqAe8NDXL1kLG/uKGdbSW2koxIR6XeaPVxkKFn1tyN/3/io08oyc0r7uhVlXh55L42bptUzad+DsA8m7quMQLASrTISYzhzaja/3VDFx/MW41r9d1j2BVACXEREpHd2vgy+RMhf0m2xlbsrSI3zMjU3aZACkxEhbwHkzILVf+djH/sYf3h1J396bRe/+MjcSEcmItKv1NJSZKhqPOQM8D5mMSTlAtAShO+sSSI/PsAXptVHOECJZtefVEBFfQurMi+Fiu2w581IhyQiIhI9dr4E408Bj6/LIgerm9hZVs/ignRcujEo/ckYWHQ9HFxH2qH3uXLxGB5fu5/i6sZIRyYi0q+UtBQZqrY9B1iYckH7or9si2dHrYcfzKslTu2k5TicNCmDKTmJ/GTvNGxsCqy6O9IhiYiIRIfKXXBoT4/jWb68tZQYj4ulBemDE5eMLHOugpgUWHknnzq5AAvc/ebuSEclItKvlLQUGYrqSqBwJYw7GeKcmSb31rn5zaYEzh3dzFmjWyIcoEQ7YwzXn1TA2oPNFI//EGx+EmoPRjosERGRoW/HS85zN+NZltQ0sXF/NcsmZBDv051mGQAxibDg47DpccZ4qvjAnFHcu3IfVQ36niAiw4eSliJD0dZnwO2DSecAYC18a00SPpfltvkaZFv6x4fm55GdFMPth06FoF+tLUVERHpj6zOQMQnSJ3RZ5OUtpXg9Lk6elDmIgcmIs/jTEAzAu3/l86dPoqE1wB2v7Ih0VCIi/UZJS5GhpqoQit+HCac7d1CBh/bGsrzUx9dn15EbF4xsfDJsxHrdfOaUCTyyN5bqMWfCu3+F1qZIhyUiIjJ0NVU7M4dPvbDLCey2l9Syoa2VZYxaWcoASi+AqRfAqr8yNd3F5Qvy+cdbeymsbIh0ZCIi/UJJS5GhxFrY/Dj4EmDCGQCUNRl+9H4iizNauGaCEkrSv65ZOpaUOC93tZ4PDeWw4eFIhyQiIjJ0bX8Bgq0w7aJOV1true2pTcR4XZzUQyvLe1fuY+Xuyh4fIt066RZnAs/V/+Ar507BGPjF81sjHZWISL9Q0lJkKDm4Dip2OJPveGMBuHl5DPV+uCqniHf36GJW+ldCjIfrThzP7/eMpil9Grz1OwiqNa+IiEintj4DCVmQv7jT1S9uLuWN7eWcNS2HRLWylMEwdimMPRHe/j2jEtx86uQCHlt7gHVFVZGOTETkuClpKTJU+Jth8xOQNArGLgPg5WIfbx1K5kO5FeTHaVBtGRjXnTiexBgv93o+BGWbYduzkQ5JRERk6PG3OC0tp5wPLvdRq5taA/zwqU1Myk7khAkZEQhQRqxTvgI1+2H9A3z29IlkJ8Xw9YfX0xrQjWgRiW66/ScyVKz4AzRUwNLPgctNXavhO2uSyI9t5tLcikhHJ8NYWoKPT51cwI9fauZjmfn43viFMz5SF2N1iYiIjEi7X4fmmi67hv/ptV3sq2zgX59aQmFl4yAHJyPBvSv3db7CTuH85Ol4n/9fnm5axg8vncWN/1rNn1/fxRfOmNS3fXVwzdKxxxquiMhxU0tLkaGgtgRe/wXkzIKsqQD8ZH0ixY0ubhxXjEf/U6Wf3bty3xGPlDgvPq+PP/o/CPtX8dJ/H4p0iCIiIkPLhochJqV93PFwmw7U8PtXtvPBuaM5ZXJWBIKTEc0Y1k2+iaTGIiYWPcp5M3O5aPYofvPidnaU1kY6OhGRY6ZUiMhQ8PIPwd8E0y8B4JViH/fuiuOGKQ1MSdTkOzLwYr1uTp+axR+qllLjzWTOtt87E0OJiIgItDbC5idhxgfbxx1v0+IP8tUH3yclzsdtF8+MUIAy0h3IOoWy1HnM2vEnaG3k1otnEh/j5kv/WUtTayDS4YmIHBMlLUUibd9KeO/fsPRGSMziULPh66uTmJrs58sz6yMdnYwgJ0zIICY2nj9xOVlVa2H785EOSUREZGjY9iy01MLsK45a9buXt7O5uIaffGgWaQm+CAQnAhjD+1O+RHxzKbzzZ7KSYvjFFXPZVFzDbU9tinR0IiLHRElLkUjyN8MTX4SUfDj9m1gL31qTxKFmF79cUkPs0WO8iwwYr9vFOTNy+FPtiVR4R8NLP9RM4iIiIgDrH4LEXBh/yhGL39hexu9f2cHlC/M5d2ZuhIITcZRmLGZ/1qnw2s+htoSzpudw42kTuHflPh5fuz/S4YmI9JmSliKR9PrtUL4VPvBriEnknzvj+O/+WL46q56Zqf5IRycj0PyxaeSkJvHz1sugZD2suz/SIYmIiERW4yGn98Gsy46YNby4upGb71vL5OxEbrtE3cJlaFgz/f85w0699AMAvnbuVJaMT+f/PbSO1XsrIxydiEjfKGkpEikHN8Cbv4Q5V8Hks1lXVMWP3k/kzNxmbpjSEOnoZIRyGcNFs0dxf9NSDibOgBdvhea6SIclIiISOesegEALzL2yfVFTa4Av3LOGptYAf/joAuJ9nggGKHJYbcI4WPZ5WHsPFL6L1+3izo8vZFRKLJ/+xyp2l2v4KRGJHvp0FYmEYMDpFh6bCuf/LxV1zXz+njVkxQb5xeIaXCbSAcpINj4zgVl5adxcchX3e77nJNfP+l6kwxIRETnKvSv39arcNUvHHtsBrIVVd8PoBTBqLgD3rNjL/asKWVdUzdVLxvLO7kO8s/vQse1fZCCc+j+w/mF4/Atw4+ukJ8Ty9+uX8OE/vsX1f3uHhz93YqQjFBHpFSUtZWRb9bf+2c+i6/tW/q3fwoE1cNlf+cfaGv765vscrG7i1imFbDug2cKldybue3DA9v35dA//UzKB5d5lnLj81xi3DxJzut+or/8PREREhrp9K6BsC1z8u/ZFL24uZV1RNefOyGF2XkoEgxPpQkwSXPwb+Pdl8NpP4exbGZ+ZwF+uXcTVf17Bp/+5ikvn5eF1q+OliAxtepcSGWxFq+DlH8GMSwjO+DAPryliX2UDVywaw+QEJSxlaEj3+fn67Hpurv0ErSbGGdvSalIeEREZYVb/DWKSnfEsgbvf3M0rW0tZOC6N06ZkRTg4kW5MOhvmfxyW/8ZJvgMLxqbxm6vmsbawigdWFRK0NsJBioh0T0lLkcHUWAUPfRKSRhP8wG/43pMbWVdUzXkzc3WnXoacj05oZFxGHD9q/RhU7oK9yyMdkoiIyOCpK4ONj8GcK8GXwL/e3sNtT21i5uhkLp2XhzEaz0eGuPN+DKnj4MHrob4cgPNnjeK7F81g44EaHl97AKvEpYgMYUpaigyWYAAe+QzU7Cf44bv45jOF/HvFPk6dnMmpkzMjHZ3IUVwGfrqwlvv8p7LBMxO76QmoPRjpsERERAbHO39yJuBZ+ln++uZuvvv4Rs6ens2Vi8fg1gDkEg1iU+Aj/4SGCnj40xDwA/DJkws4bUoW7+6p5IVNJREOUkSka0paigyWl38I25+n+Zyf8sU3fdy/qpAvnjmJ82bm6k69DFmTkwN8c3Y919d9nmYTA+/9q/2CV0REZNhqroN37sJO+wA/XtnCD5/axAWzcrnjowvwuPQVSqLIqDlw0e2w6xV45mvO5FLAuTNyWDI+nVe3lfHm9rIIByki0jl94ooMhnf/Am/+ipoZH+WDK6bw3w3FfPOCaXz13KlKWMqQd92kRmblxnNz041Qsx82PhLpkERERAbWmn9CUxU/qT6Pu97YzbXLxvH7axYQ43FHOjKRvlvwCTjpFmeM1tdvB8AYw8XzRjMrL4VnNhxk9d7KyMYoItIJzR4uMtA2PIJ9+mvszTiFD264CI+nmX9+ciknq0u4RAlj4OeLajj/hXncay7imn1PQ0o+jDsx0qGJiIj0v9ZGWt78LZtcM/j7vgy+/8HpXHfieN1oluh21vehthhe+REYIOZKXMbwkYX5NLUGeGTNfuK8bmaM1jj7IjJ0KGkpMoD8a+/H9fjn2OiezhX7P8nJ03P4wSUzyUuNi3RoIn2SGWv547IaPv76VcyM28ucDQ9j4jMga2qkQxMREek39c1+3vrXjzmnvpi/cB0PnFrB/JhXYfXhMhP3qUWa9K+J+x7s3x260ztffskfnOeXf8S8gv2snXoLHreLjy4dy91v7uY/7xZy3YluJmYl9m88A2XV3wbnOIuuH5zjiMhR1D1cZABsO1jDK/+4DddjN7LSP5VvxH2PO649ib9cu0gJS4laizNb+dGCej5efzMH3aOwq+6Gqn2RDktEROS4+QNB7ntnH5f84hkWFf6NbTEz+cl5uczP0DjOMoy4PXDpnbDok8zY/TdOW/0lvK21xHjcXHvieDISfPxrxV6KDjVEOlIREUAtLUWO2crdzl12a2HFoW3srWxgb0UDB8sr+WLLX7ja8wqrYpfR/IE7eXLGOFyaZVKGgcvHN7GrNoFLtn6T5xO+R8rKOzEnfA5SxkQ6NBERkT5raPHz8Jr93P3mbnaX1/Pz9KdIa6kjbckF4LWRDk+k/7lccNEvebdhFAs3/x8XLL+clbN+QEnmCXzypAL+9PpO/v7WHm44dQLZSbGRjlZERjglLUX6qCkA6w95eexgOlvr4tlWF0ddYDsAC337+I/nDsZ5Cnlv7PUsuu6XzoWByDDyP7PqaQgk8sGd3+HJ+B+R8vYdmCU3RDosERGRXrHWsrawikfW7OfxtfupafIzJz+Ff12awckvPAqzP+KM3SwSpdoaV3S0MxDWQ2bcVRxKnsYJ67/LWe9+hl15F7Nu8hdDictd/G35Hm48dcIgRSwi0jklLUV6UO83rCr3srLMy8pyL+sPeWkJOq0mR8c0szi1ljGjR3N57b+ZdfARmrwZvDL7ToqzTmK+EpYyDBkD359bByRx4Y7v81jCj8ha8QfMqLkw+/JIhyciInKUYNCyZt8hnt1wkGc3HqToUCMxHhfnzczl2hPHsWBMKuZfl4InDs79EWx9JtIhiwy48rR5/PekB5m1406m7fkXY4ufY1f+h4hb+BFuf7eFu5fv4crFY8hIjIl0qCIyQilpKSPOvSsP32HsbCD1poCBxBxWlHlZUeZj/SEPfmvwGMucND/XT2pgUWYr1JeRHThITuVqMneuw2Vb2T7uKtZP+jwtPs26J9Gvq7v0bc5PriRrVj7nb7iNf8T9ktkPfwoOvOfMTunxDVKUIiIinQtay96KBtYVVfGrF7dRVtuMz+3i5MmZfOmsyVwwK5ekWK9TeN0DsPs1uPB2SMqJbOAigyjgjuX9qbewfexHmL3jTiYWPsTkffdzTuo87jq0gC/+tYU/3XjO4f8rIiKDSElLGfEaAy621sWxqTaejXXx7KqPJYiTpJyb3sqNUxs4IauFhRmtxLsCUHsAyrZSt28NiY0HCOJid97FbJrwSWoTCyJdHZFBYwx8YVoDM1J8XPfON/mK+Q8fffv3BPcsx3XpHZAzM9IhiojICGOtZX9VI+uKqllXVEVNkx+v23DOjBzOnzWKM6ZmHZ18qSqEZ74G+Yth0ScjE7hIhDXEjWbl7NtYN/kmJhY+zLjiZ/iR56+0Vv6dTb+ax4yTL8Y78XTInQ0ud6TDFZERQklLGXGC1lJ0qJHtJbX8vXAsO+rjCGJwY5mY0MgluRXMSGpgSnwDybaaxIb9JO4qwr9xP4HGYtw2NItk7Gj25pxDeeoctk64rtNjhbfqFBmuzhjVwlPn+Llt3zd4fdMM/q/4bpLvPJXACV/Ae9rXIDY50iGKiMgw5w8G2bC/hrd2llN0qBG3MUzJSeSC/FSmjUri+pO6uLEcDMCjNzrPH/6zkjEy4jXGZrNh8ufYMOmzpNZuZVLJs8RvexbvS9+Hl4DYVBh/MhScBgWnQtZU5062iMgAUNJSRoTGlgAvbSnhvxsO8vLmUhpbAxhgQrzh4twKZiY1MCW+noyW/SQ37CWxaj+Jxfvx+WsBCBo39bG5lKYtpC4+j9r4sbR6lYgRaTMqPsgfP7aQV7aO4YvPL+XS0j9w2du/ofadf7Bv6idJO/UGRuXkYnRRKyLDVG9vVF6zdOwARzKyNLT4eWd3JSt2VVDT5CczMYaL545mbn4qcb7DCciu/j6zt9/B7L3LeXv2j9i93QM45TobQkhkRDGGquRpTDnnXP7x1pe45ok3uWXiQa7J3oPZ/Tpsecopl5jjJC/bkphp4yIbt4gMK0payrDTdlEaCFp2lNbxflEVm4praPEHSYzxMH1UEpNzkpiUlciC/f8mvWYTqVU7Sdq/F0+wGYAmXxo1CeOpi8ujLi6PhtgcrKvr/y4T9z04KHUTGdJW/Y0zgDOWweqKK/nV1tM5ofwRlm36JQ0b7+De4Kk87zmdWl8WyR4/sa4gMS6LzxUkxm2JcQVDj8M/+0I/p3gDJHsC3R5+aUH64V8WXT+wdRURkYhqaPHzt+V7+N3L22lqDTI5O5EPzc9kck4irl7eIBtb/Cyzd9zJzrxL2J138QBHLBK9rj1xPIcaWvj2i9vZkHEuP/7S73BV74XdrzuPXa/B+tD3obQCmHohTLsIxp4Q3a2XrQUbdJ51410kIpS0lGGnrtnPyt0VrNhVSX2zn1ivizl5Kcwdk0pBZgLeQCPjip9j3Pv/JbdiBQZLky+NipSZ1CQUUJMwHr8nIdLVEIlqCzP8LDwxg7d3XcbTVWeSX7mCK5te5qP2BXY0j+GFxsX8N7CEzcE8Wm3vLmaTPX7GxDUzOaGRqYmNzEhsINZtB7gmIiIyEHrTMrWzVqmtgSD3vVvIb1/aTlltM9NykzhnRg6jUuL6dPyMqnWcsO67lKbN592Z31NCQqQHN581GX/A8vtXdtDsD/Lzy+fiXvAJWPAJJ6lXttWZzGrHi/DuXbDiDojPgKkXwLQPwIQzwBsb6Wp0rqUBqguhap/z3FABjVXgb3SSls98FTxxziRdKWMgewaMngfjTlLLUpEBpqSlDBs7y+r465u7eeDdQvxBy9ScJJYUpDM5OxGP20V84wGmbP0TkwofxuevpTZ+DAcyT6IiZRaNsdmRDl9kWHIZyExLoyntAta1nkxGzUZyajbzuYZH+JzrEVo8SVTHj6MybjzlsWOp9GTTZD00B13OI2BoDro41OqhqCmGPQ0xPHkwg8cweE2QWUkNLEqtZWFqXaSrKiIiAygYtDy1vphfPL+VvRUNLB6fxh8/uoBtJX1//0+t2cIZ736WxphM3pj/K4Ju3wBELDK8GGP42nlTifG4+MUL26hv9vObq+YT63U7Sf/sac5j6Y3QXOskL7c8DZuehPf+Db5EJ4E580Mw8azIJjBt0ElQlmyEkg1QW3x4XUIWJGRD2njwxoPbC7lzoKUO6krg0B5Yew+88yenfMZkmPVhmPlhp/4i0q+UtJSo0tld+dKaJl7cXMLGAzW4XYZ5Y1I5eVIm2cnOB2FS/V5mb7+DscXPgTEU5pzNtnHXUJY2n4mFDw12FUSGlZW7ez/mV6s3iYMZJ3Aw4wS8rTWk1W4juX4vKQ17yKrZwFSc8WMbY7JoiMmmITaHhtgcGmOyafUktLeCaQ4attXFsaY6kVVViby3bxR37YP5Ra18cEwTHxjTjG5DiIgMD9Za3thezs+e28KG/TVMy03i7usWccbUbIwxfU5aptZs4cx3b6TVk8BLS/9Kc0zGAEUuMjx98azJJMZ6uO2pTXzsLyv5y7WLSI3vkPiPSXKSkzM/BP4W2PMGbHocNj/pdCP3JcG0C0MJzDPBEzPwgQdaoHwbHNwApRudxKpxQfoEmHoRpI6F1DFOorKjjsMOBYNQtsWp1+Yn4bWfwWv/57TAXHAtzP+ocw5E5LgpaTmM/eqFbXz5nCntz8eyPXBM23YXT8efO/u9Jy9uLqGxNcD6oirqmgPEeFycPjWL1oCl6FAD2cmxrF6/gc+ZhygofBTrjuHZ5Ct4KfkSDnmyoQTOTjds3/Qe97gv4dapTjL01q1jmZnUwBWjy8krfZX92acD8OCBTDbWxjMzqeGI51un7uO69ybz9/nbefBA5hExXjG6nFu3Ot2abp26r30fbT+3lWnb/7mNz5AycQlfWDeR0zOr2Vgbz56GGMbHN/Nlz0M8H3chr5ankBXTyp6GGC7KOcS8qheo8nv4O5eSFdNKWbOX+oCL1qBhcmITABc1P83TMRfxoZan2JZxJldU/IEv2G8CkBXTypa6OC4bVcFjxc54gDd5HuHf5hKu4zFSPX5ubbwSPwYPFj9O0sgA3/Q9SEPQRaa3lfJWL7/xX0Z4R91bPA/za/9l7c8Px9zGZc3f4z7fDwG4quW7R5yv+3w/bF92i+fh9uUnuDZxVct3eTPmSzwUOI1f+y/jPt8PWRGc0b6u7fe247Utbzt22zHbtlkRnHHU/u/z/ZB8U8ZDgdO6XNa2XXgM4XW8xfMwv/Ffxs2h+MPXhXsz5kuc3PzbI85T2/n9iuchgqHz3LY8gMGGyp7i3sgbgZlcPqqckwtv5Gveh9iTeQYPF2fwTd+D/Nl+iNMzqwF4uiSNpqCLDK+//XUzPt4Zt3VmUgOvlqe0vw5unbqv/fV/deUd3Bj8Flkxre0xh/8faXsNt71+217DHV//beVnJjW0L2vbbtP2naxNPR/ccMWUMmJbKkloPEB8Uwk11ZVM8O8iq3pd+/6qbCIV3hwSk9JY3jSeE3Pi2Wem8dtZpXx23UTOza7i5Yp0bns/iR+9n8iJb99HbPZEfvGRedz95u6j3l+u/NPb3H/jMuDI97qu3qeO9X20J315n22LoWPsx/r+fjzvwSJytG89up6ffGh2pMOIOn9+fSc3nDqx/Tl8+fRRSfzs2a28vauC/LQ4fnXlXC6em4fbdWRX7u8+tp4fXjr7iP394MkNfP+Ds45YllOxklNX30yLN4mXl/yFhrjR7eva9tH2+/ZN7zF5xnwAvrBuInfM2dlp/G3Xd5vr4rh/4db2z7nr3pvMRTmHAHi1PKV9+7brxo+unsIt3odZPOfwa+bWrWPb9/P++nXMnT2HMRvu4MTm3/IVz0P80n85D/h+yPd9X2N7XSxf8jzMEtdmrm75Lhb4lu9BLjRvUGSzjriOSaaBzbP+B7v+AVbZ6dwR+DCNQVf7tdctnof5U/BDNAZdAPwo7j6+03gVca4gN7oe5bnYC/l+y+2sCM5gRmIDm+qcpE749Uv4tVvbuvsXbuXK1VPbr8NmmL3Maf5L+3E9WG7yPILHWBaZze3XVW06Xg+2XYdd7n6NhwKntR+nq2vIX/svI84VbK9X+DWZgfZrq0+6/8vdgQvat7nF8zBFWafzULGT0M70+rmOx/Bbw7K5s/BsuJ/Lmr/HLZ6H8RjL7a2XE+cK0ho03OR5hN/7PwzAPQudz3i7/gFWBGdQlHU6V4wu56rVU7kvdG7uX7gVu/4BbrLf5AbzKNsyzuSh4gwyvX7umLOTK1dPxQD3Ldza/jpse421/b5903v8uPkj7dd4QPv3mbbXb33Axd/nb28fh79tH22xAEf83Ha9d+vWsVzjfunIF/3W/3L91AvIWhLDV961zL/teV44t5JJyUePPf72my+xIu1ivjyzHkbPd1osVmyHA2udZN+6+8ETCzmznPWZU8HdjymKphoo3cTOHZuZ2LQJgq00mzhiRk2D7JmQPR18PQ8LVvjLMxjzlVcOL3C5IGeG81h6I9SWOEnZdffBs1+HV34M8z8GS26A9IL+q4/ICKSk5TD2m5e28+VzprQ/H8v20H9Jy/A4OsbUlxjrmv28vKX0iGVfO3cqCTEevvXoelKpZf6WR7ms8F48Lss//eeQcuY3+MozxVAKoX84e3oOH7NP8p26q9v3s7kuns118Vwxupz8stfbk5YPFWe2rw9/BmgMuo8o0+aK0eVHlAtf3/Zz28XEQ8WZ3B77LCtZQnmr94iym+viOTf2WW6o+gQA5a3esG2eBDfc3nRF+/Lw7QD+G/sot9ddwX9jH2N88Ue4PXYL5U3eo/bV5hbPI/y66XJuin0ULHwH5/y0JSzBuci7wfUYuEK/eODX/suPOP4tnkf4tf/y9ueFZitgOMG1JVTiyC8cznLTvu2RDPmmon1fJ7i2HLGftt/bjte2PLz84WMQti1H7OPIYx+9rO338BjC6xj+3FaubVm4fFNxRF1/7b+8/fx+yfNoe7nw5eFxLPRshTKAz3KT+1HGF18BOH+TnzR95KjXYnmrt/1v3dlrOHzd5rp4bo/desQ24dq2e6j4yKRlZ6//8H22LWvb7mP2Sb5TfHX78qaYDJpiMqhgNlfun8b9C7fg8dfxy/UxTDFFTDZFTLVFjKtez4eDq2AP3Aa01iXwazOJSe5UiluX8YMz03msOI3Ht9Szt6KUxT96kZZAkCk5SZw1PZtH1uwHnBaibS23297rcpJju3yfOtb30Z705X22LYbw1q3H8/5+rO/BIiL9aU9FwxHPAKW1TeypaOBDf3iLjAQft35wBlcvHUuMp/MxkANhd03b9tPst2HL6pm07wEWbvpfahPG88qiP9IYl3tE+bZ9tP3+MfskK3GSlp19Hrbp+PnX9jnXGHQftQ4OXzf6cXGT+1FWcjhpGf7ZfIPrMVYyh9Gha4YveR7ll/4rWOLa0l6u7ZrBhq4VbnA9Bhy+zuh4vXOCawsnsIXbW69o/739eqnp8LXKx+yTfIeraQy6ucX3CL+uu5wTYkPXXk1wbugbZPh1Ssdrt8PXPh3jMO3H9YeOHR5f+DVTx32G7yf8Wqura8hf+y9vP99t69r2bzvsp+O12/jiw3GUt3qd62JgJbPar2nbtrm99Yr243R23dd2zTi++HKuGF3e/vdqi/kEl3NtfkPoWr3tmG1l2mINv26/YnR5++8fs0/yneDVR7x+2r7PHLmvw9r2YY+4vjdHbH/4uUNr5u3PwdQL+MCYZnLjqrj81TQufTmNXy+p4ezRLUcUXVb1JFcfvNpJWoIzKU/WNOcx+wqn5WPxe3BwPexfBe4YJ8mXMRkyJkFKft8m8mmug0O7oXy7kxwNdfuOsZlQcALkzGLWa8vYvqD3PYUAxtSs6b5AUg4svcF5FK2GlX+Ed/4MK++EGZfAyV+GUXP7dEwRcShpKVGjpqmVFzYdZPmOiqPWJcR48Pjr+ZL7ET7jeZrE3U08HDwF7+nf4gfPVfKTmEyg+OidisiQ5fcksjw4jeVhX+jun7WZm9dk8/epb/PgDsONqRvJrawgr+x9/uV7Dd4yfCUxhy/7DnJj65eZsPgi7lxRyhfuXUOCz82UnCTmjkmNXKVERKRL1Y2tvLS5hNV7nRaKXz57Cp86pYDEmGP/yuLx1/Nzz59YsvF1DmSexPJ5/0erN6W/QhYZ9joOBbT0qGVpjE8M8Om3Uvn05Ab+3+w6fK5e7Njldlo6Zk+H2X4o2+Z0267YAVuedMoYlzOZT2K2M9akL9FpmWkMBAPQ2gBN1c5Yk7Ul0NqWHPVCxgTIWwjZMzj5+bnsmVUGQOtAp0DyF0L+X+CcHzpJy1V3w8ZHnXE8T/4yjD9ZE3+J9IGSljLkVTe0cvfy3dy9fDe1TX5m56Wwfn91+3ofrUzd829m7ryLWG8lzwUW4T/t23ztxUZ+Ep8H9O1OmogMYcZwkAyqEyfx18A0zs2byJUHp/HQvPf51ToX98x+Hw7twdQd5M++X8G633GGbxK5Cz7AY3Uz+ePWIO8VVgHw+Nr9zM1PjWh1etJxHN+233sz666ISLSoanBaZ93+vNM19cSJGSzfWcHNZ08+rv1mVa5i2brvEO8+wPqJN7Jh8uewpg+ttkSkVx464xA/WZfIX7bHs7Lcyy8X1zC5k+7iXXJ5Dne3Bqdbd+UOqCmGulKoL3WSmsHWo7f1JjgtHUfNgcQcp3Vm6vgO3cwjkCRMHgXn/ABO+Qq8+1dY8Qf4xwcgf7GTvJxygdPNXES6paSlDFmV9S389c1d/OOtvdQ1+zlvZg6Ts5MYnRrH+kfX4ybAh91vcLPnEfI3l3MwYymfPXARa+0kfpI0CVgf6SqIyCAJuGNYHpwGk3OcBU/dwpXN3+X+s+pIeP1Rxq29nZuB6xPHsDbpNH5eOI01+45sKbC3oj4ywYuIjFCH6p1k5S+ed8b+Wzg2jdOmZpEW72P5zqN71vRWbHM5v/D+kXNWvkFtXD4fafkuH55yRb/ELCJHi3XDbfPrODG7hW+tTuaiF9O5eUY9N9hjvEkQmwyjF8DosGXWOpPp+ENjd7pc4InrW/fxSIhNcRKXJ3zOmXV8+W/gvmucLvIn3QKzL3dmKBeRTilpKUPO1oO1/GvFHh5Zs5/G1gAXzh7FF8+cxLTcZO5duQ8T9HOZ63W+6HmU8a4S3g9OYOuSn1CSuYy1jypRKSKOlXY6nH0RH3hxMXu+vQi2PUvd2/dzctl9nBrjpzZ2FGsSTuXXB2aw1k7kT6/vAuDWJzZy4exREY5eRIayFn+QdUVVbD5Yy87SOmqanNY/D68pIjnWS1aSj3EZCaR1nFFXANhX0cAja4pYs8/pBr5ofBord1dy6fy849qvt7War3ge4IOvPYd1tbBh4mfYOOHTrHpyJx/uj8BFpFvn57WwKKOC772XxM83JPKY+d/+27kxzizjgzHT+EDwxsHiT8OC62DjI/Dmr+CxzzqT9pz4RZj/cfB1MnO5yAinpKUMis66NHbs3vivt/fwyHv7eW9fFT6Pi4vnjuazp01gUnaSU6ClgYn7HmTG7r9xta+QDcHxfLrlq7wYXMBPMucMWl1EZOhqazm5NPR7+3vPpmbgDFh8Br6WalY8+y++nLWZk8sf5rSY+ym26ezMOIPfH5zOfe/A39/aA8B3H9ugBKaIANDUGuD5TSU89f4Blu8op77F6fqYGOMhNd5pJbOtpJa6Jn/7xBlp8V5mjU5hdn4KealxmBE+jtmG/dXc9cYunlpXjAGWFmTw9q4KLpmXd9S4eX1RYIpZtPFxJux/Ao+nkb3Z53PtnnO4Ycq5/Re8iPRKZqzlD8tqeOFAEz9427lxc8NbKfy/WXWdzjA+4rg9MOcjzkRE256DN38J//1/8Nr/wdLPwZJPQ1xapKMUGTKUtJSIKTrUwM6yerYcrAHgu49vZFpuEt+6cBpXLBxDWkKodULNAXj3L7DqbyxtrKQyeTqfafkKLwQXEpHxSUQkqrX4UngocBoLFt2Et7WWt575F+e73+Wsqqc42fcwwfhMCrPP4HvbJvDY6iD/WrEXgC/fv5ZTp2Ry8qQsspIG5y5/IGgJWosNZUAaQ0mSoLW4RnjyQ2SwbNhfzQOrCnnsvf3UNPkZlRLLpfPzOHVKFrPzUhiVEosxhvHfeJpvXjCdQNBSUtPE3op6tpbU8tbOCt7YUU5WYgwLxqVx1vRscpJjI12tQWOt5bVtZdz1xi6W76ggwefm+hPHk5kYQ3Kcl7d3HWM38JYG2P4cp636G1f5XscWetgz+kK+sGsZn5h3EXt2q/eNSCSdM7qFU3z/w7Tmf/BWqZdzn0/n0rFN3Di1gakpSl5iDEw933nsfdtJXr7yI+d5xiUw92oYf4rGvZQRT0nLYW4oTNrQ7A9woKoRgKfXHaC4ugmAP7y6E4DRKc6F+39vPoXpo5JDG9XC2odg3QOw+zVnDJNpF/FCymWUpS3khcc2DH5FRGTYafUm8WjwFB4NnsLPzp/Iq0/fw9dHbWN04dP8w9dAsyeJLelL+Hf5RNZsnMuj7zl3vmeMSuakSRnMyU9lTn4K+WnxuF29TyL6A0H+8sbu9t9vvu89ahr91Da1UtPUSm2Tn5rG1vaWXG1++PQmwGkBGuN1LmIv+M0bZCb6yE2OZVRKLLkpceSmxJCbHMeolFieWV/caesuTeYj0rXK+hYee28/D64uYnNxDT6Piwtm5fKRRWNYNiEDVzf/390uw+jUOEanxrFsYiaNLQE2HKhmzd5DPLfxIC9sOsipU7K4fGE+Z0/PIdY7xMdjO0YlNU08smY/D64uZFdZPTnJMXzjgmlcvWQsKXHeY3sPaqqG3a/DpsdhyzPQWk+6L4PfBT7EmLO+SFNMJlt2KlkpMti6aim91DhDZ/xq5k7eacjjnl2xPLIvjtNzm7lmQiNn5rbgGcCc3OG4srv4GZYWpA9cAL01bhmMexAOrod3/gwbH4P3/wMpY2DWZTD1AmcCn6E+fqfIAIhI0tIYcz7wG8AN/MVa+9MO601o/YVAA3CdtXZNd9saY9KB+4HxwB7gI9baQ4NRn6GsNjTG0r7KBpr9AVr8QVr8QQJBp+VOEOfZWosxBpcxuAxHfMF9at0B3MbgdjkPl8vgNgaPy2CBumY/9aFHXXOA8rpmiqsbOVDVRHF1I2W1zQRDrYTe2VPZ3rrgysVjKMhMIDnWy7ceXcd0byms/A/seMm5IPU3Quo4OOVrMO8aSC+gTF+yRWSA+D3xPBM8gZPnfQZXoJkXnvoP387bzpSy5fzc+xIAlalj2J++lBcapvDEW6O4K5AOGHxuF2PS48hJjiUtwUdSjAe3y2AM1DX5nSRkKBlZUd9CRd3h90WAx9cewACJsR6SY72kxHkZkxZPYqwHj8t5b35240Eumj2Kp9cXc/rUbJr8Ad7eWUFeahzldc1sLymntLbpiP0CeN2G5FgvqfFeUuJ8pMQd7saaEudt/12Gl8G+1jLGfBP4FBAAvmStfW6Aq9jvmv0BXt9WzkOrC3l5SymtAcuc/BRuu2Qml8zNIyX+2P6vxPncLB6fzuLx6ZTXNdPiD/LwmiJuuvc9UuK8XDx3NJcvzGdOfkrUdx+va/bz6tZSHl5dxGvbyghaWDw+jS+cPokPzh2Nr6/ZiYZKOPAe7Hsbdr0K+1eDDTpdJ2dfDrMu47HSsfzqsU38JCZzQOokIscv2RPg23Pr+Py0ev65M457dsVxw1upZMUGOD+vmQvymlmY0UrMAOTk2nqrHGz2UtPqHGBLXVz7+thKDwkeS5LXkuCxxHssfbgP3b9yZ8PFv4MLfgZbnnYSl2//Hpb/GuLSYfK5MOE0GLMU0ic4rTVFhrlBT1oaY9zAHcA5QBHwrjHmCWvtprBiFwCTQ4+lwB+BpT1s+w3gJWvtT40x3wj9/vXBqtdgag0EKa9rpqSmmdKaJkprmymtbaastonSmmZKQs8A//vfLQDc+drOYz7eTfe+16fysV4Xo1PiGJUayymTsxidGkdpTRP3vVvI9z84EzdBfv/Ya5wfU0Pa/k1kVG/krZj34PehO17pE2DBJ5yL0fzFejMWkUEXdMfwUnAhZ825Dqzl74//l/+dW0FuxQpml/2X2a0P8RUvtCZlUpo0g32ecWz1j2Zz4yg2V2VxsDmGgAWLM95dUqzzGJMez9z8VHKSY9hb2cDjaw8A8I3zp5EQSnR25dmNBzlpUiZPry/mnBnOLOlv76zgL9cuai/jDwQpq2umuLqJg9VNHKhq5NWtZVQ1tlLd0MKO0lpqm/zA4XE725z3q9cZlRrLqJQ4RqfEMio1jtGpsYxOiSM3JXbYtgYbjgb7WssYMwO4CpiJM9fri8aYKdbaId3/LxC0bC+t5d3dlby5o5w3tpfT0BIgM9HHdSeO57KF+UzLTe7XY2YmxnDN0rF8+ZwpvLWznIdWF/HAqkL+tWIvEzITOHVKFidNymTemNRBG4bieDS0+Nmwv4Z391Ty9s4KVu6uoDVgyU2O5XOnT+Tyhc4N6u5ZfC3VzDE7Gb9/L0kNe0mp28nrvrXwszKniHFD3gI45asw4XTIXwIeZxghW6Yb2iLRIi3GcvOMBj4/rYGXin08ti+WB/fE8a+d8cS6LQszWpmV2sq0FD9jEwPkxgXJiAkS43K+EgYsNAUMTQFo9BsOtbgob3JR0eyistlQ0eyivNn5vaLZRXF9GtWhROXNGya2x/H9reMOB7X1yBgNlmRPgBSvn5TQ87TWj5L52k4yE2PITPSFnmNIT/D1/WZMb3jjnO/Csy+HxirY+TJsexa2Pwfr7nPKxGc474U5MyBrOmRNhYxJmsxHhp1ItLRcAuyw1u4CMMbcB1wChF9IXwL801prgRXGmFRjzCicO/tdbXsJcHpo+38ArxLhpKW1lqA9PCZZIGgJWEswGPo5aGlsDdDQEqChxR96dn6ubw5QGWqNU17XQnldMxX1znNVQ+tRxzIGMhJiyE6KITs5hhmjknlgVREfnDuaJ98/wLXLxhPjceHzuIjxuEItgJxWQG3jorXF2/Z8+/POO/jzXz61Pd6gtfiDTh1oqsEVaCLB1UKCy0+8q4V400oMLZjWOmgshIZyqC9nd8VeLvDuZd4bdSQ2FHJ1bCuscmKviR/Hy8FpXHrxZTDxLEgvGLS/kYhIj4xhmx3D1oIL2Vrwca5ZmAsl62H/GrwH3iPvwHvklbzNsmDYe7M3HpJHQfJoSM6DxGyITTn8iEnmhUADe0whLXjJDyQQaPIRdPkIuJxnvzuuzzdtPG4Xo1LiGJVyuAVBvO/Ij/pA0PLdxzdwwykTqG5spbqxlWc3HmRsRjzF1Y2sL6qmor7lqH2nJ/jaHxlhP6cn+EiL9xHncxPndRPvcxPrdbf/3vaZ09ZKv60F6uGfdWNqAAz2tdYlwH3W2mZgtzFmRyiGtwewjp1qDQRpaAnQFLq+amwJUN/ip6y2mZKaJkpqnOddZXVsLamlqTUIOEPVfHhBHmdPz+GkSZl43QM7hpjbZThlchanTM6iurGVp9Yd4PmNJdz37r72GwqjUmIpyEwgPy2O/LR48tPiyEyMISHGQ2KMh8RYD/Fet/N/K/T/yxjaf+6sC3vHa722MXODNjR+Lk5DxtZgsP3c1TcHqGv2U1rj3AwpDj3vL6vkYGUVXhvAi58pmT6+tiCZZeOSmZkThztYCZX7oLgGWuqcoX+aa53Wk3UHOad4H3HN5Xw4ppTYl1q5PAZYB0Fc1Mfn8ZqdyNizb4LR82H0POe9U0SGBa/LmWn8/LwWGvzwZqmPFaU+VpZ7+duOeFqCR75/GSweA622+2sGn8uSGRMkI9ZJdqa7mkj2+HmyJIPPjz9AsifAT3eM4duTnZsdFvAHDU1BFw0BN40BFw0BFzV+D9Wtbqr8Hkrq4lgdOIvGUGOgjpJjPWQmxpCR6CMjIYb0RB+ZCT5S4n0kxrhJiPGQ4POQEOMh3ucmMcaD1+PCE3rvPvzsav/9iPfvuFSY9WHnEQxC+VYoXAmF70DRKtj+PITfI4xLh5Q8SM6HxCyITXX20f6cAt4E8MaCJ855ThmrsTNlyIpE0jIPKAz7vYjDE712Vyavh21zrLXFANbaYmNMdn8G3RePrCniaw++f1QXvWORHOshMymGzIQYpuQksmxCBhmJPrKTYtsTlDnJsWQk+PB0uMB+YFURyyZk8OT7B5iam3TMMUzJ6WLb350DFTt63oE3nixPGtUmlprEAvZnn8Y/t7k5+6QTOZQ8lVZvMt96dD2XLr7omGMUERk0Hh/kLXQebQJ+OLQbyrbCoT3OBGK1B5znvW9B3UEIHJkIPAc4p60h1RtHH+aRM1+haQC6O7a15hwf1vrp2Y0HuesTh1tsNrUGKK5uoriqkQOh54M1Tc7NtPoWtpfWUVnfwqGGlvZuV8cb018+sYgzpkXso3u4GexrrTxgRSf7GnRffeB9nnj/QJfrvW5DdlIs4zLiuXrJWGaNTmFJQTr5aZGb2TslzstHl47jo0vH0dQaYF1RNeuKqtiwv5rCQ428tq2MklAPmr5yhW5OB0NJyv6QkeAjNyWW2+0vmR0T9mevA9aHHt2JTYHEXALuVMrS5vF8LSyeNZ0/rgtyyVmnUReXT9Dt41uPrucDJ+vaUGS4i/fAuaNbOHe0c53UGoQ9dW6K6t0cbHRR1eKiMWBoDUKs2xLrbnu2pMU4ycnMGEt6TJBEjz3ifm/b2JVPlmRwWkZN+/I5yQ19inHpxtuo/2YF5XXNlNc1U1bb0t7AqK1hUUVdC7vK63h3j3N9dDzvucY4080aY/jxpbO4aslYZ4XLBdnTncfC65xl/hao3AllW6BiJ9Tsh+r9UF3oDK/RVAX+pu4P+J0ycPmOPWCRAWRsf3zb6MsBjbkCOM9a++nQ7x8HllhrvxhW5mngf621b4Z+fwn4f8CErrY1xlRZa1PD9nHIWpvWyfFvAG4I/TqVoxqE94tMoHwA9jvUjIR6joQ6wsio50ioI6iew8lIqCMMTj3HWWuzBvgYQ8ZgX2sZY+4A3rbW/ju0/K/AM9bahzvENRjXYMdrpPy/Ox46R72j89Q7Ok+9o/PUOzpPvTNY52lEXX/JwIhES8siYEzY7/lAx1viXZXxdbNtiTFmVOjO/yigtLODW2v/DPz52MPvmTFmlbV2Uc8lo9tIqOdIqCOMjHqOhDqC6jmcjIQ6wsip5yAb7Gut3hxvUK7Bjpdejz3TOeodnafe0XnqHZ2n3tF56h2dJ4kmkRi44F1gsjGmwBjjwxm4/YkOZZ4APmEcJwDVoe5I3W37BHBt6OdrgccHuiIiIiIiQ9BgX2s9AVxljIkxxhTgTO7zzkBVTkRERERGhkFvaWmt9RtjbgKeA9zA3dbajcaYz4bW3wk8A1wI7AAagOu72za0658CDxhjPgXsA64YxGqJiIiIDAmDfa0V2vcDOJP1+IEvDPWZw0VERERk6ItE93Cstc/gXCyHL7sz7GcLfKG324aWVwBn9W+kx2xId33qRyOhniOhjjAy6jkS6giq53AyEuoII6eeg2qwr7WstT8GfnwcIQ8Vej32TOeod3SeekfnqXd0nnpH56l3dJ4kagz6RDwiIiIiIiIiIiIi3YnEmJYiIiIiIiIiIiIiXVLSsheMMWOMMa8YYzYbYzYaY24OLb8i9HvQGLOowzbfNMbsMMZsNcacF7Z8oTFmfWjdb40xZrDr05Vu6vlzY8wWY8w6Y8yjxpjUsG2iqp7d1PGHofqtNcY8b4wZHbZNVNURuq5n2PqvGWOsMSYzbFlU1bObv+Wtxpj9ob/lWmPMhWHbRFUdofu/pTHmi6G6bDTG/Cxs+bCppzHm/rC/5R5jzNqwbaKqnt3UcZ4xZkWojquMMUvCtomqOkK39ZxrjHk7FPeTxpjksG2irp4SHbp5PaYbY14wxmwPPaeFlo83xjSGve/cGbavYfl67OYcDavr3OPV1/M0El9LMDK+T/SHvp4nvZ6G93e349XX8zRSX08Spay1evTwAEYBC0I/JwHbgBnAdGAq8CqwKKz8DOB9IAYoAHYC7tC6d4BlgAH+C1wQ6fr1op7nAp7Q8v8D/i9a69lNHZPDynwJuDNa69hdPUO/j8GZYGEvkBmt9ezmb3kr8LVOykddHXuo5xnAi0BMaF32cKxnhzK/AL4XrfXs5m/5fFuMOBOjvBqtdeyhnu8Cp4WWfxL4YTTXU4/oeHTzevwZ8I3Q8m9w+NpmPLChi30Ny9djN+doWF3nRuA8jbjXUg/nadh8n4jQedLraRh/d4vAeRqRryc9ovOhlpa9YK0tttauCf1cC2wG8qy1m621WzvZ5BLgPmtts7V2N87MnEuMMaNw3jjettZa4J/ApYNTi551U8/nrbX+ULEVQH7o56irZzd1rAkrlgC0DfYadXWErusZWv0r4P9xuI4QhfXsoY6dibo6Qrf1/BzwU2ttc2hdaWiT4VZPAEJ3eT8C/Ce0KOrq2U0dLdDW6jAFOBD6OerqCN3WcyrweqjYC8BloZ+jsp4SHbp5PV4C/CNU7B/08Noazq/HkXKde7yO4Tx1agSfp2HzfaI/HMN56tQIPk/D6rvb8TqG89Sp4X6eJDopadlHxpjxwHxgZTfF8oDCsN+LQsvyQj93XD7kdFPPT+LccYEor2fHOhpjfmyMKQQ+CnwvVCyq6whH1tMYczGw31r7fodiUV3PTl6vN4W6QtxtQl3+iPI6wlH1nAKcYoxZaYx5zRizOFRsuNWzzSlAibV2e+j3qK5nhzreAvw89P5zO/DNULGoriMcVc8NwMWhVVfgtPqGYVBPiQ4dXo851tpicL7sAdlhRQuMMe+F3ltPCS0bEa/HkXKde7x6eZ5gBL+WYGR8n+gPvTxPoNfTeEbAd7fj1cvzBCP89STRQ0nLPjDGJAIPA7d0uGtxVNFOltlulg8pXdXTGPNtwA/c07aok82jop6d1dFa+21r7Ric+t3UVrSTzaOijnBkPXH+dt/myA+r9qKdLIuKenbyt/wjMBGYBxTjdCmGKK4jdFpPD5AGnAD8D/BAqDXicKtnm6s53MoSoriendTxc8CXQ+8/Xwb+2la0k82joo7QaT0/CXzBGLMap+tSS1vRTjaPmnpKdOjDNVwxMNZaOx/4CnCvccZfHfavx5FynXu89FrqnZHwfaI/9OE86fU0Ar67Ha8+nKcR/XqS6KKkZS8ZY7w4bwD3WGsf6aF4EYdbkIDTrP9AaHl+J8uHjK7qaYy5FvgA8NFQU3GI0nr24m95L4e7LUZlHaHTek7EGdvlfWPMHpyY1xhjconSenb2t7TWllhrA9baIHAX0DapSVTWEbp8zRYBj1jHO0AQyGT41RNjjAf4MHB/WPGorGcXdbwWaPv5QYbpa9Zau8Vae661diFOAnpnqHjU1lOiQxf/70pC3eDausOVAoS6FFaEfl6N8zqdwjB/PY6U69zj1ZfzNFJfSzAyvk/0h76cJ72ehv93t+PVl/M0kl9PEn2UtOyFUOulvwKbrbW/7MUmTwBXGWNijDEFwGTgnVD3o1pjzAmhfX4CeHzAAu+jruppjDkf+DpwsbW2IWyTqKtnN3WcHFbsYmBL6OeoqyN0Xk9r7Xprbba1dry1djzOh9ICa+1BorCe3fwtR4UV+xBOl1SIwjpCt+8/jwFnhspMAXxAOcOvngBnA1usteHdVaKunt3U8QBwWujnM4G2LvBRV0fo9v9mdujZBXwHaJupMirrKdGhm/93T+DcMCD0/HiofJYxxh36eQLO63HXcH49jpTr3OPV1/M0El9LMDK+T/SHvp4nvZ6G93e349XX8zRSX08SpewQmA1oqD+Ak3GaRa8D1oYeF+IkRIqAZqAEeC5sm2/j3LHYStiMW8AinCTKTuD3gIl0/XpRzx04Y4O0LbszWuvZTR0fDsW7DngSZ+DiqKxjd/XsUGYPodnDo7Ge3fwt/wWsDy1/AhgVrXXsoZ4+4N+huNcAZw7HeobW/R34bCfbRFU9u/lbngysxpntciWwMFrr2EM9b8aZzXIb8NPwmKOxnnpEx6Ob12MG8BLOTYKXgPRQ+cuAjaH/j2uAD4bta1i+Hrs5R8PqOnewz9NIfC31cJ6GzfeJSJwnvZ6G93e3wT5PI/X1pEd0Poy1GqJAREREREREREREhg51DxcREREREREREZEhRUlLERERERERERERGVKUtBQREREREREREZEhRUlLERERERERERERGVKUtBQREREREREREZEhRUlLERkRjDF3G2NKjTEbIh2LiIiIyEhgjBljjHnFGLPZGLPRGHNzpGMSEZHoYay1kY5BRGTAGWNOBeqAf1prZ0U6HhEREZHhzhgzChhlrV1jjEkCVgOXWms3RTg0ERGJAmppKSIjgrX2daAy0nGIiIiIjBTW2mJr7ZrQz7XAZiAvslGJiEi0UNJSREREREREBpQxZjwwH1gZ4VBERCRKKGkpIiIiIiIiA8YYkwg8DNxira2JdDwiIhIdlLQUERERERGRAWGM8eIkLO+x1j4S6XhERCR6KGkpIiIiIiIi/c4YY4C/Aputtb+MdDwiIhJdlLQUkRHBGPMf4G1gqjGmyBjzqUjHJCIiIjLMnQR8HDjTGLM29Lgw0kGJiEh0MNbaSMcgIiIiIiIiIiIi0k4tLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLUVERERERERERGRIUdJSREREREREREREhhQlLWVEMsa8aoyxfdzGGmNe7ccYTg/t89b+2qeIiIiIdM0YMz50/fX3SMciIiIi3VPSUqJa6KKzL4/rIh3z8TDGpBpjbjPGrDXG1Bljmo0x+40xK4wxvzDGzI90jENF2N88aIyZ2E25V4bL60OOZozZY4zZE+k4RESiWbRdX/X3jeZeHO8KY8yzxphSY0yrMabCGLPJGPNvY8y1gxXHUGeM+XvYa+YH3ZS7Nqzcq4MYogwCY8ytob/t6ZGORUSGPk+kAxA5Tp1d8NwCpAC/Aao6rFt7HMeaDjQcx/bHxRgzGlgOjAd2AfcAlUAeMA2n3o3Ae5GJcEjy47zPfQr4VseVxpjJwGlh5URERKRrXSWa1g5mEEOJMebPwGdwrsGeBnYDCcAE4IPA6cA/IhXfEOUHPmmMuc1aG+hk/WfQtZmIiKAPAoly1tpbOy4L3e1PAX5trd3Tj8fa0l/7Oka34SQs7wY+ba09onu7MWYUMCoCcQ1lJUAxcL0x5nvWWn+H9Z8GDPAUcOkgxyYiIhJVOrvuGsmMMSfhJNiKgGXW2qIO6704SUs5Utt11/k4id52xpjpwEnAo8CHBj0yEREZUtQ9XEY0Y4zHGPMtY8z2UFfrQmPM/xljfJ2UPaqLSnj3BmPMNcaYlaFu23vCyuQYY/5qjCkxxjSGunYfS1ehE0PPv+uYsASw1hZba9d0Ene8MeaboePWh+J72xhzdSdl28fZNMbMM8Y8bYypMsY0GGNeM8ac2Mk2ScaY7xpjNhhjaowxtcaYncaY+40xCzsp/xFjzOvGmOrQ+Vgfii+mk7J7Qo9kY8wvQz+3mr6NA3oXkAt8oMO+vcC1wFvAxq42NsakG2P+1xizORRvtTHmJWPMuZ2UTTHG/I8x5mVjTJExpsUYU2aMecIYc0IX+z/FGPNkqHyzMeagcbr7f79DuS7HYTXGXNdZ97zenD9jzLRQd63C0PFLjDH3GmOmdnKctm5dBcaYm4zT9a0ptN9vGWNMqNwVxph3Qq+3UmPM740xsV3EfizHH2+MuTH02mkKbfNnY0xKWNnTQ+drHDDOHNmN8e99Pf8iItI7xpgYY8w3jDHrQtcPNcaYN4wxH+mkbLfje5tOhvgI/8wzxpwf+nysDlvW9ll5Wof3/qOOEfo8uc8YUx76PFlljPlAx3LdOCn0/HDHhCWAtbbVWvtCF3U7zxjzTOjYzca5dvq5MSa1q/NgnGu6nxtj9oW22WGM+Xrb52+HbS42zvVKcajsAeNcy32+k7KTjTH/NM6QQy2hsv80To+UjmV7vPbthXtwWqZ+ppN1bcv+0t0OjDFXG2eIn0Ohv91mY8x3TOfXk5cap6v+NnP4Wni1MeZLxpijvg8b59r9dmPM1lD5qtDPfzfGTAgr1+n1V9j6Y/3ucKzX7ouMM0xBdei8PGyMGRMqNyH0Wi8zzvXsK8aYuV3EPWDfHUL1bLvGCh+iyYaV6dX5F5GRQS0tZaS7FzgF+C9QA1wI/D8gG7i+D/v5KnAO8CTwCk5LT4wxGThJsQnAm6HHKOBO4Pk+xloRep5CL7thhS58XwbmA2twWmm6gPOAe40xM6213+lk00U45+FtnIvGscBlwEvGmHnW2q2h/RvgWZyEaltZPzAGp2XBG8DqsHh+AnwTKMc593XABcBPgPOMMedYa1s7xOIL1SEd55zV4HS96q3/AL/EaVX5WNjyi4Ec4BvApM42NMaMA17FaeH6RqiuCTgJ0GeNMTdaa+8K22Q68GPgdZyWA4dwzt3FwAXGmA9aa58N239bC4Ma4Algf6ie04HP03U3vL7o8vyFjv8I4MV57e4A8oEPAxcZY87oLBEO3I7z930ytM+LQ/X2GWMqgZ/inOs3cP5ffAFwA58L38lxHP9nOK/htuOfgfMlZxJwZqjMHpzzd0vo91+Hbb827PgDff5FREYM49z0fQ5n6JUtwB1APHA5cH/oGuKo4VqO0eU4LfX+i3NdNR7n/f0HOEmRvcDfw8q/2mH7ccA7OEPu/Avn/f9K4HFjzNnW2ld6EUP4tVmvGWO+F4qzEqfVYSkwB/gacKExZpm1tqbDZl6cz7zROHX247RW/CkQS9hnljHmBuBPwEGcz8pynGvbOTjXt38IK7sYeBFIwvks3IQz7NBHgUuMMWdZa1d1Uo1Or317qQp4ELjGGDPKWlsciiUG+ATwGrCtq42NMX8FPonTwvWR0P5OAH4InBW6ngzvXfNTIAisxPmsT8G5XvgNsBj4eNi+43GGY5oIvBCqn8F5vVwCPITzmjleXX13SOXYrt0XA1/HOXd3AbNxrqdmG2MuxvkOsgX4Z6guHwZeMMZMsNbWhdX/WI/fq+8OONdjl+K8R/wD53qt3SCefxGJFtZaPfQYVg+cDz8LjO+mzKuhMquB9LDlCTiJkwCQ22EbC7zaYdmtoeX1wPxOjvPn0PpfdVi+CGgNrbu1l/W6KVS+BidpczaQ0cM2fw9t8/86LI/FScAFgXlhy08PlbfAdR22uTG0/A9hy2aHlj3aybFdQFrY78tCZfeFn1ucmydPhtZ9q4u/5YtAQh9fBxYoCv3clkzND1v/LFCN82XqR13U+dXQObqqw/JUnC9GjUBO2PIUILOTWPKBA8DmDssfDh13bifbZHYSi+2irtd1EX+X5w9Iw0mqlgMzOqybiZNQXtPF62kPkNfhfJTj/D8oA6aHrYvB+QLUDGT30/H3AWM7vIZeD61b0sk52NPFeev1+ddDDz30GMkPDl8b3NrJ47qwct8MlXsG8IQtzw77TDoxbPnpdHMt1Nl7eNhnXhA4v5t4X+1i3fiw+ny/w7rz2uLv5XnJw0mYWZyE3zXAZMB0s80ZofJvAald1O1XnZyHtvMa1+G8VoUe3rDlqzt+7oatywz72QCbQ/v+aIdyV4aWbwFcYctvpZtr3x7OV9vn+NnAyaGfvxm2/qq2WHBuRHZ27d12jh4JPxcdYru5w/KJncTiwkmaWWBp2PIPdvY3CK3zAUmdxHJdF/U9lu8ObefoWK7dO/4N/xpaXgl8u8O673Zxro7n+Nd12Oao7w4dzsHpndS/1+dfDz30GBkPdQ+Xke7r1trKtl+stfU4XVZcOInF3vqztfa98AXG6X78UaAW58O5nXXuWN/Tx1jvAP4X5077/+DcfSw3xuw2xtzVsYtHqJXnx4BV1tqfdTh+E87dWINzgd3Rcmvt3zssuxsn8bekk/KNHRdYa4PW2kNhiz4Zev6RtfZgWDk/zt3mIE5ryM58NfS3OVZ34bT0+yS0t6A8B7jHWtvp5Eqh83kaTpev+8LXWWurcFpxxOLcRW5bXm2tLe+4L+t0GXsImGaMGdvJ4To7f0ft5zh0dv4+gZNs/L61dlOHY2/EOWfzjTEzOtnfD621+8PKV+F8WYsH/mit3Ry2rhm4H+dCc3o/Hf82a+2+sPJ+4G+hXzt7ffZkoM+/iMhw8f1OHteFrf8kTsLhKzaspZu1thSnFRx0/VnfV4/bsN4Lx2Avzk3Ldtba53BujPXqsyT0WfghYCdOsuUenBaC1aFuuh8zxrg7bPal0PNnQp+f4fv7O85N0Y92ccgvWWsbw8qXAo/j3DTtOKyKH+cGeceYwz/fTsRpVfm2tfaeDuXux2mdNxUnwdjRUde+fWGtfRMnYfrpUM8dcHpNHMK5qdiVmwlN5BN+LkJ+iNP69YjzZ63d2cnxgzgtLcFJVnfU2bVBi7W2tpvY+qKz7w7Hc+3+Zse/IYcngKrGaW0a7p+h53n9dPy+fnfoyUCffxGJEuoeLiNdZ91dCkPPaX3YzzudLJuGk8R5w1pb3cn6V3HGVOwVa60FvmWMaesaewKwAFiK8wXgemPM5+zh7sqLcRJ1XY0V5Q09T+9k3VHnxVrbaowp4cjzsgnn4vrqUCLwcZwL3FXW2pYOu1gQen65k31vM8YUAQXGmNQOF/FNwLpOYuw1a+1KY8x6nJkqf4Rzvlw4ibGuLAs9p3Rx/rJCz0ecP+MMyn9zaPtsnGRduDycL0TgfLn5MLDSGHM/Tveg5baTcbGOQ1fnr61+c7uoX1tXt+k4f+dwnf2/ORB6Xt3JurYEZ/4AHv9Y/t8OxvkXERk2rLVHjZ3YxhiThNM6br/tfPLCts//+f0UTmfXXn2x1nY+c3Uhhz+jemStfcUYMwVnfMvTcOp3Es612nnAtcaYD4Ru4hHadytwhTHmik526QOyjDEZ1tqKsOXV1todXcQLR37+3QP8AtgY+nx7DefzrazDtl1em4UtPzlUp9c7rDve8w9OT5hfAGcaY/bitEL9nbW2yRw9TGdb1+G5OL00bumsDE4L047XZhk4N/wvxBmyKaHDNnlhP7+Gc93yDWPMApzWrcvp+vVyrDo7f/167c7ha7POYu/s2mwwvjv0ZLDOv4hECSUtZUTreIc7pK1lQMc749052MmylNBzSR+26VEo5vtDD4wxCTjjMn4H+J0x5glrbQmQEdpkcejRlcROllV1UdZP2Hmx1gaMMWcC38MZW+r/QqtqjTH/wOny0zZOTtv5KO5i38U449+kdDh+aShhe7zuAn6LM/7V9cDqHloItJ2/c0KPrrSfP2PMh3BaVDbhtITdidP9J4jTfeY0nO7SAFhrHzHOgP9fxWmdcmNoP6txzl2ng/f3UVfnr61+nQ2CH66z10dnSXh/L9Z5w5Ydz/GrujlGr//fDtL5FxEZKXrzOQ9OK/v+cEzXUWGquljup4+TlYZa7b0RerSN+X0OTku3s3HGdP51qHgGznew7/ew20QOj5nZU7xw5PXZL40x5TjjM38JZ3xna4x5Dfgfe3iMyuP5mx3v+Qentd9PcG4m78VpxdfdDeW0UJksej5/QPsYje8CBTiJwn/idJf249TrZo68NqsxzuSJP8AZs7utFWa5MeYPOD2GjmrBegw6O3/Hc+3ep2sza60/lPTt7NpswL479GQQz7+IRAl1DxfpH50lhdouEHK62Ca3Xw5sbb219rs4LRxjODyTZdvxf2WtNd08zjjO4x+y1n7ZWjsGZxynT+OMf3QT8Mewom3xdFXvUR3KtR/ieOIL8y+criZ/wrmj/uceyrfFcXMP5y98wqYfAi3AImvtpdbar1prv2etvRXY2vEAANbap621Z+JciJ8F/ApnTMenOnSNDoIz430nu0ntph5dnb+2+s3toX7/6GL74xXp4wN9Ov8iItK9Y/mcD4aeu2pIkdLFcui/64N+Zx3P49xQhsOTxIFT/0M9fPYZa+3e44zhn9baE3ASURfhjG94KvCcMSY7LBbo+7UZ9MP5t05X9Udxuth/Eqeb+oZuNmmL472ezl/YNp/GSVj+wFq71Fr7eWvtd0LXZvd3EVeRtfZTOD1mZuEkfitwbtJ/L6xol69f08ks8B0P0039BvTavRuRPj7Qp/MvIiOAkpYiA2cL0ADMM8Z0dtF9ej8fr22Ml7YLtXdwLqZO6efjdMlau8Na+1ecFoV1OLP8tWlr1Xh6x+2MMZNwuqfstp23fu2P2KpwWkHm47R+/E8Pm6wIPffl/E0CNtmwMR0BjDEuOh8PKjy+emvty9bar+C0OvDhzKzepm180DGdbN6X8VfbHEv9+tNgHT9AL+7w9+L8i4hIN6wz1txOIM8YM7mTIm2JjjVhy7r8bAtdG6QeYzhB+tZjZqB0vDYD5/MvzRgzczACsNZWWWufsdZ+BmeSlXQOf/Z2eW3WYfmaLtb3h7twbrpn0X0rS6zTe2cjMNMYk97L/U8KPXc2TuZpPRzPWms3Wmt/x+FeN5eGFenva7NBv3aP0PHbunl3+3+0F+dfREYAJS1FBoh1ui7cAyTRYSIeY8wiuh5kvVPGmP/p6gLXGHMyzpcBP/B26PiloeMvMsZ8t7MWesaYicaYgr7E0WH7gi5iSsO5AA0fRPvu0PN3jDFt40ESGqD+dpz3o78eayy99B2cu/nn2R4G8rZO16k3gA8bYz7ZWRljzOyw1grgzO452RgzOqyMwenCdFSrPWPMWcaYuE523dY6N3ySoLaxj47oTm2MOQu4uru6dOFvOF15vm+MOWqAdGOMyxhz+jHsd6gdvwJnbLCjznMfz7+IiPTsbpwE3c/DJ6AxxmTizFbcVqbNFqAGuCT88zT03vzb44ijgs4TSf3KGHO+MebDockXO65LxOmWDUeOB/mr0PNd4dcLYdslhLrHHm9cnbVebTvHbZ9vy3F6gpxsjLm8wz4ux2mZuQ2nN89AeQXnJveHgPt6KAvwS5wbi3d31prRGJMWGguxzZ7Q8+kdys3Hme2+4/azjDHjOzluZ9cGq3CSfNeExtts20c6cMRENr0xGNfuQ+T4bcMeHDU5ZR/Pv4iMABrTUmRgfQuny+ktoUTlmzhdba7EGVj64j7s66PAz4wxW3Du0hfjDCQ+E6fbkcGZJfpA2DY34XTZvg34uDHmTZwxNkfjDKK9+P+zd99hcp71vf/f39nZvittVZesYrl3CxdwYpoBEzgmlMQmlPAjxyFAcjglCck5OSEnJOGcdAIBHBJKEiA0gwmmxRRjcO+yZdlqVtfuaiVt7/fvjxmJtayy0u7szO6+X9e118w8z30/z3ckl0cf3YVc4LX1NL/fxcCt+TUA15Nb8LuV3MNnOT9b45KU0k8jt4nQ7wDrI+LL5EY8Xk9u6sddwJ+fZh0TknI7Tm8/acOfeTO5Rej/MSJ+C7iXXNC2DLiIXN1XA2359n8NfBx4OCK+Qm6h/ReRCyy/QW5n0fH+ElgZET8k91A9BFxO7vfzWZ778P4pcovI/17kdjZ/ktxmNdeTm1r1Bk5BSml//g8ktwL3RMQd5EYvjJF7iLya3JSyqlO5bgne/w5y/5x/OyLuJLdA/6MppW9war/+kqST+wty/1+6AXg0Im4ntynhm8gFZv8v5XaNBo5s1PG35ALNhyPiVnJ/PrmO3DPFbk7PHcCNEfENchvEjQB3ppSO3kxmss4h9//+AxHxY+CZ/L2WkZuS3UDu2eEjhzuklO6IiPcDfwY8k/812kpuncAzyI3+u4vcGtyn6wvAQP65bxu5Z8SfI/f/wweB/8jXkiLi7eTW4f63iPg6uSD5bHIj2rqBt6Xcmp0FkVJKwG2n0P6fIuJycut1bo6Iwzu+N5GbBv7z5J6Z3pXv8llyz09/ExEvIfd7tBZ4DfBVcs/k470c+KuI+Cm5X4s2cr+fN5B7RjnyrJpS2hMR/wq8FXgkIr4JzCO34c+dnN6mU4V+di+F+/+A3K/ln0XEBeRHrKaUPsgp/PpLmhsMLaUCSil1RG436T8lF1itI/c32r9B7iHyVELLd5B7AH4pub8tXkTuIXQXuanOHxv/B4H8/bsi4lrgZnIB3BvIhUD7yD20/VdyD6qn6wFyD93Xknu4bgTayT0Qfzil9K2j6vndiHiY3APR28gFm5vJjYD8y/T8HceLKqW0M/9g/Jvkfu1+hdxUlr3kQsO/Ax4f1/4TETFIbmTF28mNNP0xud+7N/D80PJPyY0sWEfuIW2M3IP3nwJ/k1I6PO2IlFJb/vfyz8k9kF9L7tf/OnIP6acUWuaveUdEXAT8D3ILnf8cueBuN7mw9lhTqabMNN3/g+T+0PhacgFyGbmNEb7BKfz6S5JOLqU0FBHXAf+N3HPHb5IL8R4F3pdSOtbSLH9IbvTUfyb3vLKXXOj2AXL/rz0d/4XcmoEvIxcgZcht7DHVoeW/kBspeh25v8j9eXLh40HgEeBLwCePfr5JKf3fiPgJubX6riEXyBwi90x3C/C5Sdb1fnL/X72M3PcfIPeXcb9L7nnxyEYmKaV7I+IF5J7FXk7u/5cd5J4t/zildMw1uYsppfSeiPgWuWDy5eT+P99J7v/hf07u9+Vw290R8XPAh8j9Wr+SXBj2bnLh7dGh5XfIbZr08+R+X+aRGyjwPeCvUko/Par9fyb3XH0T8J58DR/O1/FLp/HdCv3sXvT7p5Q25MPy/0Hu9+HwX1B/kFP/9Zc0y0Wakk15JUmSJEmSJGlquKalJEmSJEmSpJJiaClJkiRJkiSppBhaSpIkSZIkSSophpaSJEmSJEmSSoqhpSRJkiRJkqSSki12AcXU0tKSVq5cWewyJElSAT344IMdKaXWYtehn/EZTJKk2c3nL02FOR1arly5kgceeKDYZUiSpAKKiGeLXYOey2cwSZJmN5+/NBWcHi5JkiRJkiSppBhaSpIkSZIkSSophpaSJEmSJEmSSoqhpSRJkiRJkqSSYmgpSZIkSZIkqaQYWkqSJEmSJEkqKYaWkiRJkiRJkkqKoaUkSZIkSZKkkmJoKUmSNIdFxPKI+EFEbIiIJyLivxyjTUTEhyNiU0Q8FhGXjTv3qojYmD/3/umtXpIkSbOVoaUkSdLcNgL895TSucBVwHsi4ryj2lwPrM3/3Ax8DCAiyoCP5s+fB9x0jL6SJEnSKTO0lCRJmsNSSntSSg/l33cDG4ClRzW7AfhsyrkHaIiIxcAVwKaU0paU0hDwhXxbSZIkaVIMLSVJkgRARKwELgXuPerUUmDHuM8788eOd1ySJEmaFENLSZIkERF1wFeA96WUuo4+fYwu6QTHj3X9myPigYh4oL29fXLFSpIkadYztJQkSZrjIqKcXGD5rymlrx6jyU5g+bjPy4DdJzj+PCmlW1JK61JK61pbW6emcEmSJM1ahpaSJElzWEQE8I/AhpTSXx2n2W3A2/K7iF8FHEop7QHuB9ZGxKqIqABuzLeVJEmSJiVb7AIkSZJUVC8C3go8HhGP5I/9PrACIKX0ceB24NXAJqAPeEf+3EhEvBf4DlAG/FNK6YlprV6SJEmzkqGlJEnSHJZSuotjr005vk0C3nOcc7eTCzUlSZKkKeP0cEmSJEmSJEklxdBSkiRJkiRJUkkxtJQkSZIkSZJUUgwtJUmSJEmSJJUUQ0tJkiRJkiRJJcXQUpIkSZIkSVJJMbSUJEmSJEmSVFIMLSVJkiRJkiSVFENLSZIkSZIkSSXF0FKSJEmSJElSSTG0lCRJkiRJklRSDC0lSZIkSZIklRRDS0mSJEmSJEklxdBSkiRJkiRJUkkxtJQkSZIkSZJUUgwtJUmSJEmSJJUUQ0tJkiRJkiRJJaWgoWVEvCoiNkbEpoh4/zHOR0R8OH/+sYi47GR9I+JNEfFERIxFxLqjrndRRNydP/94RFQV8vtJkiRJkiRJmnoFCy0jogz4KHA9cB5wU0Scd1Sz64G1+Z+bgY9NoO964PXAnUfdLwv8C/CulNL5wIuB4Sn/YpIkSZIkSZIKqpAjLa8ANqWUtqSUhoAvADcc1eYG4LMp5x6gISIWn6hvSmlDSmnjMe73CuCxlNKj+Xb7U0qjhflqkiRJkiRJkgqlkKHlUmDHuM8788cm0mYifY92FpAi4jsR8VBE/M5pVS1JkiRJkiSpqLIFvHYc41iaYJuJ9D1aFrgGeAHQB9wREQ+mlO54zg0jbiY3FZ0VK1ac5JKSJEmSJEmSplshR1ruBJaP+7wM2D3BNhPpe6z7/Sil1JFS6gNuBy47ulFK6ZaU0rqU0rrW1tYJfRFJkiRJkiRJ06eQoeX9wNqIWBURFcCNwG1HtbkNeFt+F/GrgEMppT0T7Hu07wAXRURNflOea4Enp/ILSZIkSZIkSSq8gk0PTymNRMR7yYWJZcA/pZSeiIh35c9/nNxoyFcDm8hN6X7HifoCRMQvAn8HtALfjIhHUkqvTCkdiIi/Ihd4JuD2lNI3C/X9JEmSJEmSJBVGpHSypSJnr3Xr1qUHHnig2GVIkqQCyq9xva7YdehnfAaTJGl28/lLU6GQ08MlSZIkSZIk6ZQZWkqSJEmSJEkqKYaWkiRJkiRJkkqKoaUkSZIkSZKkkmJoKUmSJEmSJKmkGFpKkiRJkiRJKimGlpIkSZIkSZJKiqGlJEmSJEmSpJJiaClJkiRJkiSppBhaSpIkSZIkSSophpaSJEmSJEmSSoqhpSRJkiRJkqSSYmgpSZIkSZIkqaQYWkqSJEmSJEkqKYaWkiRJkiRJkkqKoaUkSZIkSZKkkmJoKUmSJEmSJKmkGFpKkiRJkiRJKimGlpIkSZIkSZJKSrbYBUg6vs/du31C7d585YoCVyJJkiRJkjR9HGkpSZIkSZIkqaQYWkqSJEmSJEkqKYaWkiRJkiRJkkqKoaUkSZIkSZKkkmJoKUmSJEmSJKmkuHu4JEnSHBcR/wS8BmhLKV1wjPO/DfxK/mMWOBdoTSl1RsQ2oBsYBUZSSuump2pJkiTNZo60lCRJ0qeBMdyafQAAeJFJREFUVx3vZErpz1NKl6SULgF+D/hRSqlzXJOX5M8bWEqSJGlKGFpKkiTNcSmlO4HOkzbMuQn4fAHLkSRJkgwtJUmSNDERUUNuROZXxh1OwHcj4sGIuLk4lUmSJGm2cU1LSZIkTdRrgZ8cNTX8RSml3RGxAPheRDyVH7n5HPlA82aAFStWTE+1kiRJmrEcaSlJkqSJupGjpoanlHbnX9uAW4ErjtUxpXRLSmldSmlda2trwQuVJEnSzGZoKUmSpJOKiPnAtcDXxx2rjYj6w++BVwDri1OhJEmSZhOnh0uSJM1xEfF54MVAS0TsBP4QKAdIKX083+wXge+mlHrHdV0I3BoRkHuu/FxK6dvTVbckSZJmL0NLSZKkOS6ldNME2nwa+PRRx7YAFxemKkmSJM1lTg+XJEmSJEmSVFIMLSVJkiRJkiSVFENLSZIkSZIkSSXF0FKSJEmSJElSSTG0lCRJkiRJklRSDC0lSZIkSZIklRRDS0mSJEmSJEklxdBSkiRJkiRJUkkxtJQkSZIkSZJUUgwtJUmSJEmSJJUUQ0tJkiRJkiRJJSVb7AKkWeuBT036Emu2dx55v3nFmyZ9PUmSJEmSpJnAkZaSJEmSJEmSSoqhpSRJkiRJkqSSUtDQMiJeFREbI2JTRLz/GOcjIj6cP/9YRFx2sr4R8aaIeCIixiJi3TGuuSIieiLifxTum0mSJEmSJEkqlIKFlhFRBnwUuB44D7gpIs47qtn1wNr8z83AxybQdz3weuDO49z6r4FvTd03kSRJkiRJkjSdCrkRzxXAppTSFoCI+AJwA/DkuDY3AJ9NKSXgnohoiIjFwMrj9U0pbcgfe94NI+J1wBagt0DfSZIkSZIkSVKBFXJ6+FJgx7jPO/PHJtJmIn2fIyJqgd8F/ug065UkSZIkSZJUAgoZWj5/KCSkCbaZSN+j/RHw1ymlnhMWFXFzRDwQEQ+0t7ef5JKSJEmSJEmSplshp4fvBJaP+7wM2D3BNhUT6Hu0K4E3RsT/AxqAsYgYSCl9ZHyjlNItwC0A69atO1kQKkmSJEmSJGmaFTK0vB9YGxGrgF3AjcCbj2pzG/De/JqVVwKHUkp7IqJ9An2fI6X0c4ffR8QHgJ6jA0tJkiRJkiRJpa9goWVKaSQi3gt8BygD/iml9EREvCt//uPA7cCrgU1AH/COE/UFiIhfBP4OaAW+GRGPpJReWajvIUmSJEmSJGl6FXKkJSml28kFk+OPfXzc+wS8Z6J988dvBW49yX0/cBrlSpIkSZIkSSoBhdyIR5IkSZIkSZJOmaGlJEmSJEmSpJJiaClJkiRJkiSppBhaSpIkSZIkSSophpaSJEmSJEmSSoqhpSRJkiRJkqSSYmgpSZIkSZIkqaQYWkqSJEmSJEkqKYaWkiRJkiRJkkqKoaUkSZIkSZKkkmJoKUmSJEmSJKmkGFpKkiRJkiRJKimGlpIkSZIkSZJKiqGlJEmSJEmSpJJiaClJkiRJkiSppBhaSpIkSZIkSSophpaSJEmSJEmSSoqhpSRJkiRJkqSSYmgpSZIkSZIkqaQYWkqSJEmSJEkqKYaWkiRJkiRJkkpKttgFSHPVvVs7i12CJEmSJElSSXKkpSRJkiRJkqSSYmgpSZIkSZIkqaQYWkqSJEmSJEkqKYaWkiRJkiRJkkqKoaUkSZIkSZKkkmJoKUmSJEmSJKmkGFpKkiTNcRHxTxHRFhHrj3P+xRFxKCIeyf/873HnXhURGyNiU0S8f/qqliRJ0mxmaClJkqRPA686SZsfp5Quyf/8H4CIKAM+ClwPnAfcFBHnFbRSSZIkzQmGlpIkSXNcSulOoPM0ul4BbEopbUkpDQFfAG6Y0uIkSZI0JxlaSpIkaSKujohHI+JbEXF+/thSYMe4NjvzxyRJkqRJyRa7AEmSJJW8h4AzUko9EfFq4GvAWiCO0TYd6wIRcTNwM8CKFSsKVKYkSZJmC0daSpIk6YRSSl0ppZ78+9uB8ohoITeycvm4psuA3ce5xi0ppXUppXWtra0Fr1mSJEkzm6GlJEmSTigiFkVE5N9fQe4Zcj9wP7A2IlZFRAVwI3Bb8SqVJEnSbOH0cEmSpDkuIj4PvBhoiYidwB8C5QAppY8DbwR+IyJGgH7gxpRSAkYi4r3Ad4Ay4J9SSk8U4StIkiRpljG0lGaIoZExtnf2Ma8qy4J5VcUuR5I0i6SUbjrJ+Y8AHznOuduB2wtRlyRJkuYuQ0upxA2NBX+9ZSmPPPQEYwmqyjO858Vn0lxXWezSJEmSJEmSCsI1LaUSd0dHAw8dquOq1c3c+ILlBMG/3PssgyOjxS5NkiRJkiSpIAwtpRI2OBbcuqeZ8+p6ec1FS7hoWQM3XrGctq5BvvrQLnLLiUmSJEmSJM0uhpZSCftueyOHRrL88tKOI8fWLqjnuvMW8viuQ2zv7CtidZIkSZIkSYXhmpZSiRoYDb6+t4mL5vVwTl0/m8edu3pNMz96up37tnZyRnPthK73uXu3T6jdm69ccRrVSpIkSZIkTR1HWkol6ied8+geyfKmxR3PO1eZLeOS5Q08vusQ/UOubSlJkiRJkmYXQ0upRD3WXUtz+TBraweOef6KVU2MjCUe2n5gmiuTJEmSJEkqLENLqQSNJXiiq4YL5vURcew2i+dXs7yxmvu2dbohjyRJkiRJmlUMLaUStL2/ku7RLBfU956w3RWrmmjvHuT+bY62lCRJkiRJs4ehpVSCHu/Oba5zQf2Jdwe/cGkD2Uzw7fV7p6MsSZIkSZKkaWFoKZWg9V01LKkapKli5ITtKrIZVrfW8oONbdNUmSRJkiRJUuEVNLSMiFdFxMaI2BQR7z/G+YiID+fPPxYRl52sb0S8KSKeiIixiFg37vh1EfFgRDyef31pIb+bVCgjY7Chp4YLTzLK8rCzF9aztaOXrR0nnkouSZIkSZI0UxQstIyIMuCjwPXAecBNEXHeUc2uB9bmf24GPjaBvuuB1wN3HnWtDuC1KaULgbcD/zzV30maDpv6qhkcy5x0PcvDzl40D4AfOtpSkiRJkiTNEoUcaXkFsCmltCWlNAR8AbjhqDY3AJ9NOfcADRGx+ER9U0obUkobj75ZSunhlNLu/McngKqIqCzMV5MK5/GuGoLEeRMcadlUW5GfIt5e4MokSZIkSZKmRyFDy6XAjnGfd+aPTaTNRPqeyBuAh1NKg0efiIibI+KBiHigvd2QR6VnY08NZ1QPUpcdm3Cfl569gHu27Kdv6MRrYEqSJEmSJM0EhQwt4xjH0gTbTKTvsW8acT7wf4FfP9b5lNItKaV1KaV1ra2tE7mkNK2291eysmbglPq85JwFDI2M8dNN+wtUlSRJkiRJ0vQpZGi5E1g+7vMyYPcE20yk7/NExDLgVuBtKaXNp1GzVFRdw2UcGsmyvPp5g4RPaN3KRmoryvi+61pKkiRJkqRZoJCh5f3A2ohYFREVwI3AbUe1uQ14W34X8auAQymlPRPs+xwR0QB8E/i9lNJPpvi7SNNi+0BuGdYVpxhaVmbLuGp1M/dsdqSlJEmSJEma+QoWWqaURoD3At8BNgBfTCk9ERHvioh35ZvdDmwBNgH/ALz7RH0BIuIXI2IncDXwzYj4Tv5a7wXOBP4gIh7J/ywo1PeTCmFHfy60PNWRlgBXrGpiS0cv7d2n3leSJEmSJKmUZAt58ZTS7eSCyfHHPj7ufQLeM9G++eO3kpsCfvTxDwIfnGTJUlHt6K+kvmyEhuzo887V9z5LQ9dG9rZczXB5/fPOX7GqCYD7t3Xy6gsXF7xWSZIkSZKkQiloaCnp1Gzvr2R59SAxfiuqlFi551tc+cT/AWCkrJq7L/wgOxa/4jl9L1g6n+ryMu7bamgpSZIkSZJmtkKuaSnpFIwl2NFfwfLqoeccX9p+JwsPPMDTK27kP674Rw7Un8ULH/1dFuy/7zntyssyXHZGA/dt7ZzOsiVJkiRJkqacoaVUIjqGyhkYK2NF9cCRY5VDB1jS8WM65l/AA+f9Pm3NV/DDdX9PT80yrnr8f5MZfe76lVesbGbD3i4O9Q9Pd/mSJEmSJElTxtBSKhE/24TnZyMtl7X9EMiwfeHLOTxnfLh8Hg+c9z+p69/FOdv++TnXeMGqRlKCh549MF1lS5IkSZIkTTlDS6lEbD9q5/Dy4S6aD61nb9MLGC6f95y2+1quYlfrtZy79VOUjfYfOX7p8kbKy4J7nSIuSZIkSZJmMENLqUTs6K+gtWKYmrIxAFoOPkaQaGu8/Jjtn1z9DiqHu1i569+PHKuuKOOiZQ3ct3X/tNQsSZIkSZJUCO4eLpWIwzuHA5ASrQcfo6tmOYOVTQCs2f6l53ZIid6qRVy46WNwf+OR6eMvqKrlH5+uYeDeT1FV9rPma7b/bPTl5hVvKuh3kSRJkiRJmgxHWkolYDTB7sFKllflQsuagb1UD3XQ0XDx8TtFsK9xHTWD7XBox5HDlzUPM5yC9QfKC122JEmSJElSQRhaSiVg/1A5oylYXJXbhKeh5xkADtSffcJ+nfPPZSzKYNeDR45d0jQCwCOdDqSWJEmSJEkzk6GlVAL2DeZGRbZWDgPQ0L2JnqoljGRrT9hvtKyag3Vnwu6HIeXWwlxQNcbSmlEe7nSkpSRJkiRJmpkMLaUS0JYPLRdWDFE20k9d/y4O1p85ob77518Ag13QueXIsUuahnnE0FKSJEmSJM1QhpZSCdg3VEEZieaKEeb3biFIHKqbWGh5sG4tZMpg3/ojxy5tGmZXXxntA1GokiVJkiRJkgrG0FIqAW2D5bRUDpMJmNf7LKOZCnqql0yo71hZBTSvhb3rISVg/LqWjraUJEmSJEkzj6GlVALaBstZWJFbz7Kufwc91UshTuFfz4UXQF8H9LYBcEHjMNlIPLzf0FKSJEmSJM08hpZSCWgbLGdB5RBlowPUDLTRXbP81C6w8Pzc674nAagqg3PmjzjSUpIkSZIkzUiGllKR9Y1m6B7NsqBymLr+nQSJ7poVp3aR6kaoXQD7nzly6JKmYR47kGU0TXHBkiRJkiRJBWZoKRXZ4Z3DF1QOU9e3g0Tkpoefqpa1sH8zjI0CuXUte0YybO4qm8pyJUmSJEmSCs7QUiqyw6Hlwooh6vp301e5gLGyylO/UPNaGB2EQzuA3EhLcDMeSZIkSZI08xhaSkXWNpQfaVkxRG3/HnqrF5/ehVrOzL125KaIr64fpS47xuMHs1NRpiRJkiRJ0rQxtJSKrG2wgtqyURo5RPloH31Vi07vQhV1UL/kyLqWmYDzG0Z4zJGWkiRJkiRphjG0lIps32A5rRXD1A7sATj9kZaQW9eycyuM5qaGX9Q4woZDWYbHpqJSSZIkSZKk6eG8UanI2obKWV41SG3/XhLQV7Xw9C/Wsha2/ggObIOWtVzYNMzQMzU83VXAf9Uf+NTUXm/dO6b2epIkTdSuByElWLau2JVIkiTNeY60lIpoLEH7YDkLKoepGdjLQGULY5mK079g0xogYP8mIDfSEuDxA/79hCRJJ/UPL4VPvqzYVUiSJAlDS6moDg5nGU4ZFlQOUzuwl97TXc/ysPJqaFgOHU8DcEbtKPXlYzx2wHUtJUmSJEnSzGFoKRXRvsFcmLg020Pl8CH6K1snf9GmM+HQdhgdIQIubBhhvSMtJUmSJEnSDGJoKRVRx1AutFwduwDor1ww+Ys2rYSxUejaAcCFjcNsOJhleCwmf21JkiRJkqRpYGgpFdH+4dwIyMWjuZ3D+6ZipGXjqtxr5zYALmoaYTgFO/onsVamJEmSJEnSNDK0lIqoc6ic2rJR5g23MRZZBisaJn/RynqoaYYDWwG4qHEYgM191ZO/tiRJkiRJ0jQwtJSKqHM4S2P5CNWD7fRXtkBM0b+SjavgwDZIiWU1Y8wvH2NrX9XUXFuSJEmSJKnADC2lIuocytJcMUz1QPvUbMJzWONKGOyC/k4i4KLGETb3GlpKkiRJkqSZwdBSKqLO4SxLst1UjnRNzXqWhzUdXtcyN0X8wsZhdvRXMuRmPJIkSZIkaQYwtJSKZCTBweEsZ5XlNuEZqGyZuovXL4ayytwUcXKb8YwSbO+vnLp7SJJmjYj4p4hoi4j1xzn/KxHxWP7npxFx8bhz2yLi8Yh4JCIemL6qJUmSNJsZWkpFcnA4SyJYFfnQsqJ56i4eGWg840hoeeHhzXicIi5JOrZPA686wfmtwLUppYuAPwZuOer8S1JKl6SU1hWoPkmSJM0xhpZSkXQOZQFYlvaRgIGKxqm9QeNK6NoFI4MsqR6jPjviZjySpGNKKd0JdJ7g/E9TSgfyH+8Blk1LYZIkSZqzDC2lIukcLgdgwVgbg+UNpEx2am/QuApIcPBZImBNzQCbDS0lSZP3TuBb4z4n4LsR8WBE3FykmiRJkjTLTHFKImmi9udHWjaMtDNQOYVTww9rOCP3euBZaDmL1TUDPLa3mUE345EknaaIeAm50PKacYdflFLaHRELgO9FxFP5kZtH970ZuBlgxYoV01KvJEmSZi5HWkpFcmA4S3mMUju8f2rXszysogZqF8DBZwFYXTvAGMGzfW7GI0k6dRFxEfBJ4IaU0v7Dx1NKu/OvbcCtwBXH6p9SuiWltC6ltK61tXU6SpYkSdIMZmgpFcn+oXLOquigbGyoMKEl/GwznpRYXTMAwBaniEuSTlFErAC+Crw1pfT0uOO1EVF/+D3wCuCYO5BLkiRJp8Lp4VKRdA5nuSi7G0ahv7KpMDdpOAN23g/9nTSVB/OzI2zpq2ZtYe4mSZqhIuLzwIuBlojYCfwhUA6QUvo48L+BZuDvIwJgJL9T+ELg1vyxLPC5lNK3p/0LSJIkadYxtJSKpHMoy5qqvTAKg1O9c/hhjStzrweeJWIla2oH2NzrSEtJ0nOllG46yflfA37tGMe3ABcXqi5JkiTNXU4Pl4ogpdxIyzOinUQwVD6/MDeqXwyZcji4DYDVNQPsGqhgaGSsMPeTJEmSJEmaAoaWUhF0DgUjKcNi2hkqn0eKssLcKFMGDctzO4iTCy0TwZ5D/YW5nyRJkiRJ0hQwtJSKYG9/LqRsTfsZLG8o7M0azoCuncTYCKvym/HsPGBoKUmSJEmSSpehpVQEe/ty/+o1jHYyWNFQ2Js1roSxUWoG9tFUMUJj+TC7DhpaSpIkSZKk0mVoKRXB3oEMlQxRM9o1PSMtgbr+nQCsqRlglyMtJUmSJElSCTO0lIpgb18Zy6IdKODO4YdVN0DVfOr6dwGwunaAjp5BBodHC3tfSZIkSZKk02RoKRXB3oEM52T3AjBQ6JGWAA1nUNeXDy1rBkjA7kMDhb+vJEmSJEnSaShoaBkRr4qIjRGxKSLef4zzEREfzp9/LCIuO1nfiHhTRDwREWMRse6o6/1evv3GiHhlIb+bNBlt/RnWZvcBFH5NS4DGlVQNHyA70ntkM55dB/oKf19JkiRJkqTTkC3UhSOiDPgocB2wE7g/Im5LKT05rtn1wNr8z5XAx4ArT9J3PfB64BNH3e884EbgfGAJ8B8RcVZKyTmwKjntgxnOyLQxNlbGcLZ+0te7d2vnCc/X9zdyHlDXv4uR+lrmV5ez0814JEmSJElSiSrkSMsrgE0ppS0ppSHgC8ANR7W5AfhsyrkHaIiIxSfqm1LakFLaeIz73QB8IaU0mFLaCmzKX0cqOe0DGZbRntuEJ6Lg9+utXkIijkwRX9pQzW5DS0mSJEmSVKIKGVouBXaM+7wzf2wibSbS93TuJxXdaIL9AxkW0TE9U8OBsUw5fVULj+wgvrSxmo6eIQbcjEeSJEmSJJWgQoaWxxo+libYZiJ9T+d+RMTNEfFARDzQ3t5+kktKU69zMBgjaB7bnxtpOU16qpdS278bUmJpQzUAuxxtKUmSJEmSSlAhQ8udwPJxn5cBuyfYZiJ9T+d+pJRuSSmtSymta21tPcklpanXMZChjj5qUt+0jbQE6KlZRnZskOrBjp+FlgcMLSVJkiRJUukpZGh5P7A2IlZFRAW5TXJuO6rNbcDb8ruIXwUcSintmWDfo90G3BgRlRGxitzmPvdN5ReSpkL7QBnLIzfKd7C8cdru21OdWy2hrn8ntZVZGmvKHWkpSZIkSZJKUsF2D08pjUTEe4HvAGXAP6WUnoiId+XPfxy4HXg1uU1z+oB3nKgvQET8IvB3QCvwzYh4JKX0yvy1vwg8CYwA73HncJWi9sEMy6MNgIFpHGk5UNHMSKaK2v7cZjxLGqoNLSVJkiRJUkkqWGgJkFK6nVwwOf7Yx8e9T8B7Jto3f/xW4Nbj9PkT4E8mUbJUcO0DmaKMtCSCnuolR3YQX9ZQzRO7u+gfGqW6omz66pAkSZIkSTqJQk4Pl3QM7QMZVmXaGMlUMlpWNa337qlZRs1gG9mRPpY21gBuxiNJkiRJkkqPoaU0zdoHMqwqa8/tHB7H2vS+cHqqlxIkmg494Q7ikiRJkiSpZBlaStOsfSDDksx+hsrnTfu9e/Ob8TQffIzqijKaaivYdaBv2uuQJEmSJEk6EUNLaZq1D2RoSZ1FCS1HsjX0VzTRcugxAJa6GY8kSZIkSSpBhpbSNOsaGKE+9RQltITcaMuWg49BSixtqOZA3zC9gyNFqUWSJEmSJOlYDC2laTQ4CtUjBwGKFlr2VC+jerCDmoG9LG10XUtJkiRJklR6DC2ladQxmGFJ7AdgKFuc0LK7JreuZcvBR49sxrPb0FKSJEmSJJUQQ0tpGrUPZFhEJ1C8kZZ9VYsYLqthQecDVJWX0VJXwY5ON+ORJEmSJEmlw9BSmkbtAxkWHx5pWaTQksjQ3ngpCzofBGBFUy3bO/tIKRWnHkmSJEmSpKMYWkrTKBdadjKWrWEsU160Otqa1tHQs4nKoQOsaKqhd2iUzt6hotUjSZIkSZI0nqGlNI2OjLSsbihqHW1N6wBo7XyQFc01AGx3irgkSZIkSSoRhpbSNGofyLAs00mmyKFl5/zzGclUsbDzARbUV1KZzfCsoaUkSZIkSSoRhpbSNOoYLI2RlmOZcjoaL2ZB5wNkIljRVMP2/YaWkiRJkiSpNBhaStPoUP8I8+mBqoZil0Jb4+U0dD9N+fAhVjTVsK9rgIHh0WKXJUmSJEmSZGgpTacYOJR7U+SRlpBb1zJILOh8iBXNNSRg54H+YpclSZIkSZI0sdAyIr4SEb8QEYac0iSUDx3IvSmBkZYdDRcxmqlgQecDLG+sIYBnO3uLXZYkaZJ8bpMkSdJskJ1gu48B7wA+HBFfAj6dUnqqcGVJs0/fCDSPdeY+VDVCd3HrGSurpKPhIhZ23k9VeRkL51VNeF3Lz927/cj7Nds7j9nmylVNU1KnJOmU+dwmSZKkGW9CfwOfUvqPlNKvAJcB24DvRcRPI+IdEVFeyAKl2WL/YIZFkQ/4qucXt5i8vc1X09S1garBjtxmPJ19jI6lqb3JYDds+SFs+AbsexLS2NReX5L0HD63SZIkaTaY8LShiGgGfhX4NeBh4G/JPQx/ryCVSbPMgcEMS2I/Q2W1UFZR7HIA2N36IgAWddzNqpZaBkfGeGL3oam7wfZ74Pt/DE9+DTZ/H+6/Be75GIwMTN09JEnP43ObJEmSZrqJrmn5VeDHQA3w2pTSf0op/VtK6TeBukIWKM0Wh0dajlQ2FLuUIw7MO5eBiiYWd/yUVa21ANy9ef/UXHzzD+CxL0DjSnjx78Gr/xwufBN0bob7boExdyqXpELwuU2SJEmzwURHWn4ypXReSunPUkp7ACKiEiCltK5g1UmzSOdQhiXRCdWNxS7lZyLD3uarWNzxU+ZVltFaV8ndWyYfWjZ2PQUbvg6LL4Yrfh3qFkImC2e8CC6+CTq3wJYfTMEXkCQdg89tkiRJmvEmGlp+8BjH7p7KQqTZrnMwWBT7ydaUxnqWh+1acC1VQ520HHyM1a213L+1k+HR0193smLoIGt2fQ3mr4BLfgUyZc9tsHQdLLoInv4W9B+YXPGSpGPxuU2SJEkz3glDy4hYFBGXA9URcWlEXJb/eTG5KUeSJuhQ/zBN0UN5TUOxS3mO3a0/x2hkWbbv+6xuraN3aJTHd53mupYpsXr3bbn3l7/92Gt3RsD5vwgp5da5lCRNCZ/bJEmSNJtkT3L+leQWcV8G/NW4493A7xeoJmlWGu3vAiCqSmuk5XB5PW3NV7B83x2sWvmbQG5dy8tWnPo09taDjzC/dxtbFr+G1TXNx29Y3QjL1uU26ln7CqisP93yJUk/43ObJEmSZo0ThpYppc8An4mIN6SUvjJNNUmz00AutKTEQkuAHQteyhVPfpClw1s5e2E992zZz3tecuYpXaNspJ/l++6gu2Y57Y2X0r6184TtqyrXcfHYfbDjXjjz5ZMpX5KEz22SJEmaXU4YWkbEW1JK/wKsjIj/dvT5lNJfHaObpGPIDuWnXFfNK24hx7Bz0ctYt+HPWLn7dq5e807+7f4dDI2MUZGd6LK3sLzt+2RH+9m6+NW5KeAnMVDZQnfNcup33gdrXjahPpKk4/O5TZIkSbPJyRKJ2vxrHVB/jB9JE1Q5nA8tK0tvpOVAZQt7W65m5e5vcvXqRvqHR3l4+8Q3yant382CAw+yt+kK+qsWTrhfe8Ml0NMGB589jaolSUfxuU2SJEmzxsmmh38i//pH01OONHvVjh5kOFNOeXl1sUs5pq1LXsuLHv1dfq58I+Vlwfc3tnHl6hOsS3lYGmPl7m8ynK1j14IXn9I9O+edx+q934ZdD0HjytOqW5KU43ObJEmSZpMJzf2MiP8XEfMiojwi7oiIjoh4S6GLk2aLoTFoTAfpy84v2WnQOxe+hKFsPTWPfYYrVzVzx4a2CfVbcOBB6gb28OyiVzBaVnlK9xwtq4TWs2Df47ndxCVJk+ZzmyRJkmaDiS5Y94qUUhfwGmAncBbw2wWrSpplDgxmWMhBBssbil3KcY2WVbN5+evhydt47coxNrX18Oz+3hP2qRrsYPm+73OodhWd884/vRsvvBD6D0D37tPrL0k6ms9tkiRJmvEmGlqW519fDXw+pXTibYElPcf+wQwL4gCjJbie5XhPr7gJSLyq7+sAJx1teelTf0kmDbNt8fWnP4J04flAwN7HT6+/JOloPrdJkiRpxptoaPmNiHgKWAfcERGtwEDhypJmts/du517t3Ye+fnJsz0siIN0jlYeOVaKemuWwgVvYP5jn+KqlgG+/9TxQ8ul+77Pqt3/zp7mFzFQ2XL6N62sh8YzoO3J07+GJGk8n9skSZI0400otEwpvR+4GliXUhoGeoEbClmYNJsMDA1TH/2Mlc+AzVtf+r8gjfE/q77MvVv30z0w/Lwm1QP7uPLxD9BZfw67Wn9+8vdsORsO7oDhvslfS5LmOJ/bJEmSNBtMdKQlwLnAL0fE24A3Aq8oTEnSLDSUD+Mqaotbx0Q0roSr38uFHbfz8nQPdz7d8dzzgz1c++BvUjY2xE8v+b+kTNnk79lyFpBg/6bJX0uSBD63SZIkaYbLTqRRRPwzsAZ4BBjNH07AZwtTljS7ZIZ6cq+VMyC0BHjx75G23slf7PoEn7p3GVx0c+541x74wk00dG3kR+s+Qlfdalo7H5z8/RrPgLIK6Hhm8teSpDnO5zZJkiTNBhMKLcmtiXReSikVshhptiof6QZgZCZMDwfIVhC//M90//1r+I2dv8PwP3+H8qp5sPFbEMGPL/tb9rT+3NTdL5OFptXQsXHqrilJc9cpP7dFxD+R2228LaV0wTHOB/C35Db36QN+NaX0UP7cq/LnyoBPppQ+NPmvIEmSpLluotPD1wOLClmINJtVjeZGWg5n64pcySmYt4S2N93GJ0Zfy+Cep2DXA3D+6+Bdd7Fr4Yun/n7NZ0JPG/R2nLytJOlETue57dPAq05w/npgbf7nZuBjABFRBnw0f/484KaIOO8U7y1JkiQ9z0RHWrYAT0bEfcDg4YMppf9UkKqkWaZ29BADVDCaqSx2KafkwtVLeV/jO/lh7bv54ruu/tmJTdun/mZNq3OvO+6Fc35h6q8vSXPHKT+3pZTujIiVJ7jmDcBn86M374mIhohYDKwENqWUtgBExBfybZ+c9LeQJEnSnDbR0PIDhSxCmu3mpS4OxHyIKHYppyQieMNly/jz72xk+/4+VjTXFO5m85dDpgy232NoKUmT84ECXHMpsGPc5535Y8c6fmUB7i9JkqQ5ZkLTw1NKPwK2AeX59/cDDxWwLmlWaUwH6Yr5xS7jtLzu0qVEwK0P7yrsjcrKc8HljnsLex9JmuUK9Nx2rL91Syc4/vwLRNwcEQ9ExAPt7e2TLEeSJEmz3YRCy4j4z8CXgU/kDy0FvlagmqRZZSxBczpIX9m8YpdyWpY2VHP16ma+/NAORscKvBdX4yrY/TAMDxT2PpI0ixXouW0nsHzc52XA7hMcf56U0i0ppXUppXWtra2TLEeSJEmz3UQ34nkP8CKgCyCl9AywoFBFSbNJz3CGhXGQ/uzMDC0B3nb1Gezo7Odb6/cU9kaNq2B0CPY+Xtj7SNLsVojnttuAt0XOVcChlNIecqM410bEqoioAG7Mt5UkSZImZaKh5WBKaejwh4jIcpypP5Keq294hJoYZDhbW+xSTtt15y1idUstH/vhZnJ7MBRIw4rc625Xn5CkSTjl57aI+DxwN3B2ROyMiHdGxLsi4l35JrcDW4BNwD8A7wZIKY0A7wW+A2wAvphSemKqv5AkSZLmnoluxPOjiPh9oDoiriP3oPqNwpUlzR6jg7251/L6Ildy+soywa9fu5rf/crj3LWpo3A3qpoPdQthl6GlJE3CKT+3pZRuOsn5RG4E57HO3U4u1JQkSZKmzERHWr4faAceB36d3IPp/ypUUdJskob6cm8qZu5IS8htyLNwXiUf++Hmwt0kApZcllvXUpJ0unxukyRJ0ow3oZGWKaWxiPga8LWUkts9SqegbLg791o5s0PLymwZ77xmFX96+1NcsGQ+K1sK9H2WXApPfxsGu6Fy5o5OlaRi8blNkiRJs8EJR1rmF1v/QER0AE8BGyOiPSL+9/SUJ8185cM9AFRWVhe5ksl7y1VnsHh+Fd94bDdjhVrbcullQILdjxTm+pI0S/ncJkmSpNnkZNPD30du98kXpJSaU0pNwJXAiyLivxa6OGk2qBzppi9VEuWVxS5l0moqsvz+q89lz6EB7t/WWZibLL4k97r3scJcX5Jmr/fhc5skSZJmiZOFlm8DbkopbT18IKW0BXhL/twJRcSrImJjRGyKiPcf43xExIfz5x+LiMtO1jcimiLiexHxTP61MX+8PCI+ExGPR8SGiPi9k399qfBqxrrZT0Oxy5gyr7loMataavnuE/voGxyZ+hvUteY249nn5rOSdIom9dwmSZIklZKThZblKaXnbRWcXx+p/EQdI6IM+ChwPXAecFNEnHdUs+uBtfmfm4GPTaDv+4E7UkprgTvynwHeBFSmlC4ELgd+PSJWnuT7SQVXO9bNwZhX7DKmTETw2ouXMDgyyjcf31OYmyw8H/Y+XphrS9LsddrPbZIkSVKpOVloOXSa5wCuADallLaklIaALwA3HNXmBuCzKeceoCEiFp+k7w3AZ/LvPwO8Lv8+AbURkQWq8/V1naRGqeDmpy56MrNrQ5lF86q49qxWHt5xkHsPFOC7LbwA2p+C0QKM5JSk2Wsyz22SJElSSTlZaHlxRHQd46cbuPAkfZcCO8Z93pk/NpE2J+q7MKW0ByD/uiB//MtAL7AH2A78RUqpQIvuSRPXmLronWWhJcBLz1nI0oZqbnl2EZ1D2am9+MILYHQI9j8ztdeVpNltMs9tkiRJUkk5YWiZUipLKc07xk99Sulk04ziWJecYJuJ9D3aFcAosARYBfz3iFj9vKIibo6IByLigfb29pNcUpqk0RHmRy8DZbMvtCzLBL+0bjlDY8HHn13ElG4mvuiC3Ove9VN4UUma3Sb53CZJkiSVlJONtJyMncDycZ+XAbsn2OZEffflp5CTf23LH38z8O2U0nBKqQ34CbDu6KJSSreklNallNa1trae1heTJmpkqA+A4fK6IldSGK31lbxlWRuPdtXxnfaGqbtw81rIlMM+Q0tJkiRJkuaiQoaW9wNrI2JVRFQANwK3HdXmNuBt+V3ErwIO5ad8n6jvbcDb8+/fDnw9/3478NL8tWqBq4CnCvXlpIkYHuwHYDRbU+RKCucVrQe5ZF4P/7JzATv7K6bmotkKaD3b0FKSJEmSpDmqYKFlSmkEeC/wHWAD8MWU0hMR8a6IeFe+2e3AFmAT8A/Au0/UN9/nQ8B1EfEMcF3+M+R2G68D1pMLPT+VUnqsUN9Pmoix/EhLKmqLW0gBRcBvrNxDVdkYH9m2hJGxKbrwwgtg3xMnbydJkiRJkmadKd4947lSSreTCybHH/v4uPcJeM9E++aP7wdedozjPcCbJlmyNKViuBeAsorZO9ISoKF8lF8/Yy9/sXkZX97Two1LOyZ/0YXnw2NfgN79UNs8+etJkiRJkqQZo5DTw6U5rywfWlZUVhW5ksJ7QUMPL2k+yNf2NvNUT/XkL3h4Mx6niEuSJEmSNOcYWkoFVDHSw/5UT/0ULfVY6t6+vI0FFcN8dOti+kYn+Z+XhYaWkiRJkiTNVYaWUgFVjfbQkeZTmUnFLmVaVJeN8Z5Ve2gfKueLu1smd7G6BVC7wHUtJUmSJEmagwwtpQKqHevmYMwvdhnT6uy6fl7acojvtjeyb7B8chdbeL4jLSVJkiRJmoMMLaUCqh/roivqi13GtHvj4g6ykfjCrtbJXaj1bOh4BsamaktySZIkSZI0ExhaSoWSEg100ZuZe6FlU8UIv7Cgk58emMem3klsQtRyFgz3QdfOqStOkiRJkiSVPENLqUDKxoaoYoj+srkXWgK8dlEn87Ijkxtt2Xp27rX96akpSpIkSZIkzQiGllKBlI/0ADBYVlfkSoqjpmyM1yzs5PHuWrb1VZ7eRVryoWXHxqkrTJIkSZIklTxDS6lA0lAvACPltUWupHhe1nKQyswY32prPL0L1LZAdRO0G1pKkiRJkjSXGFpKBTIy2AdAqpi7oWVddoxrmw9xV+c8Dg6XnfoFIvKb8Tg9XJIkSZKkucTQUiqU4VxomZnDIy0Brl9wgJGU4XvtDad3gZazHGkpSZIkSdIcY2gpFUhmuJfhVEZFRXmxSymqJVVDXDa/h++1NzI8Fqd+gdazob8TejumvjhJkiRJklSSDC2lAsmO9NDBfOaVp2KXUnSvbD3AoZEsDx86jVGnhzfjcbSlJEmSJElzhqGlVCCVIz20p/nMKx8tdilFd+G8XuZnR7irc96pd249K/fqDuKSJEmSJM0ZhpZSgdSMdtOR5lOdGSt2KUVXFvDCpi4ePFRH78gp/mdn3jIor4F2N+ORJEmSJGmuMLSUCqQudXMo5hGnsYzjbHRNUxcjKcM9B+tPrWMmAy1rHWkpSZIkSdIcYmgpFUCkUealbrozpzEdepZaUzPA4spB7tp/Gr8mLWc70lKSJEmSpDnE0FIqgIqhg5QxRl/mFEcVzmIRudGWT/bU0jGUPbXOrWdB104Y7ClMcZIkSZIkqaQYWkoFUD3YAcBAtq7IlZSWa5q7ALj7wCmOtjy8g3iHoy0lSZIkSZoLDC2lAqga3A/AcJmh5XiLKodZUT3AgwdP8del+czca+eWqS9KkiRJkiSVHENLqQAqBtoBGKuoLXIlpefy+T1s7Kmm51R2EW9aBQTs31SwuiRJkiRJUukwtJQKINvXlntTbmh5tMsbehgjePjQKYy2LK+G+csNLSVJkiRJmiMMLaUCyPa305sqqa44xQ1n5oA1NQPMz47w4KmElgDNq2H/5sIUJUmSJEmSSoqhpVQAlQMdtKcG6rMjxS6l5GQCLpvfw6NdtYykU+jYfGYutEyn0kmSJEmSJM1EhpZSAVQP7aed+czLjha7lJJ0eUMPfaNlPNVdM/FOTWtg8BD07S9cYZIkSZIkqSQYWkoFUDu8n/bUYGh5HBfW91IeY6c2RfzwDuKuaylJkiRJ0qxnaCkVQP1IJx1pPnWGlsdUVZY4v76PRw6dwkZFzWtyr65rKUmSJEnSrOcuIdJUGxmkbqybQzGPTBS7mGNbs/1Lxz9Z1jSB/p2TruHCeb38886F7OnLsLhm7GcnHvjUsTuMjUJk4ImvwujQxG6y7h2TrlOSJEmSJE0/R1pKU62nDYDesvoiF1LaLqzvA+AnbRUT65Apg5pm6G0vYFWSJEmSJKkUONJSmmr50LI/Y2h5IsurB5mXHeHrWxLL04lHbl65Kj/6s7YVegwtJUmSJEma7RxpKU21nn0ADGVPYZOZOSgTcEF9H+u7a0hpgp1qW3MjLdPYydtKkiRJkqQZy9BSmmr50HI0ewqbzMxRF87r5cBwObsGJjhFvLYVxoZhoKuwhUmSJEmSpKIytJSm2Fh3LrSkorq4hcwAF+TXtXy8e4IBb92C3GtvW4EqkiRJkiRJpcDQUppiQwf3cCDVUVPuv14ns6BymIWVQ6zvqplYh9rW3Kub8UiSJEmSNKuZqkhTbKRrH+1pPvOyI8UuZUa4oL6XJ7prGJ3IupZV8yFTbmgpSZIkSdIsZ2gpTbHU00Z7aqA+O1rsUmaEC+r76B8rY1tf1ckbRwZqWwwtJUmSJEma5QwtpSmW6WujnfnMM7SckHPq+gF4qmeCa4DWtkKPoaUkSZIkSbOZoaU0lVKicqCd9tRgaDlBTRUjLKwcYkPPKaxr2bcfxvz1lSRJkiRptjK0lKbSUA/Z0QHXtDxF59T18VR3NWki61rWtkIahf4DBa9LkuaKiHhVRGyMiE0R8f5jnP/tiHgk/7M+IkYjoil/bltEPJ4/98D0Vy9JkqTZyNBSmko9bQAcLGsk679dE3ZuXT/do1l2DVScvHHdgtxrb1thi5KkOSIiyoCPAtcD5wE3RcR549uklP48pXRJSukS4PeAH6WUOsc1eUn+/LrpqluSJEmzm7GKNJV69uVess1FLmRmObeuD2BiU8RrW3OvbsYjSVPlCmBTSmlLSmkI+AJwwwna3wR8floqkyRJ0pxlaClNpXxo2VdhaHkqFlYO05AdYcNENuOpqINslZvxSNLUWQrsGPd5Z/7Y80REDfAq4CvjDifguxHxYETcXLAqJUmSNKdki12ANKvkp4cPVLQAnSduqyMi4Nz6Pp7qnsBIy4j8ZjyGlpI0ReIYx463yvBrgZ8cNTX8RSml3RGxAPheRDyVUrrzeTfJBZo3A6xYsWKyNUuSJGmWc6SlNJV69jFChtGqxmJXMuOcU9fH/uFy2gcn8Hcpta2OtJSkqbMTWD7u8zJg93Ha3shRU8NTSrvzr23AreSmmz9PSumWlNK6lNK61tbWSRctSZKk2c3QUppCqXsfHWk+NVUT2FBGz3FOXT9wCuta9h+AUXdol6QpcD+wNiJWRUQFuWDytqMbRcR84Frg6+OO1UZE/eH3wCuA9dNStSRJkmY1Q0tpCo107aU9zaemoqzYpcw4K6oHqc6M8nTvBNa1rG0BEvTvL3hdkjTbpZRGgPcC3wE2AF9MKT0REe+KiHeNa/qLwHdTSr3jji0E7oqIR4H7gG+mlL49XbVLkiRp9nJNS2kKjXXvoz01UFvpv1qnKhNwZu0Az0xkM57xO4jXLSxsYZI0B6SUbgduP+rYx4/6/Gng00cd2wJcXODyJEmSNAc50lKaQtHblgstKwwtT8dZdf0821/JwOix9oQYZ3xoKUmSJEmSZp2ChpYR8aqI2BgRmyLi/cc4HxHx4fz5xyLispP1jYimiPheRDyTf20cd+6iiLg7Ip6IiMcjoqqQ3096jrExsv0dtDOf2kqnh5+OtbX9JIJNfScZbVlRC+U10NsxPYVJkiRJkqRpVbDQMiLKgI8C1wPnATdFxHlHNbseWJv/uRn42AT6vh+4I6W0Frgj/5mIyAL/ArwrpXQ+8GJguFDfT3qe/k4yadTp4ZOwtja3Gc+EpojXtDjSUpIkSZKkWaqQIy2vADallLaklIaALwA3HNXmBuCzKeceoCEiFp+k7w3AZ/LvPwO8Lv/+FcBjKaVHAVJK+1NKowX6btLz9ewDoD3Nd3r4aarLjrGkanCCm/G0GlpKkiRJkjRLFTK0XArsGPd5Z/7YRNqcqO/ClNIegPzrgvzxs4AUEd+JiIci4nem5FtIE5UPLbvKmqjIulzs6Tqrtp9neqtI6SQNa1ug/yCMOqBakiRJkqTZppDJyrF20jg6hjhem4n0PVoWuAb4lfzrL0bEy55XVMTNEfFARDzQ3u4oLU2hnjYAhqtbi1zIzHZWbT/dI1n2DpafuGFtK5Cgb/+01CVJkiRJkqZPIUPLncDycZ+XAbsn2OZEffflp5CTf20bd60fpZQ6Ukp9wO3AZRwlpXRLSmldSmlda6vhkqZQfqQldQtO3E4ntLYut67lSaeI17bkXt2MR5IkSZKkWaeQoeX9wNqIWBURFcCNwG1HtbkNeFt+F/GrgEP5Kd8n6nsb8Pb8+7cDX8+//w5wUUTU5DfluRZ4slBfTnqenjYGqaS6bn6xK5nRllUNUZ0ZPflmPLX5v3RwXUtJkiRJkmadgu0WklIaiYj3kgsTy4B/Sik9ERHvyp//OLnRkK8GNgF9wDtO1Dd/6Q8BX4yIdwLbgTfl+xyIiL8iF3gm4PaU0jcL9f2k5+neS0c00FRXWexKZrRMwNragZOPtKyohfIaQ0tJkiRJkmahgm5xnFK6nVwwOf7Yx8e9T8B7Jto3f3w/8Ly1KvPn/gX4l0mULJ2+nn3sG5tPU01FsSuZ8dbW9fPVPc30j2aoLhs7fsPaVuhzergkSZIkSbONWxxLU2Ssey97xxpoqjO0nKyzavtJBJt7q07csLbFkZaSJEmSJM1ChpbSVOneR1tqoLnW0HKyzqyd6GY8rdB/EEaHC1+UJEmSJEmaNoaW0lQY7icz1EVbaqCp1jUtJ6suO8bSqkGemUhoSXKKuCRJkiRJs4yhpTQVevYB0E4DTY60nBJn1fbzTE8VKZ2g0ZEdxA0tJUmSJEmaTQwtpanQnQ8tnR4+ZdbW9dM9mmXPYPnxG9W05F5d11KSJEmSpFnF0FKaCj17AXLTw92IZ0qclV/X8oRTxCtqoLzW0FKSJEmSpFnG0FKaCj1tABzINFJfmS1yMbPD0qohaspG2dhzsnUtW5weLkmSJEnSLGNoKU2F7r2MkYGaZiKi2NXMCpnI7SI+oc14HGkpSZIkSdKsYmgpTYWevXSVNdJQV1PsSmaVs2oH2NFfSffwCYLg2hYYOAijQ9NWlyRJkiRJKixDS2kqdO9jfzTSVHuCTWN0ytbW9pMIHu08wZT7IzuI75+eoiRJkiRJUsEZWkpToWcfbWk+TbWVxa5kVlmb34znof0nCIMPh5Z9ThGXJEmSJGm2MLSUpkLPPnaPzKe51p3Dp1JtdoxlVYM81Hmi0LIl9+pmPJIkSZIkzRqGltJkjY2SetvZNTqPJkPLKbe2tp+H95czlo7ToLwGKmrdjEeSJEmSpFnE0FKarN4OIo3RnhoMLQvgrLp+Dg1n2NJddvxG7iAuSZIkSdKsYmgpTVbPXgDaUoPTwwvgrMPrWp5winir08MlSZIkSZpFDC2lyereB+BIywJZUjXEvPIxHj7RZjw1LTBwEEaHpq0uSZIkSZJUOIaW0mT15ELLNhppqXf38KmWCbi0aXhiO4g72lKSJEmSpFnB0FKarPz08PY0n5ZaQ8tCuKx5mKe7yugajmM3OBJauq6lJEmSJEmzgaGlNFnd+xgoq2c0U8m86myxq5mVLmseJhE82nmcX9/altyrIy0lSZIkSZoVDC2lyerZx6FsE811FUQcZySgJuWSphGCdPwp4uXVUFHnSEtJkiRJkmYJh4VJk9Wzj85opLnGqeGFUl+eOGve6MnXtTS0lCRJkiRpVnCkpTRZ3XtpSw0017lzeCFd1jzMw53ljKXjNKhtcXq4JEmSJEmzhKGlNBkpQc8+do/Op6XOkZaFdGnzMF3DGbZ0lx27QW0rDB6CkcHpLUySJEmSJE05Q0tpMga7YGSA7UP1NNc60rKQLmsaBjj+FPHDm/H07Z+miiRJkiRJUqEYWkqT0b0PgN0j82ipd6RlIa2uH2V++RgPHje0bM29uq6lJEmSJEkznqGlNBk9ewFoo9GRlgWWidwU8Yc6DS0lSZIkSZrtDC2lyehpA6AtNbim5TS4rGmYZ7qyHBqK55/MVkFlvZvxSJIkSZI0CxhaSpPRnRtp2e7u4dPi8ubcupaPHG+0ZU2LIy0lSZIkSZoFDC2lyejZy2imgi5qaHakZcFd3DRChnSCzXhaDS0lSZIkSZoFDC2lyehpo7eiBQjXtJwGdeWJs+ePnHgznsEuGBmc3sIkSZIkSdKUMrSUJqN7L4fKmqirzFJVXlbsauaEF7QM83BnlpGxY5ysbcm99rmupSRJkiRJM5mhpTQZPfvojEbXs5xGL2gZpnckw5MHs88/eXgH8R6niEuSJEmSNJMZWkqT0b2XttTg1PBpdEVLbjOe+zqOMUX8yEhLQ0tJkiRJkmayYwxVkjQhI4MwcJDdVfNn1SY8927tLHYJJ7SweowVtaPc31HOr53V/9yT2SqonOdmPJIkSZIkzXCOtJROV88+ALYP1dPi9PBp9YKWIR7YX0FKxzhZ2wK9rmkpSZIkSdJMZmgpna7uXGi5bbCOllk00nImuKJlmP2DGTZ3H2PzI0NLSZIkSZJmPENL6XR17wZgz1ija1pOs3X5dS3vP+a6lq0w2AUjA9NclSRJkiRJmiqGltLp6t4LwN7UNKvWtJwJVteN0lI5dpzQckHu1XUtJUmSJEmasQwtpdPVtZuxTDkHqKPZNS2nVQSsaxnivo5j/LrX5UPLnrbpLUqSJEmSJE0ZQ0vpdHXvYaCqlUTGNS2L4MqWYXb2lbGj96j/jNW0AnFkoyRJkiRJkjTzGFpKp6trNz0VuVF9rmk5/V64YAiAu9uP+rUvy0JNsyMtJUmSJEmawQwtpdPVvYcDZc1kAhpqDC2n21nzRmmuHOOetmOsa1m3wNBSkk5BRLwqIjZGxKaIeP8xzr84Ig5FxCP5n/890b6SJEnS6cgWuwBpRkoJuvbQPv9SmmorKMtEsSuacyLgqtYhftpeQUq5z0fULYSOZ2BsDDL+3YwknUhElAEfBa4DdgL3R8RtKaUnj2r645TSa06zryRJknRK/NO8dDoGu2C4l91jjbTWVxW7mjnr6tYh9vaXsa2n7LknahfA2DAc2lGcwiRpZrkC2JRS2pJSGgK+ANwwDX0lSZKk4zK0lE5H914AdgzNZ0G9m/AUywsXDAPw0/ajpogf3kG845lprkiSZqSlwPi/5dmZP3a0qyPi0Yj4VkScf4p9JUmSpFNiaCmdjq7dAGwenEeroWXRrKobZWHVKHe3HbWmaN3C3GvH09NflCTNPMda4yQd9fkh4IyU0sXA3wFfO4W+uYYRN0fEAxHxQHt7++nWKkmSpDnC0FI6Hd17ANjYV2doWUQRudGW9+TXtTyiohbKa2C/Iy0laQJ2AsvHfV4G7B7fIKXUlVLqyb+/HSiPiJaJ9B13jVtSSutSSutaW1unsn5JkiTNQoaW0unIj7TcOdpIa52hZTFd3TpEx2CGp7vGrWsZkZsi7vRwSZqI+4G1EbEqIiqAG4HbxjeIiEURuS3PIuIKcs+Q+yfSV5IkSTodBQ0tI+JVEbExIjZFxPuPcT4i4sP5849FxGUn6xsRTRHxvYh4Jv/aeNQ1V0RET0T8j0J+N81x3XsYrZzPIBWOtCyyFy0cAuDH+46eIr7A6eGSNAEppRHgvcB3gA3AF1NKT0TEuyLiXflmbwTWR8SjwIeBG1POMftO/7eQJEnSbFOw0DIiyoCPAtcD5wE3RcR5RzW7Hlib/7kZ+NgE+r4fuCOltBa4I/95vL8GvjXlX0gar2sPA9W5dRPdiKe4ltaMcWb9CD/ae9TvQ+0C6NkHA4eKU5gkzSAppdtTSmellNaklP4kf+zjKaWP599/JKV0fkrp4pTSVSmln56oryRJkjRZhRxpeQWwKaW0JaU0BHwBuOGoNjcAn83/Tf09QENELD5J3xuAz+TffwZ43eGLRcTrgC2Af8OvwureQ095bj0uR1oW37WLhri3o5y+kXEHj2zGs6koNUmSJEmSpNNXyNByKbBj3Oed+WMTaXOivgtTSnsA8q8LACKiFvhd4I9OVJQ7V2pKdO/hYLYZMLQsBS9eNMjQWHBv+7gp4nULcq9OEZckSZIkacYpZGgZxziWJthmIn2P9kfAXx/e2fJ43LlSkzY6Aj37aKeJqvIMdZXZYlc0572gZZiqssSPxq9rWdMCmayhpSRJkiRJM1Ah05adwPJxn5cBuyfYpuIEffdFxOKU0p78VPK2/PErgTdGxP8DGoCxiBhIKX1kKr6MdERvG6Qxdo810lpfSX4zVRVRVVluF/Ef7h0XWmbKoGkNtG8s2H0/d+/2k7Z585UrCnZ/SZIkSZJmq0KOtLwfWBsRqyKiArgRuO2oNrcBb8vvIn4VcCg/5ftEfW8D3p5//3bg6wAppZ9LKa1MKa0E/gb4UwNLFUTXHgC2jzSwoL6qyMXosBcvGmJbT5ZtPWU/O7jgHGjfULyiJEmSJEnSaSlYaJlSGgHeC3wH2AB8MaX0RES8KyLelW92O7mNczYB/wC8+0R9830+BFwXEc8A1+U/S9OnOzfod8vgPFrrXM+yVFy7aAiAO8ePtmw9Fzq3wnB/kaqSJEmSJEmno6CL8aWUbicXTI4/9vFx7xPwnon2zR/fD7zsJPf9wGmUK01MfqTlxt5aXugmPCVjZd0oK+tG+N6eCt52Zj6kXHAOkHLrWi6+uKj1SZIkSZKkiSvk9HBpdureQ8pk2dJf487hJeaVSwa5u62CQ0P5dUZbz8m9FnBdS0mSJEmSNPUMLaVT1b2H0dqFJDKGliXmFUsHGUnBDw5PEW9ak9tBvM11LSVJkiRJmkkMLaVTdWgng9WLAFhgaFlSLm0aYUHVKN/elf99yVZA85nQ/lRxC5MkSZIkSafE0FI6VYd20l2ZCy0daVlaMgGvWDLIj/ZW0j+SP9h6jiMtJUmSJEmaYQwtpVMxNgZdu+jMLgAMLUvRq5YO0j8a3LkvP0V8wblwYJs7iEuSJEmSNIMYWkqnorcdRofYF60ANNcaWpaaK1uHmV8+xncOTxFvPZsjO4hLkiRJkqQZwdBSOhWHdgKwKzXRWFNORdZ/hUpNeQZetmSQ7+2pZGB4FFrPzZ1oc11LSZIkSZJmChMX6VR05ULLrcONLKivKnIxOp7XLR+kezjDHRvaoHkNZMqh3XUtJUmSJEmaKQwtpVORH2n59ECD61mWsBctHGJh1ShfeWgnlJXndxDfWOyyJEmSJEnSBBlaSqfi0E4or2VLd7mhZQkrC3jdigF+9HQ77d2DuXUt3UFckiRJkqQZw9BSOhWHdpDmL6OtZ5CF85weXsreuHKA0bHE1x/ZBQvOy+0gPtRb7LIkSZIkSdIEGFpKp+LQLobrFjM8mlg839CylK2dN8pFy+bzlYd2waILgAT7nix2WZIkSZIkaQIMLaVTcWgnPZWLABxpOQO84bJlbNjTxUbOyB3Y93hxC5IkSZIkSRNiaClN1PAA9LZxILsQwJGWM8DrLllKTUUZn3hkGCrnw971xS5JkiRJkiRNgKGlNFFduwDYm2kBYJGhZcmbX1POL61bzm2P7WGo5VzYZ2gpSZIkSdJMYGgpTVQ+tNw52kRZJmipc/fwmeD/e9EqRlPi8ZHlsO8JGBsrdkmSJEmSJOkkssUuQJoxDu0EYPNQIwvqKynLRJELmjvu3do5oXZXrmp63rEVzTW88rxFfGNzE5fTAwe3QdPqKa5QkiRJkiRNJUdaShOVDy2f6Z/n1PAZ5j///CoeGlyW++C6lpIkSZIklTxDS2miDu2A2gVs7xplkTuHzyiXn9HEvDMuYpQMAzsfLXY5kiRJkiTpJAwtpYk6tAvmL2Nf16AjLWeg97/2UraOLeLZJ+8rdimSJEmSJOkkDC2liTq0k+G6JfQMjjjScga6YOl8uhvOoebABp7d31vsciRJkiRJ0gkYWkoTkRIc2klv1SIAR1rOUGdedBXLo52/uO1+UkrFLkeSJEmSJB2HoaU0Ef0HYLiX/dkFAI60nKHqV1wKwN6nH+Szdz9b5GokSZIkSdLxGFpKE3FgGwB7MwsBWDy/uojF6LQtuhCA1y/u4IPffJKHtx8ockGSJEmSJOlYssUuQJoR8qHls2MLgMSCeZVFLUfHdu/Wzud83jy6/Xlt3ly3iDcsauej/VW8518f4ku/8UKWNhhCS5IkSZJUShxpKU1EPrR8ZqiZptoKqsrLiluPTt/Sy6jY9wgff8vldA+M8Pq//wlP7u4qdlWSJEmSJGkcQ0tpIg5sg5oWtvdkXM9ypltyKex/hguagy/9xtVkIvilT9zNrQ/vZGzMzXkkSZIkSSoFhpbSRBzYBo0r2ds14M7hM92Sy3Kvex7hnEXz+Oq7X8jq1lr+6789ymv+7i5uf3wPvYMjxa1RkiRJkqQ5zjUtpYk4sA2WvYC9ewe4eHlDsavRZCzJ7SDOrodg1c+zeH41X3v3i/jGY7v5i+9u5N3/+hAV2QxXrmri8jMauWR5A5csb6ChpqK4dUuSJEmSNIcYWkonMzoMh3Yycv4b2d875PTwma62GRrOgN0PHzmUyQQ3XLKUV1+4mPu3dXLHhjZ+/Ew7f3vHM6T8jPFVLbX8/NoWfuWqMzhrYX2RipckSZIkaW4wtJRO5tBOSKN0VS0FcHr4bLDkUtj90PMOl5dleOGaFl64pgWA7oFhHt95iId3HOTh7Qf4/P07+Mzdz/LCNc188HUXTHfVkqTpkhJEFLsKSZKkOc3QUjqZ/M7he8sWAbDY0HLmW3oZPPk16O2A2pbjNquvKueFZ7bwwjNzbTp7h/jiAzv42A838wsfvovrL1jE5Wc0Ev7BVpIkSZKkKeVGPNLJ5EPLraOtACxvrCliMZoShzfj2f3IKXVrqq3gXdeu4dvv+zkuXdHAVx/exbfW7yUldx2XJEmSJGkqGVpKJ3NgG2TKeaa/nghY0lBd7Io0WYsvBuKYU8Qn1H1+Nf/yziu5anUTd23q4EdPt09tfZIkSZIkzXFOD5dO5sA2aFjBjoO5TXgqsmb9M17VPGhZm9tB/DRlMsFrLlrCwPAY331yH7UVWV6wqmkKi5QkFY1rWkqSJBWdoaV0Mge2QeNKdh7ooyKb4XP3bi92RZoKS9fBM9+d1B9MMxG84bJl9A6O8I3HdrOiuYaF7i4vSZIkSdKkOWRMOpkjoWU/TTUVxa5GU2XFVdDXAfs3T+oyZZngjZcvoyKb4csP7mR0zPUtJUmSJEmaLENL6UT6D8LAQUYbzmDPoX4aDC1njxVX5163/3TSl6qvKueGS5ay62A/P9zYNunrSZIkSZI01zk9XDqRg88C0Fm+hLEEjTXlRS5Ik3Vken+q5PXlDex68D+4d/jFz2nz5itXnPJ1L1w6nyeXzecHG9u4aFkDrfWVU1CtJKk4HDUvSZJUbI60lE7kwDYAdsVCABprHWk5a0TQ3ngpCw48PGWXfPWFi8mWZfjuk3un7JqSJEmSJM1FjrSUTqRzKwBbR1qAbhqdHj6rtDdexvK2H1A12MFAZcuR46e72VJ9VTk/v7aF/9jQxrP7ezmjuXaqSpUkSZIkaU5xpKV0Ivs3Qe0CtvSUkQmYX+308NmkvekyAFqncLTlNWe2Ul+V5Vvr95KS0wslaUbyv9+SJElFZ2gpnUjHM9ByFjsP9LN4fjVlmSh2RZpCB+ady0imitYDD03ZNSuyGV5+7kK2d/axYU/XlF1XkiRJkqS5xNBSOpGOp6FlLTsP9LGssbrY1WiKjWXK2d9w4ZSOtAS4bEUjTbUV/PDpdkdbSpIkSZJ0GgwtpePp3Q/9ndByFjs6+1nWWFPsilQA7Y2X0tj1FNmRvim7ZlkmuHZtKzsP9POTTfun7LqSVCgR8aqI2BgRmyLi/cc4/ysR8Vj+56cRcfG4c9si4vGIeCQiHpjeyiVJkjRbGVpKx9PxNABDTWvY1z3gSMtZqq3xcjJpdMpHW166ooF5VVk++oNNU3pdSZpqEVEGfBS4HjgPuCkizjuq2Vbg2pTSRcAfA7ccdf4lKaVLUkrrCl7wtHCUvCRJUrEZWkrHkw8t95WfQUqwvMmRlrNRe9OljEY5i/bfPaXXzZZluObMFu7esp+Hth+Y0mtL0hS7AtiUUtqSUhoCvgDcML5BSumnKaXD/zG7B1g2zTVKkiRpjiloaDmBqUYRER/On38sIi47Wd+IaIqI70XEM/nXxvzx6yLiwfz0pAcj4qWF/G6aAzqehmwV20YaARxpOUuNllXT3nQZizp+OuXXfsGqJhpqyvnYDzdP+bUlaQotBXaM+7wzf+x43gl8a9znBHw3//x1cwHqkyRJ0hyULdSFx001uo7cw+/9EXFbSunJcc2uB9bmf64EPgZceZK+7wfuSCl9KB9mvh/4XaADeG1KaXdEXAB8hxM/cEsn1vEMNK9l58FBIBdabmnvLXJRKoQ9LS/k0o1/TdVAOwNVrVN23cpsGW+96gw+8oNNbO3oZVVL7c9OPvCpKbsP694xddeSNBfFMY4dc350RLyEXGh5zbjDL8o/fy0AvhcRT6WU7jxG35uBmwFWrFgx+aqnmhunSZIklZRCjrQ86VSj/OfPppx7gIaIWHySvjcAn8m//wzwOoCU0sMppd35408AVRFRWaDvprkgv3P4s/v7yGaCRfOqil2RCmRPywsBWFyA0ZZvvfoMyjMZPvWTrVN+bUmaIjuB5eM+LwN2H90oIi4CPgnckFI6ssvY4eevlFIbcCu557jnSSndklJal1Ja19o6dX9BVBAGmJIkSUVXyNByIlONjtfmRH0XppT2AORfFxzj3m8AHk4pDZ529Zrbhgfg4LPQspYt7T2c0VxDtswlYGerg/Vn0V/RXJDQckF9Ff/pkiV86YGdHOobnvLrS9IUuB9YGxGrIqICuBG4bXyDiFgBfBV4a0rp6XHHayOi/vB74BXA+mmrXJIkSbNWIVOYiUw1Ol6bCU9Tet5NI84H/i/w68c5f3NEPBARD7S3t0/kkpqLOrdAGoOWs9jS0cvq1rpiV6RCigx7W65m0f57cr/vU+yd16yif3iUz923fcqvLUmTlVIaAd5LbmmdDcAXU0pPRMS7IuJd+Wb/G2gG/j4iHomIB/LHFwJ3RcSjwH3AN1NK357mryBJkqRZqGBrWjKxqUbHa1Nxgr77ImJxSmlPfip52+FGEbGM3LSkt6WUjrnzRUrpFuAWgHXr1jn3R8eW3zl8pPFMnt2/m5efu7DIBanQ9rS8kFW7/53Gro0cmH/ulF773MXzuObMFj79062885pVVGQdtSuptKSUbgduP+rYx8e9/zXg147RbwtwccELnA5OCZckSSophfyT80mnGuU/vy2/i/hVwKH8lO8T9b0NeHv+/duBrwNERAPwTeD3Uko/KeD30lyw/xkAdpYtZXg0sbq19iQdNNPtbb4KoCC7iENutOW+rkFuf3xPQa4vSZpKBpiSJEnFVrDQcoJTjW4HtgCbgH8A3n2ivvk+HwKui4hnyO0u/qH88fcCZwJ/kJ+29Eh+F0vp1HU8A/OXs/lgbqrwGqeHz3oDVa101p/D0vYfFeT6157VyprWWj551xaSo3kkSZIkSTqhQk4Pn8hUowS8Z6J988f3Ay87xvEPAh+cZMlSTn7n8M3tPQCscaTlnLBz4Uu5cNPHqBpoZ6Bqane2zWSCd16zmt+/9XHu29rJlVN6dUnS5PkXSpIkSaXEhdWko42NQfvT0HI2W9p7aa6toKGmothVaRpsX/QKgsTyfXcU5Pqvv2wpjTXlfPKurQW5viRJkiRJs4WhpXS0g9tguBcWns+W9l7Xs5xDuurXcKh2Fcv3fq8g168qL+MtV53Bf2zYx9busoLcQ5I0BVzGQ5IkqegMLaWj7csvn7rwfDa397ie5RyzY9F1LOh8gMrBzoJc/61Xn0F5JsOnNlUX5PqSJEmSJM0GhpbS0fY9AQQH61azv3fIkZZzzPZF15FhjGVt3y/I9RfUV/GfLlnCl7ZVc2goCnIPSdJpcHSlJElSSTG0lI62bz00r2HzwdwfXhxpObccrD+b7prlLN/7HwW7xzuvWUX/aPC5LY62lKTSZIApSZJUbIaW0tH2PZFfzzK3c/hqQ8u5JYIdC1/Oov33Ujl0oCC3OHfxPK5ZMMSnN1UzNFaQW0iSJEmSNKMZWkrjDfVC51ZYeAFbOnopLwuWNzoabq7ZtuQXyKQRzth9e8Hu8c61fewbKOP2nZUFu4ck6VQ4ulKSJKmUZItdgFRS9q4HEiy8gM339XBGcy3ZMrP9mWrN9i+ddt/eqsWcu/XTjGYqjhzbvOJNU1EWANcuGmJN/QiffLqGG5YPEi5vKUmSJEnSEaYx0nh7Hsm9LrmETW09rHETnjmrveFiagf2UtO/pyDXz0RutOX6g+Xc11FekHtIkk6Tm/JIkiQVnaGlNN7uR6B2Ab0VrWzd38v5S+YXuyIVSUfDhYxGloWdDxTsHq8/Y4DGijE++UxNwe4hSZogg0pJkqSSYmgpjbfnEVhyCRv2dpMSnL9kXrErUpGMllWzv+Eimg89TtlIf0HuUVUGb1ndz3/srmBrd1lB7iFJkiRJ0kxkaCkdNtQH7U/B4ktYv+sQABcsdaTlXLa36QWUpREWHHyoYPd465n9lGfgU5vc8EmSJEmSpMPciEc6bN96SGOw5BLWP95FS10lC+rd2Xku669ayKHaVSzafy97m648rWt87t7txzy+ZnvnkfdXN2b54rb5vPfcPhZUjZ3WfSRJU8mp4pIkScXmSEvpsF0P5l7zIy3PXzKPcEvnOW936zVUjPTQevCRgt3j9Yv2MzwGf/+Ua1tKUvEYVEqSJJUSQ0vpsJ33w7ylDNQsYlNbDxcsdT1LQVfNSrqrl7Gk/S7KRgcKco9FVcP80soB/nVzNTt7/c+yJEmSJEn+6Vg6bMf9sOwFPL2vm5GxxAXuHC6ACHYsfCmVI12c9eznCnab3zy3lwj42w21BbuHJEmSJEkzhaGlBNC9Fw5th+VXsH5XFwDnG1oqr7t2JQfq1nL+5n+gaqC9IPdYUjPGW9f085VtVTzT5U7ikjTtUjr2e0mSJBWFoaUEsOO+3OuyF/DE7kPUV2VZ3uRuzvqZZxe9grKxIS5/6v8V7B7vPqeXuvLEBx6p98/LkiRJkqQ5zdBSAth5H5RVwOKLWb+7iwuWzHcTHj3HYGUz69fczBl7vs3yPd8pyD2aKxO/fUEvP2mr4Js78zvXj41Cbzsc2pkbETxSmHU1JUmSJEkqJdliFyCVhG0/gaXrGI5yNuzp4m1XnVHsilSCnlz9/7G07U6uXP9HdM4/n96aZVN+jzef0cWmTRsZevQRxjZvINPbBmn0uY1qWqD1HFi2Dhr8Z1WSpp7D3SVJkorN0FIa6II9j8DP/Q827u1maGSMC5e5nqWeL2XK+ckl/5frf/JLvPiB9/Ddqz/LcPkU/bPSsw+23UXZzvv5o5EBelIVm0fWsnb1eVC3ELJVMDYM/Z1w4FnYcS88exc0rYZFF8DyK6amDkmaswwqJUmSSomhpbT9HkhjsPIa7tmyH4ArVzUXuSiVqt6aZdx52d/ykvtv5qX3v4sfrPs4QxWnGVymMRq7n2Zh5/3wxFbIlMHiS2Dp5fzFzkv5zJZ5/OtFB3nhguHn9x0egJ33wqY74B+vgxf8Z3jln0C2clLfT5IkSZKkUmBoKW37cW49y2Uv4O47n2BVSy2L5lcVuyqVsLbmF3DXpX/NNY/8d6675y3cdelfcqj+rAn3rx5oY0nbnSw4+BCVw10Mls+Ds38BVlwFlfUA/G7TIHe2jfLf75/Ht6/rZH7FUSOAyqtg1bWw/MrcyMt7Pwa7HoA3fQYanTIuSZIkSZrZDC2lrXfC0nWMZqu5b2snr7l4cbEr0gywa+GL+f4LPsE1j/w2r/rJjTy16q1sXPlWBipbjtk+O9LLoo57WLn731nW9gMyaZRDtat5dtErOVB/Nleufm6/6iz89RVdvOEHjfzPh+r5uyu7uG9b5zGvvXnFu1l22dlc9dgfUPGJn4df+ZLTxSXpVKV07PeSJEkqCkNLzW097bn1LF/6v3hi9yG6B0e4arVTwzUx7U3r+NaLvsQlG/+a87f8E+ds/SztTZezf/75DFQ0UzY2RF3/Tup7ttFy8FHK0ggD5Q08tfJtDJXVMljZdMLrX9w0wn89r5c/f6KOixqHubji+G13LnwZ337hWfyn9e+Fz74Obvo8rL52ar+wJEmSJEnTxNBSc9uWH+Rez3w5dz+TW8/yakNLnYKByhbuuehPeGL1r7Fm560s7vgJ52z9LGVpJHe+oomemmVsXPkWdrf+PO2Nl5Ay5azZ/qUJXf83zunjyUNZ/uzxOn73zFound973LY9tcvhHd/KhZb/+ia48XOw9uVT8TUlSZIkSZpWhpaa2575HtS0wKKLufs7D7C6tZYF81zPUqeuu24Vj5zz33iE/0akUbIjvaTIMpKtmdR1MwF/sa6LbT2N/O2WJXzg7O2srBk8fof6RfCO2+GzN8C/vQXefptTxSVJkiRJM06m2AVIRTM2CpvvgDNfxkiC+7d2OspSUyJFGcPl8yYdWB5WnYV/eOEhasrG+OAzy9nZf4J54gA1TfCWr8K8xbkRl20bpqQOSZrd0nHeS5IkqRgMLTV37bgX+vbDWa/k8V2H6B0a5eo1hpYqTUtqxviDs7ZTBnzwmeXsGSg/cYe6VnjrrZCtgn9+PXTtmZY6JUmSJEmaCoaWmruevA3KKmHtK/jxMx0AXLnK0FKla3HVMH9w1nZGUvCHG89ga1/liTs0roS3fBkGDsEXboKhvmmpU5IkSZKkyTK01NyUEmz4Bpz5Mqis55uP7WHdGY201p8kBJKKbFn1EH909nbKM4k/2riC9V0nmYK+6EJ4wydh9yPw9Xfn/tmXJD2f/32UJEkqKYaWmpt2PQhdO+Hc1/LMvm427uvmNRctLnZV0oQsrRri/5z9LM0VI/zpM8u5e8t+0on+sH3Oq+G6P4InboUf/d/pK1SSZioDTEmSpKJz93DNTY98LrfW3zmv4Rs/3kMEvPpCQ0vNHM0VI/zxOc/yd1uX8I1Hd7PrQB83XLL0+B1e+FvQ9hT88M+g5Sy44PXTV+wEfe7e7RNq9+YrVxS4EkmSJElSsTnSUnPPyCCs/wqc8xpSZT3//thurlrVzIJ5VcWuTDolNWVj/Paanbz0nAU8tP0gt9y5hV0H+4/dOAJe+zew/Cr42m/AroemtVZJkiRJkk6FoaXmno23w8BBuOQmNuzpZkt7L6+52FGWmpkyAS8/dyFvveoMOnoGee3f3cWPn2k/duNsJdz4r1C3AD5/ExzaNb3FSlJJGzclPI0VrwxJkiQBhpaai+77JMxfAatfwjce201ZJrj+AkNLzWznLp7Hu198Js21Fbz1H+/jg//+JIMjo89vWNsCb/4iDPfB534ZBrunv1hJKkXj17Hc/P3i1SFJkiTANS011+xdD8/eBdf9HwbH4CsP7uSaM1toqq0odmWa4+7d2jnpa7TWV3Lbe6/hT2/fwCfv2spPN+/nwzddwpkL6p/bcMG58KZPwb/+Enz5nXDj56DM/x1ImuvGhZajw8UrQ5IkSYChpeaae/4estVw6Vv56kO7aOse5K9+aXWxq9IMsWb7l4pdwklVV5Txx6+7gBef3crvfPkxfuHDd/G/fuFc3nLVGUTEzxqe+XJ49f+Db/53+O7/hOvdVVzSHDd+pOX4/15KkiSpKJwerrmjcys8+gW4/O2MVjXyiR9t5sKl83nRmc3Frkyaci87dyHfet/PcdXqZv7g609w4y33sLm957mNXvBrcNV74N6Pw90fLU6hklQyxoeWPiJLkiQVmyMtZ7G//t7T/Nfrzip2GaXjx38BmSy86H18a/0etu3v42O/ctmR0Wcv+tAd/OT9L5vQpf5jwz5efu7CI5//6BvrGRpJR/6486e/eCF3P7qeqzJPcs/YefxL3MBb0tePfL4q8yTLop0vj17L34y8gbsqf4trBj/M+7Jf4W9G3sAXKv74yLWXRTvXDH6YL1T8MTcO/QF3Vf4WO1Mr94yd95y2Nw79Ae/LfgWAqzJPcuPQH/CFij8+0q6lfISPxJ8duf/h17cM/S9+WPlfuGbww7SUj9AxnH3O60fiz4gLf4lffvBszq3rZ0NP9ZHa7qr8rSPfAeB92a/wL3EDHcNZHqv8NZ5MZxy5P8CmyrfwkdFfJBuJvxh+I8Bz6j+3rp9XDtzOztYX86YlHSxd/1EGsw18Y/Bi3ri4g6/tbeablb/AB4b+gg9U/A/aB8ufU+/h+n6/4ktcfOFF3P3oev5m5A1UZ8b49+oP8LF5/4UnumsA2NBTzbl1/Xzg7O2857E1AHz0os2857E1vCV9nasvvoBffXgt/WOZI+0+sHEFG3qq+bfLNx45V50Z49OXPsOND5595Pf/3y7fCMCvPHgWv5X9Ch8bez2fvvQZfvXhtXz60mee88/Srz68ll/P3MrVF1/Aex5bw0cv2nzk+NFtx/vAxhVs66vke0v/gfmbvsb1h97Pn/zihc9ps6C+ik+/4wVc+n++y4Y9XVz/tz/mN19yJr9+7RoqspncP/O/88dwaDt85/ehaj5c+pbj3vNoF/7ht3n8j1414fYTccudm7n559c859iHvvUk733pWezvGeQf79rCYzsPEhFs3NvFI9sP8sevu+C5o0iBN1+5YtprP5lf/sTd/NuvX33Sdp+7d/tJ2/zPWx9n64d+4YRtJvrfNP9fIeWNG2n5us/v4WsXFbEWSZIm4EUfuoM3Xr7cZznNWv418iz2t3ccP/CYc/Y+Do98Dl7wTlL9Ij72w82sbq3llecvOtJk18GBCV/u+0+1Pefz4LjA8rD3Zb/KVZmneF/2q3QMlz/n81WZp1gW+3lf9qtAsCz2A3Hk81WZp478HD53VeapI20PX2d828P9D1//8LnD7TqGy59z/8OvI2SO3KNjuPx5r7lrAQQbemqAOPIz/jscvv/hvvOi/zn3hyAbifdlv8p7y249cmx8/Rt6anhf9qt8eU8LAMtiP2tGN/O+7FdZ1n4n7y27lQ09NVyZeYoNPTXPq/dwfTdnvnbk9wCC/rEy1oxu5st7WtjQU3OkXe4VOobL89fgyO8VQP9Y2XPa/ez7/+xc7hXSuF+Xw0bI8FvZW4+0Ofw6Xv9Y2ZH7Ha7heG3H29BTQ/9YGcva7+Scwcee98/fYRHBwf4R/uO/X8srzlvIX37vaX7hwz/mvq2duX/mM2Xwhn+E1S+B234TnvjaCe87XvfgMTb6maRt+/sAGB4d47GdB/nygzvoGhjlT2/fwCfu3MLIGHzh/h18/r7tPLT9IGPAn9y+gc/dt537tnbSMzhStNpPZirWLT3seL/f4030v2n+v0J6vpjQv2WSJBXXroMDPstpVnOkpWa/lOBbvwtVDXDt7/C5+7bzxO4u/vJNF5PJuGaV5oYF9VV85M2X8frL9vEHX3uCX/rE3QBsbu9hTWsd3Piv8M+/CF95J4yNwIVvLFqt31q/hwefPUDf0Cg1Fbnw9hcuXExLXQWfuftZ/svL1pISfPj7uQe0sxfWs6Wjl/W7DvH1R3axZkEddVVZXnX+Iiqy/t2cpAkaN9IySMcd9XyykdySJEmaGoaWmv3u/yQ8+xN47YfZ0V/Jn3xzA9ec2cLrL1ta7MqkaffScxZy1X9r5h9/vJW//N7TXPdXP+I1Fy3h3S9Zwzm/8mX4/I3wlV+DoR64/FenpabRscT3n2rj0z/dCsBPNnVw7uJ5XLmqmdWttfyvr63nRWe2HGm/cF7Vc/q/ad1yUkrs6xrksV0HeXTHQX7r8w/TUlfBL61bzpuvXMGyxppp+S6SZocyxopdgiRJ0pxnaKnZre0p+O4fwJkvZ+ySt/I7/3gfmQg+9IYLn7cGnjRX1FRk+c2XreUvv/c0//nnVvMv9zzLbY/u5oVrmnnLZX/HK7O/Q9k3/guPP/kkj6999wk3pPjcvdtPe9RRW/cAX7x/B5+/bwe7DvYzryr3v6TffuX/396dx0dV3nsc//xmJpOFhCwkbAkgIqis4oK41mqr1LXWau2tFdv6sott7e3tS2t7a1+9/nHpbmut1ttarXq1avVq3TeoO6ggyiIS9rAFAgmQNNvM7/5xDjRiCAlkMpPk+3695jXnPOecyTPPkznznN8853mOoDA3az9Hf5iZMbQwh6GFQ/nEkUOorN7F3JU13DpnBbfOWcHhQws4fvQgxg7JJxJ+9g8m7yLSF/2rp2XUFLQUERERSTcFLaXvatgW9BrLLoDzf8dNLyzn9ZU1zPrMJPW6Egldf/aRfP20Mdw7dy33zVvLNx58n+KcL3NbUZzjV/yBwrqlzD1qFi1ZBd3y91oSSV5ZvpWH5lfxzKJNtCadkw8r5UfnHsmWnc386NFFXQ5Y7i1ixrghBYwbUkBtQzNvrt7Gm6u38/6m1ZQMiHP86JJueS89qbk1ydZdTdTUN1PX0AzADY8uAoL3W5ATozQ/m6GFOYwpy2fUIJ3jRLri+SWbeWnBEv4rXDecyupdHFo2YM8PHSIiIiLSsxS0lL6psQ7uuQh2rIcrnuCWt+v57YuVXHxMBZ87bsQ+D+vMrL0imWxf/8P7Sy/Oi/O1j42hsnoX71bVccXGy/lscjA3bLmbk164kDsGfZe6IScweGAOJXlxcuMdTxS0m7tTtf2fzF21jdcqt/LismpqG1oozM1i5omH8IXjR3JoWX6HeTwYRXlxPjl+KB8/YjCLN+zgjZU1PLVoEwAPvLWO8uJcThoziFg0c8a+dHdqG1pYs62BtdsaWLutnk11jST3mhfkwbeqgv1xmlqSH5o2JBqO1/uzp99n2ugSjhlVTEHOwQWDRfqibfXBjyVPvLuRcQMa9qRHSXLHq6s4fEgBFx9bQV5cTWYRERGRnqYWmPQ9deuDHpbVS0lc/BduXlbETc8v48Kp5cy6aLJuCxfZh7Y9FBPJctZsG8XP1h7FzOqf8p9br+PuTZ/gl60XU0sBOVlBkO/uN9Ywf+128rNjxCJGwp3GlgQ1u5rZvLOJFdW79szoXTIgzsfGlXHe5OGcOq6sRyfJiUUiTKkoYkpFERvr/snNL1by/qYdzLxjHqX52Zw/ZTifnjqcSeWFPX6OaGxJsHhDHfPX1PL2mu28umIrOxuDMotHI1QU53LquDKGFeYyaECc4rw4Nz6xhB+dO37PayTdaWhOUNvQTPXOJjbVNfJK5Vb+8NJKfj9nBRGDieWFnDK2lFPHljF1ZLEmKZJ+b+7KGq7+3wXU/bOZ7505jq8ekw+/DrZFSHLOpGE8vWgTv5tdyb9N09i4IiIiIj1NQUvpW95/Ev7+bWhpZMu5f+Yb/yjizdXL+czUcn722cl7eh+JSMeiEePQ0nwoPYNXEycyadnNXLbmHi6Ov8ZzRZfwcPw8Zq9uomZXE69WbmVXUyvJpBMxIzsrSml+nNL8bC46upyxQwo4emQxRwwtIJIBn8FhhbkAXP+pIxkyMJtHFqzn7jdWc8erqygryOaUw0o5ZVwpJx1WyuCCnP28WqCzvURbE0lWbq1n6cYdvFdVx/y121m0fgfNiWD8vBEluYwpy2dkSR4jS/IYMjCnU+etiBn52THys2N7AiuvVG7l3R+fyYK1tcxbVcNrK2q47R8ruWX2CgbEo5wwZhCnjC0Dgt6d+kFH+gt355431vCTvy9h5KA87rlyGkcMHQg7N+3ZJ0KSkw4rZWRJHvfNW8sfXlrJ2ZOGMb0XDi8hIiIi0lulNGhpZjOA3wBR4I/uPmuv7RZuPxtoAK5w9/kdHWtmJcBfgUOA1cAl7r493HY98BUgAXzb3Z9J5fuTDOEO6+bBy7+E5c/QXDqB/xl5PTc/EiMrspNfXTKFC6eW64Jc5AAlorm8M/5aVo34DJOX38x5m+9kRvSvPBg7gZLjrmLGJ2dAL/x8ZUUjzJg4jBkTh1Hb0MyzSzbz8vKtzF5WzcML1gNw2OB8JpUXMn7YQEaXDmDUoDwGD8xhYE6s3XNK0p3m1iT1Ta3U/rOF2oZmtjcEzwDjf/wMza1BgDIeizC5vJArTjqEo0cWc/SoIgYX5HTrbfIDsmOcPLaUk8eW8l1gR2MLr6+o4eXlW3h5+VaeX1oNwMk/nc2pYaB26shihhfm6JzZz6SizZaJtuxs4oZHF/HUok2cfsRgbrr0KAbuHjrB20zEw+4fEvL45scP48G3q/j7wg2s3lrPmROGUlaQnY7si4iIiPQrKQtamlkUuAX4JFAFvGlmj7n7kja7fQoYGz6OB24Fjt/Psd8HXnD3WWb2/XD9OjMbD1wKTACGA8+b2Th3T6TqPUoauUP1ElgxG194H7Z5Ec2xfB4qvJIbN5xK68YoFxw1nO98Yqxu5xLpJnUFh/Hy0b+huG4x49bcz4VVT5L72ouwqBzGnQWHnAzlx0LRyF4XxCzKi3PJsSO45NgRJJPOko07eGn5Ft5evZ3XV9TwSBjE3C0WMfJzYmTHIsQiEbY3NNPcmqR174EnAQMKwpnRZ54wiiOHDWT88IGMKcsnq4fH0hyYk8VZE4Zy1oShAKypqedjP5/DhOEDeXzhRu6btw6A0vw4kyuKmFwRBGwPKR3AyJI8crI6N5ap9C4pbLNljJ2NLTw8fz2/fv4DGpoSXDvjcL566pi9ejL/6/MbabOclx3jiyeM4qUPtvDC0mpO/8UcvvPJcXx+2giNdSkiIiKSQqlsaU0DKt19JYCZ3Q9cALRtxF4A/MXdHXjDzIrMbBhBL8p9HXsBcFp4/F3AHOC6MP1+d28CVplZZZiH11P4HjvN3XEPeuEk2iwnHRJJpzWRpKl19yNBU0v7y40tSRpbEjSG6Y17tgfbmlqDGK0RNMLPu/mVPXkwCy60o+EjFokQjRhTRhSRHYuQFTWyohHisUjw3GY5K2rEY0Fa+/sYWeH2eDTSpVtAPezZ4B5cLiSSTlNrgubWJM2JJM2tSXY2tmIfPEPeutnk1FZSuOMD8lprAVjqo/lL65U81ngiw/NK+fLHhnDZ9FF7bgEVke61vXACcyffyKwVZ3P7cZuY3jIPFv4V3roj2CGnCEpGQ/Ho4Dl/KOSVBI/cEig7ArI6d9t1OkQixsTyQiaWF+5J217fzOqaetZua2DLzia21Tezs7F1T6CyantDcI4Mz4N58ShFeXGKcrMozMsiFonwg0fe44fnjO/gL3e/zvbavP3yY2lJJFm0vo731texcF0d71bVMntZ9Z7OZ2YwbGAOIwflMXRgDoPysxmUH6d0QDZFeVnkxWPkZEXIyYqSG4+SmxU84rHgu8YMomZELFhWT86Mkqo2W49zd+qbE2yvb6Z6ZyNLNuxgwbpanl60iYbmBMeOKmbWRZM4bHDBRw9Otu5ZjIQ9Lfesm3Ha4YOZMLyQ+Wu3c+PjS/jFM8s448jBnDimlEPLBlBRnEtuVpSc8KEhaUREREQOTiqDluXAujbrVQS/zO9vn/L9HDvE3TcCuPtGMxvc5rXeaOe1etz/LVjPdX97t01g0j8y62t3icci5MSCi8TsrOBiue2F4Ma6xnApCJQm3EkknNZkEDxNJJ1/fLCl2/MVBEUt6Kfgwey2u4OS7h4+d+01/zP2MJ+LzqHSy3k5OYVl2ZPYUHI8peWHckxFIV89pITRpQO6/b2ISPt2kM/KiguZfvw1kGiBzYth/VuweQlsXwUb5sOSR2HvDu/fmg+DxqQn0weoeECc4gFxpo4sbnd7KmY+72lZ0QhTRxYH7/GEIG1XUysrqnexuqaeNTUNrN5az+qaeuavraVmVxP1zQd+M0PEgkBQxIxIBG677BhOO3zw/g+UVEhVm63HXXP/Ozy2cMOH0orysjhn0jC+MH0UUyo6mGwr0bJnMbpX0HK3soJs7vzSccxbtY3HFm7g6UWbePzdje3umxW1LgUud//g3Kl9uxAPVehURKTvm3DD0wd87MIfn0msh+/+Eeks865Gjjr7wmYXA2e5+5Xh+heBae7+rTb7PAH8t7u/Eq6/AFwLHLqvY82s1t2L2rzGdncvNrNbgNfd/Z4w/U/Ak+7+t73ydRVwVbh6OLAsBW8/U5QCW9OdCekU1VXvoHrqHVRPvUdP1dUody/rgb/TK6WqzdbO3+kNbTCdPw6Myq3rVGZdpzLrOpVZ16nMDkx75ab2lxy0VPa0rAJGtFmvADZ0cp94B8duNrNhYS/LYUB1F/4e7n47cHvX3krvZGZvufux6c6H7J/qqndQPfUOqqfeQ3WVMVLVZvuQ3tAG0//kgVG5dZ3KrOtUZl2nMus6ldmBUblJqqSyD/CbwFgzG21mcYJJch7ba5/HgMstMB2oC2/97ujYx4CZ4fJM4NE26ZeaWbaZjSYYKH5eqt6ciIiISB+RqjabiIiIiMgBS1lPS3dvNbNvAs8AUeAOd19sZl8Lt98GPAmcDVQCDcCXOjo2fOlZwANm9hVgLXBxeMxiM3uAYOD3VuBqzRwuIiIi0rEUttlERERERA5Yysa0lPQzs6vCW7Ekw6muegfVU++geuo9VFeSafQ/eWBUbl2nMus6lVnXqcy6TmV2YFRukioKWoqIiIiIiIiIiEhG0bz2IiIiIiIiIiIiklEUtOyjzGyGmS0zs0oz+36689PfmNkdZlZtZovapJWY2XNmtjx8Lm6z7fqwrpaZ2Vlt0o8xs/fCbb81M+vp99KXmdkIM5ttZkvNbLGZXROmq64yiJnlmNk8M1sY1tNPwnTVUwYys6iZLTCzx8N11ZP0Cmo7ta87vyv7m+44H/YnZlZkZg+Z2fvh/9sJKrOOmdm/h5/LRWZ2X9hmUpntxXRt1mX7KLOfh5/Pd83sETMrarOt35eZpIaCln2QmUWBW4BPAeOBz5vZ+PTmqt+5E5ixV9r3gRfcfSzwQrhOWDeXAhPCY34f1iHArcBVwNjwsfdrysFpBf7D3Y8EpgNXh/WhusosTcDp7j4FOAqYYcHsxaqnzHQNsLTNuupJMp7aTh3qzu/K/qY7zof9yW+Ap939CGAKQdmpzPbBzMqBbwPHuvtEgsnQLkVl1p470bVZV93JR9/fc8BEd58MfABcDyozSS0FLfumaUClu69092bgfuCCNOepX3H3l4BteyVfANwVLt8FfLpN+v3u3uTuqwhmZp1mZsOAge7+ugeDz/6lzTHSDdx9o7vPD5d3EjSOy1FdZRQP7ApXs8KHo3rKOGZWAZwD/LFNsupJegO1nfahu74rezTTGaA7zoc9lNWMYGYDgVOBPwG4e7O716Iy258YkGtmMSAP2IDK7CN0bdZ17ZWZuz/r7q3h6htARbisMpOUUdCybyoH1rVZrwrTJL2GuPtGCC4AgMFh+r7qqzxc3jtdUsDMDgGmAnNRXWWc8Ba7d4Bq4Dl3Vz1lppuAa4FkmzTVk/QGajt1wkF+V/Y3N3Hw58P+5FBgC/Dn8Jb6P5rZAFRm++Tu64FfAGuBjUCduz+Lyqyz1D45OF8GngqXVWaSMgpa9k3tjROhaeIz177qS/XYQ8wsH/gb8B1339HRru2kqa56gLsn3P0ogl90p5nZxA52Vz2lgZmdC1S7+9udPaSdNNWTpIv+7/ajG74r+41uPB/2JzHgaOBWd58K1BPerrsP/b7MwjEYLwBGA8OBAWZ2WUeHtJPWr8qsk9Q+2Q8z+yHB0CH37k5qZzeVmXQLBS37pipgRJv1CoJbBSS9Nodd5Amfq8P0fdVXFf/qct82XbqRmWURXITd6+4Ph8mqqwwV3io2h2A8HNVTZjkJON/MVhPcWnu6md2D6kl6B7WdOtBN35X9SXedD/uTKqAqvJMC4CGCIKbKbN8+Aaxy9y3u3gI8DJyIyqyz1D45AGY2EzgX+EJ4yzeozCSFFLTsm94ExprZaDOLEwyK+1ia8yRBHcwMl2cCj7ZJv9TMss1sNMEAxfPC2xR2mtn0cJa1y9scI90gLNc/AUvd/VdtNqmuMoiZle2endDMcgka6e+jesoo7n69u1e4+yEE3zsvuvtlqJ6kd1DbaR+667uyp/KbCbrrfNjD2U4rd98ErDOzw8OkM4AlqMw6shaYbmZ54ef0DIIxZ1VmnaP2SReZ2QzgOuB8d29os0llJikTS3cGpPu5e6uZfRN4hmAWuTvcfXGas9WvmNl9wGlAqZlVAT8GZgEPmNlXCBoZFwO4+2Ize4CgYdYKXO3uifClvk4wc1suwZghTyHd6STgi8B74XiJAD9AdZVphgF3hbMQRoAH3P1xM3sd1VNvoM+TZDy1nTrUnd+V/Z3KrGPfAu4NfzhYCXyJ8HtfZfZR7j7XzB4C5hOUwQLgdiAfldmH6Nqs6/ZRZtcD2cBzQQySN9z9ayozSSX7V49eERERERERERERkfTT7eEiIiIiIiIiIiKSURS0FBERERERERERkYyioKWIiIiIiIiIiIhkFAUtRUREREREREREJKMoaCkiIiIiIiIiIiIZRUFLEekXzCzHzOaZ2UIzW2xmP0l3nkRERET6AzOLmtkCM3s83XkREZHeQ0FLEekvmoDT3X0KcBQww8ympzdLIiIiIv3CNcDSdGdCRER6FwUtRaRf8MCucDUrfHgasyQiIiLS55lZBXAO8Md050VERHoXBS1FpN8Ib016B6gGnnP3uWnOkoiIiEhfdxNwLZBMcz5ERKSXUdBSRPoNd0+4+1FABTDNzCamOUsiIiIifZaZnQtUu/vb6c6LiIj0Pgpaiki/4+61wBxgRnpzIiIiItKnnQScb2argfuB083snvRmSUREegtz15BuItL3mVkZ0OLutWaWCzwL/NTdNYuliIiISIqZ2WnA99z93DRnRUREeolYujMgItJDhgF3mVmUoJf5AwpYioiIiIiIiGQm9bQUERERERERERGRjKIxLUVERERERERERCSjKGgpIiIiIiIiIiIiGUVBSxEREREREREREckoClqKiIiIiIiIiIhIRlHQUkRERERERERERDKKgpYiIiIiIiIiIiKSURS0FBERERERERERkYyioKWIiIiIiIiIiIhklP8H0IkqdwOZ5twAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x1440 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "for target in targets:\n",
    "    sns.distplot(target['1'], hist=True, rug=True)\n",
    "plt.title('First Sensor Measurements', fontsize = 20)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "for target in targets:\n",
    "    sns.distplot(target['2'], hist=True, rug=True)\n",
    "plt.title('Second Sensor Measurements', fontsize = 20)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "for target in targets:\n",
    "    sns.distplot(target['3'], hist=True, rug=True)\n",
    "plt.title('Third Sensor Measurements', fontsize = 20)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "for target in targets:\n",
    "    sns.distplot(target['4'], hist=True, rug=True)\n",
    "plt.title('Fourth Sensor Measurements', fontsize = 20)\n",
    "\n",
    "#sns.add_legend()\n",
    "#plt.legend()\n",
    "fig.legend(labels=['Pass','Fail'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ead904",
   "metadata": {},
   "source": [
    "3B. Perform bivariate and multivariate analysis with appropriate detailed comments after each analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac1539e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f5c650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: vmin. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3600x3600 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(50,50))  \n",
    "sns.heatmap(corrMatrix,0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac88ee32",
   "metadata": {},
   "source": [
    "Observation : \n",
    " Most of features has doesn't have correlation . Becuase already we removed features having more 70% correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503e930",
   "metadata": {},
   "source": [
    "# 4. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa2a581",
   "metadata": {},
   "source": [
    "4.A. Segregate predictors vs target attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da2d49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange data into independent variables and dependent variables\n",
    "y=df['Pass/Fail']\n",
    "X=df.drop(['Pass/Fail'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73aa15a",
   "metadata": {},
   "source": [
    "4.B. Check for target balancing and fix it if found imbalanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84a21abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    1463\n",
       " 1     104\n",
       "Name: Pass/Fail, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a4f976",
   "metadata": {},
   "source": [
    "Data is imbalance between -1,1.So we will do oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee198b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_over, y_over = oversample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "319345a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    1463\n",
       " 1    1463\n",
       "Name: Pass/Fail, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_over.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae714415",
   "metadata": {},
   "source": [
    "4.C. Perform train-test split and standardise the data or vice versa if required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11297ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc238cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = Pipeline([('scl', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "541b88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pipe_lr.fit_transform(X_train)\n",
    "X_test=pipe_lr.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd20893",
   "metadata": {},
   "source": [
    "4.D. Check if the train and test data have similar statistical characteristics when compared with original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7091970d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>3014.441551</td>\n",
       "      <td>73.480841</td>\n",
       "      <td>2743.2400</td>\n",
       "      <td>2966.66500</td>\n",
       "      <td>3011.49000</td>\n",
       "      <td>3056.54000</td>\n",
       "      <td>3356.3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>2495.866110</td>\n",
       "      <td>80.228143</td>\n",
       "      <td>2158.7500</td>\n",
       "      <td>2452.88500</td>\n",
       "      <td>2499.40500</td>\n",
       "      <td>2538.74500</td>\n",
       "      <td>2846.4400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>2200.551958</td>\n",
       "      <td>29.380973</td>\n",
       "      <td>2060.6600</td>\n",
       "      <td>2181.09995</td>\n",
       "      <td>2201.06670</td>\n",
       "      <td>2218.05550</td>\n",
       "      <td>2315.2667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>1395.383474</td>\n",
       "      <td>439.837330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1083.88580</td>\n",
       "      <td>1285.21440</td>\n",
       "      <td>1590.16990</td>\n",
       "      <td>3715.0417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>4.171281</td>\n",
       "      <td>56.103721</td>\n",
       "      <td>0.6815</td>\n",
       "      <td>1.01770</td>\n",
       "      <td>1.31680</td>\n",
       "      <td>1.51880</td>\n",
       "      <td>1114.5366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>101.116476</td>\n",
       "      <td>6.209385</td>\n",
       "      <td>82.1311</td>\n",
       "      <td>97.93780</td>\n",
       "      <td>101.51220</td>\n",
       "      <td>104.53000</td>\n",
       "      <td>129.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.121825</td>\n",
       "      <td>0.008936</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.12110</td>\n",
       "      <td>0.12240</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>1.462860</td>\n",
       "      <td>0.073849</td>\n",
       "      <td>1.1910</td>\n",
       "      <td>1.41125</td>\n",
       "      <td>1.46160</td>\n",
       "      <td>1.51685</td>\n",
       "      <td>1.6564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.000842</td>\n",
       "      <td>0.015107</td>\n",
       "      <td>-0.0534</td>\n",
       "      <td>-0.01080</td>\n",
       "      <td>-0.00130</td>\n",
       "      <td>0.00840</td>\n",
       "      <td>0.0749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.009296</td>\n",
       "      <td>-0.0349</td>\n",
       "      <td>-0.00560</td>\n",
       "      <td>0.00040</td>\n",
       "      <td>0.00590</td>\n",
       "      <td>0.0530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.964355</td>\n",
       "      <td>0.012444</td>\n",
       "      <td>0.6554</td>\n",
       "      <td>0.95810</td>\n",
       "      <td>0.96580</td>\n",
       "      <td>0.97130</td>\n",
       "      <td>0.9848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>199.956272</td>\n",
       "      <td>3.255230</td>\n",
       "      <td>182.0940</td>\n",
       "      <td>198.13095</td>\n",
       "      <td>199.53560</td>\n",
       "      <td>202.00675</td>\n",
       "      <td>272.0451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>9.005297</td>\n",
       "      <td>2.793916</td>\n",
       "      <td>2.2493</td>\n",
       "      <td>7.09675</td>\n",
       "      <td>8.96700</td>\n",
       "      <td>10.85870</td>\n",
       "      <td>19.5465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>413.084376</td>\n",
       "      <td>17.204633</td>\n",
       "      <td>333.4486</td>\n",
       "      <td>406.13100</td>\n",
       "      <td>412.21910</td>\n",
       "      <td>419.08280</td>\n",
       "      <td>824.9271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>9.907496</td>\n",
       "      <td>2.401564</td>\n",
       "      <td>4.4696</td>\n",
       "      <td>9.56855</td>\n",
       "      <td>9.85175</td>\n",
       "      <td>10.12775</td>\n",
       "      <td>102.8677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>190.046620</td>\n",
       "      <td>2.778426</td>\n",
       "      <td>169.1774</td>\n",
       "      <td>188.30065</td>\n",
       "      <td>189.66420</td>\n",
       "      <td>192.17890</td>\n",
       "      <td>215.5977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>12.481152</td>\n",
       "      <td>0.217273</td>\n",
       "      <td>9.8773</td>\n",
       "      <td>12.46000</td>\n",
       "      <td>12.49960</td>\n",
       "      <td>12.54710</td>\n",
       "      <td>12.9898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>1.405054</td>\n",
       "      <td>0.016737</td>\n",
       "      <td>1.1797</td>\n",
       "      <td>1.39650</td>\n",
       "      <td>1.40600</td>\n",
       "      <td>1.41500</td>\n",
       "      <td>1.4534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-5618.272176</td>\n",
       "      <td>626.430997</td>\n",
       "      <td>-7150.2500</td>\n",
       "      <td>-5932.62500</td>\n",
       "      <td>-5523.25000</td>\n",
       "      <td>-5356.62500</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-3806.318177</td>\n",
       "      <td>1379.280633</td>\n",
       "      <td>-9986.7500</td>\n",
       "      <td>-4370.62500</td>\n",
       "      <td>-3820.75000</td>\n",
       "      <td>-3356.37500</td>\n",
       "      <td>2363.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-298.317538</td>\n",
       "      <td>2900.846582</td>\n",
       "      <td>-14804.5000</td>\n",
       "      <td>-1474.37500</td>\n",
       "      <td>-78.75000</td>\n",
       "      <td>1376.25000</td>\n",
       "      <td>14106.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>1.203946</td>\n",
       "      <td>0.177510</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.09490</td>\n",
       "      <td>1.28300</td>\n",
       "      <td>1.30430</td>\n",
       "      <td>1.3828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>69.499093</td>\n",
       "      <td>3.458992</td>\n",
       "      <td>59.4000</td>\n",
       "      <td>67.38335</td>\n",
       "      <td>69.15560</td>\n",
       "      <td>72.25555</td>\n",
       "      <td>77.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>2.366212</td>\n",
       "      <td>0.408433</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>2.08890</td>\n",
       "      <td>2.37780</td>\n",
       "      <td>2.65560</td>\n",
       "      <td>3.5111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>3.672880</td>\n",
       "      <td>0.535050</td>\n",
       "      <td>2.0698</td>\n",
       "      <td>3.36270</td>\n",
       "      <td>3.43100</td>\n",
       "      <td>3.53125</td>\n",
       "      <td>4.8044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>85.337340</td>\n",
       "      <td>2.025908</td>\n",
       "      <td>83.1829</td>\n",
       "      <td>84.49050</td>\n",
       "      <td>85.13545</td>\n",
       "      <td>85.74190</td>\n",
       "      <td>105.6038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>8.960157</td>\n",
       "      <td>1.344036</td>\n",
       "      <td>7.6032</td>\n",
       "      <td>8.58000</td>\n",
       "      <td>8.76980</td>\n",
       "      <td>9.06060</td>\n",
       "      <td>23.3453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>66.221280</td>\n",
       "      <td>0.304044</td>\n",
       "      <td>64.9193</td>\n",
       "      <td>66.04080</td>\n",
       "      <td>66.23180</td>\n",
       "      <td>66.34305</td>\n",
       "      <td>67.9586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>86.836566</td>\n",
       "      <td>0.446613</td>\n",
       "      <td>84.7327</td>\n",
       "      <td>86.57830</td>\n",
       "      <td>86.82070</td>\n",
       "      <td>87.00240</td>\n",
       "      <td>88.4188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>68.063966</td>\n",
       "      <td>23.911897</td>\n",
       "      <td>1.4340</td>\n",
       "      <td>74.84000</td>\n",
       "      <td>78.29000</td>\n",
       "      <td>80.18000</td>\n",
       "      <td>86.1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>3.348792</td>\n",
       "      <td>2.342518</td>\n",
       "      <td>-0.0759</td>\n",
       "      <td>2.69900</td>\n",
       "      <td>3.07400</td>\n",
       "      <td>3.51500</td>\n",
       "      <td>37.8800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>355.537743</td>\n",
       "      <td>6.232884</td>\n",
       "      <td>342.7545</td>\n",
       "      <td>350.80225</td>\n",
       "      <td>353.72090</td>\n",
       "      <td>360.77180</td>\n",
       "      <td>377.2973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>10.031167</td>\n",
       "      <td>0.174982</td>\n",
       "      <td>9.4640</td>\n",
       "      <td>9.92545</td>\n",
       "      <td>10.03485</td>\n",
       "      <td>10.15245</td>\n",
       "      <td>11.0530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>136.742841</td>\n",
       "      <td>7.846746</td>\n",
       "      <td>108.8464</td>\n",
       "      <td>130.73045</td>\n",
       "      <td>136.40000</td>\n",
       "      <td>142.09095</td>\n",
       "      <td>176.3136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>1.178005</td>\n",
       "      <td>0.189585</td>\n",
       "      <td>0.4967</td>\n",
       "      <td>0.98500</td>\n",
       "      <td>1.25105</td>\n",
       "      <td>1.34020</td>\n",
       "      <td>1.5111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>139.972254</td>\n",
       "      <td>4.522806</td>\n",
       "      <td>125.7982</td>\n",
       "      <td>136.93000</td>\n",
       "      <td>140.00775</td>\n",
       "      <td>143.19410</td>\n",
       "      <td>163.2509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>4.592979</td>\n",
       "      <td>0.054880</td>\n",
       "      <td>3.7060</td>\n",
       "      <td>4.57400</td>\n",
       "      <td>4.59600</td>\n",
       "      <td>4.61700</td>\n",
       "      <td>4.7640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>2856.166560</td>\n",
       "      <td>25.716644</td>\n",
       "      <td>2801.0000</td>\n",
       "      <td>2836.00000</td>\n",
       "      <td>2854.00000</td>\n",
       "      <td>2874.00000</td>\n",
       "      <td>2936.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.928855</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.8755</td>\n",
       "      <td>0.92550</td>\n",
       "      <td>0.93100</td>\n",
       "      <td>0.93310</td>\n",
       "      <td>0.9378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.949215</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.9319</td>\n",
       "      <td>0.94670</td>\n",
       "      <td>0.94930</td>\n",
       "      <td>0.95200</td>\n",
       "      <td>0.9598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>4.593259</td>\n",
       "      <td>0.084992</td>\n",
       "      <td>4.2199</td>\n",
       "      <td>4.53190</td>\n",
       "      <td>4.57270</td>\n",
       "      <td>4.66860</td>\n",
       "      <td>4.8475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>2.951249</td>\n",
       "      <td>9.511839</td>\n",
       "      <td>-28.9882</td>\n",
       "      <td>-1.85545</td>\n",
       "      <td>0.94725</td>\n",
       "      <td>4.33770</td>\n",
       "      <td>168.1455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>10.423195</td>\n",
       "      <td>0.274351</td>\n",
       "      <td>9.4611</td>\n",
       "      <td>10.28405</td>\n",
       "      <td>10.43670</td>\n",
       "      <td>10.59055</td>\n",
       "      <td>11.7849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>116.501216</td>\n",
       "      <td>8.612494</td>\n",
       "      <td>81.4900</td>\n",
       "      <td>112.05545</td>\n",
       "      <td>116.21180</td>\n",
       "      <td>120.91820</td>\n",
       "      <td>287.1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>13.986604</td>\n",
       "      <td>7.104106</td>\n",
       "      <td>1.6591</td>\n",
       "      <td>10.38365</td>\n",
       "      <td>13.24605</td>\n",
       "      <td>16.32550</td>\n",
       "      <td>188.0923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>20.539783</td>\n",
       "      <td>4.966452</td>\n",
       "      <td>6.4482</td>\n",
       "      <td>17.37730</td>\n",
       "      <td>20.02135</td>\n",
       "      <td>22.79955</td>\n",
       "      <td>48.9882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>16.655186</td>\n",
       "      <td>306.914183</td>\n",
       "      <td>0.4137</td>\n",
       "      <td>0.89150</td>\n",
       "      <td>0.97830</td>\n",
       "      <td>1.06490</td>\n",
       "      <td>7272.8283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>147.438189</td>\n",
       "      <td>4.231976</td>\n",
       "      <td>87.0255</td>\n",
       "      <td>145.24230</td>\n",
       "      <td>147.59730</td>\n",
       "      <td>149.93590</td>\n",
       "      <td>167.8309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>104.322429</td>\n",
       "      <td>31.591384</td>\n",
       "      <td>21.4332</td>\n",
       "      <td>87.58460</td>\n",
       "      <td>102.60430</td>\n",
       "      <td>115.43960</td>\n",
       "      <td>238.4775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.002677</td>\n",
       "      <td>0.105986</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.006894</td>\n",
       "      <td>0.022121</td>\n",
       "      <td>-0.1049</td>\n",
       "      <td>-0.01920</td>\n",
       "      <td>-0.00630</td>\n",
       "      <td>0.00660</td>\n",
       "      <td>0.2315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.029383</td>\n",
       "      <td>0.032948</td>\n",
       "      <td>-0.1862</td>\n",
       "      <td>-0.05135</td>\n",
       "      <td>-0.02890</td>\n",
       "      <td>-0.00690</td>\n",
       "      <td>0.0723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.007085</td>\n",
       "      <td>0.031129</td>\n",
       "      <td>-0.1046</td>\n",
       "      <td>-0.02940</td>\n",
       "      <td>-0.00990</td>\n",
       "      <td>0.00890</td>\n",
       "      <td>0.1331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.013625</td>\n",
       "      <td>0.047504</td>\n",
       "      <td>-0.3482</td>\n",
       "      <td>-0.04730</td>\n",
       "      <td>-0.01250</td>\n",
       "      <td>0.01205</td>\n",
       "      <td>0.2492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.003415</td>\n",
       "      <td>0.022905</td>\n",
       "      <td>-0.0568</td>\n",
       "      <td>-0.01070</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.01280</td>\n",
       "      <td>0.1013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.018380</td>\n",
       "      <td>0.048862</td>\n",
       "      <td>-0.1437</td>\n",
       "      <td>-0.04295</td>\n",
       "      <td>-0.00870</td>\n",
       "      <td>0.00870</td>\n",
       "      <td>0.1186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.021130</td>\n",
       "      <td>0.016891</td>\n",
       "      <td>-0.0982</td>\n",
       "      <td>-0.02710</td>\n",
       "      <td>-0.01960</td>\n",
       "      <td>-0.01215</td>\n",
       "      <td>0.0584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.035797</td>\n",
       "      <td>-0.2129</td>\n",
       "      <td>-0.01735</td>\n",
       "      <td>0.00760</td>\n",
       "      <td>0.02680</td>\n",
       "      <td>0.1437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>7.452076</td>\n",
       "      <td>0.516087</td>\n",
       "      <td>5.8257</td>\n",
       "      <td>7.10435</td>\n",
       "      <td>7.46745</td>\n",
       "      <td>7.80735</td>\n",
       "      <td>8.9904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.133107</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>0.1174</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.13630</td>\n",
       "      <td>0.1505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>2.401872</td>\n",
       "      <td>0.037332</td>\n",
       "      <td>2.2425</td>\n",
       "      <td>2.37685</td>\n",
       "      <td>2.40390</td>\n",
       "      <td>2.42860</td>\n",
       "      <td>2.5555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.982420</td>\n",
       "      <td>0.012848</td>\n",
       "      <td>0.7749</td>\n",
       "      <td>0.97580</td>\n",
       "      <td>0.98740</td>\n",
       "      <td>0.98970</td>\n",
       "      <td>0.9935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>1807.815021</td>\n",
       "      <td>53.537262</td>\n",
       "      <td>1627.4714</td>\n",
       "      <td>1777.47030</td>\n",
       "      <td>1809.24920</td>\n",
       "      <td>1841.87300</td>\n",
       "      <td>2105.1823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.188749</td>\n",
       "      <td>0.051514</td>\n",
       "      <td>0.1113</td>\n",
       "      <td>0.16975</td>\n",
       "      <td>0.19010</td>\n",
       "      <td>0.20015</td>\n",
       "      <td>1.4727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>8827.468461</td>\n",
       "      <td>389.807042</td>\n",
       "      <td>7397.3100</td>\n",
       "      <td>8578.56995</td>\n",
       "      <td>8825.43510</td>\n",
       "      <td>9055.26000</td>\n",
       "      <td>10746.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.087515</td>\n",
       "      <td>-0.3570</td>\n",
       "      <td>-0.04265</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.05035</td>\n",
       "      <td>0.3627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>-0.0126</td>\n",
       "      <td>-0.00115</td>\n",
       "      <td>0.00040</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>0.0281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>-0.0171</td>\n",
       "      <td>-0.00160</td>\n",
       "      <td>-0.00020</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.0133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>-0.0020</td>\n",
       "      <td>-0.00010</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.0011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.062620</td>\n",
       "      <td>-0.5283</td>\n",
       "      <td>-0.02980</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02980</td>\n",
       "      <td>0.8854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>-0.0030</td>\n",
       "      <td>-0.00020</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.0023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.062847</td>\n",
       "      <td>-0.5353</td>\n",
       "      <td>-0.03530</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03360</td>\n",
       "      <td>0.2979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.009789</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>-0.0329</td>\n",
       "      <td>-0.01180</td>\n",
       "      <td>-0.01010</td>\n",
       "      <td>-0.00820</td>\n",
       "      <td>0.0203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.001759</td>\n",
       "      <td>0.087307</td>\n",
       "      <td>-0.5226</td>\n",
       "      <td>-0.04835</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.04860</td>\n",
       "      <td>0.4856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.010790</td>\n",
       "      <td>0.086591</td>\n",
       "      <td>-0.3454</td>\n",
       "      <td>-0.06440</td>\n",
       "      <td>-0.01120</td>\n",
       "      <td>0.03785</td>\n",
       "      <td>0.3938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.945424</td>\n",
       "      <td>0.012133</td>\n",
       "      <td>0.8534</td>\n",
       "      <td>0.93860</td>\n",
       "      <td>0.94640</td>\n",
       "      <td>0.95230</td>\n",
       "      <td>0.9763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>747.383792</td>\n",
       "      <td>48.949250</td>\n",
       "      <td>544.0254</td>\n",
       "      <td>721.02300</td>\n",
       "      <td>750.86140</td>\n",
       "      <td>776.78185</td>\n",
       "      <td>924.5318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.987130</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>0.8900</td>\n",
       "      <td>0.98950</td>\n",
       "      <td>0.99050</td>\n",
       "      <td>0.99090</td>\n",
       "      <td>0.9924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>58.625908</td>\n",
       "      <td>6.485174</td>\n",
       "      <td>52.8068</td>\n",
       "      <td>57.97830</td>\n",
       "      <td>58.54910</td>\n",
       "      <td>59.13390</td>\n",
       "      <td>311.7344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.598421</td>\n",
       "      <td>0.008040</td>\n",
       "      <td>0.5274</td>\n",
       "      <td>0.59420</td>\n",
       "      <td>0.59900</td>\n",
       "      <td>0.60330</td>\n",
       "      <td>0.6245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.970777</td>\n",
       "      <td>0.008949</td>\n",
       "      <td>0.8411</td>\n",
       "      <td>0.96480</td>\n",
       "      <td>0.96940</td>\n",
       "      <td>0.97830</td>\n",
       "      <td>0.9827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>6.310863</td>\n",
       "      <td>0.124304</td>\n",
       "      <td>5.1259</td>\n",
       "      <td>6.24640</td>\n",
       "      <td>6.31360</td>\n",
       "      <td>6.37585</td>\n",
       "      <td>7.5220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>15.796388</td>\n",
       "      <td>0.099332</td>\n",
       "      <td>15.4600</td>\n",
       "      <td>15.73000</td>\n",
       "      <td>15.79000</td>\n",
       "      <td>15.86000</td>\n",
       "      <td>16.0700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>3.898267</td>\n",
       "      <td>0.901520</td>\n",
       "      <td>1.6710</td>\n",
       "      <td>3.20200</td>\n",
       "      <td>3.87700</td>\n",
       "      <td>4.39200</td>\n",
       "      <td>6.8890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>2.750638</td>\n",
       "      <td>0.252744</td>\n",
       "      <td>2.3400</td>\n",
       "      <td>2.57400</td>\n",
       "      <td>2.73500</td>\n",
       "      <td>2.87300</td>\n",
       "      <td>3.9910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>3.192198</td>\n",
       "      <td>0.263414</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.07600</td>\n",
       "      <td>3.19500</td>\n",
       "      <td>3.31100</td>\n",
       "      <td>3.8950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>-0.551860</td>\n",
       "      <td>1.217366</td>\n",
       "      <td>-3.7790</td>\n",
       "      <td>-0.89880</td>\n",
       "      <td>-0.14190</td>\n",
       "      <td>0.04730</td>\n",
       "      <td>2.4580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.997808</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.9936</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>0.99775</td>\n",
       "      <td>0.99890</td>\n",
       "      <td>1.0190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>2.318513</td>\n",
       "      <td>0.053047</td>\n",
       "      <td>2.1911</td>\n",
       "      <td>2.27730</td>\n",
       "      <td>2.31240</td>\n",
       "      <td>2.35830</td>\n",
       "      <td>2.4723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>1004.043129</td>\n",
       "      <td>6.520981</td>\n",
       "      <td>980.4510</td>\n",
       "      <td>1000.04545</td>\n",
       "      <td>1004.05000</td>\n",
       "      <td>1008.67060</td>\n",
       "      <td>1020.9944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>39.389480</td>\n",
       "      <td>2.983032</td>\n",
       "      <td>33.3658</td>\n",
       "      <td>37.36890</td>\n",
       "      <td>38.90260</td>\n",
       "      <td>40.80460</td>\n",
       "      <td>64.1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>117.932355</td>\n",
       "      <td>57.454912</td>\n",
       "      <td>58.0000</td>\n",
       "      <td>92.00000</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>127.00000</td>\n",
       "      <td>994.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>138.180983</td>\n",
       "      <td>53.806875</td>\n",
       "      <td>36.1000</td>\n",
       "      <td>90.15000</td>\n",
       "      <td>134.60000</td>\n",
       "      <td>180.90000</td>\n",
       "      <td>295.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>122.670645</td>\n",
       "      <td>52.137163</td>\n",
       "      <td>19.2000</td>\n",
       "      <td>81.40000</td>\n",
       "      <td>117.70000</td>\n",
       "      <td>161.60000</td>\n",
       "      <td>334.7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>57.587810</td>\n",
       "      <td>12.291096</td>\n",
       "      <td>19.8000</td>\n",
       "      <td>51.00000</td>\n",
       "      <td>55.90010</td>\n",
       "      <td>62.90010</td>\n",
       "      <td>141.7998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>416.077185</td>\n",
       "      <td>262.221743</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>243.84110</td>\n",
       "      <td>339.56100</td>\n",
       "      <td>494.80600</td>\n",
       "      <td>1770.6909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>6.638156</td>\n",
       "      <td>3.536522</td>\n",
       "      <td>1.7400</td>\n",
       "      <td>5.11000</td>\n",
       "      <td>6.26000</td>\n",
       "      <td>7.50000</td>\n",
       "      <td>103.3900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00330</td>\n",
       "      <td>0.00390</td>\n",
       "      <td>0.00490</td>\n",
       "      <td>0.0121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.119992</td>\n",
       "      <td>0.061305</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.08390</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.13265</td>\n",
       "      <td>0.6253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.063615</td>\n",
       "      <td>0.026524</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.04805</td>\n",
       "      <td>0.05860</td>\n",
       "      <td>0.07180</td>\n",
       "      <td>0.2507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.055004</td>\n",
       "      <td>0.021831</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.04235</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.06150</td>\n",
       "      <td>0.2479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>6.812615</td>\n",
       "      <td>3.238956</td>\n",
       "      <td>1.3370</td>\n",
       "      <td>4.46550</td>\n",
       "      <td>5.95100</td>\n",
       "      <td>8.26950</td>\n",
       "      <td>22.3180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>14.041556</td>\n",
       "      <td>30.973119</td>\n",
       "      <td>2.0200</td>\n",
       "      <td>8.09600</td>\n",
       "      <td>10.99350</td>\n",
       "      <td>14.34200</td>\n",
       "      <td>536.5640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.011925</td>\n",
       "      <td>0.009337</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.00730</td>\n",
       "      <td>0.01110</td>\n",
       "      <td>0.01490</td>\n",
       "      <td>0.2389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.058089</td>\n",
       "      <td>0.079174</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.03625</td>\n",
       "      <td>0.04870</td>\n",
       "      <td>0.06670</td>\n",
       "      <td>2.2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>882.349075</td>\n",
       "      <td>982.458855</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>411.50000</td>\n",
       "      <td>623.00000</td>\n",
       "      <td>963.50000</td>\n",
       "      <td>7791.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>555.196554</td>\n",
       "      <td>574.456703</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>295.00000</td>\n",
       "      <td>438.00000</td>\n",
       "      <td>624.50000</td>\n",
       "      <td>4170.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>4064.996171</td>\n",
       "      <td>4236.854877</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1322.50000</td>\n",
       "      <td>2614.00000</td>\n",
       "      <td>5033.00000</td>\n",
       "      <td>37943.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>4793.308870</td>\n",
       "      <td>6550.267099</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>451.00000</td>\n",
       "      <td>1784.00000</td>\n",
       "      <td>6376.00000</td>\n",
       "      <td>36871.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>2.788641</td>\n",
       "      <td>1.119061</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>2.10000</td>\n",
       "      <td>2.60000</td>\n",
       "      <td>3.20000</td>\n",
       "      <td>21.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>1.235737</td>\n",
       "      <td>0.632364</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.90000</td>\n",
       "      <td>1.20000</td>\n",
       "      <td>1.50000</td>\n",
       "      <td>16.3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.124390</td>\n",
       "      <td>0.047609</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>0.09000</td>\n",
       "      <td>0.11900</td>\n",
       "      <td>0.15050</td>\n",
       "      <td>0.7250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.400468</td>\n",
       "      <td>0.197792</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.23050</td>\n",
       "      <td>0.41200</td>\n",
       "      <td>0.53600</td>\n",
       "      <td>1.1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.684331</td>\n",
       "      <td>0.157418</td>\n",
       "      <td>0.2979</td>\n",
       "      <td>0.57560</td>\n",
       "      <td>0.68600</td>\n",
       "      <td>0.79730</td>\n",
       "      <td>1.1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.120059</td>\n",
       "      <td>0.060766</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.07980</td>\n",
       "      <td>0.11250</td>\n",
       "      <td>0.14030</td>\n",
       "      <td>0.4940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.320115</td>\n",
       "      <td>0.071220</td>\n",
       "      <td>0.1287</td>\n",
       "      <td>0.27660</td>\n",
       "      <td>0.32385</td>\n",
       "      <td>0.37020</td>\n",
       "      <td>0.5484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.576193</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.2538</td>\n",
       "      <td>0.51690</td>\n",
       "      <td>0.57760</td>\n",
       "      <td>0.63450</td>\n",
       "      <td>0.8643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.778037</td>\n",
       "      <td>0.116285</td>\n",
       "      <td>0.4616</td>\n",
       "      <td>0.69220</td>\n",
       "      <td>0.76820</td>\n",
       "      <td>0.84390</td>\n",
       "      <td>1.1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.244717</td>\n",
       "      <td>0.074894</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.19630</td>\n",
       "      <td>0.24290</td>\n",
       "      <td>0.29375</td>\n",
       "      <td>0.4411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.394699</td>\n",
       "      <td>0.282823</td>\n",
       "      <td>0.0470</td>\n",
       "      <td>0.22200</td>\n",
       "      <td>0.29900</td>\n",
       "      <td>0.42300</td>\n",
       "      <td>1.8580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>19.013050</td>\n",
       "      <td>3.310585</td>\n",
       "      <td>9.4000</td>\n",
       "      <td>16.85000</td>\n",
       "      <td>18.69000</td>\n",
       "      <td>20.96500</td>\n",
       "      <td>48.6700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.546756</td>\n",
       "      <td>0.224331</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>0.37800</td>\n",
       "      <td>0.52400</td>\n",
       "      <td>0.68850</td>\n",
       "      <td>3.5730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>10.780153</td>\n",
       "      <td>4.162749</td>\n",
       "      <td>3.1700</td>\n",
       "      <td>7.73500</td>\n",
       "      <td>10.17000</td>\n",
       "      <td>13.33500</td>\n",
       "      <td>55.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>26.661515</td>\n",
       "      <td>6.833931</td>\n",
       "      <td>5.0140</td>\n",
       "      <td>21.17200</td>\n",
       "      <td>27.20050</td>\n",
       "      <td>31.68700</td>\n",
       "      <td>72.9470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.144808</td>\n",
       "      <td>0.110163</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>0.10220</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.16910</td>\n",
       "      <td>3.2283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>43.209503</td>\n",
       "      <td>21.705075</td>\n",
       "      <td>6.6130</td>\n",
       "      <td>24.71400</td>\n",
       "      <td>40.20950</td>\n",
       "      <td>57.67250</td>\n",
       "      <td>191.8300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.287013</td>\n",
       "      <td>0.394684</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>0.21950</td>\n",
       "      <td>0.25900</td>\n",
       "      <td>0.29600</td>\n",
       "      <td>4.8380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>17.598561</td>\n",
       "      <td>8.671307</td>\n",
       "      <td>3.2100</td>\n",
       "      <td>14.17500</td>\n",
       "      <td>17.23500</td>\n",
       "      <td>20.16000</td>\n",
       "      <td>199.6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>7.834537</td>\n",
       "      <td>5.093583</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.02500</td>\n",
       "      <td>6.76000</td>\n",
       "      <td>9.48500</td>\n",
       "      <td>126.5300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>73.264254</td>\n",
       "      <td>28.013323</td>\n",
       "      <td>5.3590</td>\n",
       "      <td>56.22050</td>\n",
       "      <td>73.24800</td>\n",
       "      <td>90.45250</td>\n",
       "      <td>172.3490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.088725</td>\n",
       "      <td>0.041756</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.06590</td>\n",
       "      <td>0.07970</td>\n",
       "      <td>0.09905</td>\n",
       "      <td>0.5164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.056701</td>\n",
       "      <td>0.024817</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.04395</td>\n",
       "      <td>0.05320</td>\n",
       "      <td>0.06410</td>\n",
       "      <td>0.3227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.051281</td>\n",
       "      <td>0.031358</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.03260</td>\n",
       "      <td>0.04160</td>\n",
       "      <td>0.06210</td>\n",
       "      <td>0.5941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.060280</td>\n",
       "      <td>0.052624</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.03695</td>\n",
       "      <td>0.05600</td>\n",
       "      <td>0.07340</td>\n",
       "      <td>1.2837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.083148</td>\n",
       "      <td>0.056030</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.05705</td>\n",
       "      <td>0.07540</td>\n",
       "      <td>0.09325</td>\n",
       "      <td>0.7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.081097</td>\n",
       "      <td>0.030203</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.06360</td>\n",
       "      <td>0.08250</td>\n",
       "      <td>0.09810</td>\n",
       "      <td>0.3429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.083501</td>\n",
       "      <td>0.025566</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.06980</td>\n",
       "      <td>0.08460</td>\n",
       "      <td>0.09730</td>\n",
       "      <td>0.2828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.071483</td>\n",
       "      <td>0.045944</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.04610</td>\n",
       "      <td>0.06170</td>\n",
       "      <td>0.08605</td>\n",
       "      <td>0.6744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>3.771376</td>\n",
       "      <td>1.170068</td>\n",
       "      <td>1.0340</td>\n",
       "      <td>2.94620</td>\n",
       "      <td>3.63075</td>\n",
       "      <td>4.40340</td>\n",
       "      <td>8.8015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.00230</td>\n",
       "      <td>0.00300</td>\n",
       "      <td>0.00380</td>\n",
       "      <td>0.0163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.060718</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.04020</td>\n",
       "      <td>0.06090</td>\n",
       "      <td>0.07650</td>\n",
       "      <td>0.2305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.008821</td>\n",
       "      <td>0.055937</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>0.00230</td>\n",
       "      <td>0.00550</td>\n",
       "      <td>0.9911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>122.846571</td>\n",
       "      <td>55.156003</td>\n",
       "      <td>32.2637</td>\n",
       "      <td>95.14735</td>\n",
       "      <td>119.43600</td>\n",
       "      <td>144.50280</td>\n",
       "      <td>1768.8802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>1038.656080</td>\n",
       "      <td>426.259257</td>\n",
       "      <td>168.7998</td>\n",
       "      <td>726.50000</td>\n",
       "      <td>967.29980</td>\n",
       "      <td>1252.39965</td>\n",
       "      <td>3601.2998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.019122</td>\n",
       "      <td>0.010749</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.01325</td>\n",
       "      <td>0.01650</td>\n",
       "      <td>0.02120</td>\n",
       "      <td>0.1541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.017841</td>\n",
       "      <td>0.010738</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.01265</td>\n",
       "      <td>0.01550</td>\n",
       "      <td>0.02000</td>\n",
       "      <td>0.2133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.004791</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.00370</td>\n",
       "      <td>0.00460</td>\n",
       "      <td>0.00570</td>\n",
       "      <td>0.0244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.00360</td>\n",
       "      <td>0.00440</td>\n",
       "      <td>0.00530</td>\n",
       "      <td>0.0236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>109.650967</td>\n",
       "      <td>54.597274</td>\n",
       "      <td>21.0107</td>\n",
       "      <td>76.13215</td>\n",
       "      <td>103.09360</td>\n",
       "      <td>131.75840</td>\n",
       "      <td>1119.7042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.004285</td>\n",
       "      <td>0.037472</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.00070</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.00130</td>\n",
       "      <td>0.9909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.033179</td>\n",
       "      <td>0.022255</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.02460</td>\n",
       "      <td>0.03080</td>\n",
       "      <td>0.03770</td>\n",
       "      <td>0.4517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.403848</td>\n",
       "      <td>0.120334</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.30760</td>\n",
       "      <td>0.40510</td>\n",
       "      <td>0.48095</td>\n",
       "      <td>0.9255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.070587</td>\n",
       "      <td>0.029573</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.04400</td>\n",
       "      <td>0.07060</td>\n",
       "      <td>0.09160</td>\n",
       "      <td>0.1578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>19.496877</td>\n",
       "      <td>7.326430</td>\n",
       "      <td>6.0980</td>\n",
       "      <td>13.82800</td>\n",
       "      <td>17.97700</td>\n",
       "      <td>24.65300</td>\n",
       "      <td>40.8550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>3.777486</td>\n",
       "      <td>1.149394</td>\n",
       "      <td>1.3017</td>\n",
       "      <td>2.96240</td>\n",
       "      <td>3.70350</td>\n",
       "      <td>4.37680</td>\n",
       "      <td>10.1529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>320.236157</td>\n",
       "      <td>287.521430</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>302.17760</td>\n",
       "      <td>523.62445</td>\n",
       "      <td>999.3160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>309.014570</td>\n",
       "      <td>325.243132</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>272.44870</td>\n",
       "      <td>582.80310</td>\n",
       "      <td>998.6813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>77.645599</td>\n",
       "      <td>32.567460</td>\n",
       "      <td>23.0200</td>\n",
       "      <td>56.00820</td>\n",
       "      <td>69.90545</td>\n",
       "      <td>92.83275</td>\n",
       "      <td>424.2152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>99.314795</td>\n",
       "      <td>126.116776</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>31.03385</td>\n",
       "      <td>57.96930</td>\n",
       "      <td>120.13690</td>\n",
       "      <td>994.2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>205.449868</td>\n",
       "      <td>225.643014</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.04745</td>\n",
       "      <td>151.11560</td>\n",
       "      <td>304.54180</td>\n",
       "      <td>995.7447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>54.693892</td>\n",
       "      <td>34.086853</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>36.34370</td>\n",
       "      <td>49.09090</td>\n",
       "      <td>66.66670</td>\n",
       "      <td>851.6129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>29.195480</td>\n",
       "      <td>13.331150</td>\n",
       "      <td>7.9534</td>\n",
       "      <td>20.22410</td>\n",
       "      <td>26.16785</td>\n",
       "      <td>35.26840</td>\n",
       "      <td>149.3851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>223.843226</td>\n",
       "      <td>230.303247</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>38.88265</td>\n",
       "      <td>150.34010</td>\n",
       "      <td>334.67400</td>\n",
       "      <td>999.8770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>137.890044</td>\n",
       "      <td>47.591323</td>\n",
       "      <td>11.4997</td>\n",
       "      <td>105.62215</td>\n",
       "      <td>138.25515</td>\n",
       "      <td>168.27025</td>\n",
       "      <td>492.7718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>20.116232</td>\n",
       "      <td>14.913155</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.58535</td>\n",
       "      <td>15.97380</td>\n",
       "      <td>23.68240</td>\n",
       "      <td>274.8871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>318.037084</td>\n",
       "      <td>278.866442</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>293.51850</td>\n",
       "      <td>512.39075</td>\n",
       "      <td>999.4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>205.672096</td>\n",
       "      <td>191.514533</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>82.41015</td>\n",
       "      <td>148.31750</td>\n",
       "      <td>260.07900</td>\n",
       "      <td>989.4737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>214.117076</td>\n",
       "      <td>211.695946</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>77.01180</td>\n",
       "      <td>138.77550</td>\n",
       "      <td>288.91845</td>\n",
       "      <td>996.8586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>199.761505</td>\n",
       "      <td>217.277824</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>51.18850</td>\n",
       "      <td>112.95340</td>\n",
       "      <td>283.28900</td>\n",
       "      <td>994.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>301.700889</td>\n",
       "      <td>285.226689</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>249.92700</td>\n",
       "      <td>497.38450</td>\n",
       "      <td>999.4911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>237.507454</td>\n",
       "      <td>262.273846</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>57.31690</td>\n",
       "      <td>112.27550</td>\n",
       "      <td>391.27750</td>\n",
       "      <td>995.7447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>352.553880</td>\n",
       "      <td>250.105428</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>145.15685</td>\n",
       "      <td>348.52940</td>\n",
       "      <td>507.49705</td>\n",
       "      <td>997.5186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>271.362828</td>\n",
       "      <td>226.384996</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>113.80665</td>\n",
       "      <td>219.48720</td>\n",
       "      <td>372.34190</td>\n",
       "      <td>994.0035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>262.859941</td>\n",
       "      <td>324.699975</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>536.12260</td>\n",
       "      <td>1000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>240.673807</td>\n",
       "      <td>322.911797</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>505.22575</td>\n",
       "      <td>999.2337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>55.752306</td>\n",
       "      <td>37.668964</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>35.32440</td>\n",
       "      <td>46.98610</td>\n",
       "      <td>64.22845</td>\n",
       "      <td>451.4851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>275.627217</td>\n",
       "      <td>329.601505</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>554.01070</td>\n",
       "      <td>1000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>11.610080</td>\n",
       "      <td>103.122996</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.111206</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.11340</td>\n",
       "      <td>0.1184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.008470</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.00780</td>\n",
       "      <td>0.00780</td>\n",
       "      <td>0.00900</td>\n",
       "      <td>0.0240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.00260</td>\n",
       "      <td>0.00260</td>\n",
       "      <td>0.0047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>1.018304</td>\n",
       "      <td>0.358508</td>\n",
       "      <td>0.4444</td>\n",
       "      <td>0.81410</td>\n",
       "      <td>0.91110</td>\n",
       "      <td>1.21650</td>\n",
       "      <td>3.9786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>403.476047</td>\n",
       "      <td>4.627143</td>\n",
       "      <td>372.8220</td>\n",
       "      <td>400.81400</td>\n",
       "      <td>403.12200</td>\n",
       "      <td>406.76300</td>\n",
       "      <td>421.7020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>75.415081</td>\n",
       "      <td>3.152734</td>\n",
       "      <td>71.0380</td>\n",
       "      <td>73.25400</td>\n",
       "      <td>74.08400</td>\n",
       "      <td>76.96000</td>\n",
       "      <td>83.7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>16.901595</td>\n",
       "      <td>4.542840</td>\n",
       "      <td>6.1100</td>\n",
       "      <td>14.82000</td>\n",
       "      <td>16.34000</td>\n",
       "      <td>18.40500</td>\n",
       "      <td>131.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.994995</td>\n",
       "      <td>0.083835</td>\n",
       "      <td>0.8919</td>\n",
       "      <td>0.95520</td>\n",
       "      <td>0.97270</td>\n",
       "      <td>1.00080</td>\n",
       "      <td>1.5121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.325686</td>\n",
       "      <td>0.201329</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>0.14995</td>\n",
       "      <td>0.29090</td>\n",
       "      <td>0.44360</td>\n",
       "      <td>1.0737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>262.998383</td>\n",
       "      <td>6.958289</td>\n",
       "      <td>242.2860</td>\n",
       "      <td>262.10100</td>\n",
       "      <td>264.27200</td>\n",
       "      <td>264.73300</td>\n",
       "      <td>311.4040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.674651</td>\n",
       "      <td>0.111169</td>\n",
       "      <td>0.3049</td>\n",
       "      <td>0.56710</td>\n",
       "      <td>0.65100</td>\n",
       "      <td>0.73825</td>\n",
       "      <td>1.2988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>6.221117</td>\n",
       "      <td>2.442203</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>4.98000</td>\n",
       "      <td>5.16000</td>\n",
       "      <td>7.31000</td>\n",
       "      <td>32.5800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.141069</td>\n",
       "      <td>0.074373</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.08770</td>\n",
       "      <td>0.11955</td>\n",
       "      <td>0.16685</td>\n",
       "      <td>0.6892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>530.523623</td>\n",
       "      <td>17.499736</td>\n",
       "      <td>317.1964</td>\n",
       "      <td>530.70270</td>\n",
       "      <td>532.39820</td>\n",
       "      <td>534.35640</td>\n",
       "      <td>589.5082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>2.101836</td>\n",
       "      <td>0.275112</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>1.98290</td>\n",
       "      <td>2.11860</td>\n",
       "      <td>2.29065</td>\n",
       "      <td>2.7395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>28.450165</td>\n",
       "      <td>86.304681</td>\n",
       "      <td>3.5400</td>\n",
       "      <td>7.50000</td>\n",
       "      <td>8.65000</td>\n",
       "      <td>10.13000</td>\n",
       "      <td>454.5600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.500096</td>\n",
       "      <td>0.003403</td>\n",
       "      <td>0.4778</td>\n",
       "      <td>0.49790</td>\n",
       "      <td>0.50020</td>\n",
       "      <td>0.50235</td>\n",
       "      <td>0.5098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.015317</td>\n",
       "      <td>0.017174</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.01160</td>\n",
       "      <td>0.01380</td>\n",
       "      <td>0.01650</td>\n",
       "      <td>0.4766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.021458</td>\n",
       "      <td>0.012354</td>\n",
       "      <td>-0.0169</td>\n",
       "      <td>0.01345</td>\n",
       "      <td>0.02050</td>\n",
       "      <td>0.02760</td>\n",
       "      <td>0.1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>0.016474</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.01060</td>\n",
       "      <td>0.01480</td>\n",
       "      <td>0.02030</td>\n",
       "      <td>0.0799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>1567.0</td>\n",
       "      <td>99.652345</td>\n",
       "      <td>93.864558</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>44.36860</td>\n",
       "      <td>71.90050</td>\n",
       "      <td>114.74970</td>\n",
       "      <td>737.3048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count         mean          std         min         25%         50%  \\\n",
       "0    1567.0  3014.441551    73.480841   2743.2400  2966.66500  3011.49000   \n",
       "1    1567.0  2495.866110    80.228143   2158.7500  2452.88500  2499.40500   \n",
       "2    1567.0  2200.551958    29.380973   2060.6600  2181.09995  2201.06670   \n",
       "3    1567.0  1395.383474   439.837330      0.0000  1083.88580  1285.21440   \n",
       "4    1567.0     4.171281    56.103721      0.6815     1.01770     1.31680   \n",
       "6    1567.0   101.116476     6.209385     82.1311    97.93780   101.51220   \n",
       "7    1567.0     0.121825     0.008936      0.0000     0.12110     0.12240   \n",
       "8    1567.0     1.462860     0.073849      1.1910     1.41125     1.46160   \n",
       "9    1567.0    -0.000842     0.015107     -0.0534    -0.01080    -0.00130   \n",
       "10   1567.0     0.000146     0.009296     -0.0349    -0.00560     0.00040   \n",
       "11   1567.0     0.964355     0.012444      0.6554     0.95810     0.96580   \n",
       "12   1567.0   199.956272     3.255230    182.0940   198.13095   199.53560   \n",
       "14   1567.0     9.005297     2.793916      2.2493     7.09675     8.96700   \n",
       "15   1567.0   413.084376    17.204633    333.4486   406.13100   412.21910   \n",
       "16   1567.0     9.907496     2.401564      4.4696     9.56855     9.85175   \n",
       "18   1567.0   190.046620     2.778426    169.1774   188.30065   189.66420   \n",
       "19   1567.0    12.481152     0.217273      9.8773    12.46000    12.49960   \n",
       "20   1567.0     1.405054     0.016737      1.1797     1.39650     1.40600   \n",
       "21   1567.0 -5618.272176   626.430997  -7150.2500 -5932.62500 -5523.25000   \n",
       "23   1567.0 -3806.318177  1379.280633  -9986.7500 -4370.62500 -3820.75000   \n",
       "24   1567.0  -298.317538  2900.846582 -14804.5000 -1474.37500   -78.75000   \n",
       "25   1567.0     1.203946     0.177510      0.0000     1.09490     1.28300   \n",
       "28   1567.0    69.499093     3.458992     59.4000    67.38335    69.15560   \n",
       "29   1567.0     2.366212     0.408433      0.6667     2.08890     2.37780   \n",
       "31   1567.0     3.672880     0.535050      2.0698     3.36270     3.43100   \n",
       "32   1567.0    85.337340     2.025908     83.1829    84.49050    85.13545   \n",
       "33   1567.0     8.960157     1.344036      7.6032     8.58000     8.76980   \n",
       "37   1567.0    66.221280     0.304044     64.9193    66.04080    66.23180   \n",
       "38   1567.0    86.836566     0.446613     84.7327    86.57830    86.82070   \n",
       "40   1567.0    68.063966    23.911897      1.4340    74.84000    78.29000   \n",
       "41   1567.0     3.348792     2.342518     -0.0759     2.69900     3.07400   \n",
       "43   1567.0   355.537743     6.232884    342.7545   350.80225   353.72090   \n",
       "44   1567.0    10.031167     0.174982      9.4640     9.92545    10.03485   \n",
       "45   1567.0   136.742841     7.846746    108.8464   130.73045   136.40000   \n",
       "47   1567.0     1.178005     0.189585      0.4967     0.98500     1.25105   \n",
       "48   1567.0   139.972254     4.522806    125.7982   136.93000   140.00775   \n",
       "53   1567.0     4.592979     0.054880      3.7060     4.57400     4.59600   \n",
       "55   1567.0  2856.166560    25.716644   2801.0000  2836.00000  2854.00000   \n",
       "56   1567.0     0.928855     0.006800      0.8755     0.92550     0.93100   \n",
       "57   1567.0     0.949215     0.004171      0.9319     0.94670     0.94930   \n",
       "58   1567.0     4.593259     0.084992      4.2199     4.53190     4.57270   \n",
       "59   1567.0     2.951249     9.511839    -28.9882    -1.85545     0.94725   \n",
       "61   1567.0    10.423195     0.274351      9.4611    10.28405    10.43670   \n",
       "62   1567.0   116.501216     8.612494     81.4900   112.05545   116.21180   \n",
       "63   1567.0    13.986604     7.104106      1.6591    10.38365    13.24605   \n",
       "64   1567.0    20.539783     4.966452      6.4482    17.37730    20.02135   \n",
       "67   1567.0    16.655186   306.914183      0.4137     0.89150     0.97830   \n",
       "68   1567.0   147.438189     4.231976     87.0255   145.24230   147.59730   \n",
       "71   1567.0   104.322429    31.591384     21.4332    87.58460   102.60430   \n",
       "74   1567.0     0.002677     0.105986      0.0000     0.00000     0.00000   \n",
       "75   1567.0    -0.006894     0.022121     -0.1049    -0.01920    -0.00630   \n",
       "76   1567.0    -0.029383     0.032948     -0.1862    -0.05135    -0.02890   \n",
       "77   1567.0    -0.007085     0.031129     -0.1046    -0.02940    -0.00990   \n",
       "78   1567.0    -0.013625     0.047504     -0.3482    -0.04730    -0.01250   \n",
       "79   1567.0     0.003415     0.022905     -0.0568    -0.01070     0.00060   \n",
       "80   1567.0    -0.018380     0.048862     -0.1437    -0.04295    -0.00870   \n",
       "81   1567.0    -0.021130     0.016891     -0.0982    -0.02710    -0.01960   \n",
       "82   1567.0     0.006079     0.035797     -0.2129    -0.01735     0.00760   \n",
       "83   1567.0     7.452076     0.516087      5.8257     7.10435     7.46745   \n",
       "84   1567.0     0.133107     0.005032      0.1174     0.12980     0.13300   \n",
       "86   1567.0     2.401872     0.037332      2.2425     2.37685     2.40390   \n",
       "87   1567.0     0.982420     0.012848      0.7749     0.97580     0.98740   \n",
       "88   1567.0  1807.815021    53.537262   1627.4714  1777.47030  1809.24920   \n",
       "89   1567.0     0.188749     0.051514      0.1113     0.16975     0.19010   \n",
       "90   1567.0  8827.468461   389.807042   7397.3100  8578.56995  8825.43510   \n",
       "91   1567.0     0.002431     0.087515     -0.3570    -0.04265     0.00000   \n",
       "92   1567.0     0.000507     0.003229     -0.0126    -0.00115     0.00040   \n",
       "93   1567.0    -0.000540     0.003008     -0.0171    -0.00160    -0.00020   \n",
       "94   1567.0    -0.000029     0.000174     -0.0020    -0.00010     0.00000   \n",
       "95   1567.0     0.000060     0.000104     -0.0009     0.00000     0.00000   \n",
       "99   1567.0     0.001534     0.062620     -0.5283    -0.02980     0.00000   \n",
       "100  1567.0    -0.000021     0.000355     -0.0030    -0.00020     0.00000   \n",
       "102  1567.0     0.001110     0.062847     -0.5353    -0.03530     0.00000   \n",
       "103  1567.0    -0.009789     0.003063     -0.0329    -0.01180    -0.01010   \n",
       "107  1567.0    -0.001759     0.087307     -0.5226    -0.04835     0.00000   \n",
       "108  1567.0    -0.010790     0.086591     -0.3454    -0.06440    -0.01120   \n",
       "113  1567.0     0.945424     0.012133      0.8534     0.93860     0.94640   \n",
       "114  1567.0     0.000123     0.001668      0.0000     0.00000     0.00000   \n",
       "115  1567.0   747.383792    48.949250    544.0254   721.02300   750.86140   \n",
       "116  1567.0     0.987130     0.009497      0.8900     0.98950     0.99050   \n",
       "117  1567.0    58.625908     6.485174     52.8068    57.97830    58.54910   \n",
       "118  1567.0     0.598421     0.008040      0.5274     0.59420     0.59900   \n",
       "119  1567.0     0.970777     0.008949      0.8411     0.96480     0.96940   \n",
       "120  1567.0     6.310863     0.124304      5.1259     6.24640     6.31360   \n",
       "121  1567.0    15.796388     0.099332     15.4600    15.73000    15.79000   \n",
       "122  1567.0     3.898267     0.901520      1.6710     3.20200     3.87700   \n",
       "126  1567.0     2.750638     0.252744      2.3400     2.57400     2.73500   \n",
       "128  1567.0     3.192198     0.263414      0.0000     3.07600     3.19500   \n",
       "129  1567.0    -0.551860     1.217366     -3.7790    -0.89880    -0.14190   \n",
       "131  1567.0     0.997808     0.002244      0.9936     0.99640     0.99775   \n",
       "132  1567.0     2.318513     0.053047      2.1911     2.27730     2.31240   \n",
       "133  1567.0  1004.043129     6.520981    980.4510  1000.04545  1004.05000   \n",
       "134  1567.0    39.389480     2.983032     33.3658    37.36890    38.90260   \n",
       "135  1567.0   117.932355    57.454912     58.0000    92.00000   109.00000   \n",
       "136  1567.0   138.180983    53.806875     36.1000    90.15000   134.60000   \n",
       "137  1567.0   122.670645    52.137163     19.2000    81.40000   117.70000   \n",
       "138  1567.0    57.587810    12.291096     19.8000    51.00000    55.90010   \n",
       "139  1567.0   416.077185   262.221743      0.0000   243.84110   339.56100   \n",
       "142  1567.0     6.638156     3.536522      1.7400     5.11000     6.26000   \n",
       "143  1567.0     0.004168     0.001278      0.0000     0.00330     0.00390   \n",
       "144  1567.0     0.119992     0.061305      0.0324     0.08390     0.10750   \n",
       "145  1567.0     0.063615     0.026524      0.0214     0.04805     0.05860   \n",
       "146  1567.0     0.055004     0.021831      0.0227     0.04235     0.05000   \n",
       "150  1567.0     6.812615     3.238956      1.3370     4.46550     5.95100   \n",
       "151  1567.0    14.041556    30.973119      2.0200     8.09600    10.99350   \n",
       "153  1567.0     0.011925     0.009337      0.0036     0.00730     0.01110   \n",
       "156  1567.0     0.058089     0.079174      0.0111     0.03625     0.04870   \n",
       "159  1567.0   882.349075   982.458855      0.0000   411.50000   623.00000   \n",
       "160  1567.0   555.196554   574.456703      0.0000   295.00000   438.00000   \n",
       "161  1567.0  4064.996171  4236.854877      0.0000  1322.50000  2614.00000   \n",
       "162  1567.0  4793.308870  6550.267099      0.0000   451.00000  1784.00000   \n",
       "166  1567.0     2.788641     1.119061      0.8000     2.10000     2.60000   \n",
       "167  1567.0     1.235737     0.632364      0.3000     0.90000     1.20000   \n",
       "168  1567.0     0.124390     0.047609      0.0330     0.09000     0.11900   \n",
       "169  1567.0     0.400468     0.197792      0.0460     0.23050     0.41200   \n",
       "170  1567.0     0.684331     0.157418      0.2979     0.57560     0.68600   \n",
       "171  1567.0     0.120059     0.060766      0.0089     0.07980     0.11250   \n",
       "172  1567.0     0.320115     0.071220      0.1287     0.27660     0.32385   \n",
       "173  1567.0     0.576193     0.095703      0.2538     0.51690     0.57760   \n",
       "175  1567.0     0.778037     0.116285      0.4616     0.69220     0.76820   \n",
       "176  1567.0     0.244717     0.074894      0.0735     0.19630     0.24290   \n",
       "177  1567.0     0.394699     0.282823      0.0470     0.22200     0.29900   \n",
       "180  1567.0    19.013050     3.310585      9.4000    16.85000    18.69000   \n",
       "181  1567.0     0.546756     0.224331      0.0930     0.37800     0.52400   \n",
       "182  1567.0    10.780153     4.162749      3.1700     7.73500    10.17000   \n",
       "183  1567.0    26.661515     6.833931      5.0140    21.17200    27.20050   \n",
       "184  1567.0     0.144808     0.110163      0.0297     0.10220     0.13260   \n",
       "188  1567.0    43.209503    21.705075      6.6130    24.71400    40.20950   \n",
       "195  1567.0     0.287013     0.394684      0.0800     0.21950     0.25900   \n",
       "200  1567.0    17.598561     8.671307      3.2100    14.17500    17.23500   \n",
       "201  1567.0     7.834537     5.093583      0.0000     5.02500     6.76000   \n",
       "208  1567.0    73.264254    28.013323      5.3590    56.22050    73.24800   \n",
       "210  1567.0     0.088725     0.041756      0.0319     0.06590     0.07970   \n",
       "211  1567.0     0.056701     0.024817      0.0022     0.04395     0.05320   \n",
       "212  1567.0     0.051281     0.031358      0.0071     0.03260     0.04160   \n",
       "213  1567.0     0.060280     0.052624      0.0037     0.03695     0.05600   \n",
       "214  1567.0     0.083148     0.056030      0.0193     0.05705     0.07540   \n",
       "215  1567.0     0.081097     0.030203      0.0059     0.06360     0.08250   \n",
       "216  1567.0     0.083501     0.025566      0.0097     0.06980     0.08460   \n",
       "217  1567.0     0.071483     0.045944      0.0079     0.04610     0.06170   \n",
       "218  1567.0     3.771376     1.170068      1.0340     2.94620     3.63075   \n",
       "219  1567.0     0.003252     0.001640      0.0007     0.00230     0.00300   \n",
       "221  1567.0     0.060718     0.023305      0.0200     0.04020     0.06090   \n",
       "222  1567.0     0.008821     0.055937      0.0003     0.00140     0.00230   \n",
       "223  1567.0   122.846571    55.156003     32.2637    95.14735   119.43600   \n",
       "225  1567.0  1038.656080   426.259257    168.7998   726.50000   967.29980   \n",
       "227  1567.0     0.019122     0.010749      0.0062     0.01325     0.01650   \n",
       "228  1567.0     0.017841     0.010738      0.0072     0.01265     0.01550   \n",
       "238  1567.0     0.004791     0.001697      0.0013     0.00370     0.00460   \n",
       "239  1567.0     0.004575     0.001440      0.0014     0.00360     0.00440   \n",
       "250  1567.0   109.650967    54.597274     21.0107    76.13215   103.09360   \n",
       "251  1567.0     0.004285     0.037472      0.0003     0.00070     0.00100   \n",
       "253  1567.0     0.033179     0.022255      0.0094     0.02460     0.03080   \n",
       "255  1567.0     0.403848     0.120334      0.1269     0.30760     0.40510   \n",
       "267  1567.0     0.070587     0.029573      0.0198     0.04400     0.07060   \n",
       "268  1567.0    19.496877     7.326430      6.0980    13.82800    17.97700   \n",
       "269  1567.0     3.777486     1.149394      1.3017     2.96240     3.70350   \n",
       "418  1567.0   320.236157   287.521430      0.0000     0.00000   302.17760   \n",
       "419  1567.0   309.014570   325.243132      0.0000     0.00000   272.44870   \n",
       "423  1567.0    77.645599    32.567460     23.0200    56.00820    69.90545   \n",
       "432  1567.0    99.314795   126.116776      0.0000    31.03385    57.96930   \n",
       "433  1567.0   205.449868   225.643014      0.0000    10.04745   151.11560   \n",
       "438  1567.0    54.693892    34.086853      0.0000    36.34370    49.09090   \n",
       "460  1567.0    29.195480    13.331150      7.9534    20.22410    26.16785   \n",
       "468  1567.0   223.843226   230.303247      0.0000    38.88265   150.34010   \n",
       "472  1567.0   137.890044    47.591323     11.4997   105.62215   138.25515   \n",
       "476  1567.0    20.116232    14.913155      0.0000    11.58535    15.97380   \n",
       "482  1567.0   318.037084   278.866442      0.0000     0.00000   293.51850   \n",
       "483  1567.0   205.672096   191.514533      0.0000    82.41015   148.31750   \n",
       "484  1567.0   214.117076   211.695946      0.0000    77.01180   138.77550   \n",
       "485  1567.0   199.761505   217.277824      0.0000    51.18850   112.95340   \n",
       "486  1567.0   301.700889   285.226689      0.0000     0.00000   249.92700   \n",
       "487  1567.0   237.507454   262.273846      0.0000    57.31690   112.27550   \n",
       "488  1567.0   352.553880   250.105428      0.0000   145.15685   348.52940   \n",
       "489  1567.0   271.362828   226.384996      0.0000   113.80665   219.48720   \n",
       "499  1567.0   262.859941   324.699975      0.0000     0.00000     0.00000   \n",
       "500  1567.0   240.673807   322.911797      0.0000     0.00000     0.00000   \n",
       "510  1567.0    55.752306    37.668964      0.0000    35.32440    46.98610   \n",
       "511  1567.0   275.627217   329.601505      0.0000     0.00000     0.00000   \n",
       "521  1567.0    11.610080   103.122996      0.0000     0.00000     0.00000   \n",
       "542  1567.0     0.111206     0.002736      0.1053     0.10960     0.10960   \n",
       "543  1567.0     0.008470     0.001533      0.0051     0.00780     0.00780   \n",
       "544  1567.0     0.002509     0.000295      0.0016     0.00240     0.00260   \n",
       "546  1567.0     1.018304     0.358508      0.4444     0.81410     0.91110   \n",
       "547  1567.0   403.476047     4.627143    372.8220   400.81400   403.12200   \n",
       "548  1567.0    75.415081     3.152734     71.0380    73.25400    74.08400   \n",
       "550  1567.0    16.901595     4.542840      6.1100    14.82000    16.34000   \n",
       "558  1567.0     0.994995     0.083835      0.8919     0.95520     0.97270   \n",
       "559  1567.0     0.325686     0.201329      0.0699     0.14995     0.29090   \n",
       "562  1567.0   262.998383     6.958289    242.2860   262.10100   264.27200   \n",
       "563  1567.0     0.674651     0.111169      0.3049     0.56710     0.65100   \n",
       "564  1567.0     6.221117     2.442203      0.9700     4.98000     5.16000   \n",
       "565  1567.0     0.141069     0.074373      0.0224     0.08770     0.11955   \n",
       "570  1567.0   530.523623    17.499736    317.1964   530.70270   532.39820   \n",
       "571  1567.0     2.101836     0.275112      0.9802     1.98290     2.11860   \n",
       "572  1567.0    28.450165    86.304681      3.5400     7.50000     8.65000   \n",
       "582  1567.0     0.500096     0.003403      0.4778     0.49790     0.50020   \n",
       "583  1567.0     0.015317     0.017174      0.0060     0.01160     0.01380   \n",
       "586  1567.0     0.021458     0.012354     -0.0169     0.01345     0.02050   \n",
       "587  1567.0     0.016474     0.008805      0.0032     0.01060     0.01480   \n",
       "589  1567.0    99.652345    93.864558      0.0000    44.36860    71.90050   \n",
       "\n",
       "            75%         max  \n",
       "0    3056.54000   3356.3500  \n",
       "1    2538.74500   2846.4400  \n",
       "2    2218.05550   2315.2667  \n",
       "3    1590.16990   3715.0417  \n",
       "4       1.51880   1114.5366  \n",
       "6     104.53000    129.2522  \n",
       "7       0.12380      0.1286  \n",
       "8       1.51685      1.6564  \n",
       "9       0.00840      0.0749  \n",
       "10      0.00590      0.0530  \n",
       "11      0.97130      0.9848  \n",
       "12    202.00675    272.0451  \n",
       "14     10.85870     19.5465  \n",
       "15    419.08280    824.9271  \n",
       "16     10.12775    102.8677  \n",
       "18    192.17890    215.5977  \n",
       "19     12.54710     12.9898  \n",
       "20      1.41500      1.4534  \n",
       "21  -5356.62500      0.0000  \n",
       "23  -3356.37500   2363.0000  \n",
       "24   1376.25000  14106.0000  \n",
       "25      1.30430      1.3828  \n",
       "28     72.25555     77.9000  \n",
       "29      2.65560      3.5111  \n",
       "31      3.53125      4.8044  \n",
       "32     85.74190    105.6038  \n",
       "33      9.06060     23.3453  \n",
       "37     66.34305     67.9586  \n",
       "38     87.00240     88.4188  \n",
       "40     80.18000     86.1200  \n",
       "41      3.51500     37.8800  \n",
       "43    360.77180    377.2973  \n",
       "44     10.15245     11.0530  \n",
       "45    142.09095    176.3136  \n",
       "47      1.34020      1.5111  \n",
       "48    143.19410    163.2509  \n",
       "53      4.61700      4.7640  \n",
       "55   2874.00000   2936.0000  \n",
       "56      0.93310      0.9378  \n",
       "57      0.95200      0.9598  \n",
       "58      4.66860      4.8475  \n",
       "59      4.33770    168.1455  \n",
       "61     10.59055     11.7849  \n",
       "62    120.91820    287.1509  \n",
       "63     16.32550    188.0923  \n",
       "64     22.79955     48.9882  \n",
       "67      1.06490   7272.8283  \n",
       "68    149.93590    167.8309  \n",
       "71    115.43960    238.4775  \n",
       "74      0.00000      4.1955  \n",
       "75      0.00660      0.2315  \n",
       "76     -0.00690      0.0723  \n",
       "77      0.00890      0.1331  \n",
       "78      0.01205      0.2492  \n",
       "79      0.01280      0.1013  \n",
       "80      0.00870      0.1186  \n",
       "81     -0.01215      0.0584  \n",
       "82      0.02680      0.1437  \n",
       "83      7.80735      8.9904  \n",
       "84      0.13630      0.1505  \n",
       "86      2.42860      2.5555  \n",
       "87      0.98970      0.9935  \n",
       "88   1841.87300   2105.1823  \n",
       "89      0.20015      1.4727  \n",
       "90   9055.26000  10746.6000  \n",
       "91      0.05035      0.3627  \n",
       "92      0.00200      0.0281  \n",
       "93      0.00100      0.0133  \n",
       "94      0.00010      0.0011  \n",
       "95      0.00010      0.0009  \n",
       "99      0.02980      0.8854  \n",
       "100     0.00020      0.0023  \n",
       "102     0.03360      0.2979  \n",
       "103    -0.00820      0.0203  \n",
       "107     0.04860      0.4856  \n",
       "108     0.03785      0.3938  \n",
       "113     0.95230      0.9763  \n",
       "114     0.00000      0.0414  \n",
       "115   776.78185    924.5318  \n",
       "116     0.99090      0.9924  \n",
       "117    59.13390    311.7344  \n",
       "118     0.60330      0.6245  \n",
       "119     0.97830      0.9827  \n",
       "120     6.37585      7.5220  \n",
       "121    15.86000     16.0700  \n",
       "122     4.39200      6.8890  \n",
       "126     2.87300      3.9910  \n",
       "128     3.31100      3.8950  \n",
       "129     0.04730      2.4580  \n",
       "131     0.99890      1.0190  \n",
       "132     2.35830      2.4723  \n",
       "133  1008.67060   1020.9944  \n",
       "134    40.80460     64.1287  \n",
       "135   127.00000    994.0000  \n",
       "136   180.90000    295.8000  \n",
       "137   161.60000    334.7000  \n",
       "138    62.90010    141.7998  \n",
       "139   494.80600   1770.6909  \n",
       "142     7.50000    103.3900  \n",
       "143     0.00490      0.0121  \n",
       "144     0.13265      0.6253  \n",
       "145     0.07180      0.2507  \n",
       "146     0.06150      0.2479  \n",
       "150     8.26950     22.3180  \n",
       "151    14.34200    536.5640  \n",
       "153     0.01490      0.2389  \n",
       "156     0.06670      2.2016  \n",
       "159   963.50000   7791.0000  \n",
       "160   624.50000   4170.0000  \n",
       "161  5033.00000  37943.0000  \n",
       "162  6376.00000  36871.0000  \n",
       "166     3.20000     21.1000  \n",
       "167     1.50000     16.3000  \n",
       "168     0.15050      0.7250  \n",
       "169     0.53600      1.1430  \n",
       "170     0.79730      1.1530  \n",
       "171     0.14030      0.4940  \n",
       "172     0.37020      0.5484  \n",
       "173     0.63450      0.8643  \n",
       "175     0.84390      1.1720  \n",
       "176     0.29375      0.4411  \n",
       "177     0.42300      1.8580  \n",
       "180    20.96500     48.6700  \n",
       "181     0.68850      3.5730  \n",
       "182    13.33500     55.0000  \n",
       "183    31.68700     72.9470  \n",
       "184     0.16910      3.2283  \n",
       "188    57.67250    191.8300  \n",
       "195     0.29600      4.8380  \n",
       "200    20.16000    199.6200  \n",
       "201     9.48500    126.5300  \n",
       "208    90.45250    172.3490  \n",
       "210     0.09905      0.5164  \n",
       "211     0.06410      0.3227  \n",
       "212     0.06210      0.5941  \n",
       "213     0.07340      1.2837  \n",
       "214     0.09325      0.7615  \n",
       "215     0.09810      0.3429  \n",
       "216     0.09730      0.2828  \n",
       "217     0.08605      0.6744  \n",
       "218     4.40340      8.8015  \n",
       "219     0.00380      0.0163  \n",
       "221     0.07650      0.2305  \n",
       "222     0.00550      0.9911  \n",
       "223   144.50280   1768.8802  \n",
       "225  1252.39965   3601.2998  \n",
       "227     0.02120      0.1541  \n",
       "228     0.02000      0.2133  \n",
       "238     0.00570      0.0244  \n",
       "239     0.00530      0.0236  \n",
       "250   131.75840   1119.7042  \n",
       "251     0.00130      0.9909  \n",
       "253     0.03770      0.4517  \n",
       "255     0.48095      0.9255  \n",
       "267     0.09160      0.1578  \n",
       "268    24.65300     40.8550  \n",
       "269     4.37680     10.1529  \n",
       "418   523.62445    999.3160  \n",
       "419   582.80310    998.6813  \n",
       "423    92.83275    424.2152  \n",
       "432   120.13690    994.2857  \n",
       "433   304.54180    995.7447  \n",
       "438    66.66670    851.6129  \n",
       "460    35.26840    149.3851  \n",
       "468   334.67400    999.8770  \n",
       "472   168.27025    492.7718  \n",
       "476    23.68240    274.8871  \n",
       "482   512.39075    999.4135  \n",
       "483   260.07900    989.4737  \n",
       "484   288.91845    996.8586  \n",
       "485   283.28900    994.0000  \n",
       "486   497.38450    999.4911  \n",
       "487   391.27750    995.7447  \n",
       "488   507.49705    997.5186  \n",
       "489   372.34190    994.0035  \n",
       "499   536.12260   1000.0000  \n",
       "500   505.22575    999.2337  \n",
       "510    64.22845    451.4851  \n",
       "511   554.01070   1000.0000  \n",
       "521     0.00000   1000.0000  \n",
       "542     0.11340      0.1184  \n",
       "543     0.00900      0.0240  \n",
       "544     0.00260      0.0047  \n",
       "546     1.21650      3.9786  \n",
       "547   406.76300    421.7020  \n",
       "548    76.96000     83.7200  \n",
       "550    18.40500    131.6800  \n",
       "558     1.00080      1.5121  \n",
       "559     0.44360      1.0737  \n",
       "562   264.73300    311.4040  \n",
       "563     0.73825      1.2988  \n",
       "564     7.31000     32.5800  \n",
       "565     0.16685      0.6892  \n",
       "570   534.35640    589.5082  \n",
       "571     2.29065      2.7395  \n",
       "572    10.13000    454.5600  \n",
       "582     0.50235      0.5098  \n",
       "583     0.01650      0.4766  \n",
       "586     0.02760      0.1028  \n",
       "587     0.02030      0.0799  \n",
       "589   114.74970    737.3048  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe().transpose() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3445a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>5.788948e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.317875</td>\n",
       "      <td>-0.680283</td>\n",
       "      <td>-0.136262</td>\n",
       "      <td>0.543866</td>\n",
       "      <td>4.341457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.521243e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-4.399281</td>\n",
       "      <td>-0.542260</td>\n",
       "      <td>0.054060</td>\n",
       "      <td>0.538736</td>\n",
       "      <td>4.590805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.237956e-14</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-4.552223</td>\n",
       "      <td>-0.759917</td>\n",
       "      <td>-0.033931</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>3.724207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.276362e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.709379</td>\n",
       "      <td>-0.743939</td>\n",
       "      <td>-0.236848</td>\n",
       "      <td>0.488838</td>\n",
       "      <td>5.956214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>8.653060e-18</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.049400</td>\n",
       "      <td>-0.039004</td>\n",
       "      <td>-0.030471</td>\n",
       "      <td>-0.024130</td>\n",
       "      <td>33.139103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-6.924978e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.326588</td>\n",
       "      <td>-0.445971</td>\n",
       "      <td>0.027301</td>\n",
       "      <td>0.517347</td>\n",
       "      <td>4.763205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.525386e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-21.956885</td>\n",
       "      <td>-0.187450</td>\n",
       "      <td>0.010291</td>\n",
       "      <td>0.279937</td>\n",
       "      <td>1.160781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.695190e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-4.113134</td>\n",
       "      <td>-0.657638</td>\n",
       "      <td>-0.028553</td>\n",
       "      <td>0.692957</td>\n",
       "      <td>2.787422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>4.675689e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.575289</td>\n",
       "      <td>-0.605486</td>\n",
       "      <td>0.010627</td>\n",
       "      <td>0.557514</td>\n",
       "      <td>5.306429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>7.362691e-18</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.776959</td>\n",
       "      <td>-0.635218</td>\n",
       "      <td>0.039519</td>\n",
       "      <td>0.619370</td>\n",
       "      <td>5.490123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-3.446452e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-26.541209</td>\n",
       "      <td>-0.614520</td>\n",
       "      <td>0.109016</td>\n",
       "      <td>0.634440</td>\n",
       "      <td>1.831719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.002914e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-6.797573</td>\n",
       "      <td>-0.592897</td>\n",
       "      <td>-0.102826</td>\n",
       "      <td>0.767429</td>\n",
       "      <td>9.914739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.892450e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.275035</td>\n",
       "      <td>-0.751849</td>\n",
       "      <td>0.016649</td>\n",
       "      <td>0.696288</td>\n",
       "      <td>3.912580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.359027e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-5.329464</td>\n",
       "      <td>-0.419682</td>\n",
       "      <td>-0.029693</td>\n",
       "      <td>0.351441</td>\n",
       "      <td>27.574395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.504783e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-10.238797</td>\n",
       "      <td>-0.532742</td>\n",
       "      <td>0.007969</td>\n",
       "      <td>0.524973</td>\n",
       "      <td>4.363118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.766814e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-6.769610</td>\n",
       "      <td>-0.614417</td>\n",
       "      <td>-0.164173</td>\n",
       "      <td>0.749835</td>\n",
       "      <td>9.657058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-3.223897e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-14.749173</td>\n",
       "      <td>-0.163522</td>\n",
       "      <td>0.040789</td>\n",
       "      <td>0.322986</td>\n",
       "      <td>2.817611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-4.010769e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-10.932438</td>\n",
       "      <td>-0.604693</td>\n",
       "      <td>0.051710</td>\n",
       "      <td>0.669101</td>\n",
       "      <td>3.213299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-5.033197e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.021659</td>\n",
       "      <td>-0.313541</td>\n",
       "      <td>0.021001</td>\n",
       "      <td>0.223908</td>\n",
       "      <td>6.708483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.334899e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-4.300106</td>\n",
       "      <td>-0.456785</td>\n",
       "      <td>-0.017734</td>\n",
       "      <td>0.356972</td>\n",
       "      <td>4.214688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-4.478338e-18</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-5.259198</td>\n",
       "      <td>-0.414176</td>\n",
       "      <td>0.126221</td>\n",
       "      <td>0.642760</td>\n",
       "      <td>5.417190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-4.548676e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-5.348678</td>\n",
       "      <td>0.164724</td>\n",
       "      <td>0.429648</td>\n",
       "      <td>0.528769</td>\n",
       "      <td>0.881551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>3.641876e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.704752</td>\n",
       "      <td>-0.569155</td>\n",
       "      <td>-0.053123</td>\n",
       "      <td>0.634392</td>\n",
       "      <td>2.582616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-7.914261e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-4.236385</td>\n",
       "      <td>-0.759532</td>\n",
       "      <td>0.013074</td>\n",
       "      <td>0.702981</td>\n",
       "      <td>2.827587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.608407e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.082948</td>\n",
       "      <td>-0.526427</td>\n",
       "      <td>-0.401108</td>\n",
       "      <td>-0.222095</td>\n",
       "      <td>2.305362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.546833e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.919015</td>\n",
       "      <td>-0.380389</td>\n",
       "      <td>-0.137299</td>\n",
       "      <td>0.076931</td>\n",
       "      <td>7.822077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>6.672597e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.778309</td>\n",
       "      <td>-0.276718</td>\n",
       "      <td>-0.183726</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>7.014540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.593793e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-4.561817</td>\n",
       "      <td>-0.542511</td>\n",
       "      <td>-0.018224</td>\n",
       "      <td>0.431949</td>\n",
       "      <td>5.866623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.195438e-14</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.768379</td>\n",
       "      <td>-0.513097</td>\n",
       "      <td>-0.089933</td>\n",
       "      <td>0.241878</td>\n",
       "      <td>2.682508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>3.273994e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.475696</td>\n",
       "      <td>0.011789</td>\n",
       "      <td>0.494782</td>\n",
       "      <td>0.563221</td>\n",
       "      <td>0.818543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>7.276667e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.716256</td>\n",
       "      <td>-0.381103</td>\n",
       "      <td>-0.156454</td>\n",
       "      <td>0.180264</td>\n",
       "      <td>17.654802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-3.319536e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.090494</td>\n",
       "      <td>-0.741270</td>\n",
       "      <td>-0.228676</td>\n",
       "      <td>0.860742</td>\n",
       "      <td>3.699374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.215557e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.337912</td>\n",
       "      <td>-0.692068</td>\n",
       "      <td>-0.014549</td>\n",
       "      <td>0.680787</td>\n",
       "      <td>6.178799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>6.107745e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.449858</td>\n",
       "      <td>-0.764456</td>\n",
       "      <td>-0.042998</td>\n",
       "      <td>0.618751</td>\n",
       "      <td>3.943352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-9.219557e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.471688</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>0.393372</td>\n",
       "      <td>0.865693</td>\n",
       "      <td>1.746032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-5.557187e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.019890</td>\n",
       "      <td>-0.647372</td>\n",
       "      <td>0.012745</td>\n",
       "      <td>0.669996</td>\n",
       "      <td>5.099316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.275016e-14</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-12.392614</td>\n",
       "      <td>-0.401412</td>\n",
       "      <td>0.058105</td>\n",
       "      <td>0.430095</td>\n",
       "      <td>3.734240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>6.176413e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.955335</td>\n",
       "      <td>-0.877995</td>\n",
       "      <td>-0.052034</td>\n",
       "      <td>0.702104</td>\n",
       "      <td>2.892696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.631532e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-8.905011</td>\n",
       "      <td>-0.431901</td>\n",
       "      <td>0.324921</td>\n",
       "      <td>0.637521</td>\n",
       "      <td>1.344985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>5.012128e-14</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-4.000372</td>\n",
       "      <td>-0.648336</td>\n",
       "      <td>0.096560</td>\n",
       "      <td>0.608677</td>\n",
       "      <td>2.494197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>8.912298e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-4.638327</td>\n",
       "      <td>-0.794304</td>\n",
       "      <td>-0.128104</td>\n",
       "      <td>0.829863</td>\n",
       "      <td>2.991366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-7.873779e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.294258</td>\n",
       "      <td>-0.629851</td>\n",
       "      <td>-0.294496</td>\n",
       "      <td>0.377076</td>\n",
       "      <td>15.535384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-4.067432e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.048503</td>\n",
       "      <td>-0.584095</td>\n",
       "      <td>0.014616</td>\n",
       "      <td>0.649355</td>\n",
       "      <td>4.524203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.162344e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.760381</td>\n",
       "      <td>-0.536094</td>\n",
       "      <td>-0.006174</td>\n",
       "      <td>0.510171</td>\n",
       "      <td>18.468054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>6.993845e-18</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.765064</td>\n",
       "      <td>-0.552816</td>\n",
       "      <td>-0.178454</td>\n",
       "      <td>0.360735</td>\n",
       "      <td>23.670851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>7.914261e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.333936</td>\n",
       "      <td>-0.675198</td>\n",
       "      <td>-0.146969</td>\n",
       "      <td>0.468112</td>\n",
       "      <td>4.439086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-6.983171e-18</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.069304</td>\n",
       "      <td>-0.068165</td>\n",
       "      <td>-0.067916</td>\n",
       "      <td>-0.067725</td>\n",
       "      <td>17.201556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>4.397020e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-11.363856</td>\n",
       "      <td>-0.400171</td>\n",
       "      <td>0.041035</td>\n",
       "      <td>0.487393</td>\n",
       "      <td>3.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.874324e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.443582</td>\n",
       "      <td>-0.531475</td>\n",
       "      <td>-0.050037</td>\n",
       "      <td>0.278915</td>\n",
       "      <td>3.883048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>4.254516e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.021354</td>\n",
       "      <td>-0.021354</td>\n",
       "      <td>-0.021354</td>\n",
       "      <td>-0.021354</td>\n",
       "      <td>46.829478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.467478e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.746250</td>\n",
       "      <td>-0.546377</td>\n",
       "      <td>0.014030</td>\n",
       "      <td>0.513852</td>\n",
       "      <td>9.598503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.656641e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-4.597860</td>\n",
       "      <td>-0.633605</td>\n",
       "      <td>-0.010822</td>\n",
       "      <td>0.623938</td>\n",
       "      <td>3.142018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>4.033034e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.002400</td>\n",
       "      <td>-0.710907</td>\n",
       "      <td>-0.063043</td>\n",
       "      <td>0.532251</td>\n",
       "      <td>4.348313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>3.567490e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-6.754550</td>\n",
       "      <td>-0.790235</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.555720</td>\n",
       "      <td>5.488666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-4.453037e-18</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.637438</td>\n",
       "      <td>-0.673460</td>\n",
       "      <td>-0.169266</td>\n",
       "      <td>0.538885</td>\n",
       "      <td>3.909845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.087361e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.291803</td>\n",
       "      <td>-1.060862</td>\n",
       "      <td>0.239454</td>\n",
       "      <td>0.589611</td>\n",
       "      <td>2.626335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>6.588470e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-4.479310</td>\n",
       "      <td>-0.361715</td>\n",
       "      <td>0.044891</td>\n",
       "      <td>0.537399</td>\n",
       "      <td>4.488916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>6.020455e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.696888</td>\n",
       "      <td>-0.760411</td>\n",
       "      <td>0.066174</td>\n",
       "      <td>0.646184</td>\n",
       "      <td>3.916097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.337429e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.976134</td>\n",
       "      <td>-0.618277</td>\n",
       "      <td>-0.017028</td>\n",
       "      <td>0.670532</td>\n",
       "      <td>2.822647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-3.226111e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.256312</td>\n",
       "      <td>-0.710739</td>\n",
       "      <td>-0.027780</td>\n",
       "      <td>0.634483</td>\n",
       "      <td>3.593971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-8.024973e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-4.622857</td>\n",
       "      <td>-0.709679</td>\n",
       "      <td>0.117333</td>\n",
       "      <td>0.670595</td>\n",
       "      <td>4.163064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.257044e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-10.372585</td>\n",
       "      <td>-0.439060</td>\n",
       "      <td>0.308625</td>\n",
       "      <td>0.424233</td>\n",
       "      <td>0.585080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.696415e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.390159</td>\n",
       "      <td>-0.568960</td>\n",
       "      <td>-0.004629</td>\n",
       "      <td>0.628715</td>\n",
       "      <td>5.598967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.809999e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.813545</td>\n",
       "      <td>-0.304087</td>\n",
       "      <td>-0.019296</td>\n",
       "      <td>0.106236</td>\n",
       "      <td>15.052673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.871287e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.848354</td>\n",
       "      <td>-0.631593</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.583173</td>\n",
       "      <td>5.359076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.413748e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-4.266187</td>\n",
       "      <td>-0.502905</td>\n",
       "      <td>-0.025524</td>\n",
       "      <td>0.615281</td>\n",
       "      <td>4.631598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.841341e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-4.218170</td>\n",
       "      <td>-0.544942</td>\n",
       "      <td>-0.061623</td>\n",
       "      <td>0.453918</td>\n",
       "      <td>8.895896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.636845e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-5.692734</td>\n",
       "      <td>-0.372075</td>\n",
       "      <td>0.102379</td>\n",
       "      <td>0.509054</td>\n",
       "      <td>3.830230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>3.091824e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-11.654924</td>\n",
       "      <td>-0.437026</td>\n",
       "      <td>0.153390</td>\n",
       "      <td>0.153390</td>\n",
       "      <td>6.647962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.206368e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-8.680023</td>\n",
       "      <td>-0.615100</td>\n",
       "      <td>0.281002</td>\n",
       "      <td>0.281002</td>\n",
       "      <td>7.449822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>3.537128e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-4.405853</td>\n",
       "      <td>-0.446783</td>\n",
       "      <td>-0.087718</td>\n",
       "      <td>0.271346</td>\n",
       "      <td>10.267851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.988180e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-7.590891</td>\n",
       "      <td>-0.574496</td>\n",
       "      <td>-0.073325</td>\n",
       "      <td>0.427846</td>\n",
       "      <td>5.690142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-8.931375e-18</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-7.803259</td>\n",
       "      <td>-0.592747</td>\n",
       "      <td>-0.041924</td>\n",
       "      <td>0.435651</td>\n",
       "      <td>4.402751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.133789e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-7.044210</td>\n",
       "      <td>-0.649311</td>\n",
       "      <td>-0.089389</td>\n",
       "      <td>0.676820</td>\n",
       "      <td>4.478395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.732545e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-5.780091</td>\n",
       "      <td>-0.531136</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.544062</td>\n",
       "      <td>5.479475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-6.021720e-18</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.884405</td>\n",
       "      <td>-0.606792</td>\n",
       "      <td>0.017321</td>\n",
       "      <td>0.566401</td>\n",
       "      <td>4.714748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.287242e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-7.991130</td>\n",
       "      <td>-0.578355</td>\n",
       "      <td>0.114488</td>\n",
       "      <td>0.557561</td>\n",
       "      <td>2.659983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.074963e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.128385</td>\n",
       "      <td>-0.128385</td>\n",
       "      <td>-0.128385</td>\n",
       "      <td>-0.128385</td>\n",
       "      <td>9.391683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.262041e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.541344</td>\n",
       "      <td>-0.487823</td>\n",
       "      <td>0.090799</td>\n",
       "      <td>0.595858</td>\n",
       "      <td>3.686565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.311394e-14</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-10.303727</td>\n",
       "      <td>0.279947</td>\n",
       "      <td>0.397071</td>\n",
       "      <td>0.439661</td>\n",
       "      <td>0.556784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-9.959305e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.023707</td>\n",
       "      <td>-0.100695</td>\n",
       "      <td>0.013124</td>\n",
       "      <td>0.112889</td>\n",
       "      <td>45.807391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-9.553535e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-7.989388</td>\n",
       "      <td>-0.578107</td>\n",
       "      <td>0.063190</td>\n",
       "      <td>0.592095</td>\n",
       "      <td>3.501073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.774596e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-16.037953</td>\n",
       "      <td>-0.758361</td>\n",
       "      <td>-0.118575</td>\n",
       "      <td>0.910100</td>\n",
       "      <td>1.487162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.704299e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-9.433215</td>\n",
       "      <td>-0.593925</td>\n",
       "      <td>0.006417</td>\n",
       "      <td>0.563706</td>\n",
       "      <td>9.670081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-8.553069e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.583282</td>\n",
       "      <td>-0.730477</td>\n",
       "      <td>-0.119161</td>\n",
       "      <td>0.695926</td>\n",
       "      <td>2.631759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-6.957870e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.528268</td>\n",
       "      <td>-0.778116</td>\n",
       "      <td>-0.149641</td>\n",
       "      <td>0.639244</td>\n",
       "      <td>3.718171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.891529e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.648517</td>\n",
       "      <td>-0.696198</td>\n",
       "      <td>-0.100062</td>\n",
       "      <td>0.417340</td>\n",
       "      <td>4.541554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>5.696345e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-13.103748</td>\n",
       "      <td>-0.507915</td>\n",
       "      <td>0.022866</td>\n",
       "      <td>0.512817</td>\n",
       "      <td>2.799257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-7.501849e-18</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.236881</td>\n",
       "      <td>-0.278679</td>\n",
       "      <td>0.252096</td>\n",
       "      <td>0.517437</td>\n",
       "      <td>2.594443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>3.615977e-14</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.054129</td>\n",
       "      <td>-0.682000</td>\n",
       "      <td>-0.044940</td>\n",
       "      <td>0.543116</td>\n",
       "      <td>10.393046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>7.600423e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.297082</td>\n",
       "      <td>-0.745726</td>\n",
       "      <td>-0.094517</td>\n",
       "      <td>0.794806</td>\n",
       "      <td>2.775491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.096722e-14</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.868753</td>\n",
       "      <td>-0.609136</td>\n",
       "      <td>-0.020435</td>\n",
       "      <td>0.799012</td>\n",
       "      <td>2.553411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>3.486272e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.099950</td>\n",
       "      <td>-0.705563</td>\n",
       "      <td>-0.146359</td>\n",
       "      <td>0.539911</td>\n",
       "      <td>8.935937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>9.540188e-18</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.920820</td>\n",
       "      <td>-0.425618</td>\n",
       "      <td>-0.154056</td>\n",
       "      <td>0.149455</td>\n",
       "      <td>14.031073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-3.599875e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.918416</td>\n",
       "      <td>-0.898570</td>\n",
       "      <td>-0.108654</td>\n",
       "      <td>0.799936</td>\n",
       "      <td>2.867441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-3.219090e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.130860</td>\n",
       "      <td>-0.719485</td>\n",
       "      <td>-0.017790</td>\n",
       "      <td>0.754773</td>\n",
       "      <td>4.167426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.991125e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.022715</td>\n",
       "      <td>-0.517422</td>\n",
       "      <td>-0.171190</td>\n",
       "      <td>0.369549</td>\n",
       "      <td>6.469373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-5.930635e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.350009</td>\n",
       "      <td>-0.625970</td>\n",
       "      <td>-0.295474</td>\n",
       "      <td>0.292154</td>\n",
       "      <td>4.948770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.327815e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.018984</td>\n",
       "      <td>-0.621768</td>\n",
       "      <td>-0.113496</td>\n",
       "      <td>0.401156</td>\n",
       "      <td>19.502819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.820088e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.567739</td>\n",
       "      <td>-0.748245</td>\n",
       "      <td>-0.219590</td>\n",
       "      <td>0.573392</td>\n",
       "      <td>7.093472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.751865e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.233035</td>\n",
       "      <td>-0.585514</td>\n",
       "      <td>-0.217896</td>\n",
       "      <td>0.165704</td>\n",
       "      <td>7.911009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-8.147033e-18</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.761999</td>\n",
       "      <td>-0.669463</td>\n",
       "      <td>-0.151027</td>\n",
       "      <td>0.451450</td>\n",
       "      <td>8.248730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>7.691609e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.580215</td>\n",
       "      <td>-0.635905</td>\n",
       "      <td>-0.196928</td>\n",
       "      <td>0.318614</td>\n",
       "      <td>9.047105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>5.475211e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.638872</td>\n",
       "      <td>-0.698055</td>\n",
       "      <td>-0.260900</td>\n",
       "      <td>0.392852</td>\n",
       "      <td>4.874130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>5.203063e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.528610</td>\n",
       "      <td>-0.215893</td>\n",
       "      <td>-0.070077</td>\n",
       "      <td>0.069331</td>\n",
       "      <td>26.130788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.470767e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.183131</td>\n",
       "      <td>-0.720093</td>\n",
       "      <td>-0.025537</td>\n",
       "      <td>0.491976</td>\n",
       "      <td>23.916238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>5.649790e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.108837</td>\n",
       "      <td>-0.472554</td>\n",
       "      <td>-0.213686</td>\n",
       "      <td>0.233890</td>\n",
       "      <td>28.440828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-3.633273e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.816758</td>\n",
       "      <td>-0.464770</td>\n",
       "      <td>-0.282317</td>\n",
       "      <td>-0.020142</td>\n",
       "      <td>5.473016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-5.657697e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.917554</td>\n",
       "      <td>-0.451776</td>\n",
       "      <td>-0.238057</td>\n",
       "      <td>-0.014753</td>\n",
       "      <td>5.004085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.958324e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.044427</td>\n",
       "      <td>-0.675684</td>\n",
       "      <td>-0.267640</td>\n",
       "      <td>0.226325</td>\n",
       "      <td>8.963593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.773624e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.753014</td>\n",
       "      <td>-0.677455</td>\n",
       "      <td>-0.461243</td>\n",
       "      <td>0.362390</td>\n",
       "      <td>5.277204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.870781e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.199902</td>\n",
       "      <td>-0.453509</td>\n",
       "      <td>-0.166435</td>\n",
       "      <td>0.178054</td>\n",
       "      <td>10.455311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.115347e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.239095</td>\n",
       "      <td>-0.461434</td>\n",
       "      <td>-0.072603</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>19.498536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>3.600887e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.811372</td>\n",
       "      <td>-0.675678</td>\n",
       "      <td>-0.137718</td>\n",
       "      <td>0.499864</td>\n",
       "      <td>11.976344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.712304e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.768168</td>\n",
       "      <td>-0.846491</td>\n",
       "      <td>0.034223</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>3.848945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>8.086310e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.427326</td>\n",
       "      <td>-0.739264</td>\n",
       "      <td>-0.026533</td>\n",
       "      <td>0.656260</td>\n",
       "      <td>2.770593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.493701e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.952329</td>\n",
       "      <td>-0.656779</td>\n",
       "      <td>-0.085471</td>\n",
       "      <td>0.429584</td>\n",
       "      <td>6.575096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>4.062384e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.935237</td>\n",
       "      <td>-0.568347</td>\n",
       "      <td>0.016660</td>\n",
       "      <td>0.762096</td>\n",
       "      <td>3.328217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.760164e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.544499</td>\n",
       "      <td>-0.626334</td>\n",
       "      <td>-0.026183</td>\n",
       "      <td>0.555248</td>\n",
       "      <td>3.178293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>7.500331e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.613084</td>\n",
       "      <td>-0.718133</td>\n",
       "      <td>-0.007526</td>\n",
       "      <td>0.628087</td>\n",
       "      <td>3.528131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-7.531198e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.277351</td>\n",
       "      <td>-0.629279</td>\n",
       "      <td>-0.072734</td>\n",
       "      <td>0.680555</td>\n",
       "      <td>2.676299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>9.503185e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.220369</td>\n",
       "      <td>-0.578681</td>\n",
       "      <td>-0.364785</td>\n",
       "      <td>0.091526</td>\n",
       "      <td>5.235724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>5.721140e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.974103</td>\n",
       "      <td>-0.665215</td>\n",
       "      <td>-0.083946</td>\n",
       "      <td>0.643046</td>\n",
       "      <td>9.742589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.709517e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.865439</td>\n",
       "      <td>-0.742468</td>\n",
       "      <td>-0.101958</td>\n",
       "      <td>0.606286</td>\n",
       "      <td>14.457454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>5.428404e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.966924</td>\n",
       "      <td>-0.733762</td>\n",
       "      <td>-0.147810</td>\n",
       "      <td>0.645223</td>\n",
       "      <td>11.837567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>5.586537e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.397618</td>\n",
       "      <td>-0.797405</td>\n",
       "      <td>0.214343</td>\n",
       "      <td>0.719757</td>\n",
       "      <td>6.985100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.265988e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.151961</td>\n",
       "      <td>-0.430659</td>\n",
       "      <td>-0.142732</td>\n",
       "      <td>0.245129</td>\n",
       "      <td>30.496287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.727485e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.756583</td>\n",
       "      <td>-0.815443</td>\n",
       "      <td>-0.066852</td>\n",
       "      <td>0.571446</td>\n",
       "      <td>6.756976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-6.965460e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.489764</td>\n",
       "      <td>-0.139400</td>\n",
       "      <td>-0.074684</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>10.128247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.496828e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.506127</td>\n",
       "      <td>-0.337387</td>\n",
       "      <td>-0.070275</td>\n",
       "      <td>0.243508</td>\n",
       "      <td>16.412237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.945077e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.477156</td>\n",
       "      <td>-0.550154</td>\n",
       "      <td>-0.218692</td>\n",
       "      <td>0.251183</td>\n",
       "      <td>21.566754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.126831e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.422044</td>\n",
       "      <td>-0.592485</td>\n",
       "      <td>0.022388</td>\n",
       "      <td>0.660430</td>\n",
       "      <td>3.448330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>5.589067e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.111152</td>\n",
       "      <td>-0.515057</td>\n",
       "      <td>-0.245009</td>\n",
       "      <td>0.236383</td>\n",
       "      <td>7.604470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>5.748466e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.696269</td>\n",
       "      <td>-0.468163</td>\n",
       "      <td>-0.192238</td>\n",
       "      <td>0.192143</td>\n",
       "      <td>8.402910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.943143e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.479332</td>\n",
       "      <td>-0.661578</td>\n",
       "      <td>-0.335470</td>\n",
       "      <td>0.509597</td>\n",
       "      <td>17.954729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>5.957202e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.014653</td>\n",
       "      <td>-0.402577</td>\n",
       "      <td>-0.135519</td>\n",
       "      <td>0.181300</td>\n",
       "      <td>20.217253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-4.156378e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.283080</td>\n",
       "      <td>-0.508262</td>\n",
       "      <td>-0.115487</td>\n",
       "      <td>0.272995</td>\n",
       "      <td>14.633956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.267091e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.455899</td>\n",
       "      <td>-0.619590</td>\n",
       "      <td>0.047320</td>\n",
       "      <td>0.580848</td>\n",
       "      <td>8.791807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.651581e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.766396</td>\n",
       "      <td>-0.568492</td>\n",
       "      <td>-0.028383</td>\n",
       "      <td>0.556506</td>\n",
       "      <td>7.216933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.651581e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.539500</td>\n",
       "      <td>-0.612290</td>\n",
       "      <td>-0.263697</td>\n",
       "      <td>0.428747</td>\n",
       "      <td>14.265756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.179964e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.305560</td>\n",
       "      <td>-0.727063</td>\n",
       "      <td>-0.107191</td>\n",
       "      <td>0.575102</td>\n",
       "      <td>4.275483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.052283e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.723667</td>\n",
       "      <td>-0.640175</td>\n",
       "      <td>-0.166147</td>\n",
       "      <td>0.375599</td>\n",
       "      <td>8.840377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>4.153469e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.544242</td>\n",
       "      <td>-0.799674</td>\n",
       "      <td>-0.088781</td>\n",
       "      <td>0.569731</td>\n",
       "      <td>6.331711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.163265e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.152129</td>\n",
       "      <td>-0.140865</td>\n",
       "      <td>-0.129600</td>\n",
       "      <td>-0.083416</td>\n",
       "      <td>11.008750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.731118e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.779081</td>\n",
       "      <td>-0.517167</td>\n",
       "      <td>-0.063069</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>32.553305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.603931e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.966004</td>\n",
       "      <td>-0.651278</td>\n",
       "      <td>-0.213912</td>\n",
       "      <td>0.412264</td>\n",
       "      <td>5.559085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-5.417018e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.126959</td>\n",
       "      <td>-0.518946</td>\n",
       "      <td>-0.246941</td>\n",
       "      <td>0.177069</td>\n",
       "      <td>10.705293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-6.515096e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.050223</td>\n",
       "      <td>-0.518788</td>\n",
       "      <td>-0.253070</td>\n",
       "      <td>0.249895</td>\n",
       "      <td>18.508498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>9.918127e-18</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.049055</td>\n",
       "      <td>-0.649079</td>\n",
       "      <td>-0.143933</td>\n",
       "      <td>0.548839</td>\n",
       "      <td>11.286800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.140079e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.962502</td>\n",
       "      <td>-0.586070</td>\n",
       "      <td>-0.148114</td>\n",
       "      <td>0.414971</td>\n",
       "      <td>11.926948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.208807e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.706693</td>\n",
       "      <td>-0.623585</td>\n",
       "      <td>-0.131595</td>\n",
       "      <td>0.444682</td>\n",
       "      <td>19.035589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>3.581405e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.120579</td>\n",
       "      <td>-0.106974</td>\n",
       "      <td>-0.096770</td>\n",
       "      <td>-0.086567</td>\n",
       "      <td>33.572458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.677100e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.025282</td>\n",
       "      <td>-0.387510</td>\n",
       "      <td>-0.130566</td>\n",
       "      <td>0.190615</td>\n",
       "      <td>19.154023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>6.069793e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.414753</td>\n",
       "      <td>-0.783292</td>\n",
       "      <td>-0.022989</td>\n",
       "      <td>0.622321</td>\n",
       "      <td>4.542314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>3.821009e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.608524</td>\n",
       "      <td>-0.972208</td>\n",
       "      <td>0.009721</td>\n",
       "      <td>0.710637</td>\n",
       "      <td>2.848917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.649051e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.817495</td>\n",
       "      <td>-0.842162</td>\n",
       "      <td>-0.216955</td>\n",
       "      <td>0.725184</td>\n",
       "      <td>2.848664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.559069e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.096666</td>\n",
       "      <td>-0.710952</td>\n",
       "      <td>-0.095721</td>\n",
       "      <td>0.496963</td>\n",
       "      <td>5.315245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>8.845350e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.057830</td>\n",
       "      <td>-1.057830</td>\n",
       "      <td>-0.099840</td>\n",
       "      <td>0.683457</td>\n",
       "      <td>2.402380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.979577e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.014199</td>\n",
       "      <td>-1.014199</td>\n",
       "      <td>-0.007414</td>\n",
       "      <td>0.791701</td>\n",
       "      <td>2.107307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>8.668241e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.723441</td>\n",
       "      <td>-0.683069</td>\n",
       "      <td>-0.278605</td>\n",
       "      <td>0.549946</td>\n",
       "      <td>10.608134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.139826e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.860489</td>\n",
       "      <td>-0.586710</td>\n",
       "      <td>-0.339348</td>\n",
       "      <td>0.171051</td>\n",
       "      <td>7.647334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-5.404367e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.909059</td>\n",
       "      <td>-0.867240</td>\n",
       "      <td>-0.285144</td>\n",
       "      <td>0.415882</td>\n",
       "      <td>3.172550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.025970e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.336052</td>\n",
       "      <td>-0.475452</td>\n",
       "      <td>-0.157814</td>\n",
       "      <td>0.268443</td>\n",
       "      <td>19.103647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>8.389926e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.633675</td>\n",
       "      <td>-0.658614</td>\n",
       "      <td>-0.278395</td>\n",
       "      <td>0.472951</td>\n",
       "      <td>7.311566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.692063e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.889621</td>\n",
       "      <td>-0.749303</td>\n",
       "      <td>-0.388207</td>\n",
       "      <td>0.405611</td>\n",
       "      <td>3.450791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>3.926769e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.648596</td>\n",
       "      <td>-0.693006</td>\n",
       "      <td>-0.015308</td>\n",
       "      <td>0.651986</td>\n",
       "      <td>7.349617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-3.411886e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.311472</td>\n",
       "      <td>-0.564329</td>\n",
       "      <td>-0.273325</td>\n",
       "      <td>0.283704</td>\n",
       "      <td>16.162206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>4.527929e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.109917</td>\n",
       "      <td>-1.109917</td>\n",
       "      <td>-0.092794</td>\n",
       "      <td>0.713752</td>\n",
       "      <td>2.447038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>5.303162e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.132982</td>\n",
       "      <td>-0.638322</td>\n",
       "      <td>-0.267473</td>\n",
       "      <td>0.330843</td>\n",
       "      <td>4.739949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.597843e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.002200</td>\n",
       "      <td>-0.654587</td>\n",
       "      <td>-0.312842</td>\n",
       "      <td>0.395796</td>\n",
       "      <td>3.949625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.226518e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.902646</td>\n",
       "      <td>-0.656604</td>\n",
       "      <td>-0.371591</td>\n",
       "      <td>0.297826</td>\n",
       "      <td>3.753302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>2.722425e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.142204</td>\n",
       "      <td>-1.142204</td>\n",
       "      <td>-0.190854</td>\n",
       "      <td>0.668321</td>\n",
       "      <td>2.517876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>4.819906e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.887463</td>\n",
       "      <td>-0.614237</td>\n",
       "      <td>-0.464505</td>\n",
       "      <td>0.497745</td>\n",
       "      <td>2.937577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.805504e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.294781</td>\n",
       "      <td>-0.980161</td>\n",
       "      <td>0.035022</td>\n",
       "      <td>0.655616</td>\n",
       "      <td>2.613527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.619286e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.221308</td>\n",
       "      <td>-0.686209</td>\n",
       "      <td>-0.226447</td>\n",
       "      <td>0.393966</td>\n",
       "      <td>3.160511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>6.942689e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.746360</td>\n",
       "      <td>-0.746360</td>\n",
       "      <td>-0.746360</td>\n",
       "      <td>0.825867</td>\n",
       "      <td>2.357846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.138561e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.740204</td>\n",
       "      <td>-0.740204</td>\n",
       "      <td>-0.740204</td>\n",
       "      <td>0.747247</td>\n",
       "      <td>2.232419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-6.537868e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.349572</td>\n",
       "      <td>-0.562671</td>\n",
       "      <td>-0.270185</td>\n",
       "      <td>0.124802</td>\n",
       "      <td>8.175457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-6.193453e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.896319</td>\n",
       "      <td>-0.896319</td>\n",
       "      <td>-0.691461</td>\n",
       "      <td>0.988376</td>\n",
       "      <td>2.034152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-6.527747e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.140396</td>\n",
       "      <td>-0.140396</td>\n",
       "      <td>-0.140396</td>\n",
       "      <td>-0.140396</td>\n",
       "      <td>7.898866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-5.705858e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.243642</td>\n",
       "      <td>-0.537110</td>\n",
       "      <td>-0.537110</td>\n",
       "      <td>0.613807</td>\n",
       "      <td>2.955327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-3.305368e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.407297</td>\n",
       "      <td>-0.398175</td>\n",
       "      <td>-0.398175</td>\n",
       "      <td>0.048297</td>\n",
       "      <td>11.656558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>8.762362e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.377761</td>\n",
       "      <td>-0.451191</td>\n",
       "      <td>0.280452</td>\n",
       "      <td>0.280452</td>\n",
       "      <td>7.962699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-6.391120e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.621381</td>\n",
       "      <td>-0.606436</td>\n",
       "      <td>-0.302341</td>\n",
       "      <td>0.477658</td>\n",
       "      <td>8.372954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>7.210074e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-7.194216</td>\n",
       "      <td>-0.697835</td>\n",
       "      <td>-0.162195</td>\n",
       "      <td>0.714023</td>\n",
       "      <td>4.149850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-5.192057e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.363359</td>\n",
       "      <td>-0.670686</td>\n",
       "      <td>-0.411246</td>\n",
       "      <td>0.463504</td>\n",
       "      <td>2.600757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.811070e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.668507</td>\n",
       "      <td>-0.423216</td>\n",
       "      <td>-0.155523</td>\n",
       "      <td>0.222722</td>\n",
       "      <td>16.902883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>3.774961e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.096079</td>\n",
       "      <td>-0.447228</td>\n",
       "      <td>-0.240170</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>5.261223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-2.306471e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.277654</td>\n",
       "      <td>-0.842064</td>\n",
       "      <td>-0.189910</td>\n",
       "      <td>0.623068</td>\n",
       "      <td>3.662967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>3.056833e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.887032</td>\n",
       "      <td>-0.089554</td>\n",
       "      <td>0.250035</td>\n",
       "      <td>0.256385</td>\n",
       "      <td>5.293657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>4.693906e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.794341</td>\n",
       "      <td>-0.862554</td>\n",
       "      <td>-0.226196</td>\n",
       "      <td>0.475887</td>\n",
       "      <td>6.452349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.487719e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-2.167262</td>\n",
       "      <td>-0.535293</td>\n",
       "      <td>-0.462037</td>\n",
       "      <td>0.485197</td>\n",
       "      <td>10.697214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>8.957941e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.416675</td>\n",
       "      <td>-0.675988</td>\n",
       "      <td>-0.302714</td>\n",
       "      <td>0.260419</td>\n",
       "      <td>5.584696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.475113e-15</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-11.081199</td>\n",
       "      <td>0.016802</td>\n",
       "      <td>0.085720</td>\n",
       "      <td>0.201461</td>\n",
       "      <td>3.058086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-3.828347e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.483387</td>\n",
       "      <td>-0.369614</td>\n",
       "      <td>0.171020</td>\n",
       "      <td>0.631444</td>\n",
       "      <td>2.028850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-5.566296e-18</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.270755</td>\n",
       "      <td>-0.220065</td>\n",
       "      <td>-0.203126</td>\n",
       "      <td>-0.182546</td>\n",
       "      <td>5.698335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>1.365868e-14</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-6.281437</td>\n",
       "      <td>-0.630429</td>\n",
       "      <td>0.037671</td>\n",
       "      <td>0.622258</td>\n",
       "      <td>2.626556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-6.184281e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-0.813403</td>\n",
       "      <td>-0.332476</td>\n",
       "      <td>-0.131361</td>\n",
       "      <td>0.139707</td>\n",
       "      <td>39.881784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-1.282272e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-3.187290</td>\n",
       "      <td>-0.608299</td>\n",
       "      <td>-0.074139</td>\n",
       "      <td>0.541397</td>\n",
       "      <td>6.803173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-3.518911e-16</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.609150</td>\n",
       "      <td>-0.718013</td>\n",
       "      <td>-0.139353</td>\n",
       "      <td>0.482707</td>\n",
       "      <td>7.267498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2194.0</td>\n",
       "      <td>-6.730158e-17</td>\n",
       "      <td>1.000228</td>\n",
       "      <td>-1.127377</td>\n",
       "      <td>-0.618918</td>\n",
       "      <td>-0.286974</td>\n",
       "      <td>0.188515</td>\n",
       "      <td>7.078848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count          mean       std        min       25%       50%       75%  \\\n",
       "0    2194.0  5.788948e-16  1.000228  -3.317875 -0.680283 -0.136262  0.543866   \n",
       "1    2194.0  1.521243e-15  1.000228  -4.399281 -0.542260  0.054060  0.538736   \n",
       "2    2194.0  1.237956e-14  1.000228  -4.552223 -0.759917 -0.033931  0.588571   \n",
       "3    2194.0 -2.276362e-16  1.000228  -1.709379 -0.743939 -0.236848  0.488838   \n",
       "4    2194.0  8.653060e-18  1.000228  -0.049400 -0.039004 -0.030471 -0.024130   \n",
       "5    2194.0 -6.924978e-16  1.000228  -3.326588 -0.445971  0.027301  0.517347   \n",
       "6    2194.0 -1.525386e-15  1.000228 -21.956885 -0.187450  0.010291  0.279937   \n",
       "7    2194.0  1.695190e-15  1.000228  -4.113134 -0.657638 -0.028553  0.692957   \n",
       "8    2194.0  4.675689e-17  1.000228  -3.575289 -0.605486  0.010627  0.557514   \n",
       "9    2194.0  7.362691e-18  1.000228  -3.776959 -0.635218  0.039519  0.619370   \n",
       "10   2194.0 -3.446452e-15  1.000228 -26.541209 -0.614520  0.109016  0.634440   \n",
       "11   2194.0  1.002914e-15  1.000228  -6.797573 -0.592897 -0.102826  0.767429   \n",
       "12   2194.0  2.892450e-16  1.000228  -2.275035 -0.751849  0.016649  0.696288   \n",
       "13   2194.0  1.359027e-15  1.000228  -5.329464 -0.419682 -0.029693  0.351441   \n",
       "14   2194.0  2.504783e-15  1.000228 -10.238797 -0.532742  0.007969  0.524973   \n",
       "15   2194.0  2.766814e-15  1.000228  -6.769610 -0.614417 -0.164173  0.749835   \n",
       "16   2194.0 -3.223897e-15  1.000228 -14.749173 -0.163522  0.040789  0.322986   \n",
       "17   2194.0 -4.010769e-16  1.000228 -10.932438 -0.604693  0.051710  0.669101   \n",
       "18   2194.0 -5.033197e-16  1.000228  -2.021659 -0.313541  0.021001  0.223908   \n",
       "19   2194.0 -1.334899e-16  1.000228  -4.300106 -0.456785 -0.017734  0.356972   \n",
       "20   2194.0 -4.478338e-18  1.000228  -5.259198 -0.414176  0.126221  0.642760   \n",
       "21   2194.0 -4.548676e-16  1.000228  -5.348678  0.164724  0.429648  0.528769   \n",
       "22   2194.0  3.641876e-16  1.000228  -2.704752 -0.569155 -0.053123  0.634392   \n",
       "23   2194.0 -7.914261e-17  1.000228  -4.236385 -0.759532  0.013074  0.702981   \n",
       "24   2194.0  1.608407e-16  1.000228  -3.082948 -0.526427 -0.401108 -0.222095   \n",
       "25   2194.0  2.546833e-15  1.000228  -0.919015 -0.380389 -0.137299  0.076931   \n",
       "26   2194.0  6.672597e-16  1.000228  -0.778309 -0.276718 -0.183726  0.000129   \n",
       "27   2194.0  2.593793e-15  1.000228  -4.561817 -0.542511 -0.018224  0.431949   \n",
       "28   2194.0 -2.195438e-14  1.000228  -3.768379 -0.513097 -0.089933  0.241878   \n",
       "29   2194.0  3.273994e-16  1.000228  -2.475696  0.011789  0.494782  0.563221   \n",
       "30   2194.0  7.276667e-17  1.000228  -1.716256 -0.381103 -0.156454  0.180264   \n",
       "31   2194.0 -3.319536e-16  1.000228  -2.090494 -0.741270 -0.228676  0.860742   \n",
       "32   2194.0 -2.215557e-15  1.000228  -3.337912 -0.692068 -0.014549  0.680787   \n",
       "33   2194.0  6.107745e-16  1.000228  -3.449858 -0.764456 -0.042998  0.618751   \n",
       "34   2194.0 -9.219557e-16  1.000228  -2.471688 -0.998139  0.393372  0.865693   \n",
       "35   2194.0 -5.557187e-15  1.000228  -3.019890 -0.647372  0.012745  0.669996   \n",
       "36   2194.0 -1.275016e-14  1.000228 -12.392614 -0.401412  0.058105  0.430095   \n",
       "37   2194.0  6.176413e-15  1.000228  -1.955335 -0.877995 -0.052034  0.702104   \n",
       "38   2194.0 -1.631532e-15  1.000228  -8.905011 -0.431901  0.324921  0.637521   \n",
       "39   2194.0  5.012128e-14  1.000228  -4.000372 -0.648336  0.096560  0.608677   \n",
       "40   2194.0  8.912298e-15  1.000228  -4.638327 -0.794304 -0.128104  0.829863   \n",
       "41   2194.0 -7.873779e-17  1.000228  -3.294258 -0.629851 -0.294496  0.377076   \n",
       "42   2194.0 -4.067432e-15  1.000228  -3.048503 -0.584095  0.014616  0.649355   \n",
       "43   2194.0 -1.162344e-15  1.000228  -3.760381 -0.536094 -0.006174  0.510171   \n",
       "44   2194.0  6.993845e-18  1.000228  -1.765064 -0.552816 -0.178454  0.360735   \n",
       "45   2194.0  7.914261e-17  1.000228  -2.333936 -0.675198 -0.146969  0.468112   \n",
       "46   2194.0 -6.983171e-18  1.000228  -0.069304 -0.068165 -0.067916 -0.067725   \n",
       "47   2194.0  4.397020e-15  1.000228 -11.363856 -0.400171  0.041035  0.487393   \n",
       "48   2194.0  1.874324e-16  1.000228  -2.443582 -0.531475 -0.050037  0.278915   \n",
       "49   2194.0  4.254516e-16  1.000228  -0.021354 -0.021354 -0.021354 -0.021354   \n",
       "50   2194.0 -1.467478e-17  1.000228  -3.746250 -0.546377  0.014030  0.513852   \n",
       "51   2194.0 -2.656641e-17  1.000228  -4.597860 -0.633605 -0.010822  0.623938   \n",
       "52   2194.0  4.033034e-17  1.000228  -3.002400 -0.710907 -0.063043  0.532251   \n",
       "53   2194.0  3.567490e-17  1.000228  -6.754550 -0.790235  0.001353  0.555720   \n",
       "54   2194.0 -4.453037e-18  1.000228  -2.637438 -0.673460 -0.169266  0.538885   \n",
       "55   2194.0  2.087361e-17  1.000228  -2.291803 -1.060862  0.239454  0.589611   \n",
       "56   2194.0  6.588470e-17  1.000228  -4.479310 -0.361715  0.044891  0.537399   \n",
       "57   2194.0  6.020455e-17  1.000228  -3.696888 -0.760411  0.066174  0.646184   \n",
       "58   2194.0 -1.337429e-15  1.000228  -2.976134 -0.618277 -0.017028  0.670532   \n",
       "59   2194.0 -3.226111e-16  1.000228  -3.256312 -0.710739 -0.027780  0.634483   \n",
       "60   2194.0 -8.024973e-15  1.000228  -4.622857 -0.709679  0.117333  0.670595   \n",
       "61   2194.0  1.257044e-15  1.000228 -10.372585 -0.439060  0.308625  0.424233   \n",
       "62   2194.0  2.696415e-15  1.000228  -3.390159 -0.568960 -0.004629  0.628715   \n",
       "63   2194.0 -1.809999e-16  1.000228  -0.813545 -0.304087 -0.019296  0.106236   \n",
       "64   2194.0 -1.871287e-15  1.000228  -3.848354 -0.631593  0.002445  0.583173   \n",
       "65   2194.0  2.413748e-17  1.000228  -4.266187 -0.502905 -0.025524  0.615281   \n",
       "66   2194.0 -2.841341e-17  1.000228  -4.218170 -0.544942 -0.061623  0.453918   \n",
       "67   2194.0  2.636845e-17  1.000228  -5.692734 -0.372075  0.102379  0.509054   \n",
       "68   2194.0  3.091824e-17  1.000228 -11.654924 -0.437026  0.153390  0.153390   \n",
       "69   2194.0 -1.206368e-16  1.000228  -8.680023 -0.615100  0.281002  0.281002   \n",
       "70   2194.0  3.537128e-17  1.000228  -4.405853 -0.446783 -0.087718  0.271346   \n",
       "71   2194.0  1.988180e-16  1.000228  -7.590891 -0.574496 -0.073325  0.427846   \n",
       "72   2194.0 -8.931375e-18  1.000228  -7.803259 -0.592747 -0.041924  0.435651   \n",
       "73   2194.0 -2.133789e-16  1.000228  -7.044210 -0.649311 -0.089389  0.676820   \n",
       "74   2194.0 -2.732545e-17  1.000228  -5.780091 -0.531136  0.056300  0.544062   \n",
       "75   2194.0 -6.021720e-18  1.000228  -3.884405 -0.606792  0.017321  0.566401   \n",
       "76   2194.0 -2.287242e-17  1.000228  -7.991130 -0.578355  0.114488  0.557561   \n",
       "77   2194.0  2.074963e-16  1.000228  -0.128385 -0.128385 -0.128385 -0.128385   \n",
       "78   2194.0 -2.262041e-15  1.000228  -3.541344 -0.487823  0.090799  0.595858   \n",
       "79   2194.0  1.311394e-14  1.000228 -10.303727  0.279947  0.397071  0.439661   \n",
       "80   2194.0 -9.959305e-16  1.000228  -1.023707 -0.100695  0.013124  0.112889   \n",
       "81   2194.0 -9.553535e-15  1.000228  -7.989388 -0.578107  0.063190  0.592095   \n",
       "82   2194.0 -2.774596e-15  1.000228 -16.037953 -0.758361 -0.118575  0.910100   \n",
       "83   2194.0 -1.704299e-15  1.000228  -9.433215 -0.593925  0.006417  0.563706   \n",
       "84   2194.0 -8.553069e-15  1.000228  -3.583282 -0.730477 -0.119161  0.695926   \n",
       "85   2194.0 -6.957870e-17  1.000228  -2.528268 -0.778116 -0.149641  0.639244   \n",
       "86   2194.0  1.891529e-16  1.000228  -1.648517 -0.696198 -0.100062  0.417340   \n",
       "87   2194.0  5.696345e-16  1.000228 -13.103748 -0.507915  0.022866  0.512817   \n",
       "88   2194.0 -7.501849e-18  1.000228  -3.236881 -0.278679  0.252096  0.517437   \n",
       "89   2194.0  3.615977e-14  1.000228  -2.054129 -0.682000 -0.044940  0.543116   \n",
       "90   2194.0  7.600423e-15  1.000228  -2.297082 -0.745726 -0.094517  0.794806   \n",
       "91   2194.0 -1.096722e-14  1.000228  -3.868753 -0.609136 -0.020435  0.799012   \n",
       "92   2194.0  3.486272e-16  1.000228  -2.099950 -0.705563 -0.146359  0.539911   \n",
       "93   2194.0  9.540188e-18  1.000228  -0.920820 -0.425618 -0.154056  0.149455   \n",
       "94   2194.0 -3.599875e-16  1.000228  -1.918416 -0.898570 -0.108654  0.799936   \n",
       "95   2194.0 -3.219090e-16  1.000228  -2.130860 -0.719485 -0.017790  0.754773   \n",
       "96   2194.0  2.991125e-16  1.000228  -3.022715 -0.517422 -0.171190  0.369549   \n",
       "97   2194.0 -5.930635e-17  1.000228  -1.350009 -0.625970 -0.295474  0.292154   \n",
       "98   2194.0 -1.327815e-16  1.000228  -2.018984 -0.621768 -0.113496  0.401156   \n",
       "99   2194.0 -2.820088e-16  1.000228  -3.567739 -0.748245 -0.219590  0.573392   \n",
       "100  2194.0  1.751865e-16  1.000228  -1.233035 -0.585514 -0.217896  0.165704   \n",
       "101  2194.0 -8.147033e-18  1.000228  -1.761999 -0.669463 -0.151027  0.451450   \n",
       "102  2194.0  7.691609e-17  1.000228  -1.580215 -0.635905 -0.196928  0.318614   \n",
       "103  2194.0  5.475211e-17  1.000228  -1.638872 -0.698055 -0.260900  0.392852   \n",
       "104  2194.0  5.203063e-17  1.000228  -0.528610 -0.215893 -0.070077  0.069331   \n",
       "105  2194.0 -1.470767e-16  1.000228  -1.183131 -0.720093 -0.025537  0.491976   \n",
       "106  2194.0  5.649790e-17  1.000228  -1.108837 -0.472554 -0.213686  0.233890   \n",
       "107  2194.0 -3.633273e-17  1.000228  -0.816758 -0.464770 -0.282317 -0.020142   \n",
       "108  2194.0 -5.657697e-17  1.000228  -0.917554 -0.451776 -0.238057 -0.014753   \n",
       "109  2194.0 -1.958324e-17  1.000228  -1.044427 -0.675684 -0.267640  0.226325   \n",
       "110  2194.0 -1.773624e-17  1.000228  -0.753014 -0.677455 -0.461243  0.362390   \n",
       "111  2194.0  1.870781e-16  1.000228  -1.199902 -0.453509 -0.166435  0.178054   \n",
       "112  2194.0  1.115347e-16  1.000228  -1.239095 -0.461434 -0.072603  0.316228   \n",
       "113  2194.0  3.600887e-16  1.000228  -1.811372 -0.675678 -0.137718  0.499864   \n",
       "114  2194.0 -2.712304e-17  1.000228  -1.768168 -0.846491  0.034223  0.705000   \n",
       "115  2194.0  8.086310e-17  1.000228  -2.427326 -0.739264 -0.026533  0.656260   \n",
       "116  2194.0  2.493701e-16  1.000228  -1.952329 -0.656779 -0.085471  0.429584   \n",
       "117  2194.0  4.062384e-16  1.000228  -2.935237 -0.568347  0.016660  0.762096   \n",
       "118  2194.0 -1.760164e-15  1.000228  -3.544499 -0.626334 -0.026183  0.555248   \n",
       "119  2194.0  7.500331e-16  1.000228  -2.613084 -0.718133 -0.007526  0.628087   \n",
       "120  2194.0 -7.531198e-16  1.000228  -2.277351 -0.629279 -0.072734  0.680555   \n",
       "121  2194.0  9.503185e-17  1.000228  -1.220369 -0.578681 -0.364785  0.091526   \n",
       "122  2194.0  5.721140e-16  1.000228  -2.974103 -0.665215 -0.083946  0.643046   \n",
       "123  2194.0 -1.709517e-16  1.000228  -1.865439 -0.742468 -0.101958  0.606286   \n",
       "124  2194.0  5.428404e-16  1.000228  -1.966924 -0.733762 -0.147810  0.645223   \n",
       "125  2194.0  5.586537e-17  1.000228  -3.397618 -0.797405  0.214343  0.719757   \n",
       "126  2194.0  2.265988e-16  1.000228  -1.151961 -0.430659 -0.142732  0.245129   \n",
       "127  2194.0 -2.727485e-17  1.000228  -1.756583 -0.815443 -0.066852  0.571446   \n",
       "128  2194.0 -6.965460e-17  1.000228  -0.489764 -0.139400 -0.074684  0.001191   \n",
       "129  2194.0  1.496828e-16  1.000228  -1.506127 -0.337387 -0.070275  0.243508   \n",
       "130  2194.0  2.945077e-17  1.000228  -1.477156 -0.550154 -0.218692  0.251183   \n",
       "131  2194.0 -2.126831e-16  1.000228  -2.422044 -0.592485  0.022388  0.660430   \n",
       "132  2194.0  5.589067e-17  1.000228  -1.111152 -0.515057 -0.245009  0.236383   \n",
       "133  2194.0  5.748466e-17  1.000228  -1.696269 -0.468163 -0.192238  0.192143   \n",
       "134  2194.0 -1.943143e-16  1.000228  -1.479332 -0.661578 -0.335470  0.509597   \n",
       "135  2194.0  5.957202e-17  1.000228  -1.014653 -0.402577 -0.135519  0.181300   \n",
       "136  2194.0 -4.156378e-17  1.000228  -1.283080 -0.508262 -0.115487  0.272995   \n",
       "137  2194.0  1.267091e-16  1.000228  -2.455899 -0.619590  0.047320  0.580848   \n",
       "138  2194.0 -2.651581e-17  1.000228  -2.766396 -0.568492 -0.028383  0.556506   \n",
       "139  2194.0 -2.651581e-17  1.000228  -1.539500 -0.612290 -0.263697  0.428747   \n",
       "140  2194.0  2.179964e-16  1.000228  -2.305560 -0.727063 -0.107191  0.575102   \n",
       "141  2194.0 -1.052283e-16  1.000228  -1.723667 -0.640175 -0.166147  0.375599   \n",
       "142  2194.0  4.153469e-16  1.000228  -1.544242 -0.799674 -0.088781  0.569731   \n",
       "143  2194.0  2.163265e-17  1.000228  -0.152129 -0.140865 -0.129600 -0.083416   \n",
       "144  2194.0  1.731118e-16  1.000228  -1.779081 -0.517167 -0.063069  0.437273   \n",
       "145  2194.0 -1.603931e-16  1.000228  -1.966004 -0.651278 -0.213912  0.412264   \n",
       "146  2194.0 -5.417018e-17  1.000228  -1.126959 -0.518946 -0.246941  0.177069   \n",
       "147  2194.0 -6.515096e-17  1.000228  -1.050223 -0.518788 -0.253070  0.249895   \n",
       "148  2194.0  9.918127e-18  1.000228  -2.049055 -0.649079 -0.143933  0.548839   \n",
       "149  2194.0 -1.140079e-16  1.000228  -1.962502 -0.586070 -0.148114  0.414971   \n",
       "150  2194.0  2.208807e-16  1.000228  -1.706693 -0.623585 -0.131595  0.444682   \n",
       "151  2194.0  3.581405e-17  1.000228  -0.120579 -0.106974 -0.096770 -0.086567   \n",
       "152  2194.0 -1.677100e-16  1.000228  -1.025282 -0.387510 -0.130566  0.190615   \n",
       "153  2194.0  6.069793e-17  1.000228  -2.414753 -0.783292 -0.022989  0.622321   \n",
       "154  2194.0  3.821009e-16  1.000228  -1.608524 -0.972208  0.009721  0.710637   \n",
       "155  2194.0 -2.649051e-16  1.000228  -1.817495 -0.842162 -0.216955  0.725184   \n",
       "156  2194.0 -1.559069e-16  1.000228  -2.096666 -0.710952 -0.095721  0.496963   \n",
       "157  2194.0  8.845350e-17  1.000228  -1.057830 -1.057830 -0.099840  0.683457   \n",
       "158  2194.0  1.979577e-16  1.000228  -1.014199 -1.014199 -0.007414  0.791701   \n",
       "159  2194.0  8.668241e-17  1.000228  -1.723441 -0.683069 -0.278605  0.549946   \n",
       "160  2194.0 -1.139826e-16  1.000228  -0.860489 -0.586710 -0.339348  0.171051   \n",
       "161  2194.0 -5.404367e-17  1.000228  -0.909059 -0.867240 -0.285144  0.415882   \n",
       "162  2194.0 -1.025970e-16  1.000228  -1.336052 -0.475452 -0.157814  0.268443   \n",
       "163  2194.0  8.389926e-17  1.000228  -1.633675 -0.658614 -0.278395  0.472951   \n",
       "164  2194.0 -2.692063e-17  1.000228  -0.889621 -0.749303 -0.388207  0.405611   \n",
       "165  2194.0  3.926769e-17  1.000228  -2.648596 -0.693006 -0.015308  0.651986   \n",
       "166  2194.0 -3.411886e-17  1.000228  -1.311472 -0.564329 -0.273325  0.283704   \n",
       "167  2194.0  4.527929e-16  1.000228  -1.109917 -1.109917 -0.092794  0.713752   \n",
       "168  2194.0  5.303162e-17  1.000228  -1.132982 -0.638322 -0.267473  0.330843   \n",
       "169  2194.0  1.597843e-16  1.000228  -1.002200 -0.654587 -0.312842  0.395796   \n",
       "170  2194.0 -2.226518e-17  1.000228  -0.902646 -0.656604 -0.371591  0.297826   \n",
       "171  2194.0  2.722425e-17  1.000228  -1.142204 -1.142204 -0.190854  0.668321   \n",
       "172  2194.0  4.819906e-17  1.000228  -0.887463 -0.614237 -0.464505  0.497745   \n",
       "173  2194.0  1.805504e-16  1.000228  -1.294781 -0.980161  0.035022  0.655616   \n",
       "174  2194.0  1.619286e-16  1.000228  -1.221308 -0.686209 -0.226447  0.393966   \n",
       "175  2194.0  6.942689e-17  1.000228  -0.746360 -0.746360 -0.746360  0.825867   \n",
       "176  2194.0  1.138561e-16  1.000228  -0.740204 -0.740204 -0.740204  0.747247   \n",
       "177  2194.0 -6.537868e-17  1.000228  -1.349572 -0.562671 -0.270185  0.124802   \n",
       "178  2194.0 -6.193453e-17  1.000228  -0.896319 -0.896319 -0.691461  0.988376   \n",
       "179  2194.0 -6.527747e-16  1.000228  -0.140396 -0.140396 -0.140396 -0.140396   \n",
       "180  2194.0 -5.705858e-15  1.000228  -2.243642 -0.537110 -0.537110  0.613807   \n",
       "181  2194.0 -3.305368e-16  1.000228  -2.407297 -0.398175 -0.398175  0.048297   \n",
       "182  2194.0  8.762362e-16  1.000228  -3.377761 -0.451191  0.280452  0.280452   \n",
       "183  2194.0 -6.391120e-17  1.000228  -1.621381 -0.606436 -0.302341  0.477658   \n",
       "184  2194.0  7.210074e-15  1.000228  -7.194216 -0.697835 -0.162195  0.714023   \n",
       "185  2194.0 -5.192057e-16  1.000228  -1.363359 -0.670686 -0.411246  0.463504   \n",
       "186  2194.0  1.811070e-16  1.000228  -1.668507 -0.423216 -0.155523  0.222722   \n",
       "187  2194.0  3.774961e-17  1.000228  -1.096079 -0.447228 -0.240170  0.004815   \n",
       "188  2194.0 -2.306471e-16  1.000228  -1.277654 -0.842064 -0.189910  0.623068   \n",
       "189  2194.0  3.056833e-15  1.000228  -2.887032 -0.089554  0.250035  0.256385   \n",
       "190  2194.0  4.693906e-16  1.000228  -3.794341 -0.862554 -0.226196  0.475887   \n",
       "191  2194.0 -1.487719e-17  1.000228  -2.167262 -0.535293 -0.462037  0.485197   \n",
       "192  2194.0  8.957941e-17  1.000228  -1.416675 -0.675988 -0.302714  0.260419   \n",
       "193  2194.0 -1.475113e-15  1.000228 -11.081199  0.016802  0.085720  0.201461   \n",
       "194  2194.0 -3.828347e-16  1.000228  -3.483387 -0.369614  0.171020  0.631444   \n",
       "195  2194.0 -5.566296e-18  1.000228  -0.270755 -0.220065 -0.203126 -0.182546   \n",
       "196  2194.0  1.365868e-14  1.000228  -6.281437 -0.630429  0.037671  0.622258   \n",
       "197  2194.0 -6.184281e-17  1.000228  -0.813403 -0.332476 -0.131361  0.139707   \n",
       "198  2194.0 -1.282272e-16  1.000228  -3.187290 -0.608299 -0.074139  0.541397   \n",
       "199  2194.0 -3.518911e-16  1.000228  -1.609150 -0.718013 -0.139353  0.482707   \n",
       "200  2194.0 -6.730158e-17  1.000228  -1.127377 -0.618918 -0.286974  0.188515   \n",
       "\n",
       "           max  \n",
       "0     4.341457  \n",
       "1     4.590805  \n",
       "2     3.724207  \n",
       "3     5.956214  \n",
       "4    33.139103  \n",
       "5     4.763205  \n",
       "6     1.160781  \n",
       "7     2.787422  \n",
       "8     5.306429  \n",
       "9     5.490123  \n",
       "10    1.831719  \n",
       "11    9.914739  \n",
       "12    3.912580  \n",
       "13   27.574395  \n",
       "14    4.363118  \n",
       "15    9.657058  \n",
       "16    2.817611  \n",
       "17    3.213299  \n",
       "18    6.708483  \n",
       "19    4.214688  \n",
       "20    5.417190  \n",
       "21    0.881551  \n",
       "22    2.582616  \n",
       "23    2.827587  \n",
       "24    2.305362  \n",
       "25    7.822077  \n",
       "26    7.014540  \n",
       "27    5.866623  \n",
       "28    2.682508  \n",
       "29    0.818543  \n",
       "30   17.654802  \n",
       "31    3.699374  \n",
       "32    6.178799  \n",
       "33    3.943352  \n",
       "34    1.746032  \n",
       "35    5.099316  \n",
       "36    3.734240  \n",
       "37    2.892696  \n",
       "38    1.344985  \n",
       "39    2.494197  \n",
       "40    2.991366  \n",
       "41   15.535384  \n",
       "42    4.524203  \n",
       "43   18.468054  \n",
       "44   23.670851  \n",
       "45    4.439086  \n",
       "46   17.201556  \n",
       "47    3.940000  \n",
       "48    3.883048  \n",
       "49   46.829478  \n",
       "50    9.598503  \n",
       "51    3.142018  \n",
       "52    4.348313  \n",
       "53    5.488666  \n",
       "54    3.909845  \n",
       "55    2.626335  \n",
       "56    4.488916  \n",
       "57    3.916097  \n",
       "58    2.822647  \n",
       "59    3.593971  \n",
       "60    4.163064  \n",
       "61    0.585080  \n",
       "62    5.598967  \n",
       "63   15.052673  \n",
       "64    5.359076  \n",
       "65    4.631598  \n",
       "66    8.895896  \n",
       "67    3.830230  \n",
       "68    6.647962  \n",
       "69    7.449822  \n",
       "70   10.267851  \n",
       "71    5.690142  \n",
       "72    4.402751  \n",
       "73    4.478395  \n",
       "74    5.479475  \n",
       "75    4.714748  \n",
       "76    2.659983  \n",
       "77    9.391683  \n",
       "78    3.686565  \n",
       "79    0.556784  \n",
       "80   45.807391  \n",
       "81    3.501073  \n",
       "82    1.487162  \n",
       "83    9.670081  \n",
       "84    2.631759  \n",
       "85    3.718171  \n",
       "86    4.541554  \n",
       "87    2.799257  \n",
       "88    2.594443  \n",
       "89   10.393046  \n",
       "90    2.775491  \n",
       "91    2.553411  \n",
       "92    8.935937  \n",
       "93   14.031073  \n",
       "94    2.867441  \n",
       "95    4.167426  \n",
       "96    6.469373  \n",
       "97    4.948770  \n",
       "98   19.502819  \n",
       "99    7.093472  \n",
       "100   7.911009  \n",
       "101   8.248730  \n",
       "102   9.047105  \n",
       "103   4.874130  \n",
       "104  26.130788  \n",
       "105  23.916238  \n",
       "106  28.440828  \n",
       "107   5.473016  \n",
       "108   5.004085  \n",
       "109   8.963593  \n",
       "110   5.277204  \n",
       "111  10.455311  \n",
       "112  19.498536  \n",
       "113  11.976344  \n",
       "114   3.848945  \n",
       "115   2.770593  \n",
       "116   6.575096  \n",
       "117   3.328217  \n",
       "118   3.178293  \n",
       "119   3.528131  \n",
       "120   2.676299  \n",
       "121   5.235724  \n",
       "122   9.742589  \n",
       "123  14.457454  \n",
       "124  11.837567  \n",
       "125   6.985100  \n",
       "126  30.496287  \n",
       "127   6.756976  \n",
       "128  10.128247  \n",
       "129  16.412237  \n",
       "130  21.566754  \n",
       "131   3.448330  \n",
       "132   7.604470  \n",
       "133   8.402910  \n",
       "134  17.954729  \n",
       "135  20.217253  \n",
       "136  14.633956  \n",
       "137   8.791807  \n",
       "138   7.216933  \n",
       "139  14.265756  \n",
       "140   4.275483  \n",
       "141   8.840377  \n",
       "142   6.331711  \n",
       "143  11.008750  \n",
       "144  32.553305  \n",
       "145   5.559085  \n",
       "146  10.705293  \n",
       "147  18.508498  \n",
       "148  11.286800  \n",
       "149  11.926948  \n",
       "150  19.035589  \n",
       "151  33.572458  \n",
       "152  19.154023  \n",
       "153   4.542314  \n",
       "154   2.848917  \n",
       "155   2.848664  \n",
       "156   5.315245  \n",
       "157   2.402380  \n",
       "158   2.107307  \n",
       "159  10.608134  \n",
       "160   7.647334  \n",
       "161   3.172550  \n",
       "162  19.103647  \n",
       "163   7.311566  \n",
       "164   3.450791  \n",
       "165   7.349617  \n",
       "166  16.162206  \n",
       "167   2.447038  \n",
       "168   4.739949  \n",
       "169   3.949625  \n",
       "170   3.753302  \n",
       "171   2.517876  \n",
       "172   2.937577  \n",
       "173   2.613527  \n",
       "174   3.160511  \n",
       "175   2.357846  \n",
       "176   2.232419  \n",
       "177   8.175457  \n",
       "178   2.034152  \n",
       "179   7.898866  \n",
       "180   2.955327  \n",
       "181  11.656558  \n",
       "182   7.962699  \n",
       "183   8.372954  \n",
       "184   4.149850  \n",
       "185   2.600757  \n",
       "186  16.902883  \n",
       "187   5.261223  \n",
       "188   3.662967  \n",
       "189   5.293657  \n",
       "190   6.452349  \n",
       "191  10.697214  \n",
       "192   5.584696  \n",
       "193   3.058086  \n",
       "194   2.028850  \n",
       "195   5.698335  \n",
       "196   2.626556  \n",
       "197  39.881784  \n",
       "198   6.803173  \n",
       "199   7.267498  \n",
       "200   7.078848  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df = pd.DataFrame(X_train)\n",
    "X_train_df.describe().transpose()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25fe6e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>732.0</td>\n",
       "      <td>7.276890e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.185075</td>\n",
       "      <td>-0.659571</td>\n",
       "      <td>-0.144246</td>\n",
       "      <td>0.533917</td>\n",
       "      <td>3.427766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-6.716698e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.090055</td>\n",
       "      <td>-0.560818</td>\n",
       "      <td>0.045983</td>\n",
       "      <td>0.546470</td>\n",
       "      <td>3.668981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-5.575382e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-4.851195</td>\n",
       "      <td>-0.750024</td>\n",
       "      <td>-0.061808</td>\n",
       "      <td>0.562047</td>\n",
       "      <td>4.019677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.158757e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.415972</td>\n",
       "      <td>-0.726693</td>\n",
       "      <td>-0.267616</td>\n",
       "      <td>0.578864</td>\n",
       "      <td>5.684693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-3.036240e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.063052</td>\n",
       "      <td>-0.057284</td>\n",
       "      <td>-0.052366</td>\n",
       "      <td>-0.049429</td>\n",
       "      <td>19.107991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.201866e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.425556</td>\n",
       "      <td>-0.467071</td>\n",
       "      <td>0.062325</td>\n",
       "      <td>0.522790</td>\n",
       "      <td>3.391505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>732.0</td>\n",
       "      <td>6.291643e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-13.222649</td>\n",
       "      <td>-0.083778</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.220513</td>\n",
       "      <td>0.644348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>732.0</td>\n",
       "      <td>6.819075e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.435507</td>\n",
       "      <td>-0.743003</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.735872</td>\n",
       "      <td>2.897130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.896893e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.135579</td>\n",
       "      <td>-0.560479</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>0.634048</td>\n",
       "      <td>3.082219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.171649e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.497506</td>\n",
       "      <td>-0.617140</td>\n",
       "      <td>0.036029</td>\n",
       "      <td>0.624951</td>\n",
       "      <td>4.212024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-8.576170e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-8.180336</td>\n",
       "      <td>-0.708438</td>\n",
       "      <td>0.135470</td>\n",
       "      <td>0.716249</td>\n",
       "      <td>1.974998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.629139e-14</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.380168</td>\n",
       "      <td>-0.433949</td>\n",
       "      <td>-0.085992</td>\n",
       "      <td>0.526910</td>\n",
       "      <td>19.553673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.190185e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.345378</td>\n",
       "      <td>-0.690853</td>\n",
       "      <td>0.034786</td>\n",
       "      <td>0.672560</td>\n",
       "      <td>3.411210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.762931e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.551107</td>\n",
       "      <td>-0.715358</td>\n",
       "      <td>-0.095542</td>\n",
       "      <td>0.776909</td>\n",
       "      <td>4.256996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.034672e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.570599</td>\n",
       "      <td>-0.118443</td>\n",
       "      <td>-0.038858</td>\n",
       "      <td>0.035118</td>\n",
       "      <td>26.776608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.578974e-14</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-7.874998</td>\n",
       "      <td>-0.546228</td>\n",
       "      <td>-0.110609</td>\n",
       "      <td>0.769809</td>\n",
       "      <td>3.911860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.501304e-14</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-21.977564</td>\n",
       "      <td>-0.308487</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.409513</td>\n",
       "      <td>3.458809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.991232e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-12.389230</td>\n",
       "      <td>-0.493644</td>\n",
       "      <td>0.046314</td>\n",
       "      <td>0.582138</td>\n",
       "      <td>2.520199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.656993e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.047355</td>\n",
       "      <td>-0.310341</td>\n",
       "      <td>0.014714</td>\n",
       "      <td>0.216412</td>\n",
       "      <td>5.071232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-4.299650e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.876199</td>\n",
       "      <td>-0.426771</td>\n",
       "      <td>-0.047622</td>\n",
       "      <td>0.334715</td>\n",
       "      <td>4.430227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.938603e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-5.614495</td>\n",
       "      <td>-0.449679</td>\n",
       "      <td>0.130314</td>\n",
       "      <td>0.607380</td>\n",
       "      <td>5.569701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>732.0</td>\n",
       "      <td>4.606971e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-4.443771</td>\n",
       "      <td>0.250044</td>\n",
       "      <td>0.412176</td>\n",
       "      <td>0.511085</td>\n",
       "      <td>0.868856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.496485e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.639376</td>\n",
       "      <td>-0.587840</td>\n",
       "      <td>-0.057983</td>\n",
       "      <td>0.600969</td>\n",
       "      <td>2.581156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-3.608225e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.637893</td>\n",
       "      <td>-0.733707</td>\n",
       "      <td>0.050431</td>\n",
       "      <td>0.718516</td>\n",
       "      <td>2.606198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>732.0</td>\n",
       "      <td>6.699256e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.938705</td>\n",
       "      <td>-0.516527</td>\n",
       "      <td>-0.405051</td>\n",
       "      <td>-0.227947</td>\n",
       "      <td>2.335483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.254420e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.911798</td>\n",
       "      <td>-0.361151</td>\n",
       "      <td>-0.125767</td>\n",
       "      <td>0.074081</td>\n",
       "      <td>7.669562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.054747e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.799442</td>\n",
       "      <td>-0.267178</td>\n",
       "      <td>-0.179685</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>7.286988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.081284e-14</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-4.275707</td>\n",
       "      <td>-0.655529</td>\n",
       "      <td>0.027309</td>\n",
       "      <td>0.430981</td>\n",
       "      <td>3.154671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-4.406008e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.775545</td>\n",
       "      <td>-0.538025</td>\n",
       "      <td>-0.033807</td>\n",
       "      <td>0.297125</td>\n",
       "      <td>2.775740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.354411e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.457605</td>\n",
       "      <td>0.021847</td>\n",
       "      <td>0.491472</td>\n",
       "      <td>0.564731</td>\n",
       "      <td>0.763685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.921657e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.105704</td>\n",
       "      <td>-0.422285</td>\n",
       "      <td>-0.175847</td>\n",
       "      <td>0.254348</td>\n",
       "      <td>21.104615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-3.463835e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.037386</td>\n",
       "      <td>-0.737076</td>\n",
       "      <td>-0.248721</td>\n",
       "      <td>0.850270</td>\n",
       "      <td>3.481195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>732.0</td>\n",
       "      <td>6.074945e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.112456</td>\n",
       "      <td>-0.686249</td>\n",
       "      <td>-0.043301</td>\n",
       "      <td>0.696652</td>\n",
       "      <td>3.555083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.534611e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.302729</td>\n",
       "      <td>-0.773028</td>\n",
       "      <td>-0.072406</td>\n",
       "      <td>0.645383</td>\n",
       "      <td>4.593731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.457051e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.763727</td>\n",
       "      <td>-0.926510</td>\n",
       "      <td>0.373915</td>\n",
       "      <td>0.837536</td>\n",
       "      <td>1.754579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-5.362286e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.455293</td>\n",
       "      <td>-0.723254</td>\n",
       "      <td>0.017568</td>\n",
       "      <td>0.642579</td>\n",
       "      <td>2.312778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>732.0</td>\n",
       "      <td>6.394399e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-18.744682</td>\n",
       "      <td>-0.391638</td>\n",
       "      <td>0.051876</td>\n",
       "      <td>0.395072</td>\n",
       "      <td>3.536631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>732.0</td>\n",
       "      <td>6.977570e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.918338</td>\n",
       "      <td>-0.831044</td>\n",
       "      <td>-0.094489</td>\n",
       "      <td>0.650834</td>\n",
       "      <td>2.781581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>732.0</td>\n",
       "      <td>5.837619e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-9.155001</td>\n",
       "      <td>-0.463451</td>\n",
       "      <td>0.312580</td>\n",
       "      <td>0.657483</td>\n",
       "      <td>1.399024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>732.0</td>\n",
       "      <td>9.530097e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.166583</td>\n",
       "      <td>-0.618171</td>\n",
       "      <td>0.061406</td>\n",
       "      <td>0.595359</td>\n",
       "      <td>2.318571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.405372e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-4.640294</td>\n",
       "      <td>-0.780281</td>\n",
       "      <td>-0.140817</td>\n",
       "      <td>0.811653</td>\n",
       "      <td>2.996589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.698702e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.552386</td>\n",
       "      <td>-0.619970</td>\n",
       "      <td>-0.332963</td>\n",
       "      <td>0.427176</td>\n",
       "      <td>7.823519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.009798</td>\n",
       "      <td>-0.506307</td>\n",
       "      <td>0.038131</td>\n",
       "      <td>0.570059</td>\n",
       "      <td>4.168450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.286686e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.193962</td>\n",
       "      <td>-0.497806</td>\n",
       "      <td>-0.006672</td>\n",
       "      <td>0.499370</td>\n",
       "      <td>7.652856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.442642e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.556241</td>\n",
       "      <td>-0.612820</td>\n",
       "      <td>-0.217335</td>\n",
       "      <td>0.367036</td>\n",
       "      <td>10.373750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-8.148461e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.397078</td>\n",
       "      <td>-0.687895</td>\n",
       "      <td>-0.182378</td>\n",
       "      <td>0.477407</td>\n",
       "      <td>3.362541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.630450e-18</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.084019</td>\n",
       "      <td>-0.083166</td>\n",
       "      <td>-0.082982</td>\n",
       "      <td>-0.082787</td>\n",
       "      <td>12.058247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.777570e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-10.212381</td>\n",
       "      <td>-0.362795</td>\n",
       "      <td>0.059622</td>\n",
       "      <td>0.492789</td>\n",
       "      <td>1.947710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.771503e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.395015</td>\n",
       "      <td>-0.543190</td>\n",
       "      <td>-0.096982</td>\n",
       "      <td>0.247487</td>\n",
       "      <td>3.745319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>732.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.846580e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.212554</td>\n",
       "      <td>-0.492907</td>\n",
       "      <td>-0.009342</td>\n",
       "      <td>0.415806</td>\n",
       "      <td>7.704981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>732.0</td>\n",
       "      <td>5.353944e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-4.505535</td>\n",
       "      <td>-0.519077</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>0.611411</td>\n",
       "      <td>2.543304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.941374e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.568568</td>\n",
       "      <td>-0.692804</td>\n",
       "      <td>-0.106580</td>\n",
       "      <td>0.489659</td>\n",
       "      <td>3.162718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-3.185066e-18</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.036358</td>\n",
       "      <td>-0.804255</td>\n",
       "      <td>0.042950</td>\n",
       "      <td>0.511707</td>\n",
       "      <td>3.208141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.305381e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.872838</td>\n",
       "      <td>-0.687628</td>\n",
       "      <td>-0.183502</td>\n",
       "      <td>0.518158</td>\n",
       "      <td>3.732216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.236109e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.088963</td>\n",
       "      <td>-1.074517</td>\n",
       "      <td>0.217898</td>\n",
       "      <td>0.553180</td>\n",
       "      <td>2.513295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.918623e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.711700</td>\n",
       "      <td>-0.334124</td>\n",
       "      <td>0.044040</td>\n",
       "      <td>0.582794</td>\n",
       "      <td>2.924304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-7.280151e-18</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-6.041211</td>\n",
       "      <td>-0.663720</td>\n",
       "      <td>0.058424</td>\n",
       "      <td>0.598654</td>\n",
       "      <td>3.589211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>732.0</td>\n",
       "      <td>5.770515e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.982964</td>\n",
       "      <td>-0.625841</td>\n",
       "      <td>0.004367</td>\n",
       "      <td>0.594927</td>\n",
       "      <td>3.013489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.019029e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.291545</td>\n",
       "      <td>-0.700361</td>\n",
       "      <td>-0.104019</td>\n",
       "      <td>0.677031</td>\n",
       "      <td>2.830194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.254796e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.516490</td>\n",
       "      <td>-0.707048</td>\n",
       "      <td>0.062082</td>\n",
       "      <td>0.692537</td>\n",
       "      <td>3.910299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-4.406615e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-14.329567</td>\n",
       "      <td>-0.590495</td>\n",
       "      <td>0.341553</td>\n",
       "      <td>0.500346</td>\n",
       "      <td>0.762700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-3.627942e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.341563</td>\n",
       "      <td>-0.608094</td>\n",
       "      <td>-0.015574</td>\n",
       "      <td>0.628301</td>\n",
       "      <td>2.724604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.920140e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.972258</td>\n",
       "      <td>-0.294237</td>\n",
       "      <td>-0.024131</td>\n",
       "      <td>0.116740</td>\n",
       "      <td>14.643658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.465434e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.923223</td>\n",
       "      <td>-0.630289</td>\n",
       "      <td>-0.003250</td>\n",
       "      <td>0.537239</td>\n",
       "      <td>4.443237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>732.0</td>\n",
       "      <td>7.735160e-18</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-4.246654</td>\n",
       "      <td>-0.407436</td>\n",
       "      <td>0.022406</td>\n",
       "      <td>0.592886</td>\n",
       "      <td>3.398897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.137524e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.499116</td>\n",
       "      <td>-0.575995</td>\n",
       "      <td>-0.067626</td>\n",
       "      <td>0.440743</td>\n",
       "      <td>5.270247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.199212e-18</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-4.343274</td>\n",
       "      <td>-0.404920</td>\n",
       "      <td>0.011329</td>\n",
       "      <td>0.459597</td>\n",
       "      <td>4.365932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-5.088522e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-6.006282</td>\n",
       "      <td>-0.469004</td>\n",
       "      <td>0.223155</td>\n",
       "      <td>0.223155</td>\n",
       "      <td>5.760433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-3.354936e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-4.484188</td>\n",
       "      <td>-0.723173</td>\n",
       "      <td>0.217080</td>\n",
       "      <td>0.217080</td>\n",
       "      <td>5.858602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.719556e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-6.548086</td>\n",
       "      <td>-0.473827</td>\n",
       "      <td>-0.050757</td>\n",
       "      <td>0.329576</td>\n",
       "      <td>10.838388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.213358e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-5.858533</td>\n",
       "      <td>-0.553864</td>\n",
       "      <td>-0.048657</td>\n",
       "      <td>0.456549</td>\n",
       "      <td>5.508615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-3.511156e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-4.374931</td>\n",
       "      <td>-0.567847</td>\n",
       "      <td>-0.050930</td>\n",
       "      <td>0.483243</td>\n",
       "      <td>4.662676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-9.737202e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.105933</td>\n",
       "      <td>-0.652182</td>\n",
       "      <td>-0.107957</td>\n",
       "      <td>0.667377</td>\n",
       "      <td>8.718921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.334694e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-5.724386</td>\n",
       "      <td>-0.540771</td>\n",
       "      <td>0.093822</td>\n",
       "      <td>0.574776</td>\n",
       "      <td>3.093104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-4.095085e-18</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.608984</td>\n",
       "      <td>-0.598263</td>\n",
       "      <td>-0.071736</td>\n",
       "      <td>0.521888</td>\n",
       "      <td>3.695334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-9.529414e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-7.242029</td>\n",
       "      <td>-0.500005</td>\n",
       "      <td>0.185692</td>\n",
       "      <td>0.554303</td>\n",
       "      <td>2.405288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.478285e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.149048</td>\n",
       "      <td>-0.149048</td>\n",
       "      <td>-0.149048</td>\n",
       "      <td>-0.149048</td>\n",
       "      <td>12.171939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.031639e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-4.043446</td>\n",
       "      <td>-0.437831</td>\n",
       "      <td>0.078679</td>\n",
       "      <td>0.601231</td>\n",
       "      <td>3.514551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.354336e-14</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-7.983195</td>\n",
       "      <td>0.260041</td>\n",
       "      <td>0.391407</td>\n",
       "      <td>0.435196</td>\n",
       "      <td>0.599404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-5.788478e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-4.268388</td>\n",
       "      <td>-0.375433</td>\n",
       "      <td>0.140430</td>\n",
       "      <td>0.657114</td>\n",
       "      <td>2.793325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>732.0</td>\n",
       "      <td>4.349587e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-9.224776</td>\n",
       "      <td>-0.570810</td>\n",
       "      <td>0.065893</td>\n",
       "      <td>0.585650</td>\n",
       "      <td>3.028512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.016157e-14</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-14.960777</td>\n",
       "      <td>-0.709055</td>\n",
       "      <td>-0.097442</td>\n",
       "      <td>0.883445</td>\n",
       "      <td>1.310420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>732.0</td>\n",
       "      <td>6.285197e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-4.488376</td>\n",
       "      <td>-0.624212</td>\n",
       "      <td>0.057147</td>\n",
       "      <td>0.565816</td>\n",
       "      <td>3.748231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>732.0</td>\n",
       "      <td>8.796849e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.581480</td>\n",
       "      <td>-0.616514</td>\n",
       "      <td>-0.105313</td>\n",
       "      <td>0.610368</td>\n",
       "      <td>2.655173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.242176e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.573434</td>\n",
       "      <td>-0.778604</td>\n",
       "      <td>-0.169188</td>\n",
       "      <td>0.587265</td>\n",
       "      <td>3.767408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.269419e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.739294</td>\n",
       "      <td>-0.719423</td>\n",
       "      <td>-0.078420</td>\n",
       "      <td>0.488857</td>\n",
       "      <td>5.022981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.536223e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.473528</td>\n",
       "      <td>-0.671204</td>\n",
       "      <td>0.022616</td>\n",
       "      <td>0.645310</td>\n",
       "      <td>3.699732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-4.580428e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.012376</td>\n",
       "      <td>-0.209860</td>\n",
       "      <td>0.292981</td>\n",
       "      <td>0.544357</td>\n",
       "      <td>2.512052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.999969e-14</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.845192</td>\n",
       "      <td>-0.587676</td>\n",
       "      <td>-0.067325</td>\n",
       "      <td>0.409664</td>\n",
       "      <td>9.125549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.647134e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.232987</td>\n",
       "      <td>-0.716892</td>\n",
       "      <td>-0.063832</td>\n",
       "      <td>0.779630</td>\n",
       "      <td>2.770839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.715653e-14</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.026880</td>\n",
       "      <td>-0.602058</td>\n",
       "      <td>-0.003985</td>\n",
       "      <td>0.805272</td>\n",
       "      <td>2.570626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.007088e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.013700</td>\n",
       "      <td>-0.751895</td>\n",
       "      <td>-0.153074</td>\n",
       "      <td>0.523045</td>\n",
       "      <td>8.564228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.066501e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.074865</td>\n",
       "      <td>-0.491660</td>\n",
       "      <td>-0.161178</td>\n",
       "      <td>0.213045</td>\n",
       "      <td>10.103221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.178241e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.816204</td>\n",
       "      <td>-0.935039</td>\n",
       "      <td>-0.042791</td>\n",
       "      <td>0.770485</td>\n",
       "      <td>2.861173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-4.010150e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.045929</td>\n",
       "      <td>-0.784083</td>\n",
       "      <td>0.038519</td>\n",
       "      <td>0.746408</td>\n",
       "      <td>3.174015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>732.0</td>\n",
       "      <td>6.830189e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.212951</td>\n",
       "      <td>-0.617010</td>\n",
       "      <td>-0.213757</td>\n",
       "      <td>0.508730</td>\n",
       "      <td>4.524474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>732.0</td>\n",
       "      <td>7.477322e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.620439</td>\n",
       "      <td>-0.560350</td>\n",
       "      <td>-0.318274</td>\n",
       "      <td>0.314667</td>\n",
       "      <td>5.143995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.115200e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.150203</td>\n",
       "      <td>-0.370684</td>\n",
       "      <td>-0.104578</td>\n",
       "      <td>0.165639</td>\n",
       "      <td>22.718225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.680502e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.561399</td>\n",
       "      <td>-0.767247</td>\n",
       "      <td>-0.156026</td>\n",
       "      <td>0.629829</td>\n",
       "      <td>7.003990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.910544e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.401795</td>\n",
       "      <td>-0.580015</td>\n",
       "      <td>-0.193899</td>\n",
       "      <td>0.192645</td>\n",
       "      <td>5.854124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.944859e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.811842</td>\n",
       "      <td>-0.661575</td>\n",
       "      <td>-0.126165</td>\n",
       "      <td>0.375853</td>\n",
       "      <td>8.094964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.875660e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.506428</td>\n",
       "      <td>-0.601184</td>\n",
       "      <td>-0.189710</td>\n",
       "      <td>0.320776</td>\n",
       "      <td>10.045720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>732.0</td>\n",
       "      <td>9.024354e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.610976</td>\n",
       "      <td>-0.710332</td>\n",
       "      <td>-0.290053</td>\n",
       "      <td>0.497854</td>\n",
       "      <td>4.075785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.051334e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.363210</td>\n",
       "      <td>-0.197859</td>\n",
       "      <td>-0.084565</td>\n",
       "      <td>0.022617</td>\n",
       "      <td>15.055099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>732.0</td>\n",
       "      <td>5.293276e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.799593</td>\n",
       "      <td>-0.542823</td>\n",
       "      <td>-0.064700</td>\n",
       "      <td>0.254048</td>\n",
       "      <td>20.034165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.715910e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.479430</td>\n",
       "      <td>-0.248446</td>\n",
       "      <td>-0.141689</td>\n",
       "      <td>0.038827</td>\n",
       "      <td>20.746782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.814496e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.800093</td>\n",
       "      <td>-0.478590</td>\n",
       "      <td>-0.307135</td>\n",
       "      <td>0.011251</td>\n",
       "      <td>5.547678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>732.0</td>\n",
       "      <td>4.095085e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.834243</td>\n",
       "      <td>-0.452966</td>\n",
       "      <td>-0.228359</td>\n",
       "      <td>-0.022816</td>\n",
       "      <td>4.890459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-4.474259e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.988733</td>\n",
       "      <td>-0.636606</td>\n",
       "      <td>-0.301653</td>\n",
       "      <td>0.189673</td>\n",
       "      <td>6.282106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>732.0</td>\n",
       "      <td>4.550094e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.772857</td>\n",
       "      <td>-0.686351</td>\n",
       "      <td>-0.466765</td>\n",
       "      <td>0.368430</td>\n",
       "      <td>5.352990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.191629e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.951776</td>\n",
       "      <td>-0.442947</td>\n",
       "      <td>-0.160264</td>\n",
       "      <td>0.178955</td>\n",
       "      <td>10.298997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.070293e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.202954</td>\n",
       "      <td>-0.518011</td>\n",
       "      <td>-0.107046</td>\n",
       "      <td>0.303920</td>\n",
       "      <td>9.756126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.006854e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.576970</td>\n",
       "      <td>-0.683686</td>\n",
       "      <td>-0.139731</td>\n",
       "      <td>0.499041</td>\n",
       "      <td>4.770832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.607700e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.720467</td>\n",
       "      <td>-0.814756</td>\n",
       "      <td>0.077998</td>\n",
       "      <td>0.710312</td>\n",
       "      <td>3.125542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.068776e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.328501</td>\n",
       "      <td>-0.660873</td>\n",
       "      <td>-0.030029</td>\n",
       "      <td>0.668798</td>\n",
       "      <td>2.827384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.032375e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.848083</td>\n",
       "      <td>-0.642807</td>\n",
       "      <td>-0.142179</td>\n",
       "      <td>0.372378</td>\n",
       "      <td>6.024311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>732.0</td>\n",
       "      <td>8.296339e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.795032</td>\n",
       "      <td>-0.533253</td>\n",
       "      <td>-0.015939</td>\n",
       "      <td>0.771789</td>\n",
       "      <td>2.582387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.166866e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.429627</td>\n",
       "      <td>-0.637522</td>\n",
       "      <td>0.013648</td>\n",
       "      <td>0.604842</td>\n",
       "      <td>3.108847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.570086e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.883343</td>\n",
       "      <td>-0.776394</td>\n",
       "      <td>-0.011401</td>\n",
       "      <td>0.628090</td>\n",
       "      <td>3.166539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-9.979874e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.273299</td>\n",
       "      <td>-0.655759</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.630112</td>\n",
       "      <td>2.564842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.727023e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.251713</td>\n",
       "      <td>-0.592459</td>\n",
       "      <td>-0.375123</td>\n",
       "      <td>0.081283</td>\n",
       "      <td>5.308224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.574099e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.571139</td>\n",
       "      <td>-0.734427</td>\n",
       "      <td>-0.119066</td>\n",
       "      <td>0.668096</td>\n",
       "      <td>2.735959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.235351e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.186794</td>\n",
       "      <td>-0.818505</td>\n",
       "      <td>-0.105594</td>\n",
       "      <td>0.704874</td>\n",
       "      <td>3.341394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.011638e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.066563</td>\n",
       "      <td>-0.764664</td>\n",
       "      <td>-0.068333</td>\n",
       "      <td>0.747127</td>\n",
       "      <td>3.472888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-9.653784e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.237960</td>\n",
       "      <td>-0.781040</td>\n",
       "      <td>0.190342</td>\n",
       "      <td>0.755363</td>\n",
       "      <td>2.416291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.547032e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.792324</td>\n",
       "      <td>-0.698020</td>\n",
       "      <td>-0.185280</td>\n",
       "      <td>0.474142</td>\n",
       "      <td>7.560878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.494472e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.557295</td>\n",
       "      <td>-0.826489</td>\n",
       "      <td>-0.044746</td>\n",
       "      <td>0.563395</td>\n",
       "      <td>3.147231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-4.923107e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.410399</td>\n",
       "      <td>-0.196292</td>\n",
       "      <td>-0.113139</td>\n",
       "      <td>-0.058508</td>\n",
       "      <td>7.234791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.202552e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.162652</td>\n",
       "      <td>-0.283215</td>\n",
       "      <td>-0.085544</td>\n",
       "      <td>0.124752</td>\n",
       "      <td>13.641746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.448447e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.224237</td>\n",
       "      <td>-0.598922</td>\n",
       "      <td>-0.240229</td>\n",
       "      <td>0.250816</td>\n",
       "      <td>8.832594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.765437e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.410812</td>\n",
       "      <td>-0.588096</td>\n",
       "      <td>-0.058888</td>\n",
       "      <td>0.579604</td>\n",
       "      <td>3.543727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-4.783476e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.098408</td>\n",
       "      <td>-0.509865</td>\n",
       "      <td>-0.286452</td>\n",
       "      <td>0.264994</td>\n",
       "      <td>6.979465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.208312e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.066437</td>\n",
       "      <td>-0.507329</td>\n",
       "      <td>-0.145586</td>\n",
       "      <td>0.264991</td>\n",
       "      <td>7.845298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.038938e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.533388</td>\n",
       "      <td>-0.691497</td>\n",
       "      <td>-0.307350</td>\n",
       "      <td>0.442094</td>\n",
       "      <td>4.060071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-4.656263e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.137652</td>\n",
       "      <td>-0.446616</td>\n",
       "      <td>-0.156947</td>\n",
       "      <td>0.225880</td>\n",
       "      <td>9.483251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.843809e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.455679</td>\n",
       "      <td>-0.516600</td>\n",
       "      <td>-0.132864</td>\n",
       "      <td>0.286623</td>\n",
       "      <td>10.773802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.972728e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.545539</td>\n",
       "      <td>-0.647078</td>\n",
       "      <td>0.060941</td>\n",
       "      <td>0.599764</td>\n",
       "      <td>3.264377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>732.0</td>\n",
       "      <td>4.052428e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.757049</td>\n",
       "      <td>-0.667331</td>\n",
       "      <td>0.027341</td>\n",
       "      <td>0.584219</td>\n",
       "      <td>4.841579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-5.975791e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.424839</td>\n",
       "      <td>-0.604164</td>\n",
       "      <td>-0.238226</td>\n",
       "      <td>0.456322</td>\n",
       "      <td>10.384732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.629955e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.066389</td>\n",
       "      <td>-0.740187</td>\n",
       "      <td>-0.083187</td>\n",
       "      <td>0.656964</td>\n",
       "      <td>3.371040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.579641e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.551490</td>\n",
       "      <td>-0.570706</td>\n",
       "      <td>-0.166853</td>\n",
       "      <td>0.294692</td>\n",
       "      <td>7.448650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-5.475280e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.497465</td>\n",
       "      <td>-0.793353</td>\n",
       "      <td>-0.098350</td>\n",
       "      <td>0.532891</td>\n",
       "      <td>6.143013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.145107e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.157033</td>\n",
       "      <td>-0.140227</td>\n",
       "      <td>-0.121555</td>\n",
       "      <td>-0.052465</td>\n",
       "      <td>18.295463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-3.064489e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.872614</td>\n",
       "      <td>-0.680935</td>\n",
       "      <td>-0.066748</td>\n",
       "      <td>0.630338</td>\n",
       "      <td>4.021518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.982849e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.777655</td>\n",
       "      <td>-0.684209</td>\n",
       "      <td>-0.154057</td>\n",
       "      <td>0.437419</td>\n",
       "      <td>5.738040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.321569e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.193634</td>\n",
       "      <td>-0.603619</td>\n",
       "      <td>-0.258687</td>\n",
       "      <td>0.297289</td>\n",
       "      <td>8.328304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.176958e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.179019</td>\n",
       "      <td>-0.586888</td>\n",
       "      <td>-0.279857</td>\n",
       "      <td>0.315015</td>\n",
       "      <td>9.468377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.417121e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.999803</td>\n",
       "      <td>-0.629579</td>\n",
       "      <td>-0.115745</td>\n",
       "      <td>0.512274</td>\n",
       "      <td>6.278632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-8.220504e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.059287</td>\n",
       "      <td>-0.603695</td>\n",
       "      <td>-0.140552</td>\n",
       "      <td>0.454918</td>\n",
       "      <td>7.666715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.396383e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.791074</td>\n",
       "      <td>-0.717212</td>\n",
       "      <td>-0.106650</td>\n",
       "      <td>0.564592</td>\n",
       "      <td>5.408623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>732.0</td>\n",
       "      <td>5.740702e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.135567</td>\n",
       "      <td>-0.117813</td>\n",
       "      <td>-0.104498</td>\n",
       "      <td>-0.091182</td>\n",
       "      <td>26.060148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.883622e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.896846</td>\n",
       "      <td>-0.326160</td>\n",
       "      <td>-0.147603</td>\n",
       "      <td>0.118484</td>\n",
       "      <td>14.588680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.523786e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.106356</td>\n",
       "      <td>-0.788034</td>\n",
       "      <td>-0.058329</td>\n",
       "      <td>0.686653</td>\n",
       "      <td>2.703228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.273268e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.624878</td>\n",
       "      <td>-0.919622</td>\n",
       "      <td>-0.003939</td>\n",
       "      <td>0.714469</td>\n",
       "      <td>2.912436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-4.356004e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.832710</td>\n",
       "      <td>-0.818288</td>\n",
       "      <td>-0.195494</td>\n",
       "      <td>0.721314</td>\n",
       "      <td>2.608406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-4.169403e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.065731</td>\n",
       "      <td>-0.733325</td>\n",
       "      <td>-0.083605</td>\n",
       "      <td>0.494990</td>\n",
       "      <td>3.919758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-8.523843e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.116422</td>\n",
       "      <td>-1.116422</td>\n",
       "      <td>-0.087548</td>\n",
       "      <td>0.699474</td>\n",
       "      <td>2.417869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.139565e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.955571</td>\n",
       "      <td>-0.955571</td>\n",
       "      <td>-0.089399</td>\n",
       "      <td>0.823201</td>\n",
       "      <td>2.257063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>732.0</td>\n",
       "      <td>4.712381e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.534798</td>\n",
       "      <td>-0.720042</td>\n",
       "      <td>-0.307936</td>\n",
       "      <td>0.649725</td>\n",
       "      <td>3.695483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-3.018229e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.787345</td>\n",
       "      <td>-0.533760</td>\n",
       "      <td>-0.307827</td>\n",
       "      <td>0.148465</td>\n",
       "      <td>6.862925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.011638e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.924464</td>\n",
       "      <td>-0.880745</td>\n",
       "      <td>-0.274999</td>\n",
       "      <td>0.398950</td>\n",
       "      <td>3.087230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.501531e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.118656</td>\n",
       "      <td>-0.491374</td>\n",
       "      <td>-0.181162</td>\n",
       "      <td>0.233943</td>\n",
       "      <td>9.907672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.517719e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.494034</td>\n",
       "      <td>-0.614811</td>\n",
       "      <td>-0.237595</td>\n",
       "      <td>0.335852</td>\n",
       "      <td>8.830148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.714890e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.879252</td>\n",
       "      <td>-0.755221</td>\n",
       "      <td>-0.393478</td>\n",
       "      <td>0.359591</td>\n",
       "      <td>3.344192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.314598e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.358341</td>\n",
       "      <td>-0.670253</td>\n",
       "      <td>-0.048151</td>\n",
       "      <td>0.624369</td>\n",
       "      <td>5.875308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>732.0</td>\n",
       "      <td>7.826162e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.094712</td>\n",
       "      <td>-0.568016</td>\n",
       "      <td>-0.256006</td>\n",
       "      <td>0.319337</td>\n",
       "      <td>8.326555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.298294e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.095838</td>\n",
       "      <td>-1.095838</td>\n",
       "      <td>-0.092652</td>\n",
       "      <td>0.777247</td>\n",
       "      <td>2.449112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.078897e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.147523</td>\n",
       "      <td>-0.656415</td>\n",
       "      <td>-0.329805</td>\n",
       "      <td>0.368593</td>\n",
       "      <td>4.252493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.048038e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.993273</td>\n",
       "      <td>-0.640915</td>\n",
       "      <td>-0.336524</td>\n",
       "      <td>0.439800</td>\n",
       "      <td>3.954924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-8.815808e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.916054</td>\n",
       "      <td>-0.677538</td>\n",
       "      <td>-0.393733</td>\n",
       "      <td>0.339789</td>\n",
       "      <td>3.533812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.126907e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.134947</td>\n",
       "      <td>-1.134947</td>\n",
       "      <td>-0.210565</td>\n",
       "      <td>0.723722</td>\n",
       "      <td>2.460720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>732.0</td>\n",
       "      <td>6.552136e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.888294</td>\n",
       "      <td>-0.611983</td>\n",
       "      <td>-0.464544</td>\n",
       "      <td>0.499819</td>\n",
       "      <td>3.132154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>732.0</td>\n",
       "      <td>4.590856e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.193387</td>\n",
       "      <td>-0.942949</td>\n",
       "      <td>-0.019612</td>\n",
       "      <td>0.633836</td>\n",
       "      <td>2.635098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.256847e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.223295</td>\n",
       "      <td>-0.709895</td>\n",
       "      <td>-0.226157</td>\n",
       "      <td>0.409456</td>\n",
       "      <td>3.086447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.835205e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.770401</td>\n",
       "      <td>-0.770401</td>\n",
       "      <td>-0.770401</td>\n",
       "      <td>0.751589</td>\n",
       "      <td>2.334492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.214379e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.765128</td>\n",
       "      <td>-0.765128</td>\n",
       "      <td>-0.765128</td>\n",
       "      <td>0.776577</td>\n",
       "      <td>2.226177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.309931e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.378420</td>\n",
       "      <td>-0.580687</td>\n",
       "      <td>-0.280955</td>\n",
       "      <td>0.152892</td>\n",
       "      <td>7.965921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-1.164824e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.924582</td>\n",
       "      <td>-0.924582</td>\n",
       "      <td>-0.228407</td>\n",
       "      <td>0.968038</td>\n",
       "      <td>1.943023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-5.429779e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.170927</td>\n",
       "      <td>-0.170927</td>\n",
       "      <td>-0.170927</td>\n",
       "      <td>-0.170927</td>\n",
       "      <td>6.321682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-7.979045e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.006551</td>\n",
       "      <td>-0.585329</td>\n",
       "      <td>-0.585329</td>\n",
       "      <td>0.723691</td>\n",
       "      <td>2.705922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.162812e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.571308</td>\n",
       "      <td>-0.422681</td>\n",
       "      <td>-0.422681</td>\n",
       "      <td>0.213950</td>\n",
       "      <td>4.431625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.075946e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.409557</td>\n",
       "      <td>-0.398039</td>\n",
       "      <td>0.354841</td>\n",
       "      <td>0.354841</td>\n",
       "      <td>3.366359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.510631e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.682038</td>\n",
       "      <td>-0.585549</td>\n",
       "      <td>-0.256999</td>\n",
       "      <td>0.585139</td>\n",
       "      <td>3.822698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.500075e-14</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-4.782607</td>\n",
       "      <td>-0.760595</td>\n",
       "      <td>-0.228995</td>\n",
       "      <td>0.706142</td>\n",
       "      <td>3.822038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.371509e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.337159</td>\n",
       "      <td>-0.705722</td>\n",
       "      <td>-0.460045</td>\n",
       "      <td>0.675259</td>\n",
       "      <td>2.388590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.426717e-18</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.762837</td>\n",
       "      <td>-0.451032</td>\n",
       "      <td>-0.133604</td>\n",
       "      <td>0.247837</td>\n",
       "      <td>12.907276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>732.0</td>\n",
       "      <td>4.977803e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.925451</td>\n",
       "      <td>-0.444112</td>\n",
       "      <td>-0.274832</td>\n",
       "      <td>-0.004350</td>\n",
       "      <td>4.574652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>732.0</td>\n",
       "      <td>4.368091e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.340175</td>\n",
       "      <td>-0.865005</td>\n",
       "      <td>-0.124061</td>\n",
       "      <td>0.543895</td>\n",
       "      <td>3.237366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.635525e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.715001</td>\n",
       "      <td>-0.184915</td>\n",
       "      <td>0.231004</td>\n",
       "      <td>0.279377</td>\n",
       "      <td>6.546437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>732.0</td>\n",
       "      <td>2.390316e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.187463</td>\n",
       "      <td>-0.939153</td>\n",
       "      <td>-0.216587</td>\n",
       "      <td>0.551201</td>\n",
       "      <td>4.311492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.976782e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.017019</td>\n",
       "      <td>-0.513640</td>\n",
       "      <td>-0.443895</td>\n",
       "      <td>0.467625</td>\n",
       "      <td>5.155030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>732.0</td>\n",
       "      <td>7.894414e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.355163</td>\n",
       "      <td>-0.666843</td>\n",
       "      <td>-0.331115</td>\n",
       "      <td>0.339813</td>\n",
       "      <td>5.673505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.316703e-15</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-13.369234</td>\n",
       "      <td>0.005480</td>\n",
       "      <td>0.097649</td>\n",
       "      <td>0.230072</td>\n",
       "      <td>0.511306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.865043e-16</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-3.500528</td>\n",
       "      <td>-0.355553</td>\n",
       "      <td>0.154718</td>\n",
       "      <td>0.663081</td>\n",
       "      <td>2.081914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.503573e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.236055</td>\n",
       "      <td>-0.191684</td>\n",
       "      <td>-0.173872</td>\n",
       "      <td>-0.150229</td>\n",
       "      <td>6.793760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-2.127973e-14</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-5.594854</td>\n",
       "      <td>-0.627799</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>0.634547</td>\n",
       "      <td>2.582950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>732.0</td>\n",
       "      <td>1.816246e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-0.506310</td>\n",
       "      <td>-0.240788</td>\n",
       "      <td>-0.119090</td>\n",
       "      <td>0.046861</td>\n",
       "      <td>25.481641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-5.930290e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-2.230140</td>\n",
       "      <td>-0.656224</td>\n",
       "      <td>-0.083891</td>\n",
       "      <td>0.555776</td>\n",
       "      <td>6.817772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>732.0</td>\n",
       "      <td>-3.139565e-17</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.563968</td>\n",
       "      <td>-0.723800</td>\n",
       "      <td>-0.159688</td>\n",
       "      <td>0.452435</td>\n",
       "      <td>4.605263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>732.0</td>\n",
       "      <td>3.185066e-18</td>\n",
       "      <td>1.000684</td>\n",
       "      <td>-1.177220</td>\n",
       "      <td>-0.621775</td>\n",
       "      <td>-0.294523</td>\n",
       "      <td>0.223711</td>\n",
       "      <td>7.660460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count          mean       std        min       25%       50%       75%  \\\n",
       "0    732.0  7.276890e-15  1.000684  -2.185075 -0.659571 -0.144246  0.533917   \n",
       "1    732.0 -6.716698e-16  1.000684  -3.090055 -0.560818  0.045983  0.546470   \n",
       "2    732.0 -5.575382e-16  1.000684  -4.851195 -0.750024 -0.061808  0.562047   \n",
       "3    732.0  1.158757e-16  1.000684  -3.415972 -0.726693 -0.267616  0.578864   \n",
       "4    732.0 -3.036240e-17  1.000684  -0.063052 -0.057284 -0.052366 -0.049429   \n",
       "5    732.0  2.201866e-15  1.000684  -3.425556 -0.467071  0.062325  0.522790   \n",
       "6    732.0  6.291643e-16  1.000684 -13.222649 -0.083778  0.057500  0.220513   \n",
       "7    732.0  6.819075e-16  1.000684  -3.435507 -0.743003  0.003613  0.735872   \n",
       "8    732.0  2.896893e-17  1.000684  -3.135579 -0.560479  0.004618  0.634048   \n",
       "9    732.0  1.171649e-17  1.000684  -3.497506 -0.617140  0.036029  0.624951   \n",
       "10   732.0 -8.576170e-16  1.000684  -8.180336 -0.708438  0.135470  0.716249   \n",
       "11   732.0 -1.629139e-14  1.000684  -2.380168 -0.433949 -0.085992  0.526910   \n",
       "12   732.0  3.190185e-16  1.000684  -2.345378 -0.690853  0.034786  0.672560   \n",
       "13   732.0 -2.762931e-15  1.000684  -2.551107 -0.715358 -0.095542  0.776909   \n",
       "14   732.0  1.034672e-16  1.000684  -0.570599 -0.118443 -0.038858  0.035118   \n",
       "15   732.0  1.578974e-14  1.000684  -7.874998 -0.546228 -0.110609  0.769809   \n",
       "16   732.0 -1.501304e-14  1.000684 -21.977564 -0.308487  0.001682  0.409513   \n",
       "17   732.0 -2.991232e-15  1.000684 -12.389230 -0.493644  0.046314  0.582138   \n",
       "18   732.0 -1.656993e-17  1.000684  -2.047355 -0.310341  0.014714  0.216412   \n",
       "19   732.0 -4.299650e-16  1.000684  -3.876199 -0.426771 -0.047622  0.334715   \n",
       "20   732.0 -2.938603e-17  1.000684  -5.614495 -0.449679  0.130314  0.607380   \n",
       "21   732.0  4.606971e-16  1.000684  -4.443771  0.250044  0.412176  0.511085   \n",
       "22   732.0  2.496485e-16  1.000684  -2.639376 -0.587840 -0.057983  0.600969   \n",
       "23   732.0 -3.608225e-16  1.000684  -3.637893 -0.733707  0.050431  0.718516   \n",
       "24   732.0  6.699256e-16  1.000684  -2.938705 -0.516527 -0.405051 -0.227947   \n",
       "25   732.0 -2.254420e-15  1.000684  -0.911798 -0.361151 -0.125767  0.074081   \n",
       "26   732.0 -2.054747e-16  1.000684  -0.799442 -0.267178 -0.179685  0.000155   \n",
       "27   732.0 -1.081284e-14  1.000684  -4.275707 -0.655529  0.027309  0.430981   \n",
       "28   732.0 -4.406008e-17  1.000684  -3.775545 -0.538025 -0.033807  0.297125   \n",
       "29   732.0 -1.354411e-16  1.000684  -2.457605  0.021847  0.491472  0.564731   \n",
       "30   732.0  1.921657e-16  1.000684  -2.105704 -0.422285 -0.175847  0.254348   \n",
       "31   732.0 -3.463835e-15  1.000684  -2.037386 -0.737076 -0.248721  0.850270   \n",
       "32   732.0  6.074945e-16  1.000684  -3.112456 -0.686249 -0.043301  0.696652   \n",
       "33   732.0 -2.534611e-15  1.000684  -2.302729 -0.773028 -0.072406  0.645383   \n",
       "34   732.0  2.457051e-16  1.000684  -3.763727 -0.926510  0.373915  0.837536   \n",
       "35   732.0 -5.362286e-15  1.000684  -2.455293 -0.723254  0.017568  0.642579   \n",
       "36   732.0  6.394399e-16  1.000684 -18.744682 -0.391638  0.051876  0.395072   \n",
       "37   732.0  6.977570e-16  1.000684  -1.918338 -0.831044 -0.094489  0.650834   \n",
       "38   732.0  5.837619e-15  1.000684  -9.155001 -0.463451  0.312580  0.657483   \n",
       "39   732.0  9.530097e-15  1.000684  -3.166583 -0.618171  0.061406  0.595359   \n",
       "40   732.0  1.405372e-15  1.000684  -4.640294 -0.780281 -0.140817  0.811653   \n",
       "41   732.0  1.698702e-17  1.000684  -2.552386 -0.619970 -0.332963  0.427176   \n",
       "42   732.0  2.775558e-17  1.000684  -3.009798 -0.506307  0.038131  0.570059   \n",
       "43   732.0  1.286686e-15  1.000684  -3.193962 -0.497806 -0.006672  0.499370   \n",
       "44   732.0 -2.442642e-16  1.000684  -1.556241 -0.612820 -0.217335  0.367036   \n",
       "45   732.0 -8.148461e-17  1.000684  -2.397078 -0.687895 -0.182378  0.477407   \n",
       "46   732.0  1.630450e-18  1.000684  -0.084019 -0.083166 -0.082982 -0.082787   \n",
       "47   732.0 -1.777570e-16  1.000684 -10.212381 -0.362795  0.059622  0.492789   \n",
       "48   732.0 -1.771503e-16  1.000684  -2.395015 -0.543190 -0.096982  0.247487   \n",
       "49   732.0  0.000000e+00  0.000000   0.000000  0.000000  0.000000  0.000000   \n",
       "50   732.0  1.846580e-17  1.000684  -3.212554 -0.492907 -0.009342  0.415806   \n",
       "51   732.0  5.353944e-17  1.000684  -4.505535 -0.519077  0.014948  0.611411   \n",
       "52   732.0 -1.941374e-17  1.000684  -2.568568 -0.692804 -0.106580  0.489659   \n",
       "53   732.0 -3.185066e-18  1.000684  -2.036358 -0.804255  0.042950  0.511707   \n",
       "54   732.0  2.305381e-17  1.000684  -1.872838 -0.687628 -0.183502  0.518158   \n",
       "55   732.0  1.236109e-17  1.000684  -2.088963 -1.074517  0.217898  0.553180   \n",
       "56   732.0 -1.918623e-17  1.000684  -3.711700 -0.334124  0.044040  0.582794   \n",
       "57   732.0 -7.280151e-18  1.000684  -6.041211 -0.663720  0.058424  0.598654   \n",
       "58   732.0  5.770515e-16  1.000684  -2.982964 -0.625841  0.004367  0.594927   \n",
       "59   732.0 -2.019029e-15  1.000684  -3.291545 -0.700361 -0.104019  0.677031   \n",
       "60   732.0  3.254796e-15  1.000684  -3.516490 -0.707048  0.062082  0.692537   \n",
       "61   732.0 -4.406615e-15  1.000684 -14.329567 -0.590495  0.341553  0.500346   \n",
       "62   732.0 -3.627942e-16  1.000684  -3.341563 -0.608094 -0.015574  0.628301   \n",
       "63   732.0 -1.920140e-16  1.000684  -0.972258 -0.294237 -0.024131  0.116740   \n",
       "64   732.0  1.465434e-15  1.000684  -3.923223 -0.630289 -0.003250  0.537239   \n",
       "65   732.0  7.735160e-18  1.000684  -4.246654 -0.407436  0.022406  0.592886   \n",
       "66   732.0  1.137524e-17  1.000684  -3.499116 -0.575995 -0.067626  0.440743   \n",
       "67   732.0  2.199212e-18  1.000684  -4.343274 -0.404920  0.011329  0.459597   \n",
       "68   732.0 -5.088522e-17  1.000684  -6.006282 -0.469004  0.223155  0.223155   \n",
       "69   732.0 -3.354936e-16  1.000684  -4.484188 -0.723173  0.217080  0.217080   \n",
       "70   732.0 -1.719556e-17  1.000684  -6.548086 -0.473827 -0.050757  0.329576   \n",
       "71   732.0 -1.213358e-17  1.000684  -5.858533 -0.553864 -0.048657  0.456549   \n",
       "72   732.0 -3.511156e-17  1.000684  -4.374931 -0.567847 -0.050930  0.483243   \n",
       "73   732.0 -9.737202e-17  1.000684  -2.105933 -0.652182 -0.107957  0.667377   \n",
       "74   732.0 -1.334694e-17  1.000684  -5.724386 -0.540771  0.093822  0.574776   \n",
       "75   732.0 -4.095085e-18  1.000684  -2.608984 -0.598263 -0.071736  0.521888   \n",
       "76   732.0 -9.529414e-16  1.000684  -7.242029 -0.500005  0.185692  0.554303   \n",
       "77   732.0 -2.478285e-16  1.000684  -0.149048 -0.149048 -0.149048 -0.149048   \n",
       "78   732.0  1.031639e-16  1.000684  -4.043446 -0.437831  0.078679  0.601231   \n",
       "79   732.0  1.354336e-14  1.000684  -7.983195  0.260041  0.391407  0.435196   \n",
       "80   732.0 -5.788478e-16  1.000684  -4.268388 -0.375433  0.140430  0.657114   \n",
       "81   732.0  4.349587e-15  1.000684  -9.224776 -0.570810  0.065893  0.585650   \n",
       "82   732.0  1.016157e-14  1.000684 -14.960777 -0.709055 -0.097442  0.883445   \n",
       "83   732.0  6.285197e-15  1.000684  -4.488376 -0.624212  0.057147  0.565816   \n",
       "84   732.0  8.796849e-15  1.000684  -3.581480 -0.616514 -0.105313  0.610368   \n",
       "85   732.0 -1.242176e-16  1.000684  -2.573434 -0.778604 -0.169188  0.587265   \n",
       "86   732.0  1.269419e-15  1.000684  -1.739294 -0.719423 -0.078420  0.488857   \n",
       "87   732.0 -2.536223e-15  1.000684  -2.473528 -0.671204  0.022616  0.645310   \n",
       "88   732.0 -4.580428e-17  1.000684  -3.012376 -0.209860  0.292981  0.544357   \n",
       "89   732.0  1.999969e-14  1.000684  -1.845192 -0.587676 -0.067325  0.409664   \n",
       "90   732.0  1.647134e-16  1.000684  -2.232987 -0.716892 -0.063832  0.779630   \n",
       "91   732.0  3.715653e-14  1.000684  -3.026880 -0.602058 -0.003985  0.805272   \n",
       "92   732.0  1.007088e-15  1.000684  -2.013700 -0.751895 -0.153074  0.523045   \n",
       "93   732.0  2.066501e-17  1.000684  -1.074865 -0.491660 -0.161178  0.213045   \n",
       "94   732.0  3.178241e-16  1.000684  -1.816204 -0.935039 -0.042791  0.770485   \n",
       "95   732.0 -4.010150e-16  1.000684  -2.045929 -0.784083  0.038519  0.746408   \n",
       "96   732.0  6.830189e-16  1.000684  -3.212951 -0.617010 -0.213757  0.508730   \n",
       "97   732.0  7.477322e-17  1.000684  -1.620439 -0.560350 -0.318274  0.314667   \n",
       "98   732.0  1.115200e-16  1.000684  -1.150203 -0.370684 -0.104578  0.165639   \n",
       "99   732.0 -1.680502e-16  1.000684  -3.561399 -0.767247 -0.156026  0.629829   \n",
       "100  732.0 -2.910544e-16  1.000684  -1.401795 -0.580015 -0.193899  0.192645   \n",
       "101  732.0 -2.944859e-16  1.000684  -1.811842 -0.661575 -0.126165  0.375853   \n",
       "102  732.0 -2.875660e-16  1.000684  -1.506428 -0.601184 -0.189710  0.320776   \n",
       "103  732.0  9.024354e-17  1.000684  -1.610976 -0.710332 -0.290053  0.497854   \n",
       "104  732.0  2.051334e-17  1.000684  -0.363210 -0.197859 -0.084565  0.022617   \n",
       "105  732.0  5.293276e-17  1.000684  -0.799593 -0.542823 -0.064700  0.254048   \n",
       "106  732.0  3.715910e-17  1.000684  -0.479430 -0.248446 -0.141689  0.038827   \n",
       "107  732.0  3.814496e-17  1.000684  -0.800093 -0.478590 -0.307135  0.011251   \n",
       "108  732.0  4.095085e-17  1.000684  -0.834243 -0.452966 -0.228359 -0.022816   \n",
       "109  732.0 -4.474259e-17  1.000684  -0.988733 -0.636606 -0.301653  0.189673   \n",
       "110  732.0  4.550094e-17  1.000684  -0.772857 -0.686351 -0.466765  0.368430   \n",
       "111  732.0  2.191629e-16  1.000684  -0.951776 -0.442947 -0.160264  0.178955   \n",
       "112  732.0 -2.070293e-17  1.000684  -1.202954 -0.518011 -0.107046  0.303920   \n",
       "113  732.0  3.006854e-16  1.000684  -1.576970 -0.683686 -0.139731  0.499041   \n",
       "114  732.0  1.607700e-17  1.000684  -1.720467 -0.814756  0.077998  0.710312   \n",
       "115  732.0  2.068776e-16  1.000684  -2.328501 -0.660873 -0.030029  0.668798   \n",
       "116  732.0 -2.032375e-16  1.000684  -1.848083 -0.642807 -0.142179  0.372378   \n",
       "117  732.0  8.296339e-17  1.000684  -2.795032 -0.533253 -0.015939  0.771789   \n",
       "118  732.0  3.166866e-16  1.000684  -3.429627 -0.637522  0.013648  0.604842   \n",
       "119  732.0 -1.570086e-15  1.000684  -2.883343 -0.776394 -0.011401  0.628090   \n",
       "120  732.0 -9.979874e-17  1.000684  -2.273299 -0.655759  0.002312  0.630112   \n",
       "121  732.0 -2.727023e-16  1.000684  -1.251713 -0.592459 -0.375123  0.081283   \n",
       "122  732.0  3.574099e-16  1.000684  -2.571139 -0.734427 -0.119066  0.668096   \n",
       "123  732.0  1.235351e-16  1.000684  -2.186794 -0.818505 -0.105594  0.704874   \n",
       "124  732.0  1.011638e-16  1.000684  -2.066563 -0.764664 -0.068333  0.747127   \n",
       "125  732.0 -9.653784e-17  1.000684  -3.237960 -0.781040  0.190342  0.755363   \n",
       "126  732.0  1.547032e-16  1.000684  -1.792324 -0.698020 -0.185280  0.474142   \n",
       "127  732.0  3.494472e-16  1.000684  -1.557295 -0.826489 -0.044746  0.563395   \n",
       "128  732.0 -4.923107e-17  1.000684  -0.410399 -0.196292 -0.113139 -0.058508   \n",
       "129  732.0 -1.202552e-16  1.000684  -1.162652 -0.283215 -0.085544  0.124752   \n",
       "130  732.0 -1.448447e-16  1.000684  -1.224237 -0.598922 -0.240229  0.250816   \n",
       "131  732.0 -1.765437e-16  1.000684  -2.410812 -0.588096 -0.058888  0.579604   \n",
       "132  732.0 -4.783476e-16  1.000684  -1.098408 -0.509865 -0.286452  0.264994   \n",
       "133  732.0  2.208312e-16  1.000684  -2.066437 -0.507329 -0.145586  0.264991   \n",
       "134  732.0 -1.038938e-16  1.000684  -1.533388 -0.691497 -0.307350  0.442094   \n",
       "135  732.0 -4.656263e-17  1.000684  -1.137652 -0.446616 -0.156947  0.225880   \n",
       "136  732.0  2.843809e-16  1.000684  -1.455679 -0.516600 -0.132864  0.286623   \n",
       "137  732.0  2.972728e-17  1.000684  -2.545539 -0.647078  0.060941  0.599764   \n",
       "138  732.0  4.052428e-17  1.000684  -2.757049 -0.667331  0.027341  0.584219   \n",
       "139  732.0 -5.975791e-17  1.000684  -1.424839 -0.604164 -0.238226  0.456322   \n",
       "140  732.0 -2.629955e-16  1.000684  -2.066389 -0.740187 -0.083187  0.656964   \n",
       "141  732.0  1.579641e-16  1.000684  -1.551490 -0.570706 -0.166853  0.294692   \n",
       "142  732.0 -5.475280e-17  1.000684  -1.497465 -0.793353 -0.098350  0.532891   \n",
       "143  732.0  1.145107e-17  1.000684  -0.157033 -0.140227 -0.121555 -0.052465   \n",
       "144  732.0 -3.064489e-16  1.000684  -1.872614 -0.680935 -0.066748  0.630338   \n",
       "145  732.0  3.982849e-16  1.000684  -1.777655 -0.684209 -0.154057  0.437419   \n",
       "146  732.0  3.321569e-16  1.000684  -1.193634 -0.603619 -0.258687  0.297289   \n",
       "147  732.0 -1.176958e-16  1.000684  -1.179019 -0.586888 -0.279857  0.315015   \n",
       "148  732.0  3.417121e-16  1.000684  -1.999803 -0.629579 -0.115745  0.512274   \n",
       "149  732.0 -8.220504e-17  1.000684  -2.059287 -0.603695 -0.140552  0.454918   \n",
       "150  732.0 -2.396383e-16  1.000684  -1.791074 -0.717212 -0.106650  0.564592   \n",
       "151  732.0  5.740702e-17  1.000684  -0.135567 -0.117813 -0.104498 -0.091182   \n",
       "152  732.0 -2.883622e-17  1.000684  -0.896846 -0.326160 -0.147603  0.118484   \n",
       "153  732.0  2.523786e-16  1.000684  -2.106356 -0.788034 -0.058329  0.686653   \n",
       "154  732.0  1.273268e-16  1.000684  -1.624878 -0.919622 -0.003939  0.714469   \n",
       "155  732.0 -4.356004e-17  1.000684  -1.832710 -0.818288 -0.195494  0.721314   \n",
       "156  732.0 -4.169403e-16  1.000684  -2.065731 -0.733325 -0.083605  0.494990   \n",
       "157  732.0 -8.523843e-17  1.000684  -1.116422 -1.116422 -0.087548  0.699474   \n",
       "158  732.0  3.139565e-17  1.000684  -0.955571 -0.955571 -0.089399  0.823201   \n",
       "159  732.0  4.712381e-16  1.000684  -1.534798 -0.720042 -0.307936  0.649725   \n",
       "160  732.0 -3.018229e-17  1.000684  -0.787345 -0.533760 -0.307827  0.148465   \n",
       "161  732.0 -1.011638e-16  1.000684  -0.924464 -0.880745 -0.274999  0.398950   \n",
       "162  732.0  1.501531e-16  1.000684  -1.118656 -0.491374 -0.181162  0.233943   \n",
       "163  732.0 -2.517719e-16  1.000684  -1.494034 -0.614811 -0.237595  0.335852   \n",
       "164  732.0 -2.714890e-17  1.000684  -0.879252 -0.755221 -0.393478  0.359591   \n",
       "165  732.0  1.314598e-16  1.000684  -2.358341 -0.670253 -0.048151  0.624369   \n",
       "166  732.0  7.826162e-17  1.000684  -1.094712 -0.568016 -0.256006  0.319337   \n",
       "167  732.0 -1.298294e-16  1.000684  -1.095838 -1.095838 -0.092652  0.777247   \n",
       "168  732.0  3.078897e-17  1.000684  -1.147523 -0.656415 -0.329805  0.368593   \n",
       "169  732.0  1.048038e-16  1.000684  -0.993273 -0.640915 -0.336524  0.439800   \n",
       "170  732.0 -8.815808e-17  1.000684  -0.916054 -0.677538 -0.393733  0.339789   \n",
       "171  732.0  1.126907e-16  1.000684  -1.134947 -1.134947 -0.210565  0.723722   \n",
       "172  732.0  6.552136e-17  1.000684  -0.888294 -0.611983 -0.464544  0.499819   \n",
       "173  732.0  4.590856e-17  1.000684  -1.193387 -0.942949 -0.019612  0.633836   \n",
       "174  732.0  2.256847e-16  1.000684  -1.223295 -0.709895 -0.226157  0.409456   \n",
       "175  732.0 -1.835205e-17  1.000684  -0.770401 -0.770401 -0.770401  0.751589   \n",
       "176  732.0  2.214379e-16  1.000684  -0.765128 -0.765128 -0.765128  0.776577   \n",
       "177  732.0  2.309931e-16  1.000684  -1.378420 -0.580687 -0.280955  0.152892   \n",
       "178  732.0 -1.164824e-16  1.000684  -0.924582 -0.924582 -0.228407  0.968038   \n",
       "179  732.0 -5.429779e-17  1.000684  -0.170927 -0.170927 -0.170927 -0.170927   \n",
       "180  732.0 -7.979045e-15  1.000684  -2.006551 -0.585329 -0.585329  0.723691   \n",
       "181  732.0  2.162812e-16  1.000684  -2.571308 -0.422681 -0.422681  0.213950   \n",
       "182  732.0  1.075946e-15  1.000684  -3.409557 -0.398039  0.354841  0.354841   \n",
       "183  732.0  1.510631e-16  1.000684  -1.682038 -0.585549 -0.256999  0.585139   \n",
       "184  732.0  1.500075e-14  1.000684  -4.782607 -0.760595 -0.228995  0.706142   \n",
       "185  732.0  2.371509e-15  1.000684  -1.337159 -0.705722 -0.460045  0.675259   \n",
       "186  732.0 -2.426717e-18  1.000684  -1.762837 -0.451032 -0.133604  0.247837   \n",
       "187  732.0  4.977803e-16  1.000684  -0.925451 -0.444112 -0.274832 -0.004350   \n",
       "188  732.0  4.368091e-17  1.000684  -1.340175 -0.865005 -0.124061  0.543895   \n",
       "189  732.0  3.635525e-15  1.000684  -2.715001 -0.184915  0.231004  0.279377   \n",
       "190  732.0  2.390316e-16  1.000684  -3.187463 -0.939153 -0.216587  0.551201   \n",
       "191  732.0  3.976782e-16  1.000684  -2.017019 -0.513640 -0.443895  0.467625   \n",
       "192  732.0  7.894414e-17  1.000684  -1.355163 -0.666843 -0.331115  0.339813   \n",
       "193  732.0  1.316703e-15  1.000684 -13.369234  0.005480  0.097649  0.230072   \n",
       "194  732.0 -2.865043e-16  1.000684  -3.500528 -0.355553  0.154718  0.663081   \n",
       "195  732.0  3.503573e-17  1.000684  -0.236055 -0.191684 -0.173872 -0.150229   \n",
       "196  732.0 -2.127973e-14  1.000684  -5.594854 -0.627799  0.003374  0.634547   \n",
       "197  732.0  1.816246e-17  1.000684  -0.506310 -0.240788 -0.119090  0.046861   \n",
       "198  732.0 -5.930290e-17  1.000684  -2.230140 -0.656224 -0.083891  0.555776   \n",
       "199  732.0 -3.139565e-17  1.000684  -1.563968 -0.723800 -0.159688  0.452435   \n",
       "200  732.0  3.185066e-18  1.000684  -1.177220 -0.621775 -0.294523  0.223711   \n",
       "\n",
       "           max  \n",
       "0     3.427766  \n",
       "1     3.668981  \n",
       "2     4.019677  \n",
       "3     5.684693  \n",
       "4    19.107991  \n",
       "5     3.391505  \n",
       "6     0.644348  \n",
       "7     2.897130  \n",
       "8     3.082219  \n",
       "9     4.212024  \n",
       "10    1.974998  \n",
       "11   19.553673  \n",
       "12    3.411210  \n",
       "13    4.256996  \n",
       "14   26.776608  \n",
       "15    3.911860  \n",
       "16    3.458809  \n",
       "17    2.520199  \n",
       "18    5.071232  \n",
       "19    4.430227  \n",
       "20    5.569701  \n",
       "21    0.868856  \n",
       "22    2.581156  \n",
       "23    2.606198  \n",
       "24    2.335483  \n",
       "25    7.669562  \n",
       "26    7.286988  \n",
       "27    3.154671  \n",
       "28    2.775740  \n",
       "29    0.763685  \n",
       "30   21.104615  \n",
       "31    3.481195  \n",
       "32    3.555083  \n",
       "33    4.593731  \n",
       "34    1.754579  \n",
       "35    2.312778  \n",
       "36    3.536631  \n",
       "37    2.781581  \n",
       "38    1.399024  \n",
       "39    2.318571  \n",
       "40    2.996589  \n",
       "41    7.823519  \n",
       "42    4.168450  \n",
       "43    7.652856  \n",
       "44   10.373750  \n",
       "45    3.362541  \n",
       "46   12.058247  \n",
       "47    1.947710  \n",
       "48    3.745319  \n",
       "49    0.000000  \n",
       "50    7.704981  \n",
       "51    2.543304  \n",
       "52    3.162718  \n",
       "53    3.208141  \n",
       "54    3.732216  \n",
       "55    2.513295  \n",
       "56    2.924304  \n",
       "57    3.589211  \n",
       "58    3.013489  \n",
       "59    2.830194  \n",
       "60    3.910299  \n",
       "61    0.762700  \n",
       "62    2.724604  \n",
       "63   14.643658  \n",
       "64    4.443237  \n",
       "65    3.398897  \n",
       "66    5.270247  \n",
       "67    4.365932  \n",
       "68    5.760433  \n",
       "69    5.858602  \n",
       "70   10.838388  \n",
       "71    5.508615  \n",
       "72    4.662676  \n",
       "73    8.718921  \n",
       "74    3.093104  \n",
       "75    3.695334  \n",
       "76    2.405288  \n",
       "77   12.171939  \n",
       "78    3.514551  \n",
       "79    0.599404  \n",
       "80    2.793325  \n",
       "81    3.028512  \n",
       "82    1.310420  \n",
       "83    3.748231  \n",
       "84    2.655173  \n",
       "85    3.767408  \n",
       "86    5.022981  \n",
       "87    3.699732  \n",
       "88    2.512052  \n",
       "89    9.125549  \n",
       "90    2.770839  \n",
       "91    2.570626  \n",
       "92    8.564228  \n",
       "93   10.103221  \n",
       "94    2.861173  \n",
       "95    3.174015  \n",
       "96    4.524474  \n",
       "97    5.143995  \n",
       "98   22.718225  \n",
       "99    7.003990  \n",
       "100   5.854124  \n",
       "101   8.094964  \n",
       "102  10.045720  \n",
       "103   4.075785  \n",
       "104  15.055099  \n",
       "105  20.034165  \n",
       "106  20.746782  \n",
       "107   5.547678  \n",
       "108   4.890459  \n",
       "109   6.282106  \n",
       "110   5.352990  \n",
       "111  10.298997  \n",
       "112   9.756126  \n",
       "113   4.770832  \n",
       "114   3.125542  \n",
       "115   2.827384  \n",
       "116   6.024311  \n",
       "117   2.582387  \n",
       "118   3.108847  \n",
       "119   3.166539  \n",
       "120   2.564842  \n",
       "121   5.308224  \n",
       "122   2.735959  \n",
       "123   3.341394  \n",
       "124   3.472888  \n",
       "125   2.416291  \n",
       "126   7.560878  \n",
       "127   3.147231  \n",
       "128   7.234791  \n",
       "129  13.641746  \n",
       "130   8.832594  \n",
       "131   3.543727  \n",
       "132   6.979465  \n",
       "133   7.845298  \n",
       "134   4.060071  \n",
       "135   9.483251  \n",
       "136  10.773802  \n",
       "137   3.264377  \n",
       "138   4.841579  \n",
       "139  10.384732  \n",
       "140   3.371040  \n",
       "141   7.448650  \n",
       "142   6.143013  \n",
       "143  18.295463  \n",
       "144   4.021518  \n",
       "145   5.738040  \n",
       "146   8.328304  \n",
       "147   9.468377  \n",
       "148   6.278632  \n",
       "149   7.666715  \n",
       "150   5.408623  \n",
       "151  26.060148  \n",
       "152  14.588680  \n",
       "153   2.703228  \n",
       "154   2.912436  \n",
       "155   2.608406  \n",
       "156   3.919758  \n",
       "157   2.417869  \n",
       "158   2.257063  \n",
       "159   3.695483  \n",
       "160   6.862925  \n",
       "161   3.087230  \n",
       "162   9.907672  \n",
       "163   8.830148  \n",
       "164   3.344192  \n",
       "165   5.875308  \n",
       "166   8.326555  \n",
       "167   2.449112  \n",
       "168   4.252493  \n",
       "169   3.954924  \n",
       "170   3.533812  \n",
       "171   2.460720  \n",
       "172   3.132154  \n",
       "173   2.635098  \n",
       "174   3.086447  \n",
       "175   2.334492  \n",
       "176   2.226177  \n",
       "177   7.965921  \n",
       "178   1.943023  \n",
       "179   6.321682  \n",
       "180   2.705922  \n",
       "181   4.431625  \n",
       "182   3.366359  \n",
       "183   3.822698  \n",
       "184   3.822038  \n",
       "185   2.388590  \n",
       "186  12.907276  \n",
       "187   4.574652  \n",
       "188   3.237366  \n",
       "189   6.546437  \n",
       "190   4.311492  \n",
       "191   5.155030  \n",
       "192   5.673505  \n",
       "193   0.511306  \n",
       "194   2.081914  \n",
       "195   6.793760  \n",
       "196   2.582950  \n",
       "197  25.481641  \n",
       "198   6.817772  \n",
       "199   4.605263  \n",
       "200   7.660460  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df = pd.DataFrame(X_test)\n",
    "X_test_df.describe().transpose()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e1699",
   "metadata": {},
   "outputs": [],
   "source": [
    "Observation : \n",
    "    Train,Test data all the features are scaled to new values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e694c1a",
   "metadata": {},
   "source": [
    "# 5. Model training, testing and tuning: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f843fd",
   "metadata": {},
   "source": [
    "5.A. Use any Supervised Learning technique to train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968ea0ea",
   "metadata": {},
   "source": [
    "LogisticRegression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59925343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training data: 0.9567000911577028\n",
      "Accuracy on Test data: 0.8579234972677595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logit = LogisticRegression()\n",
    "logit.fit(X_train, y_train)\n",
    "logit_pred = logit.predict(X_test)\n",
    "Accuracy = logit.score(X_test, y_test) \n",
    "print('Accuracy on Training data:',logit.score(X_train, y_train) )\n",
    "print('Accuracy on Test data:',logit.score(X_test, y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e5b3c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.86      0.85      0.86       363\n",
      "           1       0.86      0.86      0.86       369\n",
      "\n",
      "    accuracy                           0.86       732\n",
      "   macro avg       0.86      0.86      0.86       732\n",
      "weighted avg       0.86      0.86      0.86       732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = logit.predict(X_test)\n",
    "print(metrics.classification_report(y_test, logit_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "262c07de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Method  accuracy\n",
       "1  Logit Regression  0.857923"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "results_df = pd.DataFrame({'Method':['Logit Regression'], 'accuracy': Accuracy},index={'1'})\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4be99",
   "metadata": {},
   "source": [
    "5.B. Use cross validation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79301681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "num_folds = 50\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, y, cv=kfold)\n",
    "accuracy=np.mean(abs(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "854e1c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:  0.9335080645161291\n",
      "Standard Deviation:  0.07012100252276122\n"
     ]
    }
   ],
   "source": [
    "print('Average accuracy: ',accuracy)\n",
    "print('Standard Deviation: ',results.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9ec9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7fd0566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Method  accuracy\n",
       "1          Logit Regression  0.857923\n",
       "2  Logit Regression K Fold   0.933508"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Logit Regression K Fold '], 'accuracy': [accuracy]},index={'2'})\n",
    "tempResultsDf\n",
    "results_df = pd.concat([results_df, tempResultsDf])\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec896d4",
   "metadata": {},
   "source": [
    "Bootstrap samples - To Give Confidence to Deploy in Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8cbc4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "values =df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a5adbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9290254237288136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9265477439664218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9315508021390374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.934020618556701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9295624332977588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9272918861959958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9216101694915254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9294605809128631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916241062308478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9091869060190074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9368983957219251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9291754756871036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9252136752136753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9454926624737946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9232386961093586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9188900747065102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9141039236479321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9118895966029724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9140708915145005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9241306638566913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9365750528541226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9157894736842105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9234042553191489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9219409282700421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9233193277310925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9318658280922432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9236401673640168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9135416666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9158004158004158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9288747346072187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9197080291970803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9258872651356994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9190871369294605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9336188436830836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9262381454162276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9288702928870293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9257322175732218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9271047227926078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9109947643979057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9342379958246346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9241164241164241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9138297872340425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911578947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9256900212314225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9228362877997914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9094827586206896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9242902208201893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9224318658280922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9372384937238494\n",
      "0.9221748400852878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 50              # Number of bootstrap samples to create\n",
    "n_size = int(len(df) * 0.50)    # picking only 50 % of the given data in every bootstrap sample\n",
    "\n",
    "# run bootstrap\n",
    "stats = list()\n",
    "for i in range(n_iterations):\n",
    "\t# prepare train and test sets\n",
    "\ttrain = resample(values, n_samples=n_size)  # Sampling with replacement \n",
    "\ttest = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n",
    "    # fit model\n",
    "\tmodel = LogisticRegression()\n",
    "\tmodel.fit(train[:,:-1], train[:,-1])\n",
    "    # evaluate model\n",
    "\tpredictions = model.predict(test[:,:-1])\n",
    "\tscore = accuracy_score(test[:,-1], predictions)    # caution, overall accuracy score can mislead when classes are imbalanced\n",
    "\tprint(score)\n",
    "\tstats.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e425f6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.0 confidence interval 91.0% and 93.7%\n"
     ]
    }
   ],
   "source": [
    "# plot scores\n",
    "pyplot.hist(stats)\n",
    "pyplot.show()\n",
    "# confidence intervals\n",
    "alpha = 0.95                             # for 95% confidence \n",
    "p = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\n",
    "lower = max(0.0, np.percentile(stats, p))  \n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, np.percentile(stats, p))\n",
    "print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d29cd62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loocv = LeaveOneOut()\n",
    "# enumerate splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4fe0272e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "results=cross_val_score(model,X,y,cv=loocv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6e16165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9329929802169751\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "caa25215",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "569f1c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit Regression loocv</td>\n",
       "      <td>0.932993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Method  accuracy\n",
       "1          Logit Regression  0.857923\n",
       "2  Logit Regression K Fold   0.933508\n",
       "3   Logit Regression loocv   0.932993"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Logit Regression loocv '], 'accuracy': [accuracy]},index={'3'})\n",
    "tempResultsDf\n",
    "results_df = pd.concat([results_df, tempResultsDf])\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caecb9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "StratifiedKFold Is not tested here because Target imbalance already done "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ff7ae",
   "metadata": {},
   "source": [
    "5.C. Apply hyper-parameter tuning techniques to get the best accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a6efc",
   "metadata": {},
   "source": [
    "Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "353810a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f47f6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model_GV = GridSearchCV(logit,param_grid,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76ba2546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "70 fits failed out of a total of 140.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.78852636        nan 0.85003736        nan 0.89287879\n",
      "        nan 0.91748028        nan 0.92020756        nan 0.92066418\n",
      "        nan 0.92157534]\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=LogisticRegression(),\n",
       "             param_grid={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                         'penalty': ['l1', 'l2']})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_model_GV.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85701407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9215753424657533\n",
      "{'C': 1000.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# print the best score and estimator \n",
    "print(logit_model_GV.best_score_)\n",
    "print(logit_model_GV.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fc38177",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=logit_model_GV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "86220c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit Regression loocv</td>\n",
       "      <td>0.932993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logit Regression Grid SearchCV</td>\n",
       "      <td>0.921575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Method  accuracy\n",
       "1                 Logit Regression  0.857923\n",
       "2         Logit Regression K Fold   0.933508\n",
       "3          Logit Regression loocv   0.932993\n",
       "4  Logit Regression Grid SearchCV   0.921575"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Logit Regression Grid SearchCV '], 'accuracy': [accuracy]},index={'4'})\n",
    "tempResultsDf\n",
    "results_df = pd.concat([results_df, tempResultsDf])\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19d4c0",
   "metadata": {},
   "source": [
    "RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b74bd833",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf312e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomCV_model = RandomizedSearchCV(logit, param_distributions=param_grid, n_iter=20) #default cv = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1381326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:292: UserWarning: The total space of parameters 14 is smaller than n_iter=20. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "35 fits failed out of a total of 70.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.78988049        nan 0.84821044        nan 0.88877794\n",
      "        nan 0.91157051        nan 0.91794968        nan 0.91658086\n",
      "        nan 0.91703852]\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=LogisticRegression(), n_iter=20,\n",
       "                   param_distributions={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                                        'penalty': ['l1', 'l2']})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomCV_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58281c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9179496780769911\n",
      "{'C': 10.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# print the best score and estimator \n",
    "print(randomCV_model.best_score_)\n",
    "print(randomCV_model.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03bd36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy =randomCV_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb5a36b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit Regression loocv</td>\n",
       "      <td>0.932993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logit Regression Grid SearchCV</td>\n",
       "      <td>0.921575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logit Regression Random SearchCV</td>\n",
       "      <td>0.917950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Method  accuracy\n",
       "1                   Logit Regression  0.857923\n",
       "2           Logit Regression K Fold   0.933508\n",
       "3            Logit Regression loocv   0.932993\n",
       "4    Logit Regression Grid SearchCV   0.921575\n",
       "5  Logit Regression Random SearchCV   0.917950"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Logit Regression Random SearchCV '], 'accuracy': [accuracy]},index={'5'})\n",
    "tempResultsDf\n",
    "results_df = pd.concat([results_df, tempResultsDf])\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7097d",
   "metadata": {},
   "source": [
    "5.D. Use any other technique/method which can enhance the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc001a80",
   "metadata": {},
   "source": [
    "standardisation/normalisation  - Already done \n",
    "target balancing  - Already done using upsampling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b44864",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b23e35fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2926, 201)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_over.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5690cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=150)\n",
    "pca.fit(X_over)\n",
    "pca_model = pca.transform(X_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6256a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PCA=pca_model\n",
    "y_PCA= y_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "768c19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into training and test set in 70:30 ratio\n",
    "X_train_PCA, X_test_PCA, y_train_PCA, y_test_PCA = train_test_split(X_PCA, y_PCA, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2555af71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training data: 0.6841385597082954\n",
      "Accuracy on Test data: 0.6338797814207651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logit_PCA = LogisticRegression()\n",
    "logit_PCA.fit(X_train_PCA, y_train_PCA)\n",
    "logit_pred_PCA = logit_PCA.predict(X_test_PCA)\n",
    "\n",
    "print('Accuracy on Training data:',logit_PCA.score(X_train_PCA, y_train_PCA) )\n",
    "print('Accuracy on Test data:',logit_PCA.score(X_test_PCA, y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b4d6ba25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6338797814207651"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy =logit_PCA.score(X_test_PCA, y_test_PCA)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "49256900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit Regression loocv</td>\n",
       "      <td>0.932993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logit Regression Grid SearchCV</td>\n",
       "      <td>0.921575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logit Regression Random SearchCV</td>\n",
       "      <td>0.917950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logit Regression PCA</td>\n",
       "      <td>0.633880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Method  accuracy\n",
       "1                   Logit Regression  0.857923\n",
       "2           Logit Regression K Fold   0.933508\n",
       "3            Logit Regression loocv   0.932993\n",
       "4    Logit Regression Grid SearchCV   0.921575\n",
       "5  Logit Regression Random SearchCV   0.917950\n",
       "6              Logit Regression PCA   0.633880"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Logit Regression PCA '], 'accuracy': [accuracy]},index={'6'})\n",
    "tempResultsDf\n",
    "results_df = pd.concat([results_df, tempResultsDf])\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68ad9a",
   "metadata": {},
   "source": [
    "5.E Display and explain the classification report in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d6129dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.86      0.85      0.86       363\n",
      "           1       0.86      0.86      0.86       369\n",
      "\n",
      "    accuracy                           0.86       732\n",
      "   macro avg       0.86      0.86      0.86       732\n",
      "weighted avg       0.86      0.86      0.86       732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = logit.predict(X_test)\n",
    "print(metrics.classification_report(y_test, logit_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36607e7",
   "metadata": {},
   "source": [
    "Observartion : \n",
    "    Without K-fold/Grid Search CV accuracy score is low as 86\n",
    "    PCA reduced accuracy due to features reduction \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa823f58",
   "metadata": {},
   "source": [
    "KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d4503",
   "metadata": {},
   "source": [
    "5.F. Apply the above steps for all possible models that you have learnt so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f5222f",
   "metadata": {},
   "source": [
    "KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "54151c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors= 5 , metric = 'euclidean' ) \n",
    "fitted_model=KNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "826a3ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.83      0.91       363\n",
      "           1       0.86      1.00      0.92       369\n",
      "\n",
      "    accuracy                           0.92       732\n",
      "   macro avg       0.93      0.92      0.92       732\n",
      "weighted avg       0.93      0.92      0.92       732\n",
      "\n",
      "Accuracy Score 91.66666666666666\n"
     ]
    }
   ],
   "source": [
    "y_pred = fitted_model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "accuracy_score=metrics.accuracy_score(y_test,y_pred)*100\n",
    "print(\"Accuracy Score\",accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a80c8613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit Regression loocv</td>\n",
       "      <td>0.932993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logit Regression Grid SearchCV</td>\n",
       "      <td>0.921575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logit Regression Random SearchCV</td>\n",
       "      <td>0.917950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logit Regression PCA</td>\n",
       "      <td>0.633880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Method  accuracy\n",
       "1                   Logit Regression  0.857923\n",
       "2           Logit Regression K Fold   0.933508\n",
       "3            Logit Regression loocv   0.932993\n",
       "4    Logit Regression Grid SearchCV   0.921575\n",
       "5  Logit Regression Random SearchCV   0.917950\n",
       "6              Logit Regression PCA   0.633880\n",
       "7                               KNN   0.916667"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['KNN '], 'accuracy': [accuracy_score/100]},index={'7'})\n",
    "tempResultsDf\n",
    "results_df = pd.concat([results_df, tempResultsDf])\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9bd4fe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 50\n",
    "seed = 7\n",
    "\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "model = KNeighborsClassifier()\n",
    "results = cross_val_score(model, X, y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b172396b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.347% (6.638%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "64973b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy =results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5e96c9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit Regression loocv</td>\n",
       "      <td>0.932993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logit Regression Grid SearchCV</td>\n",
       "      <td>0.921575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logit Regression Random SearchCV</td>\n",
       "      <td>0.917950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logit Regression PCA</td>\n",
       "      <td>0.633880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN  K Fold</td>\n",
       "      <td>0.933468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Method  accuracy\n",
       "1                   Logit Regression  0.857923\n",
       "2           Logit Regression K Fold   0.933508\n",
       "3            Logit Regression loocv   0.932993\n",
       "4    Logit Regression Grid SearchCV   0.921575\n",
       "5  Logit Regression Random SearchCV   0.917950\n",
       "6              Logit Regression PCA   0.633880\n",
       "7                               KNN   0.916667\n",
       "8                        KNN  K Fold  0.933468"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['KNN  K Fold'], 'accuracy': [accuracy]},index={'8'})\n",
    "tempResultsDf\n",
    "results_df = pd.concat([results_df, tempResultsDf])\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e11405ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = list(range(1, 31))\n",
    "param_grid = dict(n_neighbors=k_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d6e95c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(KNN, param_grid, cv=10, scoring='accuracy', return_train_score=False,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c501e9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "grid_search=grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a5be463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for our training dataset with tuning is : 96.81%\n"
     ]
    }
   ],
   "source": [
    "accuracy = grid_search.best_score_ *100\n",
    "print(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf968c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit Regression loocv</td>\n",
       "      <td>0.932993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logit Regression Grid SearchCV</td>\n",
       "      <td>0.921575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logit Regression Random SearchCV</td>\n",
       "      <td>0.917950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logit Regression PCA</td>\n",
       "      <td>0.633880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN  K Fold</td>\n",
       "      <td>0.933468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNN  Grid Serach CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Method  accuracy\n",
       "1                   Logit Regression  0.857923\n",
       "2           Logit Regression K Fold   0.933508\n",
       "3            Logit Regression loocv   0.932993\n",
       "4    Logit Regression Grid SearchCV   0.921575\n",
       "5  Logit Regression Random SearchCV   0.917950\n",
       "6              Logit Regression PCA   0.633880\n",
       "7                               KNN   0.916667\n",
       "8                        KNN  K Fold  0.933468\n",
       "9                KNN  Grid Serach CV  0.968095"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['KNN  Grid Serach CV'], 'accuracy': [accuracy/100]},index={'9'})\n",
    "tempResultsDf\n",
    "results_df = pd.concat([results_df, tempResultsDf])\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506386ba",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ad61b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    " dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "edba3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model=dTree.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62f975e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.71      0.94      0.81       363\n",
      "           1       0.91      0.62      0.74       369\n",
      "\n",
      "    accuracy                           0.78       732\n",
      "   macro avg       0.81      0.78      0.77       732\n",
      "weighted avg       0.81      0.78      0.77       732\n",
      "\n",
      "Accuracy  77.8688524590164\n"
     ]
    }
   ],
   "source": [
    "y_pred = fitted_model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "accuracy_score=metrics.accuracy_score(y_test,y_pred)*100\n",
    "print(\"Accuracy \",accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09ef3738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit Regression loocv</td>\n",
       "      <td>0.932993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logit Regression Grid SearchCV</td>\n",
       "      <td>0.921575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logit Regression Random SearchCV</td>\n",
       "      <td>0.917950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logit Regression PCA</td>\n",
       "      <td>0.633880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN  K Fold</td>\n",
       "      <td>0.933468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNN  Grid Serach CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.778689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Method  accuracy\n",
       "1                   Logit Regression  0.857923\n",
       "2           Logit Regression K Fold   0.933508\n",
       "3            Logit Regression loocv   0.932993\n",
       "4    Logit Regression Grid SearchCV   0.921575\n",
       "5  Logit Regression Random SearchCV   0.917950\n",
       "6              Logit Regression PCA   0.633880\n",
       "7                               KNN   0.916667\n",
       "8                        KNN  K Fold  0.933468\n",
       "9                KNN  Grid Serach CV  0.968095\n",
       "9                      Decision Tree  0.778689"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Decision Tree'], 'accuracy': [accuracy_score/100]},index={'9'})\n",
    "tempResultsDf\n",
    "results_df = pd.concat([results_df, tempResultsDf])\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6fa5f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 50\n",
    "seed = 7\n",
    "\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "model = DecisionTreeClassifier()\n",
    "results = cross_val_score(model, X, y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0895478b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.667% (10.604%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "230eb55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score =results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5efd9a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit Regression loocv</td>\n",
       "      <td>0.932993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logit Regression Grid SearchCV</td>\n",
       "      <td>0.921575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logit Regression Random SearchCV</td>\n",
       "      <td>0.917950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logit Regression PCA</td>\n",
       "      <td>0.633880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN  K Fold</td>\n",
       "      <td>0.933468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNN  Grid Serach CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.778689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Decision Tree K fold</td>\n",
       "      <td>0.866673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Method  accuracy\n",
       "1                    Logit Regression  0.857923\n",
       "2            Logit Regression K Fold   0.933508\n",
       "3             Logit Regression loocv   0.932993\n",
       "4     Logit Regression Grid SearchCV   0.921575\n",
       "5   Logit Regression Random SearchCV   0.917950\n",
       "6               Logit Regression PCA   0.633880\n",
       "7                                KNN   0.916667\n",
       "8                         KNN  K Fold  0.933468\n",
       "9                 KNN  Grid Serach CV  0.968095\n",
       "9                       Decision Tree  0.778689\n",
       "10               Decision Tree K fold  0.866673"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Decision Tree K fold'], 'accuracy': [accuracy_score]},index={'10'})\n",
    "tempResultsDf\n",
    "results_df = pd.concat([results_df, tempResultsDf])\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc28a06",
   "metadata": {},
   "source": [
    "Grid Search CV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "38ef1d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = list(range(1,df.shape[1]+1,1))\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [2,4,6,8,10,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dd0a0a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 294 candidates, totalling 882 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=DecisionTreeClassifier(random_state=42),\n",
       "             param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                            13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "                                            22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "                                            31, ...],\n",
       "                         'min_samples_split': [2, 3, 4]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}\n",
    "grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), params, verbose=1, cv=3)\n",
    "grid_search_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e76226b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for our training dataset with tuning is : 96.81%\n"
     ]
    }
   ],
   "source": [
    "accuracy = grid_search.best_score_ *100\n",
    "print(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1c740011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit Regression loocv</td>\n",
       "      <td>0.932993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logit Regression Grid SearchCV</td>\n",
       "      <td>0.921575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logit Regression Random SearchCV</td>\n",
       "      <td>0.917950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logit Regression PCA</td>\n",
       "      <td>0.633880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN  K Fold</td>\n",
       "      <td>0.933468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNN  Grid Serach CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.778689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Decision Tree K fold</td>\n",
       "      <td>0.866673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Decision Tree Grid Search CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Method  accuracy\n",
       "1                    Logit Regression  0.857923\n",
       "2            Logit Regression K Fold   0.933508\n",
       "3             Logit Regression loocv   0.932993\n",
       "4     Logit Regression Grid SearchCV   0.921575\n",
       "5   Logit Regression Random SearchCV   0.917950\n",
       "6               Logit Regression PCA   0.633880\n",
       "7                                KNN   0.916667\n",
       "8                         KNN  K Fold  0.933468\n",
       "9                 KNN  Grid Serach CV  0.968095\n",
       "9                       Decision Tree  0.778689\n",
       "10               Decision Tree K fold  0.866673\n",
       "11       Decision Tree Grid Search CV  0.968095"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['Decision Tree Grid Search CV'], 'accuracy': [accuracy/100]},index={'11'})\n",
    "tempResultsDf\n",
    "results_df = pd.concat([results_df, tempResultsDf])\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef80e8",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f7490ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_m = svm.SVC(gamma=0.025, C=3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8c4fc764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      1.00      1.00       363\n",
      "           1       1.00      0.99      1.00       369\n",
      "\n",
      "    accuracy                           1.00       732\n",
      "   macro avg       1.00      1.00      1.00       732\n",
      "weighted avg       1.00      1.00      1.00       732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fitted_model=svm_m.fit(X_train, y_train)\n",
    "y_pred = fitted_model.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "accuracy_score=metrics.accuracy_score(y_test,y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "af91ec23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.72677595628416"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "37dc7ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit Regression loocv</td>\n",
       "      <td>0.932993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logit Regression Grid SearchCV</td>\n",
       "      <td>0.921575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logit Regression Random SearchCV</td>\n",
       "      <td>0.917950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logit Regression PCA</td>\n",
       "      <td>0.633880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN  K Fold</td>\n",
       "      <td>0.933468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNN  Grid Serach CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.778689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Decision Tree K fold</td>\n",
       "      <td>0.866673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Decision Tree Grid Search CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.997268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Method  accuracy\n",
       "1                    Logit Regression  0.857923\n",
       "2            Logit Regression K Fold   0.933508\n",
       "3             Logit Regression loocv   0.932993\n",
       "4     Logit Regression Grid SearchCV   0.921575\n",
       "5   Logit Regression Random SearchCV   0.917950\n",
       "6               Logit Regression PCA   0.633880\n",
       "7                                KNN   0.916667\n",
       "8                         KNN  K Fold  0.933468\n",
       "9                 KNN  Grid Serach CV  0.968095\n",
       "9                       Decision Tree  0.778689\n",
       "10               Decision Tree K fold  0.866673\n",
       "11       Decision Tree Grid Search CV  0.968095\n",
       "12                                SVM  0.997268"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['SVM'], 'accuracy': [accuracy_score/100]},index={'12'})\n",
    "tempResultsDf\n",
    "results_df = pd.concat([results_df, tempResultsDf])\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "79170a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 50\n",
    "seed = 7\n",
    "\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "results = cross_val_score(svm_m, X_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "08f935b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000% (0.000%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "eed6269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score =results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5fa3775f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit Regression loocv</td>\n",
       "      <td>0.932993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logit Regression Grid SearchCV</td>\n",
       "      <td>0.921575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logit Regression Random SearchCV</td>\n",
       "      <td>0.917950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logit Regression PCA</td>\n",
       "      <td>0.633880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN  K Fold</td>\n",
       "      <td>0.933468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNN  Grid Serach CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.778689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Decision Tree K fold</td>\n",
       "      <td>0.866673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Decision Tree Grid Search CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.997268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVM K Fold</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Method  accuracy\n",
       "1                    Logit Regression  0.857923\n",
       "2            Logit Regression K Fold   0.933508\n",
       "3             Logit Regression loocv   0.932993\n",
       "4     Logit Regression Grid SearchCV   0.921575\n",
       "5   Logit Regression Random SearchCV   0.917950\n",
       "6               Logit Regression PCA   0.633880\n",
       "7                                KNN   0.916667\n",
       "8                         KNN  K Fold  0.933468\n",
       "9                 KNN  Grid Serach CV  0.968095\n",
       "9                       Decision Tree  0.778689\n",
       "10               Decision Tree K fold  0.866673\n",
       "11       Decision Tree Grid Search CV  0.968095\n",
       "12                                SVM  0.997268\n",
       "12                         SVM K Fold  1.000000"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['SVM K Fold'], 'accuracy': [accuracy_score]},index={'12'})\n",
    "tempResultsDf\n",
    "results_df = pd.concat([results_df, tempResultsDf])\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb48e37",
   "metadata": {},
   "source": [
    "Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1543f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b2be035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9aff9c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[CV 1/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.672 total time=   0.4s\n",
      "[CV 2/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.663 total time=   0.4s\n",
      "[CV 3/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.649 total time=   0.4s\n",
      "[CV 4/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.672 total time=   0.4s\n",
      "[CV 5/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.662 total time=   0.4s\n",
      "[CV 1/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.672 total time=   0.4s\n",
      "[CV 2/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.663 total time=   0.4s\n",
      "[CV 3/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.649 total time=   0.4s\n",
      "[CV 4/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.672 total time=   0.4s\n",
      "[CV 5/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.662 total time=   0.4s\n",
      "[CV 1/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.941 total time=   0.4s\n",
      "[CV 2/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.938 total time=   0.4s\n",
      "[CV 3/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.952 total time=   0.4s\n",
      "[CV 4/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.916 total time=   0.4s\n",
      "[CV 5/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.913 total time=   0.4s\n",
      "[CV 1/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.738 total time=   0.4s\n",
      "[CV 2/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.777 total time=   0.4s\n",
      "[CV 3/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.747 total time=   0.4s\n",
      "[CV 4/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.770 total time=   0.4s\n",
      "[CV 5/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.749 total time=   0.4s\n",
      "[CV 1/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.501 total time=   0.4s\n",
      "[CV 2/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.501 total time=   0.4s\n",
      "[CV 3/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.501 total time=   0.4s\n",
      "[CV 4/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.501 total time=   0.4s\n",
      "[CV 5/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.502 total time=   0.4s\n",
      "[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=1.000 total time=   0.3s\n",
      "[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=1.000 total time=   0.3s\n",
      "[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=1.000 total time=   0.3s\n",
      "[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=1.000 total time=   0.3s\n",
      "[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=1.000 total time=   0.3s\n",
      "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.3s\n",
      "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.3s\n",
      "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.3s\n",
      "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.3s\n",
      "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.3s\n",
      "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.2s\n",
      "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.3s\n",
      "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.995 total time=   0.2s\n",
      "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 1/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.847 total time=   0.3s\n",
      "[CV 2/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.884 total time=   0.3s\n",
      "[CV 3/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.870 total time=   0.3s\n",
      "[CV 4/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.854 total time=   0.3s\n",
      "[CV 5/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.849 total time=   0.3s\n",
      "[CV 1/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.738 total time=   0.4s\n",
      "[CV 2/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.784 total time=   0.4s\n",
      "[CV 3/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.761 total time=   0.4s\n",
      "[CV 4/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.765 total time=   0.4s\n",
      "[CV 5/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.758 total time=   0.4s\n",
      "[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.2s\n",
      "[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.2s\n",
      "[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 1/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.959 total time=   0.2s\n",
      "[CV 2/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.968 total time=   0.2s\n",
      "[CV 3/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.959 total time=   0.2s\n",
      "[CV 4/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.957 total time=   0.2s\n",
      "[CV 5/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.938 total time=   0.2s\n",
      "[CV 1/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.829 total time=   0.3s\n",
      "[CV 2/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.854 total time=   0.3s\n",
      "[CV 3/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.843 total time=   0.3s\n",
      "[CV 4/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.820 total time=   0.3s\n",
      "[CV 5/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.840 total time=   0.3s\n",
      "[CV 1/5] END ........C=100, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 2/5] END ........C=100, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 3/5] END ........C=100, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 4/5] END ........C=100, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 5/5] END ........C=100, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 1/5] END ......C=100, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 2/5] END ......C=100, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 3/5] END ......C=100, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 4/5] END ......C=100, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 5/5] END ......C=100, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 1/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.2s\n",
      "[CV 2/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.2s\n",
      "[CV 3/5] END .....C=100, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 4/5] END .....C=100, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 5/5] END .....C=100, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 1/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.977 total time=   0.2s\n",
      "[CV 2/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.989 total time=   0.2s\n",
      "[CV 3/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.975 total time=   0.2s\n",
      "[CV 4/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.970 total time=   0.2s\n",
      "[CV 5/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.950 total time=   0.2s\n",
      "[CV 1/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.893 total time=   0.2s\n",
      "[CV 2/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.916 total time=   0.2s\n",
      "[CV 3/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.895 total time=   0.2s\n",
      "[CV 4/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.911 total time=   0.2s\n",
      "[CV 5/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.897 total time=   0.2s\n",
      "[CV 1/5] END .......C=1000, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 2/5] END .......C=1000, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 3/5] END .......C=1000, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 4/5] END .......C=1000, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 5/5] END .......C=1000, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 1/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 2/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 3/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 4/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 5/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 1/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.2s\n",
      "[CV 2/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.2s\n",
      "[CV 3/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 4/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 5/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s\n",
      "[CV 1/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.977 total time=   0.2s\n",
      "[CV 2/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.989 total time=   0.2s\n",
      "[CV 3/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.975 total time=   0.2s\n",
      "[CV 4/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.970 total time=   0.2s\n",
      "[CV 5/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.950 total time=   0.2s\n",
      "[CV 1/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.941 total time=   0.2s\n",
      "[CV 2/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.952 total time=   0.2s\n",
      "[CV 3/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.938 total time=   0.2s\n",
      "[CV 4/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.932 total time=   0.2s\n",
      "[CV 5/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.929 total time=   0.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10, 100, 1000],\n",
       "                         'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
       "                         'kernel': ['rbf']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8ee54eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "26f488c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions_Train=grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bac8a920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit Regression loocv</td>\n",
       "      <td>0.932993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logit Regression Grid SearchCV</td>\n",
       "      <td>0.921575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logit Regression Random SearchCV</td>\n",
       "      <td>0.917950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logit Regression PCA</td>\n",
       "      <td>0.633880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN  K Fold</td>\n",
       "      <td>0.933468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNN  Grid Serach CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.778689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Decision Tree K fold</td>\n",
       "      <td>0.866673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Decision Tree Grid Search CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.997268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVM K Fold</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SVM K Fold Grid Search CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Method  accuracy\n",
       "1                    Logit Regression  0.857923\n",
       "2            Logit Regression K Fold   0.933508\n",
       "3             Logit Regression loocv   0.932993\n",
       "4     Logit Regression Grid SearchCV   0.921575\n",
       "5   Logit Regression Random SearchCV   0.917950\n",
       "6               Logit Regression PCA   0.633880\n",
       "7                                KNN   0.916667\n",
       "8                         KNN  K Fold  0.933468\n",
       "9                 KNN  Grid Serach CV  0.968095\n",
       "9                       Decision Tree  0.778689\n",
       "10               Decision Tree K fold  0.866673\n",
       "11       Decision Tree Grid Search CV  0.968095\n",
       "12                                SVM  0.997268\n",
       "12                         SVM K Fold  1.000000\n",
       "13         SVM K Fold Grid Search CV   0.968095"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store the accuracy results for each model in a dataframe for final comparison\n",
    "tempResultsDf = pd.DataFrame({'Method':['SVM K Fold Grid Search CV '], 'accuracy': [accuracy/100]},index={'13'})\n",
    "tempResultsDf\n",
    "results_df = pd.concat([results_df, tempResultsDf])\n",
    "results_df = results_df[['Method', 'accuracy']]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e7145",
   "metadata": {},
   "source": [
    "# 6. Post Training and Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ddd333",
   "metadata": {},
   "source": [
    "6.A. Display and compare all the models designed with their train and test accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1406ea98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logit Regression</td>\n",
       "      <td>0.857923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logit Regression K Fold</td>\n",
       "      <td>0.933508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logit Regression loocv</td>\n",
       "      <td>0.932993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logit Regression Grid SearchCV</td>\n",
       "      <td>0.921575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logit Regression Random SearchCV</td>\n",
       "      <td>0.917950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logit Regression PCA</td>\n",
       "      <td>0.633880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN  K Fold</td>\n",
       "      <td>0.933468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNN  Grid Serach CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.778689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Decision Tree K fold</td>\n",
       "      <td>0.866673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Decision Tree Grid Search CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.997268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVM K Fold</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SVM K Fold Grid Search CV</td>\n",
       "      <td>0.968095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Method  accuracy\n",
       "1                    Logit Regression  0.857923\n",
       "2            Logit Regression K Fold   0.933508\n",
       "3             Logit Regression loocv   0.932993\n",
       "4     Logit Regression Grid SearchCV   0.921575\n",
       "5   Logit Regression Random SearchCV   0.917950\n",
       "6               Logit Regression PCA   0.633880\n",
       "7                                KNN   0.916667\n",
       "8                         KNN  K Fold  0.933468\n",
       "9                 KNN  Grid Serach CV  0.968095\n",
       "9                       Decision Tree  0.778689\n",
       "10               Decision Tree K fold  0.866673\n",
       "11       Decision Tree Grid Search CV  0.968095\n",
       "12                                SVM  0.997268\n",
       "12                         SVM K Fold  1.000000\n",
       "13         SVM K Fold Grid Search CV   0.968095"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8aafe",
   "metadata": {},
   "source": [
    "6.B. Select the final best trained model along with your detailed comments for selecting this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f262faf2",
   "metadata": {},
   "source": [
    "1.SVM model is overfitting\n",
    "2.Logit is  best model with all folding/Hyper testing\n",
    "3.Also confidence level 95 % in boot strap sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddaba9d",
   "metadata": {},
   "source": [
    "6.C. Pickle the selected model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aecac1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(logit,open('models/Best_performing_model_'+'logit'+'.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa3ddb",
   "metadata": {},
   "source": [
    "6.D  Write your conclusion on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413f8f8",
   "metadata": {},
   "source": [
    "We selected the logit model. Becuase bootstrap smapling giving 95.0 confidence interval 91.0% and 93.7%\n",
    "Actual Data set has lot of features which is continuous values\n",
    "Lot features having Std as 0 which menas it has unique values .It is removed to build the model \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>FEATURISATION & MODEL TUNING</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>




<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>



<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/*
 * Webkit scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar,
[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-corner {
  background: var(--jp-scrollbar-background-color);
}

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-thumb {
  background: rgb(var(--jp-scrollbar-thumb-color));
  border: var(--jp-scrollbar-thumb-margin) solid transparent;
  background-clip: content-box;
  border-radius: var(--jp-scrollbar-thumb-radius);
}

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-track:horizontal {
  border-left: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
  border-right: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
}

[data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-track:vertical {
  border-top: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
  border-bottom: var(--jp-scrollbar-endpad) solid
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar */

[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar::-webkit-scrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar::-webkit-scrollbar,
[data-jp-theme-scrollbars='true']
  .CodeMirror-hscrollbar::-webkit-scrollbar-corner,
[data-jp-theme-scrollbars='true']
  .CodeMirror-vscrollbar::-webkit-scrollbar-corner {
  background-color: transparent;
}

[data-jp-theme-scrollbars='true']
  .CodeMirror-hscrollbar::-webkit-scrollbar-thumb,
[data-jp-theme-scrollbars='true']
  .CodeMirror-vscrollbar::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
  border: var(--jp-scrollbar-thumb-margin) solid transparent;
  background-clip: content-box;
  border-radius: var(--jp-scrollbar-thumb-radius);
}

[data-jp-theme-scrollbars='true']
  .CodeMirror-hscrollbar::-webkit-scrollbar-track:horizontal {
  border-left: var(--jp-scrollbar-endpad) solid transparent;
  border-right: var(--jp-scrollbar-endpad) solid transparent;
}

[data-jp-theme-scrollbars='true']
  .CodeMirror-vscrollbar::-webkit-scrollbar-track:vertical {
  border-top: var(--jp-scrollbar-endpad) solid transparent;
  border-bottom: var(--jp-scrollbar-endpad) solid transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0px solid transparent;
  border-right: 0px solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0px solid transparent;
  border-bottom: 0px solid transparent;
}

/*
 * Phosphor
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-Widget, /* </DEPRECATED> */
.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  cursor: default;
}


/* <DEPRECATED> */ .p-Widget.p-mod-hidden, /* </DEPRECATED> */
.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-CommandPalette, /* </DEPRECATED> */
.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-CommandPalette-search, /* </DEPRECATED> */
.lm-CommandPalette-search {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-CommandPalette-content, /* </DEPRECATED> */
.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}


/* <DEPRECATED> */ .p-CommandPalette-header, /* </DEPRECATED> */
.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}


/* <DEPRECATED> */ .p-CommandPalette-item, /* </DEPRECATED> */
.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}


/* <DEPRECATED> */ .p-CommandPalette-itemIcon, /* </DEPRECATED> */
.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-CommandPalette-itemContent, /* </DEPRECATED> */
.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}


/* <DEPRECATED> */ .p-CommandPalette-itemShortcut, /* </DEPRECATED> */
.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-CommandPalette-itemLabel, /* </DEPRECATED> */
.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
	border:1px solid transparent;
  background-color: transparent;
  position: absolute;
	z-index:1;
	right:3%;
	top: 0;
	bottom: 0;
	margin: auto;
	padding: 7px 0;
	display: none;
	vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
	content: "X";
	display: block;
	width: 15px;
	height: 15px;
	text-align: center;
	color:#000;
	font-weight: normal;
	font-size: 12px;
	cursor: pointer;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-DockPanel, /* </DEPRECATED> */
.lm-DockPanel {
  z-index: 0;
}


/* <DEPRECATED> */ .p-DockPanel-widget, /* </DEPRECATED> */
.lm-DockPanel-widget {
  z-index: 0;
}


/* <DEPRECATED> */ .p-DockPanel-tabBar, /* </DEPRECATED> */
.lm-DockPanel-tabBar {
  z-index: 1;
}


/* <DEPRECATED> */ .p-DockPanel-handle, /* </DEPRECATED> */
.lm-DockPanel-handle {
  z-index: 2;
}


/* <DEPRECATED> */ .p-DockPanel-handle.p-mod-hidden, /* </DEPRECATED> */
.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}


/* <DEPRECATED> */ .p-DockPanel-handle:after, /* </DEPRECATED> */
.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='horizontal'],
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='vertical'],
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='horizontal']:after,
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}


/* <DEPRECATED> */
.p-DockPanel-handle[data-orientation='vertical']:after,
/* </DEPRECATED> */
.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}


/* <DEPRECATED> */ .p-DockPanel-overlay, /* </DEPRECATED> */
.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}


/* <DEPRECATED> */ .p-DockPanel-overlay.p-mod-hidden, /* </DEPRECATED> */
.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-Menu, /* </DEPRECATED> */
.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-Menu-content, /* </DEPRECATED> */
.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}


/* <DEPRECATED> */ .p-Menu-item, /* </DEPRECATED> */
.lm-Menu-item {
  display: table-row;
}


/* <DEPRECATED> */
.p-Menu-item.p-mod-hidden,
.p-Menu-item.p-mod-collapsed,
/* </DEPRECATED> */
.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}


/* <DEPRECATED> */
.p-Menu-itemIcon,
.p-Menu-itemSubmenuIcon,
/* </DEPRECATED> */
.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}


/* <DEPRECATED> */ .p-Menu-itemLabel, /* </DEPRECATED> */
.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}


/* <DEPRECATED> */ .p-Menu-itemShortcut, /* </DEPRECATED> */
.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-MenuBar, /* </DEPRECATED> */
.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-MenuBar-content, /* </DEPRECATED> */
.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}


/* <DEPRECATED> */ .p--MenuBar-item, /* </DEPRECATED> */
.lm-MenuBar-item {
  box-sizing: border-box;
}


/* <DEPRECATED> */
.p-MenuBar-itemIcon,
.p-MenuBar-itemLabel,
/* </DEPRECATED> */
.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-ScrollBar, /* </DEPRECATED> */
.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */
.p-ScrollBar[data-orientation='horizontal'],
/* </DEPRECATED> */
.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}


/* <DEPRECATED> */
.p-ScrollBar[data-orientation='vertical'],
/* </DEPRECATED> */
.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}


/* <DEPRECATED> */ .p-ScrollBar-button, /* </DEPRECATED> */
.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-ScrollBar-track, /* </DEPRECATED> */
.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}


/* <DEPRECATED> */ .p-ScrollBar-thumb, /* </DEPRECATED> */
.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-SplitPanel-child, /* </DEPRECATED> */
.lm-SplitPanel-child {
  z-index: 0;
}


/* <DEPRECATED> */ .p-SplitPanel-handle, /* </DEPRECATED> */
.lm-SplitPanel-handle {
  z-index: 1;
}


/* <DEPRECATED> */ .p-SplitPanel-handle.p-mod-hidden, /* </DEPRECATED> */
.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}


/* <DEPRECATED> */ .p-SplitPanel-handle:after, /* </DEPRECATED> */
.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='horizontal'] > .p-SplitPanel-handle,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='vertical'] > .p-SplitPanel-handle,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='horizontal'] > .p-SplitPanel-handle:after,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}


/* <DEPRECATED> */
.p-SplitPanel[data-orientation='vertical'] > .p-SplitPanel-handle:after,
/* </DEPRECATED> */
.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-TabBar, /* </DEPRECATED> */
.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}


/* <DEPRECATED> */ .p-TabBar[data-orientation='horizontal'], /* </DEPRECATED> */
.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
}


/* <DEPRECATED> */ .p-TabBar[data-orientation='vertical'], /* </DEPRECATED> */
.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
}


/* <DEPRECATED> */ .p-TabBar-content, /* </DEPRECATED> */
.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}


/* <DEPRECATED> */
.p-TabBar[data-orientation='horizontal'] > .p-TabBar-content,
/* </DEPRECATED> */
.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}


/* <DEPRECATED> */
.p-TabBar[data-orientation='vertical'] > .p-TabBar-content,
/* </DEPRECATED> */
.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}


/* <DEPRECATED> */ .p-TabBar-tab, /* </DEPRECATED> */
.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
}


/* <DEPRECATED> */
.p-TabBar-tabIcon,
.p-TabBar-tabCloseIcon,
/* </DEPRECATED> */
.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}


/* <DEPRECATED> */ .p-TabBar-tabLabel, /* </DEPRECATED> */
.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}


.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing : border-box;
}


/* <DEPRECATED> */ .p-TabBar-tab.p-mod-hidden, /* </DEPRECATED> */
.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}


/* <DEPRECATED> */ .p-TabBar.p-mod-dragging .p-TabBar-tab, /* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}


/* <DEPRECATED> */
.p-TabBar.p-mod-dragging[data-orientation='horizontal'] .p-TabBar-tab,
/* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}


/* <DEPRECATED> */
.p-TabBar.p-mod-dragging[data-orientation='vertical'] .p-TabBar-tab,
/* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}


/* <DEPRECATED> */
.p-TabBar.p-mod-dragging .p-TabBar-tab.p-mod-dragging,
/* </DEPRECATED> */
.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ .p-TabPanel-tabBar, /* </DEPRECATED> */
.lm-TabPanel-tabBar {
  z-index: 1;
}


/* <DEPRECATED> */ .p-TabPanel-stackedPanel, /* </DEPRECATED> */
.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

@charset "UTF-8";
html{
  -webkit-box-sizing:border-box;
          box-sizing:border-box; }

*,
*::before,
*::after{
  -webkit-box-sizing:inherit;
          box-sizing:inherit; }

body{
  font-size:14px;
  font-weight:400;
  letter-spacing:0;
  line-height:1.28581;
  text-transform:none;
  color:#182026;
  font-family:-apple-system, "BlinkMacSystemFont", "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Open Sans", "Helvetica Neue", "Icons16", sans-serif; }

p{
  margin-bottom:10px;
  margin-top:0; }

small{
  font-size:12px; }

strong{
  font-weight:600; }

::-moz-selection{
  background:rgba(125, 188, 255, 0.6); }

::selection{
  background:rgba(125, 188, 255, 0.6); }
.bp3-heading{
  color:#182026;
  font-weight:600;
  margin:0 0 10px;
  padding:0; }
  .bp3-dark .bp3-heading{
    color:#f5f8fa; }

h1.bp3-heading, .bp3-running-text h1{
  font-size:36px;
  line-height:40px; }

h2.bp3-heading, .bp3-running-text h2{
  font-size:28px;
  line-height:32px; }

h3.bp3-heading, .bp3-running-text h3{
  font-size:22px;
  line-height:25px; }

h4.bp3-heading, .bp3-running-text h4{
  font-size:18px;
  line-height:21px; }

h5.bp3-heading, .bp3-running-text h5{
  font-size:16px;
  line-height:19px; }

h6.bp3-heading, .bp3-running-text h6{
  font-size:14px;
  line-height:16px; }
.bp3-ui-text{
  font-size:14px;
  font-weight:400;
  letter-spacing:0;
  line-height:1.28581;
  text-transform:none; }

.bp3-monospace-text{
  font-family:monospace;
  text-transform:none; }

.bp3-text-muted{
  color:#5c7080; }
  .bp3-dark .bp3-text-muted{
    color:#a7b6c2; }

.bp3-text-disabled{
  color:rgba(92, 112, 128, 0.6); }
  .bp3-dark .bp3-text-disabled{
    color:rgba(167, 182, 194, 0.6); }

.bp3-text-overflow-ellipsis{
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal; }
.bp3-running-text{
  font-size:14px;
  line-height:1.5; }
  .bp3-running-text h1{
    color:#182026;
    font-weight:600;
    margin-bottom:20px;
    margin-top:40px; }
    .bp3-dark .bp3-running-text h1{
      color:#f5f8fa; }
  .bp3-running-text h2{
    color:#182026;
    font-weight:600;
    margin-bottom:20px;
    margin-top:40px; }
    .bp3-dark .bp3-running-text h2{
      color:#f5f8fa; }
  .bp3-running-text h3{
    color:#182026;
    font-weight:600;
    margin-bottom:20px;
    margin-top:40px; }
    .bp3-dark .bp3-running-text h3{
      color:#f5f8fa; }
  .bp3-running-text h4{
    color:#182026;
    font-weight:600;
    margin-bottom:20px;
    margin-top:40px; }
    .bp3-dark .bp3-running-text h4{
      color:#f5f8fa; }
  .bp3-running-text h5{
    color:#182026;
    font-weight:600;
    margin-bottom:20px;
    margin-top:40px; }
    .bp3-dark .bp3-running-text h5{
      color:#f5f8fa; }
  .bp3-running-text h6{
    color:#182026;
    font-weight:600;
    margin-bottom:20px;
    margin-top:40px; }
    .bp3-dark .bp3-running-text h6{
      color:#f5f8fa; }
  .bp3-running-text hr{
    border:none;
    border-bottom:1px solid rgba(16, 22, 26, 0.15);
    margin:20px 0; }
    .bp3-dark .bp3-running-text hr{
      border-color:rgba(255, 255, 255, 0.15); }
  .bp3-running-text p{
    margin:0 0 10px;
    padding:0; }

.bp3-text-large{
  font-size:16px; }

.bp3-text-small{
  font-size:12px; }
a{
  color:#106ba3;
  text-decoration:none; }
  a:hover{
    color:#106ba3;
    cursor:pointer;
    text-decoration:underline; }
  a .bp3-icon, a .bp3-icon-standard, a .bp3-icon-large{
    color:inherit; }
  a code,
  .bp3-dark a code{
    color:inherit; }
  .bp3-dark a,
  .bp3-dark a:hover{
    color:#48aff0; }
    .bp3-dark a .bp3-icon, .bp3-dark a .bp3-icon-standard, .bp3-dark a .bp3-icon-large,
    .bp3-dark a:hover .bp3-icon,
    .bp3-dark a:hover .bp3-icon-standard,
    .bp3-dark a:hover .bp3-icon-large{
      color:inherit; }
.bp3-running-text code, .bp3-code{
  font-family:monospace;
  text-transform:none;
  background:rgba(255, 255, 255, 0.7);
  border-radius:3px;
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2);
  color:#5c7080;
  font-size:smaller;
  padding:2px 5px; }
  .bp3-dark .bp3-running-text code, .bp3-running-text .bp3-dark code, .bp3-dark .bp3-code{
    background:rgba(16, 22, 26, 0.3);
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
    color:#a7b6c2; }
  .bp3-running-text a > code, a > .bp3-code{
    color:#137cbd; }
    .bp3-dark .bp3-running-text a > code, .bp3-running-text .bp3-dark a > code, .bp3-dark a > .bp3-code{
      color:inherit; }

.bp3-running-text pre, .bp3-code-block{
  font-family:monospace;
  text-transform:none;
  background:rgba(255, 255, 255, 0.7);
  border-radius:3px;
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
  color:#182026;
  display:block;
  font-size:13px;
  line-height:1.4;
  margin:10px 0;
  padding:13px 15px 12px;
  word-break:break-all;
  word-wrap:break-word; }
  .bp3-dark .bp3-running-text pre, .bp3-running-text .bp3-dark pre, .bp3-dark .bp3-code-block{
    background:rgba(16, 22, 26, 0.3);
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
    color:#f5f8fa; }
  .bp3-running-text pre > code, .bp3-code-block > code{
    background:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    color:inherit;
    font-size:inherit;
    padding:0; }

.bp3-running-text kbd, .bp3-key{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  background:#ffffff;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
  color:#5c7080;
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  font-family:inherit;
  font-size:12px;
  height:24px;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  line-height:24px;
  min-width:24px;
  padding:3px 6px;
  vertical-align:middle; }
  .bp3-running-text kbd .bp3-icon, .bp3-key .bp3-icon, .bp3-running-text kbd .bp3-icon-standard, .bp3-key .bp3-icon-standard, .bp3-running-text kbd .bp3-icon-large, .bp3-key .bp3-icon-large{
    margin-right:5px; }
  .bp3-dark .bp3-running-text kbd, .bp3-running-text .bp3-dark kbd, .bp3-dark .bp3-key{
    background:#394b59;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
    color:#a7b6c2; }
.bp3-running-text blockquote, .bp3-blockquote{
  border-left:solid 4px rgba(167, 182, 194, 0.5);
  margin:0 0 10px;
  padding:0 20px; }
  .bp3-dark .bp3-running-text blockquote, .bp3-running-text .bp3-dark blockquote, .bp3-dark .bp3-blockquote{
    border-color:rgba(115, 134, 148, 0.5); }
.bp3-running-text ul,
.bp3-running-text ol, .bp3-list{
  margin:10px 0;
  padding-left:30px; }
  .bp3-running-text ul li:not(:last-child), .bp3-running-text ol li:not(:last-child), .bp3-list li:not(:last-child){
    margin-bottom:5px; }
  .bp3-running-text ul ol, .bp3-running-text ol ol, .bp3-list ol,
  .bp3-running-text ul ul,
  .bp3-running-text ol ul,
  .bp3-list ul{
    margin-top:5px; }

.bp3-list-unstyled{
  list-style:none;
  margin:0;
  padding:0; }
  .bp3-list-unstyled li{
    padding:0; }
.bp3-rtl{
  text-align:right; }

.bp3-dark{
  color:#f5f8fa; }

:focus{
  outline:rgba(19, 124, 189, 0.6) auto 2px;
  outline-offset:2px;
  -moz-outline-radius:6px; }

.bp3-focus-disabled :focus{
  outline:none !important; }
  .bp3-focus-disabled :focus ~ .bp3-control-indicator{
    outline:none !important; }

.bp3-alert{
  max-width:400px;
  padding:20px; }

.bp3-alert-body{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex; }
  .bp3-alert-body .bp3-icon{
    font-size:40px;
    margin-right:20px;
    margin-top:0; }

.bp3-alert-contents{
  word-break:break-word; }

.bp3-alert-footer{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:reverse;
      -ms-flex-direction:row-reverse;
          flex-direction:row-reverse;
  margin-top:10px; }
  .bp3-alert-footer .bp3-button{
    margin-left:10px; }
.bp3-breadcrumbs{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  cursor:default;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -ms-flex-wrap:wrap;
      flex-wrap:wrap;
  height:30px;
  list-style:none;
  margin:0;
  padding:0; }
  .bp3-breadcrumbs > li{
    -webkit-box-align:center;
        -ms-flex-align:center;
            align-items:center;
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex; }
    .bp3-breadcrumbs > li::after{
      background:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M10.71 7.29l-4-4a1.003 1.003 0 00-1.42 1.42L8.59 8 5.3 11.29c-.19.18-.3.43-.3.71a1.003 1.003 0 001.71.71l4-4c.18-.18.29-.43.29-.71 0-.28-.11-.53-.29-.71z' fill='%235C7080'/%3e%3c/svg%3e");
      content:"";
      display:block;
      height:16px;
      margin:0 5px;
      width:16px; }
    .bp3-breadcrumbs > li:last-of-type::after{
      display:none; }

.bp3-breadcrumb,
.bp3-breadcrumb-current,
.bp3-breadcrumbs-collapsed{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  font-size:16px; }

.bp3-breadcrumb,
.bp3-breadcrumbs-collapsed{
  color:#5c7080; }

.bp3-breadcrumb:hover{
  text-decoration:none; }

.bp3-breadcrumb.bp3-disabled{
  color:rgba(92, 112, 128, 0.6);
  cursor:not-allowed; }

.bp3-breadcrumb .bp3-icon{
  margin-right:5px; }

.bp3-breadcrumb-current{
  color:inherit;
  font-weight:600; }
  .bp3-breadcrumb-current .bp3-input{
    font-size:inherit;
    font-weight:inherit;
    vertical-align:baseline; }

.bp3-breadcrumbs-collapsed{
  background:#ced9e0;
  border:none;
  border-radius:3px;
  cursor:pointer;
  margin-right:2px;
  padding:1px 5px;
  vertical-align:text-bottom; }
  .bp3-breadcrumbs-collapsed::before{
    background:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cg fill='%235C7080'%3e%3ccircle cx='2' cy='8.03' r='2'/%3e%3ccircle cx='14' cy='8.03' r='2'/%3e%3ccircle cx='8' cy='8.03' r='2'/%3e%3c/g%3e%3c/svg%3e") center no-repeat;
    content:"";
    display:block;
    height:16px;
    width:16px; }
  .bp3-breadcrumbs-collapsed:hover{
    background:#bfccd6;
    color:#182026;
    text-decoration:none; }

.bp3-dark .bp3-breadcrumb,
.bp3-dark .bp3-breadcrumbs-collapsed{
  color:#a7b6c2; }

.bp3-dark .bp3-breadcrumbs > li::after{
  color:#a7b6c2; }

.bp3-dark .bp3-breadcrumb.bp3-disabled{
  color:rgba(167, 182, 194, 0.6); }

.bp3-dark .bp3-breadcrumb-current{
  color:#f5f8fa; }

.bp3-dark .bp3-breadcrumbs-collapsed{
  background:rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-breadcrumbs-collapsed:hover{
    background:rgba(16, 22, 26, 0.6);
    color:#f5f8fa; }
.bp3-button{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  border:none;
  border-radius:3px;
  cursor:pointer;
  font-size:14px;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  padding:5px 10px;
  text-align:left;
  vertical-align:middle;
  min-height:30px;
  min-width:30px; }
  .bp3-button > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-button > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-button::before,
  .bp3-button > *{
    margin-right:7px; }
  .bp3-button:empty::before,
  .bp3-button > :last-child{
    margin-right:0; }
  .bp3-button:empty{
    padding:0 !important; }
  .bp3-button:disabled, .bp3-button.bp3-disabled{
    cursor:not-allowed; }
  .bp3-button.bp3-fill{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    width:100%; }
  .bp3-button.bp3-align-right,
  .bp3-align-right .bp3-button{
    text-align:right; }
  .bp3-button.bp3-align-left,
  .bp3-align-left .bp3-button{
    text-align:left; }
  .bp3-button:not([class*="bp3-intent-"]){
    background-color:#f5f8fa;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    color:#182026; }
    .bp3-button:not([class*="bp3-intent-"]):hover{
      background-clip:padding-box;
      background-color:#ebf1f5;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); }
    .bp3-button:not([class*="bp3-intent-"]):active, .bp3-button:not([class*="bp3-intent-"]).bp3-active{
      background-color:#d8e1e8;
      background-image:none;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-button:not([class*="bp3-intent-"]):disabled, .bp3-button:not([class*="bp3-intent-"]).bp3-disabled{
      background-color:rgba(206, 217, 224, 0.5);
      background-image:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(92, 112, 128, 0.6);
      cursor:not-allowed;
      outline:none; }
      .bp3-button:not([class*="bp3-intent-"]):disabled.bp3-active, .bp3-button:not([class*="bp3-intent-"]):disabled.bp3-active:hover, .bp3-button:not([class*="bp3-intent-"]).bp3-disabled.bp3-active, .bp3-button:not([class*="bp3-intent-"]).bp3-disabled.bp3-active:hover{
        background:rgba(206, 217, 224, 0.7); }
  .bp3-button.bp3-intent-primary{
    background-color:#137cbd;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    color:#ffffff; }
    .bp3-button.bp3-intent-primary:hover, .bp3-button.bp3-intent-primary:active, .bp3-button.bp3-intent-primary.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-primary:hover{
      background-color:#106ba3;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-primary:active, .bp3-button.bp3-intent-primary.bp3-active{
      background-color:#0e5a8a;
      background-image:none;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-primary:disabled, .bp3-button.bp3-intent-primary.bp3-disabled{
      background-color:rgba(19, 124, 189, 0.5);
      background-image:none;
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button.bp3-intent-success{
    background-color:#0f9960;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    color:#ffffff; }
    .bp3-button.bp3-intent-success:hover, .bp3-button.bp3-intent-success:active, .bp3-button.bp3-intent-success.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-success:hover{
      background-color:#0d8050;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-success:active, .bp3-button.bp3-intent-success.bp3-active{
      background-color:#0a6640;
      background-image:none;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-success:disabled, .bp3-button.bp3-intent-success.bp3-disabled{
      background-color:rgba(15, 153, 96, 0.5);
      background-image:none;
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button.bp3-intent-warning{
    background-color:#d9822b;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    color:#ffffff; }
    .bp3-button.bp3-intent-warning:hover, .bp3-button.bp3-intent-warning:active, .bp3-button.bp3-intent-warning.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-warning:hover{
      background-color:#bf7326;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-warning:active, .bp3-button.bp3-intent-warning.bp3-active{
      background-color:#a66321;
      background-image:none;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-warning:disabled, .bp3-button.bp3-intent-warning.bp3-disabled{
      background-color:rgba(217, 130, 43, 0.5);
      background-image:none;
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button.bp3-intent-danger{
    background-color:#db3737;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    color:#ffffff; }
    .bp3-button.bp3-intent-danger:hover, .bp3-button.bp3-intent-danger:active, .bp3-button.bp3-intent-danger.bp3-active{
      color:#ffffff; }
    .bp3-button.bp3-intent-danger:hover{
      background-color:#c23030;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-danger:active, .bp3-button.bp3-intent-danger.bp3-active{
      background-color:#a82a2a;
      background-image:none;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-button.bp3-intent-danger:disabled, .bp3-button.bp3-intent-danger.bp3-disabled{
      background-color:rgba(219, 55, 55, 0.5);
      background-image:none;
      border-color:transparent;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(255, 255, 255, 0.6); }
  .bp3-button[class*="bp3-intent-"] .bp3-button-spinner .bp3-spinner-head{
    stroke:#ffffff; }
  .bp3-button.bp3-large,
  .bp3-large .bp3-button{
    min-height:40px;
    min-width:40px;
    font-size:16px;
    padding:5px 15px; }
    .bp3-button.bp3-large::before,
    .bp3-button.bp3-large > *,
    .bp3-large .bp3-button::before,
    .bp3-large .bp3-button > *{
      margin-right:10px; }
    .bp3-button.bp3-large:empty::before,
    .bp3-button.bp3-large > :last-child,
    .bp3-large .bp3-button:empty::before,
    .bp3-large .bp3-button > :last-child{
      margin-right:0; }
  .bp3-button.bp3-small,
  .bp3-small .bp3-button{
    min-height:24px;
    min-width:24px;
    padding:0 7px; }
  .bp3-button.bp3-loading{
    position:relative; }
    .bp3-button.bp3-loading[class*="bp3-icon-"]::before{
      visibility:hidden; }
    .bp3-button.bp3-loading .bp3-button-spinner{
      margin:0;
      position:absolute; }
    .bp3-button.bp3-loading > :not(.bp3-button-spinner){
      visibility:hidden; }
  .bp3-button[class*="bp3-icon-"]::before{
    font-family:"Icons16", sans-serif;
    font-size:16px;
    font-style:normal;
    font-weight:400;
    line-height:1;
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased;
    color:#5c7080; }
  .bp3-button .bp3-icon, .bp3-button .bp3-icon-standard, .bp3-button .bp3-icon-large{
    color:#5c7080; }
    .bp3-button .bp3-icon.bp3-align-right, .bp3-button .bp3-icon-standard.bp3-align-right, .bp3-button .bp3-icon-large.bp3-align-right{
      margin-left:7px; }
  .bp3-button .bp3-icon:first-child:last-child,
  .bp3-button .bp3-spinner + .bp3-icon:last-child{
    margin:0 -7px; }
  .bp3-dark .bp3-button:not([class*="bp3-intent-"]){
    background-color:#394b59;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    color:#f5f8fa; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):hover, .bp3-dark .bp3-button:not([class*="bp3-intent-"]):active, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-active{
      color:#f5f8fa; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):hover{
      background-color:#30404d;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):active, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-active{
      background-color:#202b33;
      background-image:none;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]):disabled, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-disabled{
      background-color:rgba(57, 75, 89, 0.5);
      background-image:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(167, 182, 194, 0.6); }
      .bp3-dark .bp3-button:not([class*="bp3-intent-"]):disabled.bp3-active, .bp3-dark .bp3-button:not([class*="bp3-intent-"]).bp3-disabled.bp3-active{
        background:rgba(57, 75, 89, 0.7); }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-button-spinner .bp3-spinner-head{
      background:rgba(16, 22, 26, 0.5);
      stroke:#8a9ba8; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"])[class*="bp3-icon-"]::before{
      color:#a7b6c2; }
    .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-icon, .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-icon-standard, .bp3-dark .bp3-button:not([class*="bp3-intent-"]) .bp3-icon-large{
      color:#a7b6c2; }
  .bp3-dark .bp3-button[class*="bp3-intent-"]{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-button[class*="bp3-intent-"]:hover{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-button[class*="bp3-intent-"]:active, .bp3-dark .bp3-button[class*="bp3-intent-"].bp3-active{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-dark .bp3-button[class*="bp3-intent-"]:disabled, .bp3-dark .bp3-button[class*="bp3-intent-"].bp3-disabled{
      background-image:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(255, 255, 255, 0.3); }
    .bp3-dark .bp3-button[class*="bp3-intent-"] .bp3-button-spinner .bp3-spinner-head{
      stroke:#8a9ba8; }
  .bp3-button:disabled::before,
  .bp3-button:disabled .bp3-icon, .bp3-button:disabled .bp3-icon-standard, .bp3-button:disabled .bp3-icon-large, .bp3-button.bp3-disabled::before,
  .bp3-button.bp3-disabled .bp3-icon, .bp3-button.bp3-disabled .bp3-icon-standard, .bp3-button.bp3-disabled .bp3-icon-large, .bp3-button[class*="bp3-intent-"]::before,
  .bp3-button[class*="bp3-intent-"] .bp3-icon, .bp3-button[class*="bp3-intent-"] .bp3-icon-standard, .bp3-button[class*="bp3-intent-"] .bp3-icon-large{
    color:inherit !important; }
  .bp3-button.bp3-minimal{
    background:none;
    -webkit-box-shadow:none;
            box-shadow:none; }
    .bp3-button.bp3-minimal:hover{
      background:rgba(167, 182, 194, 0.3);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#182026;
      text-decoration:none; }
    .bp3-button.bp3-minimal:active, .bp3-button.bp3-minimal.bp3-active{
      background:rgba(115, 134, 148, 0.3);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#182026; }
    .bp3-button.bp3-minimal:disabled, .bp3-button.bp3-minimal:disabled:hover, .bp3-button.bp3-minimal.bp3-disabled, .bp3-button.bp3-minimal.bp3-disabled:hover{
      background:none;
      color:rgba(92, 112, 128, 0.6);
      cursor:not-allowed; }
      .bp3-button.bp3-minimal:disabled.bp3-active, .bp3-button.bp3-minimal:disabled:hover.bp3-active, .bp3-button.bp3-minimal.bp3-disabled.bp3-active, .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{
        background:rgba(115, 134, 148, 0.3); }
    .bp3-dark .bp3-button.bp3-minimal{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:inherit; }
      .bp3-dark .bp3-button.bp3-minimal:hover, .bp3-dark .bp3-button.bp3-minimal:active, .bp3-dark .bp3-button.bp3-minimal.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none; }
      .bp3-dark .bp3-button.bp3-minimal:hover{
        background:rgba(138, 155, 168, 0.15); }
      .bp3-dark .bp3-button.bp3-minimal:active, .bp3-dark .bp3-button.bp3-minimal.bp3-active{
        background:rgba(138, 155, 168, 0.3);
        color:#f5f8fa; }
      .bp3-dark .bp3-button.bp3-minimal:disabled, .bp3-dark .bp3-button.bp3-minimal:disabled:hover, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover{
        background:none;
        color:rgba(167, 182, 194, 0.6);
        cursor:not-allowed; }
        .bp3-dark .bp3-button.bp3-minimal:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal:disabled:hover.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{
          background:rgba(138, 155, 168, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-primary{
      color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:hover, .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:hover{
        background:rgba(19, 124, 189, 0.15);
        color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{
        background:rgba(19, 124, 189, 0.3);
        color:#106ba3; }
      .bp3-button.bp3-minimal.bp3-intent-primary:disabled, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{
        background:none;
        color:rgba(16, 107, 163, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{
          background:rgba(19, 124, 189, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{
        stroke:#106ba3; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary{
        color:#48aff0; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:hover{
          background:rgba(19, 124, 189, 0.2);
          color:#48aff0; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{
          background:rgba(19, 124, 189, 0.3);
          color:#48aff0; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{
          background:none;
          color:rgba(72, 175, 240, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{
            background:rgba(19, 124, 189, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-success{
      color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:hover, .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:hover{
        background:rgba(15, 153, 96, 0.15);
        color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{
        background:rgba(15, 153, 96, 0.3);
        color:#0d8050; }
      .bp3-button.bp3-minimal.bp3-intent-success:disabled, .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{
        background:none;
        color:rgba(13, 128, 80, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{
          background:rgba(15, 153, 96, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{
        stroke:#0d8050; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success{
        color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:hover{
          background:rgba(15, 153, 96, 0.2);
          color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{
          background:rgba(15, 153, 96, 0.3);
          color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{
          background:none;
          color:rgba(61, 204, 145, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{
            background:rgba(15, 153, 96, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-warning{
      color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:hover, .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:hover{
        background:rgba(217, 130, 43, 0.15);
        color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{
        background:rgba(217, 130, 43, 0.3);
        color:#bf7326; }
      .bp3-button.bp3-minimal.bp3-intent-warning:disabled, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{
        background:none;
        color:rgba(191, 115, 38, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{
          background:rgba(217, 130, 43, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{
        stroke:#bf7326; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning{
        color:#ffb366; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:hover{
          background:rgba(217, 130, 43, 0.2);
          color:#ffb366; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{
          background:rgba(217, 130, 43, 0.3);
          color:#ffb366; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{
          background:none;
          color:rgba(255, 179, 102, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{
            background:rgba(217, 130, 43, 0.3); }
    .bp3-button.bp3-minimal.bp3-intent-danger{
      color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:hover, .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:hover{
        background:rgba(219, 55, 55, 0.15);
        color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{
        background:rgba(219, 55, 55, 0.3);
        color:#c23030; }
      .bp3-button.bp3-minimal.bp3-intent-danger:disabled, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{
        background:none;
        color:rgba(194, 48, 48, 0.5); }
        .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{
          background:rgba(219, 55, 55, 0.3); }
      .bp3-button.bp3-minimal.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{
        stroke:#c23030; }
      .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger{
        color:#ff7373; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:hover{
          background:rgba(219, 55, 55, 0.2);
          color:#ff7373; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{
          background:rgba(219, 55, 55, 0.3);
          color:#ff7373; }
        .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{
          background:none;
          color:rgba(255, 115, 115, 0.5); }
          .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{
            background:rgba(219, 55, 55, 0.3); }
  .bp3-button.bp3-outlined{
    background:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    border:1px solid rgba(24, 32, 38, 0.2);
    -webkit-box-sizing:border-box;
            box-sizing:border-box; }
    .bp3-button.bp3-outlined:hover{
      background:rgba(167, 182, 194, 0.3);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#182026;
      text-decoration:none; }
    .bp3-button.bp3-outlined:active, .bp3-button.bp3-outlined.bp3-active{
      background:rgba(115, 134, 148, 0.3);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#182026; }
    .bp3-button.bp3-outlined:disabled, .bp3-button.bp3-outlined:disabled:hover, .bp3-button.bp3-outlined.bp3-disabled, .bp3-button.bp3-outlined.bp3-disabled:hover{
      background:none;
      color:rgba(92, 112, 128, 0.6);
      cursor:not-allowed; }
      .bp3-button.bp3-outlined:disabled.bp3-active, .bp3-button.bp3-outlined:disabled:hover.bp3-active, .bp3-button.bp3-outlined.bp3-disabled.bp3-active, .bp3-button.bp3-outlined.bp3-disabled:hover.bp3-active{
        background:rgba(115, 134, 148, 0.3); }
    .bp3-dark .bp3-button.bp3-outlined{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:inherit; }
      .bp3-dark .bp3-button.bp3-outlined:hover, .bp3-dark .bp3-button.bp3-outlined:active, .bp3-dark .bp3-button.bp3-outlined.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none; }
      .bp3-dark .bp3-button.bp3-outlined:hover{
        background:rgba(138, 155, 168, 0.15); }
      .bp3-dark .bp3-button.bp3-outlined:active, .bp3-dark .bp3-button.bp3-outlined.bp3-active{
        background:rgba(138, 155, 168, 0.3);
        color:#f5f8fa; }
      .bp3-dark .bp3-button.bp3-outlined:disabled, .bp3-dark .bp3-button.bp3-outlined:disabled:hover, .bp3-dark .bp3-button.bp3-outlined.bp3-disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-disabled:hover{
        background:none;
        color:rgba(167, 182, 194, 0.6);
        cursor:not-allowed; }
        .bp3-dark .bp3-button.bp3-outlined:disabled.bp3-active, .bp3-dark .bp3-button.bp3-outlined:disabled:hover.bp3-active, .bp3-dark .bp3-button.bp3-outlined.bp3-disabled.bp3-active, .bp3-dark .bp3-button.bp3-outlined.bp3-disabled:hover.bp3-active{
          background:rgba(138, 155, 168, 0.3); }
    .bp3-button.bp3-outlined.bp3-intent-primary{
      color:#106ba3; }
      .bp3-button.bp3-outlined.bp3-intent-primary:hover, .bp3-button.bp3-outlined.bp3-intent-primary:active, .bp3-button.bp3-outlined.bp3-intent-primary.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#106ba3; }
      .bp3-button.bp3-outlined.bp3-intent-primary:hover{
        background:rgba(19, 124, 189, 0.15);
        color:#106ba3; }
      .bp3-button.bp3-outlined.bp3-intent-primary:active, .bp3-button.bp3-outlined.bp3-intent-primary.bp3-active{
        background:rgba(19, 124, 189, 0.3);
        color:#106ba3; }
      .bp3-button.bp3-outlined.bp3-intent-primary:disabled, .bp3-button.bp3-outlined.bp3-intent-primary.bp3-disabled{
        background:none;
        color:rgba(16, 107, 163, 0.5); }
        .bp3-button.bp3-outlined.bp3-intent-primary:disabled.bp3-active, .bp3-button.bp3-outlined.bp3-intent-primary.bp3-disabled.bp3-active{
          background:rgba(19, 124, 189, 0.3); }
      .bp3-button.bp3-outlined.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{
        stroke:#106ba3; }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary{
        color:#48aff0; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary:hover{
          background:rgba(19, 124, 189, 0.2);
          color:#48aff0; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary:active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary.bp3-active{
          background:rgba(19, 124, 189, 0.3);
          color:#48aff0; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary.bp3-disabled{
          background:none;
          color:rgba(72, 175, 240, 0.5); }
          .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary.bp3-disabled.bp3-active{
            background:rgba(19, 124, 189, 0.3); }
    .bp3-button.bp3-outlined.bp3-intent-success{
      color:#0d8050; }
      .bp3-button.bp3-outlined.bp3-intent-success:hover, .bp3-button.bp3-outlined.bp3-intent-success:active, .bp3-button.bp3-outlined.bp3-intent-success.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#0d8050; }
      .bp3-button.bp3-outlined.bp3-intent-success:hover{
        background:rgba(15, 153, 96, 0.15);
        color:#0d8050; }
      .bp3-button.bp3-outlined.bp3-intent-success:active, .bp3-button.bp3-outlined.bp3-intent-success.bp3-active{
        background:rgba(15, 153, 96, 0.3);
        color:#0d8050; }
      .bp3-button.bp3-outlined.bp3-intent-success:disabled, .bp3-button.bp3-outlined.bp3-intent-success.bp3-disabled{
        background:none;
        color:rgba(13, 128, 80, 0.5); }
        .bp3-button.bp3-outlined.bp3-intent-success:disabled.bp3-active, .bp3-button.bp3-outlined.bp3-intent-success.bp3-disabled.bp3-active{
          background:rgba(15, 153, 96, 0.3); }
      .bp3-button.bp3-outlined.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{
        stroke:#0d8050; }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success{
        color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success:hover{
          background:rgba(15, 153, 96, 0.2);
          color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success:active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success.bp3-active{
          background:rgba(15, 153, 96, 0.3);
          color:#3dcc91; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success.bp3-disabled{
          background:none;
          color:rgba(61, 204, 145, 0.5); }
          .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success.bp3-disabled.bp3-active{
            background:rgba(15, 153, 96, 0.3); }
    .bp3-button.bp3-outlined.bp3-intent-warning{
      color:#bf7326; }
      .bp3-button.bp3-outlined.bp3-intent-warning:hover, .bp3-button.bp3-outlined.bp3-intent-warning:active, .bp3-button.bp3-outlined.bp3-intent-warning.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#bf7326; }
      .bp3-button.bp3-outlined.bp3-intent-warning:hover{
        background:rgba(217, 130, 43, 0.15);
        color:#bf7326; }
      .bp3-button.bp3-outlined.bp3-intent-warning:active, .bp3-button.bp3-outlined.bp3-intent-warning.bp3-active{
        background:rgba(217, 130, 43, 0.3);
        color:#bf7326; }
      .bp3-button.bp3-outlined.bp3-intent-warning:disabled, .bp3-button.bp3-outlined.bp3-intent-warning.bp3-disabled{
        background:none;
        color:rgba(191, 115, 38, 0.5); }
        .bp3-button.bp3-outlined.bp3-intent-warning:disabled.bp3-active, .bp3-button.bp3-outlined.bp3-intent-warning.bp3-disabled.bp3-active{
          background:rgba(217, 130, 43, 0.3); }
      .bp3-button.bp3-outlined.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{
        stroke:#bf7326; }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning{
        color:#ffb366; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning:hover{
          background:rgba(217, 130, 43, 0.2);
          color:#ffb366; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning:active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning.bp3-active{
          background:rgba(217, 130, 43, 0.3);
          color:#ffb366; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning.bp3-disabled{
          background:none;
          color:rgba(255, 179, 102, 0.5); }
          .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning.bp3-disabled.bp3-active{
            background:rgba(217, 130, 43, 0.3); }
    .bp3-button.bp3-outlined.bp3-intent-danger{
      color:#c23030; }
      .bp3-button.bp3-outlined.bp3-intent-danger:hover, .bp3-button.bp3-outlined.bp3-intent-danger:active, .bp3-button.bp3-outlined.bp3-intent-danger.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#c23030; }
      .bp3-button.bp3-outlined.bp3-intent-danger:hover{
        background:rgba(219, 55, 55, 0.15);
        color:#c23030; }
      .bp3-button.bp3-outlined.bp3-intent-danger:active, .bp3-button.bp3-outlined.bp3-intent-danger.bp3-active{
        background:rgba(219, 55, 55, 0.3);
        color:#c23030; }
      .bp3-button.bp3-outlined.bp3-intent-danger:disabled, .bp3-button.bp3-outlined.bp3-intent-danger.bp3-disabled{
        background:none;
        color:rgba(194, 48, 48, 0.5); }
        .bp3-button.bp3-outlined.bp3-intent-danger:disabled.bp3-active, .bp3-button.bp3-outlined.bp3-intent-danger.bp3-disabled.bp3-active{
          background:rgba(219, 55, 55, 0.3); }
      .bp3-button.bp3-outlined.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{
        stroke:#c23030; }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger{
        color:#ff7373; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger:hover{
          background:rgba(219, 55, 55, 0.2);
          color:#ff7373; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger:active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger.bp3-active{
          background:rgba(219, 55, 55, 0.3);
          color:#ff7373; }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger.bp3-disabled{
          background:none;
          color:rgba(255, 115, 115, 0.5); }
          .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger.bp3-disabled.bp3-active{
            background:rgba(219, 55, 55, 0.3); }
    .bp3-button.bp3-outlined:disabled, .bp3-button.bp3-outlined.bp3-disabled, .bp3-button.bp3-outlined:disabled:hover, .bp3-button.bp3-outlined.bp3-disabled:hover{
      border-color:rgba(92, 112, 128, 0.1); }
    .bp3-dark .bp3-button.bp3-outlined{
      border-color:rgba(255, 255, 255, 0.4); }
      .bp3-dark .bp3-button.bp3-outlined:disabled, .bp3-dark .bp3-button.bp3-outlined:disabled:hover, .bp3-dark .bp3-button.bp3-outlined.bp3-disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-disabled:hover{
        border-color:rgba(255, 255, 255, 0.2); }
    .bp3-button.bp3-outlined.bp3-intent-primary{
      border-color:rgba(16, 107, 163, 0.6); }
      .bp3-button.bp3-outlined.bp3-intent-primary:disabled, .bp3-button.bp3-outlined.bp3-intent-primary.bp3-disabled{
        border-color:rgba(16, 107, 163, 0.2); }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary{
        border-color:rgba(72, 175, 240, 0.6); }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-primary.bp3-disabled{
          border-color:rgba(72, 175, 240, 0.2); }
    .bp3-button.bp3-outlined.bp3-intent-success{
      border-color:rgba(13, 128, 80, 0.6); }
      .bp3-button.bp3-outlined.bp3-intent-success:disabled, .bp3-button.bp3-outlined.bp3-intent-success.bp3-disabled{
        border-color:rgba(13, 128, 80, 0.2); }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success{
        border-color:rgba(61, 204, 145, 0.6); }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-success.bp3-disabled{
          border-color:rgba(61, 204, 145, 0.2); }
    .bp3-button.bp3-outlined.bp3-intent-warning{
      border-color:rgba(191, 115, 38, 0.6); }
      .bp3-button.bp3-outlined.bp3-intent-warning:disabled, .bp3-button.bp3-outlined.bp3-intent-warning.bp3-disabled{
        border-color:rgba(191, 115, 38, 0.2); }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning{
        border-color:rgba(255, 179, 102, 0.6); }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-warning.bp3-disabled{
          border-color:rgba(255, 179, 102, 0.2); }
    .bp3-button.bp3-outlined.bp3-intent-danger{
      border-color:rgba(194, 48, 48, 0.6); }
      .bp3-button.bp3-outlined.bp3-intent-danger:disabled, .bp3-button.bp3-outlined.bp3-intent-danger.bp3-disabled{
        border-color:rgba(194, 48, 48, 0.2); }
      .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger{
        border-color:rgba(255, 115, 115, 0.6); }
        .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger:disabled, .bp3-dark .bp3-button.bp3-outlined.bp3-intent-danger.bp3-disabled{
          border-color:rgba(255, 115, 115, 0.2); }

a.bp3-button{
  text-align:center;
  text-decoration:none;
  -webkit-transition:none;
  transition:none; }
  a.bp3-button, a.bp3-button:hover, a.bp3-button:active{
    color:#182026; }
  a.bp3-button.bp3-disabled{
    color:rgba(92, 112, 128, 0.6); }

.bp3-button-text{
  -webkit-box-flex:0;
      -ms-flex:0 1 auto;
          flex:0 1 auto; }

.bp3-button.bp3-align-left .bp3-button-text, .bp3-button.bp3-align-right .bp3-button-text,
.bp3-button-group.bp3-align-left .bp3-button-text,
.bp3-button-group.bp3-align-right .bp3-button-text{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto; }
.bp3-button-group{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex; }
  .bp3-button-group .bp3-button{
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    position:relative;
    z-index:4; }
    .bp3-button-group .bp3-button:focus{
      z-index:5; }
    .bp3-button-group .bp3-button:hover{
      z-index:6; }
    .bp3-button-group .bp3-button:active, .bp3-button-group .bp3-button.bp3-active{
      z-index:7; }
    .bp3-button-group .bp3-button:disabled, .bp3-button-group .bp3-button.bp3-disabled{
      z-index:3; }
    .bp3-button-group .bp3-button[class*="bp3-intent-"]{
      z-index:9; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:focus{
        z-index:10; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:hover{
        z-index:11; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:active, .bp3-button-group .bp3-button[class*="bp3-intent-"].bp3-active{
        z-index:12; }
      .bp3-button-group .bp3-button[class*="bp3-intent-"]:disabled, .bp3-button-group .bp3-button[class*="bp3-intent-"].bp3-disabled{
        z-index:8; }
  .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:first-child) .bp3-button,
  .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:first-child){
    border-bottom-left-radius:0;
    border-top-left-radius:0; }
  .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button,
  .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:last-child){
    border-bottom-right-radius:0;
    border-top-right-radius:0;
    margin-right:-1px; }
  .bp3-button-group.bp3-minimal .bp3-button{
    background:none;
    -webkit-box-shadow:none;
            box-shadow:none; }
    .bp3-button-group.bp3-minimal .bp3-button:hover{
      background:rgba(167, 182, 194, 0.3);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#182026;
      text-decoration:none; }
    .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-active{
      background:rgba(115, 134, 148, 0.3);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#182026; }
    .bp3-button-group.bp3-minimal .bp3-button:disabled, .bp3-button-group.bp3-minimal .bp3-button:disabled:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{
      background:none;
      color:rgba(92, 112, 128, 0.6);
      cursor:not-allowed; }
      .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{
        background:rgba(115, 134, 148, 0.3); }
    .bp3-dark .bp3-button-group.bp3-minimal .bp3-button{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:inherit; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover{
        background:rgba(138, 155, 168, 0.15); }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{
        background:rgba(138, 155, 168, 0.3);
        color:#f5f8fa; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{
        background:none;
        color:rgba(167, 182, 194, 0.6);
        cursor:not-allowed; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{
          background:rgba(138, 155, 168, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{
      color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{
        background:rgba(19, 124, 189, 0.15);
        color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{
        background:rgba(19, 124, 189, 0.3);
        color:#106ba3; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{
        background:none;
        color:rgba(16, 107, 163, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{
          background:rgba(19, 124, 189, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{
        stroke:#106ba3; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{
        color:#48aff0; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{
          background:rgba(19, 124, 189, 0.2);
          color:#48aff0; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{
          background:rgba(19, 124, 189, 0.3);
          color:#48aff0; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{
          background:none;
          color:rgba(72, 175, 240, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{
            background:rgba(19, 124, 189, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{
      color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{
        background:rgba(15, 153, 96, 0.15);
        color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{
        background:rgba(15, 153, 96, 0.3);
        color:#0d8050; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{
        background:none;
        color:rgba(13, 128, 80, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{
          background:rgba(15, 153, 96, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{
        stroke:#0d8050; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{
        color:#3dcc91; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{
          background:rgba(15, 153, 96, 0.2);
          color:#3dcc91; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{
          background:rgba(15, 153, 96, 0.3);
          color:#3dcc91; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{
          background:none;
          color:rgba(61, 204, 145, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{
            background:rgba(15, 153, 96, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{
      color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{
        background:rgba(217, 130, 43, 0.15);
        color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{
        background:rgba(217, 130, 43, 0.3);
        color:#bf7326; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{
        background:none;
        color:rgba(191, 115, 38, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{
          background:rgba(217, 130, 43, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{
        stroke:#bf7326; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{
        color:#ffb366; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{
          background:rgba(217, 130, 43, 0.2);
          color:#ffb366; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{
          background:rgba(217, 130, 43, 0.3);
          color:#ffb366; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{
          background:none;
          color:rgba(255, 179, 102, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{
            background:rgba(217, 130, 43, 0.3); }
    .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{
      color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{
        background:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{
        background:rgba(219, 55, 55, 0.15);
        color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{
        background:rgba(219, 55, 55, 0.3);
        color:#c23030; }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{
        background:none;
        color:rgba(194, 48, 48, 0.5); }
        .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{
          background:rgba(219, 55, 55, 0.3); }
      .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{
        stroke:#c23030; }
      .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{
        color:#ff7373; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{
          background:rgba(219, 55, 55, 0.2);
          color:#ff7373; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{
          background:rgba(219, 55, 55, 0.3);
          color:#ff7373; }
        .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{
          background:none;
          color:rgba(255, 115, 115, 0.5); }
          .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{
            background:rgba(219, 55, 55, 0.3); }
  .bp3-button-group .bp3-popover-wrapper,
  .bp3-button-group .bp3-popover-target{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-button-group.bp3-fill{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    width:100%; }
  .bp3-button-group .bp3-button.bp3-fill,
  .bp3-button-group.bp3-fill .bp3-button:not(.bp3-fixed){
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-button-group.bp3-vertical{
    -webkit-box-align:stretch;
        -ms-flex-align:stretch;
            align-items:stretch;
    -webkit-box-orient:vertical;
    -webkit-box-direction:normal;
        -ms-flex-direction:column;
            flex-direction:column;
    vertical-align:top; }
    .bp3-button-group.bp3-vertical.bp3-fill{
      height:100%;
      width:unset; }
    .bp3-button-group.bp3-vertical .bp3-button{
      margin-right:0 !important;
      width:100%; }
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:first-child .bp3-button,
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:first-child{
      border-radius:3px 3px 0 0; }
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:last-child .bp3-button,
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:last-child{
      border-radius:0 0 3px 3px; }
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button,
    .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:not(:last-child){
      margin-bottom:-1px; }
  .bp3-button-group.bp3-align-left .bp3-button{
    text-align:left; }
  .bp3-dark .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button,
  .bp3-dark .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:last-child){
    margin-right:1px; }
  .bp3-dark .bp3-button-group.bp3-vertical > .bp3-popover-wrapper:not(:last-child) .bp3-button,
  .bp3-dark .bp3-button-group.bp3-vertical > .bp3-button:not(:last-child){
    margin-bottom:1px; }
.bp3-callout{
  font-size:14px;
  line-height:1.5;
  background-color:rgba(138, 155, 168, 0.15);
  border-radius:3px;
  padding:10px 12px 9px;
  position:relative;
  width:100%; }
  .bp3-callout[class*="bp3-icon-"]{
    padding-left:40px; }
    .bp3-callout[class*="bp3-icon-"]::before{
      font-family:"Icons20", sans-serif;
      font-size:20px;
      font-style:normal;
      font-weight:400;
      line-height:1;
      -moz-osx-font-smoothing:grayscale;
      -webkit-font-smoothing:antialiased;
      color:#5c7080;
      left:10px;
      position:absolute;
      top:10px; }
  .bp3-callout.bp3-callout-icon{
    padding-left:40px; }
    .bp3-callout.bp3-callout-icon > .bp3-icon:first-child{
      color:#5c7080;
      left:10px;
      position:absolute;
      top:10px; }
  .bp3-callout .bp3-heading{
    line-height:20px;
    margin-bottom:5px;
    margin-top:0; }
    .bp3-callout .bp3-heading:last-child{
      margin-bottom:0; }
  .bp3-dark .bp3-callout{
    background-color:rgba(138, 155, 168, 0.2); }
    .bp3-dark .bp3-callout[class*="bp3-icon-"]::before{
      color:#a7b6c2; }
  .bp3-callout.bp3-intent-primary{
    background-color:rgba(19, 124, 189, 0.15); }
    .bp3-callout.bp3-intent-primary[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-primary > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-primary .bp3-heading{
      color:#106ba3; }
    .bp3-dark .bp3-callout.bp3-intent-primary{
      background-color:rgba(19, 124, 189, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-primary[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-primary > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-primary .bp3-heading{
        color:#48aff0; }
  .bp3-callout.bp3-intent-success{
    background-color:rgba(15, 153, 96, 0.15); }
    .bp3-callout.bp3-intent-success[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-success > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-success .bp3-heading{
      color:#0d8050; }
    .bp3-dark .bp3-callout.bp3-intent-success{
      background-color:rgba(15, 153, 96, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-success[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-success > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-success .bp3-heading{
        color:#3dcc91; }
  .bp3-callout.bp3-intent-warning{
    background-color:rgba(217, 130, 43, 0.15); }
    .bp3-callout.bp3-intent-warning[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-warning > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-warning .bp3-heading{
      color:#bf7326; }
    .bp3-dark .bp3-callout.bp3-intent-warning{
      background-color:rgba(217, 130, 43, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-warning[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-warning > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-warning .bp3-heading{
        color:#ffb366; }
  .bp3-callout.bp3-intent-danger{
    background-color:rgba(219, 55, 55, 0.15); }
    .bp3-callout.bp3-intent-danger[class*="bp3-icon-"]::before,
    .bp3-callout.bp3-intent-danger > .bp3-icon:first-child,
    .bp3-callout.bp3-intent-danger .bp3-heading{
      color:#c23030; }
    .bp3-dark .bp3-callout.bp3-intent-danger{
      background-color:rgba(219, 55, 55, 0.25); }
      .bp3-dark .bp3-callout.bp3-intent-danger[class*="bp3-icon-"]::before,
      .bp3-dark .bp3-callout.bp3-intent-danger > .bp3-icon:first-child,
      .bp3-dark .bp3-callout.bp3-intent-danger .bp3-heading{
        color:#ff7373; }
  .bp3-running-text .bp3-callout{
    margin:20px 0; }
.bp3-card{
  background-color:#ffffff;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
  padding:20px;
  -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-card.bp3-dark,
  .bp3-dark .bp3-card{
    background-color:#30404d;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); }

.bp3-elevation-0{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); }
  .bp3-elevation-0.bp3-dark,
  .bp3-dark .bp3-elevation-0{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); }

.bp3-elevation-1{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-1.bp3-dark,
  .bp3-dark .bp3-elevation-1{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-elevation-2{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 1px 1px rgba(16, 22, 26, 0.2), 0 2px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 1px 1px rgba(16, 22, 26, 0.2), 0 2px 6px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-2.bp3-dark,
  .bp3-dark .bp3-elevation-2{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.4), 0 2px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.4), 0 2px 6px rgba(16, 22, 26, 0.4); }

.bp3-elevation-3{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-3.bp3-dark,
  .bp3-dark .bp3-elevation-3{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }

.bp3-elevation-4{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); }
  .bp3-elevation-4.bp3-dark,
  .bp3-dark .bp3-elevation-4{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); }

.bp3-card.bp3-interactive:hover{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  cursor:pointer; }
  .bp3-card.bp3-interactive:hover.bp3-dark,
  .bp3-dark .bp3-card.bp3-interactive:hover{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }

.bp3-card.bp3-interactive:active{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
  opacity:0.9;
  -webkit-transition-duration:0;
          transition-duration:0; }
  .bp3-card.bp3-interactive:active.bp3-dark,
  .bp3-dark .bp3-card.bp3-interactive:active{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-collapse{
  height:0;
  overflow-y:hidden;
  -webkit-transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-collapse .bp3-collapse-body{
    -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-collapse .bp3-collapse-body[aria-hidden="true"]{
      display:none; }

.bp3-context-menu .bp3-popover-target{
  display:block; }

.bp3-context-menu-popover-target{
  position:fixed; }

.bp3-divider{
  border-bottom:1px solid rgba(16, 22, 26, 0.15);
  border-right:1px solid rgba(16, 22, 26, 0.15);
  margin:5px; }
  .bp3-dark .bp3-divider{
    border-color:rgba(16, 22, 26, 0.4); }
.bp3-dialog-container{
  opacity:1;
  -webkit-transform:scale(1);
          transform:scale(1);
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  min-height:100%;
  pointer-events:none;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none;
  width:100%; }
  .bp3-dialog-container.bp3-overlay-enter > .bp3-dialog, .bp3-dialog-container.bp3-overlay-appear > .bp3-dialog{
    opacity:0;
    -webkit-transform:scale(0.5);
            transform:scale(0.5); }
  .bp3-dialog-container.bp3-overlay-enter-active > .bp3-dialog, .bp3-dialog-container.bp3-overlay-appear-active > .bp3-dialog{
    opacity:1;
    -webkit-transform:scale(1);
            transform:scale(1);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-property:opacity, -webkit-transform;
    transition-property:opacity, -webkit-transform;
    transition-property:opacity, transform;
    transition-property:opacity, transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); }
  .bp3-dialog-container.bp3-overlay-exit > .bp3-dialog{
    opacity:1;
    -webkit-transform:scale(1);
            transform:scale(1); }
  .bp3-dialog-container.bp3-overlay-exit-active > .bp3-dialog{
    opacity:0;
    -webkit-transform:scale(0.5);
            transform:scale(0.5);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-property:opacity, -webkit-transform;
    transition-property:opacity, -webkit-transform;
    transition-property:opacity, transform;
    transition-property:opacity, transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); }

.bp3-dialog{
  background:#ebf1f5;
  border-radius:6px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin:30px 0;
  padding-bottom:20px;
  pointer-events:all;
  -webkit-user-select:text;
     -moz-user-select:text;
      -ms-user-select:text;
          user-select:text;
  width:500px; }
  .bp3-dialog:focus{
    outline:0; }
  .bp3-dialog.bp3-dark,
  .bp3-dark .bp3-dialog{
    background:#293742;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
    color:#f5f8fa; }

.bp3-dialog-header{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  background:#ffffff;
  border-radius:6px 6px 0 0;
  -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
          box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  min-height:40px;
  padding-left:20px;
  padding-right:5px; }
  .bp3-dialog-header .bp3-icon-large,
  .bp3-dialog-header .bp3-icon{
    color:#5c7080;
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    margin-right:10px; }
  .bp3-dialog-header .bp3-heading{
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    line-height:inherit;
    margin:0; }
    .bp3-dialog-header .bp3-heading:last-child{
      margin-right:20px; }
  .bp3-dark .bp3-dialog-header{
    background:#30404d;
    -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.4);
            box-shadow:0 1px 0 rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-dialog-header .bp3-icon-large,
    .bp3-dark .bp3-dialog-header .bp3-icon{
      color:#a7b6c2; }

.bp3-dialog-body{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  line-height:18px;
  margin:20px; }

.bp3-dialog-footer{
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  margin:0 20px; }

.bp3-dialog-footer-actions{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-pack:end;
      -ms-flex-pack:end;
          justify-content:flex-end; }
  .bp3-dialog-footer-actions .bp3-button{
    margin-left:10px; }
.bp3-drawer{
  background:#ffffff;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin:0;
  padding:0; }
  .bp3-drawer:focus{
    outline:0; }
  .bp3-drawer.bp3-position-top{
    height:50%;
    left:0;
    right:0;
    top:0; }
    .bp3-drawer.bp3-position-top.bp3-overlay-enter, .bp3-drawer.bp3-position-top.bp3-overlay-appear{
      -webkit-transform:translateY(-100%);
              transform:translateY(-100%); }
    .bp3-drawer.bp3-position-top.bp3-overlay-enter-active, .bp3-drawer.bp3-position-top.bp3-overlay-appear-active{
      -webkit-transform:translateY(0);
              transform:translateY(0);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-drawer.bp3-position-top.bp3-overlay-exit{
      -webkit-transform:translateY(0);
              transform:translateY(0); }
    .bp3-drawer.bp3-position-top.bp3-overlay-exit-active{
      -webkit-transform:translateY(-100%);
              transform:translateY(-100%);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-drawer.bp3-position-bottom{
    bottom:0;
    height:50%;
    left:0;
    right:0; }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-enter, .bp3-drawer.bp3-position-bottom.bp3-overlay-appear{
      -webkit-transform:translateY(100%);
              transform:translateY(100%); }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-enter-active, .bp3-drawer.bp3-position-bottom.bp3-overlay-appear-active{
      -webkit-transform:translateY(0);
              transform:translateY(0);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-exit{
      -webkit-transform:translateY(0);
              transform:translateY(0); }
    .bp3-drawer.bp3-position-bottom.bp3-overlay-exit-active{
      -webkit-transform:translateY(100%);
              transform:translateY(100%);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-drawer.bp3-position-left{
    bottom:0;
    left:0;
    top:0;
    width:50%; }
    .bp3-drawer.bp3-position-left.bp3-overlay-enter, .bp3-drawer.bp3-position-left.bp3-overlay-appear{
      -webkit-transform:translateX(-100%);
              transform:translateX(-100%); }
    .bp3-drawer.bp3-position-left.bp3-overlay-enter-active, .bp3-drawer.bp3-position-left.bp3-overlay-appear-active{
      -webkit-transform:translateX(0);
              transform:translateX(0);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-drawer.bp3-position-left.bp3-overlay-exit{
      -webkit-transform:translateX(0);
              transform:translateX(0); }
    .bp3-drawer.bp3-position-left.bp3-overlay-exit-active{
      -webkit-transform:translateX(-100%);
              transform:translateX(-100%);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-drawer.bp3-position-right{
    bottom:0;
    right:0;
    top:0;
    width:50%; }
    .bp3-drawer.bp3-position-right.bp3-overlay-enter, .bp3-drawer.bp3-position-right.bp3-overlay-appear{
      -webkit-transform:translateX(100%);
              transform:translateX(100%); }
    .bp3-drawer.bp3-position-right.bp3-overlay-enter-active, .bp3-drawer.bp3-position-right.bp3-overlay-appear-active{
      -webkit-transform:translateX(0);
              transform:translateX(0);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-drawer.bp3-position-right.bp3-overlay-exit{
      -webkit-transform:translateX(0);
              transform:translateX(0); }
    .bp3-drawer.bp3-position-right.bp3-overlay-exit-active{
      -webkit-transform:translateX(100%);
              transform:translateX(100%);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
  .bp3-position-right):not(.bp3-vertical){
    bottom:0;
    right:0;
    top:0;
    width:50%; }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-enter, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-appear{
      -webkit-transform:translateX(100%);
              transform:translateX(100%); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-enter-active, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-appear-active{
      -webkit-transform:translateX(0);
              transform:translateX(0);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-exit{
      -webkit-transform:translateX(0);
              transform:translateX(0); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right):not(.bp3-vertical).bp3-overlay-exit-active{
      -webkit-transform:translateX(100%);
              transform:translateX(100%);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
  .bp3-position-right).bp3-vertical{
    bottom:0;
    height:50%;
    left:0;
    right:0; }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-enter, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-appear{
      -webkit-transform:translateY(100%);
              transform:translateY(100%); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-enter-active, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-appear-active{
      -webkit-transform:translateY(0);
              transform:translateY(0);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:200ms;
              transition-duration:200ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-exit{
      -webkit-transform:translateY(0);
              transform:translateY(0); }
    .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(
    .bp3-position-right).bp3-vertical.bp3-overlay-exit-active{
      -webkit-transform:translateY(100%);
              transform:translateY(100%);
      -webkit-transition-delay:0;
              transition-delay:0;
      -webkit-transition-duration:100ms;
              transition-duration:100ms;
      -webkit-transition-property:-webkit-transform;
      transition-property:-webkit-transform;
      transition-property:transform;
      transition-property:transform, -webkit-transform;
      -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
              transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-drawer.bp3-dark,
  .bp3-dark .bp3-drawer{
    background:#30404d;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
    color:#f5f8fa; }

.bp3-drawer-header{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  border-radius:0;
  -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
          box-shadow:0 1px 0 rgba(16, 22, 26, 0.15);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  min-height:40px;
  padding:5px;
  padding-left:20px;
  position:relative; }
  .bp3-drawer-header .bp3-icon-large,
  .bp3-drawer-header .bp3-icon{
    color:#5c7080;
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    margin-right:10px; }
  .bp3-drawer-header .bp3-heading{
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    line-height:inherit;
    margin:0; }
    .bp3-drawer-header .bp3-heading:last-child{
      margin-right:20px; }
  .bp3-dark .bp3-drawer-header{
    -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.4);
            box-shadow:0 1px 0 rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-drawer-header .bp3-icon-large,
    .bp3-dark .bp3-drawer-header .bp3-icon{
      color:#a7b6c2; }

.bp3-drawer-body{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  line-height:18px;
  overflow:auto; }

.bp3-drawer-footer{
  -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
          box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  padding:10px 20px;
  position:relative; }
  .bp3-dark .bp3-drawer-footer{
    -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.4); }
.bp3-editable-text{
  cursor:text;
  display:inline-block;
  max-width:100%;
  position:relative;
  vertical-align:top;
  white-space:nowrap; }
  .bp3-editable-text::before{
    bottom:-3px;
    left:-3px;
    position:absolute;
    right:-3px;
    top:-3px;
    border-radius:3px;
    content:"";
    -webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-editable-text:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15); }
  .bp3-editable-text.bp3-editable-text-editing::before{
    background-color:#ffffff;
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-editable-text.bp3-disabled::before{
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-editable-text.bp3-intent-primary .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{
    color:#137cbd; }
  .bp3-editable-text.bp3-intent-primary:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(19, 124, 189, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(19, 124, 189, 0.4); }
  .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-editable-text.bp3-intent-success .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{
    color:#0f9960; }
  .bp3-editable-text.bp3-intent-success:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px rgba(15, 153, 96, 0.4);
            box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px rgba(15, 153, 96, 0.4); }
  .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-editable-text.bp3-intent-warning .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{
    color:#d9822b; }
  .bp3-editable-text.bp3-intent-warning:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px rgba(217, 130, 43, 0.4);
            box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px rgba(217, 130, 43, 0.4); }
  .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-editable-text.bp3-intent-danger .bp3-editable-text-input,
  .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{
    color:#db3737; }
  .bp3-editable-text.bp3-intent-danger:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px rgba(219, 55, 55, 0.4);
            box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px rgba(219, 55, 55, 0.4); }
  .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-editable-text:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(255, 255, 255, 0.15);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(255, 255, 255, 0.15); }
  .bp3-dark .bp3-editable-text.bp3-editable-text-editing::before{
    background-color:rgba(16, 22, 26, 0.3);
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-disabled::before{
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-dark .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{
    color:#48aff0; }
  .bp3-dark .bp3-editable-text.bp3-intent-primary:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(72, 175, 240, 0), 0 0 0 0 rgba(72, 175, 240, 0), inset 0 0 0 1px rgba(72, 175, 240, 0.4);
            box-shadow:0 0 0 0 rgba(72, 175, 240, 0), 0 0 0 0 rgba(72, 175, 240, 0), inset 0 0 0 1px rgba(72, 175, 240, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #48aff0, 0 0 0 3px rgba(72, 175, 240, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #48aff0, 0 0 0 3px rgba(72, 175, 240, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{
    color:#3dcc91; }
  .bp3-dark .bp3-editable-text.bp3-intent-success:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(61, 204, 145, 0), 0 0 0 0 rgba(61, 204, 145, 0), inset 0 0 0 1px rgba(61, 204, 145, 0.4);
            box-shadow:0 0 0 0 rgba(61, 204, 145, 0), 0 0 0 0 rgba(61, 204, 145, 0), inset 0 0 0 1px rgba(61, 204, 145, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #3dcc91, 0 0 0 3px rgba(61, 204, 145, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #3dcc91, 0 0 0 3px rgba(61, 204, 145, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{
    color:#ffb366; }
  .bp3-dark .bp3-editable-text.bp3-intent-warning:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(255, 179, 102, 0), 0 0 0 0 rgba(255, 179, 102, 0), inset 0 0 0 1px rgba(255, 179, 102, 0.4);
            box-shadow:0 0 0 0 rgba(255, 179, 102, 0), 0 0 0 0 rgba(255, 179, 102, 0), inset 0 0 0 1px rgba(255, 179, 102, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #ffb366, 0 0 0 3px rgba(255, 179, 102, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #ffb366, 0 0 0 3px rgba(255, 179, 102, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{
    color:#ff7373; }
  .bp3-dark .bp3-editable-text.bp3-intent-danger:hover::before{
    -webkit-box-shadow:0 0 0 0 rgba(255, 115, 115, 0), 0 0 0 0 rgba(255, 115, 115, 0), inset 0 0 0 1px rgba(255, 115, 115, 0.4);
            box-shadow:0 0 0 0 rgba(255, 115, 115, 0), 0 0 0 0 rgba(255, 115, 115, 0), inset 0 0 0 1px rgba(255, 115, 115, 0.4); }
  .bp3-dark .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{
    -webkit-box-shadow:0 0 0 1px #ff7373, 0 0 0 3px rgba(255, 115, 115, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #ff7373, 0 0 0 3px rgba(255, 115, 115, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-editable-text-input,
.bp3-editable-text-content{
  color:inherit;
  display:inherit;
  font:inherit;
  letter-spacing:inherit;
  max-width:inherit;
  min-width:inherit;
  position:relative;
  resize:none;
  text-transform:inherit;
  vertical-align:top; }

.bp3-editable-text-input{
  background:none;
  border:none;
  -webkit-box-shadow:none;
          box-shadow:none;
  padding:0;
  white-space:pre-wrap;
  width:100%; }
  .bp3-editable-text-input::-webkit-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-editable-text-input::-moz-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-editable-text-input:-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-editable-text-input::-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-editable-text-input::placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-editable-text-input:focus{
    outline:none; }
  .bp3-editable-text-input::-ms-clear{
    display:none; }

.bp3-editable-text-content{
  overflow:hidden;
  padding-right:2px;
  text-overflow:ellipsis;
  white-space:pre; }
  .bp3-editable-text-editing > .bp3-editable-text-content{
    left:0;
    position:absolute;
    visibility:hidden; }
  .bp3-editable-text-placeholder > .bp3-editable-text-content{
    color:rgba(92, 112, 128, 0.6); }
    .bp3-dark .bp3-editable-text-placeholder > .bp3-editable-text-content{
      color:rgba(167, 182, 194, 0.6); }

.bp3-editable-text.bp3-multiline{
  display:block; }
  .bp3-editable-text.bp3-multiline .bp3-editable-text-content{
    overflow:auto;
    white-space:pre-wrap;
    word-wrap:break-word; }
.bp3-divider{
  border-bottom:1px solid rgba(16, 22, 26, 0.15);
  border-right:1px solid rgba(16, 22, 26, 0.15);
  margin:5px; }
  .bp3-dark .bp3-divider{
    border-color:rgba(16, 22, 26, 0.4); }
.bp3-control-group{
  -webkit-transform:translateZ(0);
          transform:translateZ(0);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:stretch;
      -ms-flex-align:stretch;
          align-items:stretch; }
  .bp3-control-group > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-control-group > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-control-group .bp3-button,
  .bp3-control-group .bp3-html-select,
  .bp3-control-group .bp3-input,
  .bp3-control-group .bp3-select{
    position:relative; }
  .bp3-control-group .bp3-input{
    border-radius:inherit;
    z-index:2; }
    .bp3-control-group .bp3-input:focus{
      border-radius:3px;
      z-index:14; }
    .bp3-control-group .bp3-input[class*="bp3-intent"]{
      z-index:13; }
      .bp3-control-group .bp3-input[class*="bp3-intent"]:focus{
        z-index:15; }
    .bp3-control-group .bp3-input[readonly], .bp3-control-group .bp3-input:disabled, .bp3-control-group .bp3-input.bp3-disabled{
      z-index:1; }
  .bp3-control-group .bp3-input-group[class*="bp3-intent"] .bp3-input{
    z-index:13; }
    .bp3-control-group .bp3-input-group[class*="bp3-intent"] .bp3-input:focus{
      z-index:15; }
  .bp3-control-group .bp3-button,
  .bp3-control-group .bp3-html-select select,
  .bp3-control-group .bp3-select select{
    -webkit-transform:translateZ(0);
            transform:translateZ(0);
    border-radius:inherit;
    z-index:4; }
    .bp3-control-group .bp3-button:focus,
    .bp3-control-group .bp3-html-select select:focus,
    .bp3-control-group .bp3-select select:focus{
      z-index:5; }
    .bp3-control-group .bp3-button:hover,
    .bp3-control-group .bp3-html-select select:hover,
    .bp3-control-group .bp3-select select:hover{
      z-index:6; }
    .bp3-control-group .bp3-button:active,
    .bp3-control-group .bp3-html-select select:active,
    .bp3-control-group .bp3-select select:active{
      z-index:7; }
    .bp3-control-group .bp3-button[readonly], .bp3-control-group .bp3-button:disabled, .bp3-control-group .bp3-button.bp3-disabled,
    .bp3-control-group .bp3-html-select select[readonly],
    .bp3-control-group .bp3-html-select select:disabled,
    .bp3-control-group .bp3-html-select select.bp3-disabled,
    .bp3-control-group .bp3-select select[readonly],
    .bp3-control-group .bp3-select select:disabled,
    .bp3-control-group .bp3-select select.bp3-disabled{
      z-index:3; }
    .bp3-control-group .bp3-button[class*="bp3-intent"],
    .bp3-control-group .bp3-html-select select[class*="bp3-intent"],
    .bp3-control-group .bp3-select select[class*="bp3-intent"]{
      z-index:9; }
      .bp3-control-group .bp3-button[class*="bp3-intent"]:focus,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:focus,
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:focus{
        z-index:10; }
      .bp3-control-group .bp3-button[class*="bp3-intent"]:hover,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:hover,
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:hover{
        z-index:11; }
      .bp3-control-group .bp3-button[class*="bp3-intent"]:active,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:active,
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:active{
        z-index:12; }
      .bp3-control-group .bp3-button[class*="bp3-intent"][readonly], .bp3-control-group .bp3-button[class*="bp3-intent"]:disabled, .bp3-control-group .bp3-button[class*="bp3-intent"].bp3-disabled,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"][readonly],
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"]:disabled,
      .bp3-control-group .bp3-html-select select[class*="bp3-intent"].bp3-disabled,
      .bp3-control-group .bp3-select select[class*="bp3-intent"][readonly],
      .bp3-control-group .bp3-select select[class*="bp3-intent"]:disabled,
      .bp3-control-group .bp3-select select[class*="bp3-intent"].bp3-disabled{
        z-index:8; }
  .bp3-control-group .bp3-input-group > .bp3-icon,
  .bp3-control-group .bp3-input-group > .bp3-button,
  .bp3-control-group .bp3-input-group > .bp3-input-action{
    z-index:16; }
  .bp3-control-group .bp3-select::after,
  .bp3-control-group .bp3-html-select::after,
  .bp3-control-group .bp3-select > .bp3-icon,
  .bp3-control-group .bp3-html-select > .bp3-icon{
    z-index:17; }
  .bp3-control-group .bp3-select:focus-within{
    z-index:5; }
  .bp3-control-group:not(.bp3-vertical) > *:not(.bp3-divider){
    margin-right:-1px; }
  .bp3-control-group:not(.bp3-vertical) > .bp3-divider:not(:first-child){
    margin-left:6px; }
  .bp3-dark .bp3-control-group:not(.bp3-vertical) > *:not(.bp3-divider){
    margin-right:0; }
  .bp3-dark .bp3-control-group:not(.bp3-vertical) > .bp3-button + .bp3-button{
    margin-left:1px; }
  .bp3-control-group .bp3-popover-wrapper,
  .bp3-control-group .bp3-popover-target{
    border-radius:inherit; }
  .bp3-control-group > :first-child{
    border-radius:3px 0 0 3px; }
  .bp3-control-group > :last-child{
    border-radius:0 3px 3px 0;
    margin-right:0; }
  .bp3-control-group > :only-child{
    border-radius:3px;
    margin-right:0; }
  .bp3-control-group .bp3-input-group .bp3-button{
    border-radius:3px; }
  .bp3-control-group .bp3-numeric-input:not(:first-child) .bp3-input-group{
    border-bottom-left-radius:0;
    border-top-left-radius:0; }
  .bp3-control-group.bp3-fill{
    width:100%; }
  .bp3-control-group > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-control-group.bp3-fill > *:not(.bp3-fixed){
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto; }
  .bp3-control-group.bp3-vertical{
    -webkit-box-orient:vertical;
    -webkit-box-direction:normal;
        -ms-flex-direction:column;
            flex-direction:column; }
    .bp3-control-group.bp3-vertical > *{
      margin-top:-1px; }
    .bp3-control-group.bp3-vertical > :first-child{
      border-radius:3px 3px 0 0;
      margin-top:0; }
    .bp3-control-group.bp3-vertical > :last-child{
      border-radius:0 0 3px 3px; }
.bp3-control{
  cursor:pointer;
  display:block;
  margin-bottom:10px;
  position:relative;
  text-transform:none; }
  .bp3-control input:checked ~ .bp3-control-indicator{
    background-color:#137cbd;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    color:#ffffff; }
  .bp3-control:hover input:checked ~ .bp3-control-indicator{
    background-color:#106ba3;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); }
  .bp3-control input:not(:disabled):active:checked ~ .bp3-control-indicator{
    background:#0e5a8a;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-control input:disabled:checked ~ .bp3-control-indicator{
    background:rgba(19, 124, 189, 0.5);
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-dark .bp3-control input:checked ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control:hover input:checked ~ .bp3-control-indicator{
    background-color:#106ba3;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control input:not(:disabled):active:checked ~ .bp3-control-indicator{
    background-color:#0e5a8a;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-control input:disabled:checked ~ .bp3-control-indicator{
    background:rgba(14, 90, 138, 0.5);
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-control:not(.bp3-align-right){
    padding-left:26px; }
    .bp3-control:not(.bp3-align-right) .bp3-control-indicator{
      margin-left:-26px; }
  .bp3-control.bp3-align-right{
    padding-right:26px; }
    .bp3-control.bp3-align-right .bp3-control-indicator{
      margin-right:-26px; }
  .bp3-control.bp3-disabled{
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed; }
  .bp3-control.bp3-inline{
    display:inline-block;
    margin-right:20px; }
  .bp3-control input{
    left:0;
    opacity:0;
    position:absolute;
    top:0;
    z-index:-1; }
  .bp3-control .bp3-control-indicator{
    background-clip:padding-box;
    background-color:#f5f8fa;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
    border:none;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    cursor:pointer;
    display:inline-block;
    font-size:16px;
    height:1em;
    margin-right:10px;
    margin-top:-3px;
    position:relative;
    -webkit-user-select:none;
       -moz-user-select:none;
        -ms-user-select:none;
            user-select:none;
    vertical-align:middle;
    width:1em; }
    .bp3-control .bp3-control-indicator::before{
      content:"";
      display:block;
      height:1em;
      width:1em; }
  .bp3-control:hover .bp3-control-indicator{
    background-color:#ebf1f5; }
  .bp3-control input:not(:disabled):active ~ .bp3-control-indicator{
    background:#d8e1e8;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-control input:disabled ~ .bp3-control-indicator{
    background:rgba(206, 217, 224, 0.5);
    -webkit-box-shadow:none;
            box-shadow:none;
    cursor:not-allowed; }
  .bp3-control input:focus ~ .bp3-control-indicator{
    outline:rgba(19, 124, 189, 0.6) auto 2px;
    outline-offset:2px;
    -moz-outline-radius:6px; }
  .bp3-control.bp3-align-right .bp3-control-indicator{
    float:right;
    margin-left:10px;
    margin-top:1px; }
  .bp3-control.bp3-large{
    font-size:16px; }
    .bp3-control.bp3-large:not(.bp3-align-right){
      padding-left:30px; }
      .bp3-control.bp3-large:not(.bp3-align-right) .bp3-control-indicator{
        margin-left:-30px; }
    .bp3-control.bp3-large.bp3-align-right{
      padding-right:30px; }
      .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{
        margin-right:-30px; }
    .bp3-control.bp3-large .bp3-control-indicator{
      font-size:20px; }
    .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{
      margin-top:0; }
  .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator{
    background-color:#137cbd;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
    color:#ffffff; }
  .bp3-control.bp3-checkbox:hover input:indeterminate ~ .bp3-control-indicator{
    background-color:#106ba3;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); }
  .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate ~ .bp3-control-indicator{
    background:#0e5a8a;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{
    background:rgba(19, 124, 189, 0.5);
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-dark .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-checkbox:hover input:indeterminate ~ .bp3-control-indicator{
    background-color:#106ba3;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate ~ .bp3-control-indicator{
    background-color:#0e5a8a;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{
    background:rgba(14, 90, 138, 0.5);
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-control.bp3-checkbox .bp3-control-indicator{
    border-radius:3px; }
  .bp3-control.bp3-checkbox input:checked ~ .bp3-control-indicator::before{
    background-image:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M12 5c-.28 0-.53.11-.71.29L7 9.59l-2.29-2.3a1.003 1.003 0 00-1.42 1.42l3 3c.18.18.43.29.71.29s.53-.11.71-.29l5-5A1.003 1.003 0 0012 5z' fill='white'/%3e%3c/svg%3e"); }
  .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator::before{
    background-image:url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M11 7H5c-.55 0-1 .45-1 1s.45 1 1 1h6c.55 0 1-.45 1-1s-.45-1-1-1z' fill='white'/%3e%3c/svg%3e"); }
  .bp3-control.bp3-radio .bp3-control-indicator{
    border-radius:50%; }
  .bp3-control.bp3-radio input:checked ~ .bp3-control-indicator::before{
    background-image:radial-gradient(#ffffff, #ffffff 28%, transparent 32%); }
  .bp3-control.bp3-radio input:checked:disabled ~ .bp3-control-indicator::before{
    opacity:0.5; }
  .bp3-control.bp3-radio input:focus ~ .bp3-control-indicator{
    -moz-outline-radius:16px; }
  .bp3-control.bp3-switch input ~ .bp3-control-indicator{
    background:rgba(167, 182, 194, 0.5); }
  .bp3-control.bp3-switch:hover input ~ .bp3-control-indicator{
    background:rgba(115, 134, 148, 0.5); }
  .bp3-control.bp3-switch input:not(:disabled):active ~ .bp3-control-indicator{
    background:rgba(92, 112, 128, 0.5); }
  .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator{
    background:rgba(206, 217, 224, 0.5); }
    .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator::before{
      background:rgba(255, 255, 255, 0.8); }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator{
    background:#137cbd; }
  .bp3-control.bp3-switch:hover input:checked ~ .bp3-control-indicator{
    background:#106ba3; }
  .bp3-control.bp3-switch input:checked:not(:disabled):active ~ .bp3-control-indicator{
    background:#0e5a8a; }
  .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator{
    background:rgba(19, 124, 189, 0.5); }
    .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator::before{
      background:rgba(255, 255, 255, 0.8); }
  .bp3-control.bp3-switch:not(.bp3-align-right){
    padding-left:38px; }
    .bp3-control.bp3-switch:not(.bp3-align-right) .bp3-control-indicator{
      margin-left:-38px; }
  .bp3-control.bp3-switch.bp3-align-right{
    padding-right:38px; }
    .bp3-control.bp3-switch.bp3-align-right .bp3-control-indicator{
      margin-right:-38px; }
  .bp3-control.bp3-switch .bp3-control-indicator{
    border:none;
    border-radius:1.75em;
    -webkit-box-shadow:none !important;
            box-shadow:none !important;
    min-width:1.75em;
    -webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
    width:auto; }
    .bp3-control.bp3-switch .bp3-control-indicator::before{
      background:#ffffff;
      border-radius:50%;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
      height:calc(1em - 4px);
      left:0;
      margin:2px;
      position:absolute;
      -webkit-transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
      transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
      width:calc(1em - 4px); }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator::before{
    left:calc(100% - 1em); }
  .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right){
    padding-left:45px; }
    .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right) .bp3-control-indicator{
      margin-left:-45px; }
  .bp3-control.bp3-switch.bp3-large.bp3-align-right{
    padding-right:45px; }
    .bp3-control.bp3-switch.bp3-large.bp3-align-right .bp3-control-indicator{
      margin-right:-45px; }
  .bp3-dark .bp3-control.bp3-switch input ~ .bp3-control-indicator{
    background:rgba(16, 22, 26, 0.5); }
  .bp3-dark .bp3-control.bp3-switch:hover input ~ .bp3-control-indicator{
    background:rgba(16, 22, 26, 0.7); }
  .bp3-dark .bp3-control.bp3-switch input:not(:disabled):active ~ .bp3-control-indicator{
    background:rgba(16, 22, 26, 0.9); }
  .bp3-dark .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator{
    background:rgba(57, 75, 89, 0.5); }
    .bp3-dark .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator::before{
      background:rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator{
    background:#137cbd; }
  .bp3-dark .bp3-control.bp3-switch:hover input:checked ~ .bp3-control-indicator{
    background:#106ba3; }
  .bp3-dark .bp3-control.bp3-switch input:checked:not(:disabled):active ~ .bp3-control-indicator{
    background:#0e5a8a; }
  .bp3-dark .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator{
    background:rgba(14, 90, 138, 0.5); }
    .bp3-dark .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator::before{
      background:rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-switch .bp3-control-indicator::before{
    background:#394b59;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator::before{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-control.bp3-switch .bp3-switch-inner-text{
    font-size:0.7em;
    text-align:center; }
  .bp3-control.bp3-switch .bp3-control-indicator-child:first-child{
    line-height:0;
    margin-left:0.5em;
    margin-right:1.2em;
    visibility:hidden; }
  .bp3-control.bp3-switch .bp3-control-indicator-child:last-child{
    line-height:1em;
    margin-left:1.2em;
    margin-right:0.5em;
    visibility:visible; }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator .bp3-control-indicator-child:first-child{
    line-height:1em;
    visibility:visible; }
  .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator .bp3-control-indicator-child:last-child{
    line-height:0;
    visibility:hidden; }
  .bp3-dark .bp3-control{
    color:#f5f8fa; }
    .bp3-dark .bp3-control.bp3-disabled{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-control .bp3-control-indicator{
      background-color:#394b59;
      background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
      background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-control:hover .bp3-control-indicator{
      background-color:#30404d; }
    .bp3-dark .bp3-control input:not(:disabled):active ~ .bp3-control-indicator{
      background:#202b33;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-dark .bp3-control input:disabled ~ .bp3-control-indicator{
      background:rgba(57, 75, 89, 0.5);
      -webkit-box-shadow:none;
              box-shadow:none;
      cursor:not-allowed; }
    .bp3-dark .bp3-control.bp3-checkbox input:disabled:checked ~ .bp3-control-indicator, .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{
      color:rgba(167, 182, 194, 0.6); }
.bp3-file-input{
  cursor:pointer;
  display:inline-block;
  height:30px;
  position:relative; }
  .bp3-file-input input{
    margin:0;
    min-width:200px;
    opacity:0; }
    .bp3-file-input input:disabled + .bp3-file-upload-input,
    .bp3-file-input input.bp3-disabled + .bp3-file-upload-input{
      background:rgba(206, 217, 224, 0.5);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(92, 112, 128, 0.6);
      cursor:not-allowed;
      resize:none; }
      .bp3-file-input input:disabled + .bp3-file-upload-input::after,
      .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after{
        background-color:rgba(206, 217, 224, 0.5);
        background-image:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:rgba(92, 112, 128, 0.6);
        cursor:not-allowed;
        outline:none; }
        .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active, .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active:hover,
        .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active,
        .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active:hover{
          background:rgba(206, 217, 224, 0.7); }
      .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input, .bp3-dark
      .bp3-file-input input.bp3-disabled + .bp3-file-upload-input{
        background:rgba(57, 75, 89, 0.5);
        -webkit-box-shadow:none;
                box-shadow:none;
        color:rgba(167, 182, 194, 0.6); }
        .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input::after, .bp3-dark
        .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after{
          background-color:rgba(57, 75, 89, 0.5);
          background-image:none;
          -webkit-box-shadow:none;
                  box-shadow:none;
          color:rgba(167, 182, 194, 0.6); }
          .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active, .bp3-dark
          .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active{
            background:rgba(57, 75, 89, 0.7); }
  .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{
    color:#182026; }
  .bp3-dark .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{
    color:#f5f8fa; }
  .bp3-file-input.bp3-fill{
    width:100%; }
  .bp3-file-input.bp3-large,
  .bp3-large .bp3-file-input{
    height:40px; }
  .bp3-file-input .bp3-file-upload-input-custom-text::after{
    content:attr(bp3-button-text); }

.bp3-file-upload-input{
  -webkit-appearance:none;
     -moz-appearance:none;
          appearance:none;
  background:#ffffff;
  border:none;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
  color:#182026;
  font-size:14px;
  font-weight:400;
  height:30px;
  line-height:30px;
  outline:none;
  padding:0 10px;
  -webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  vertical-align:middle;
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal;
  color:rgba(92, 112, 128, 0.6);
  left:0;
  padding-right:80px;
  position:absolute;
  right:0;
  top:0;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-file-upload-input::-webkit-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-file-upload-input::-moz-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-file-upload-input:-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-file-upload-input::-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-file-upload-input::placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-file-upload-input:focus, .bp3-file-upload-input.bp3-active{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-file-upload-input[type="search"], .bp3-file-upload-input.bp3-round{
    border-radius:30px;
    -webkit-box-sizing:border-box;
            box-sizing:border-box;
    padding-left:10px; }
  .bp3-file-upload-input[readonly]{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); }
  .bp3-file-upload-input:disabled, .bp3-file-upload-input.bp3-disabled{
    background:rgba(206, 217, 224, 0.5);
    -webkit-box-shadow:none;
            box-shadow:none;
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed;
    resize:none; }
  .bp3-file-upload-input::after{
    background-color:#f5f8fa;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    color:#182026;
    min-height:24px;
    min-width:24px;
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    border-radius:3px;
    content:"Browse";
    line-height:24px;
    margin:3px;
    position:absolute;
    right:0;
    text-align:center;
    top:0;
    width:70px; }
    .bp3-file-upload-input::after:hover{
      background-clip:padding-box;
      background-color:#ebf1f5;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); }
    .bp3-file-upload-input::after:active, .bp3-file-upload-input::after.bp3-active{
      background-color:#d8e1e8;
      background-image:none;
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-file-upload-input::after:disabled, .bp3-file-upload-input::after.bp3-disabled{
      background-color:rgba(206, 217, 224, 0.5);
      background-image:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(92, 112, 128, 0.6);
      cursor:not-allowed;
      outline:none; }
      .bp3-file-upload-input::after:disabled.bp3-active, .bp3-file-upload-input::after:disabled.bp3-active:hover, .bp3-file-upload-input::after.bp3-disabled.bp3-active, .bp3-file-upload-input::after.bp3-disabled.bp3-active:hover{
        background:rgba(206, 217, 224, 0.7); }
  .bp3-file-upload-input:hover::after{
    background-clip:padding-box;
    background-color:#ebf1f5;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); }
  .bp3-file-upload-input:active::after{
    background-color:#d8e1e8;
    background-image:none;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-large .bp3-file-upload-input{
    font-size:16px;
    height:40px;
    line-height:40px;
    padding-right:95px; }
    .bp3-large .bp3-file-upload-input[type="search"], .bp3-large .bp3-file-upload-input.bp3-round{
      padding:0 15px; }
    .bp3-large .bp3-file-upload-input::after{
      min-height:30px;
      min-width:30px;
      line-height:30px;
      margin:5px;
      width:85px; }
  .bp3-dark .bp3-file-upload-input{
    background:rgba(16, 22, 26, 0.3);
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    color:#f5f8fa;
    color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-file-upload-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-file-upload-input:disabled, .bp3-dark .bp3-file-upload-input.bp3-disabled{
      background:rgba(57, 75, 89, 0.5);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-file-upload-input::after{
      background-color:#394b59;
      background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
      background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
      color:#f5f8fa; }
      .bp3-dark .bp3-file-upload-input::after:hover, .bp3-dark .bp3-file-upload-input::after:active, .bp3-dark .bp3-file-upload-input::after.bp3-active{
        color:#f5f8fa; }
      .bp3-dark .bp3-file-upload-input::after:hover{
        background-color:#30404d;
        -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-file-upload-input::after:active, .bp3-dark .bp3-file-upload-input::after.bp3-active{
        background-color:#202b33;
        background-image:none;
        -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
                box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
      .bp3-dark .bp3-file-upload-input::after:disabled, .bp3-dark .bp3-file-upload-input::after.bp3-disabled{
        background-color:rgba(57, 75, 89, 0.5);
        background-image:none;
        -webkit-box-shadow:none;
                box-shadow:none;
        color:rgba(167, 182, 194, 0.6); }
        .bp3-dark .bp3-file-upload-input::after:disabled.bp3-active, .bp3-dark .bp3-file-upload-input::after.bp3-disabled.bp3-active{
          background:rgba(57, 75, 89, 0.7); }
      .bp3-dark .bp3-file-upload-input::after .bp3-button-spinner .bp3-spinner-head{
        background:rgba(16, 22, 26, 0.5);
        stroke:#8a9ba8; }
    .bp3-dark .bp3-file-upload-input:hover::after{
      background-color:#30404d;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-file-upload-input:active::after{
      background-color:#202b33;
      background-image:none;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
.bp3-file-upload-input::after{
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); }
.bp3-form-group{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin:0 0 15px; }
  .bp3-form-group label.bp3-label{
    margin-bottom:5px; }
  .bp3-form-group .bp3-control{
    margin-top:7px; }
  .bp3-form-group .bp3-form-helper-text{
    color:#5c7080;
    font-size:12px;
    margin-top:5px; }
  .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{
    color:#106ba3; }
  .bp3-form-group.bp3-intent-success .bp3-form-helper-text{
    color:#0d8050; }
  .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{
    color:#bf7326; }
  .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{
    color:#c23030; }
  .bp3-form-group.bp3-inline{
    -webkit-box-align:start;
        -ms-flex-align:start;
            align-items:flex-start;
    -webkit-box-orient:horizontal;
    -webkit-box-direction:normal;
        -ms-flex-direction:row;
            flex-direction:row; }
    .bp3-form-group.bp3-inline.bp3-large label.bp3-label{
      line-height:40px;
      margin:0 10px 0 0; }
    .bp3-form-group.bp3-inline label.bp3-label{
      line-height:30px;
      margin:0 10px 0 0; }
  .bp3-form-group.bp3-disabled .bp3-label,
  .bp3-form-group.bp3-disabled .bp3-text-muted,
  .bp3-form-group.bp3-disabled .bp3-form-helper-text{
    color:rgba(92, 112, 128, 0.6) !important; }
  .bp3-dark .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{
    color:#48aff0; }
  .bp3-dark .bp3-form-group.bp3-intent-success .bp3-form-helper-text{
    color:#3dcc91; }
  .bp3-dark .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{
    color:#ffb366; }
  .bp3-dark .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{
    color:#ff7373; }
  .bp3-dark .bp3-form-group .bp3-form-helper-text{
    color:#a7b6c2; }
  .bp3-dark .bp3-form-group.bp3-disabled .bp3-label,
  .bp3-dark .bp3-form-group.bp3-disabled .bp3-text-muted,
  .bp3-dark .bp3-form-group.bp3-disabled .bp3-form-helper-text{
    color:rgba(167, 182, 194, 0.6) !important; }
.bp3-input-group{
  display:block;
  position:relative; }
  .bp3-input-group .bp3-input{
    position:relative;
    width:100%; }
    .bp3-input-group .bp3-input:not(:first-child){
      padding-left:30px; }
    .bp3-input-group .bp3-input:not(:last-child){
      padding-right:30px; }
  .bp3-input-group .bp3-input-action,
  .bp3-input-group > .bp3-input-left-container,
  .bp3-input-group > .bp3-button,
  .bp3-input-group > .bp3-icon{
    position:absolute;
    top:0; }
    .bp3-input-group .bp3-input-action:first-child,
    .bp3-input-group > .bp3-input-left-container:first-child,
    .bp3-input-group > .bp3-button:first-child,
    .bp3-input-group > .bp3-icon:first-child{
      left:0; }
    .bp3-input-group .bp3-input-action:last-child,
    .bp3-input-group > .bp3-input-left-container:last-child,
    .bp3-input-group > .bp3-button:last-child,
    .bp3-input-group > .bp3-icon:last-child{
      right:0; }
  .bp3-input-group .bp3-button{
    min-height:24px;
    min-width:24px;
    margin:3px;
    padding:0 7px; }
    .bp3-input-group .bp3-button:empty{
      padding:0; }
  .bp3-input-group > .bp3-input-left-container,
  .bp3-input-group > .bp3-icon{
    z-index:1; }
  .bp3-input-group > .bp3-input-left-container > .bp3-icon,
  .bp3-input-group > .bp3-icon{
    color:#5c7080; }
    .bp3-input-group > .bp3-input-left-container > .bp3-icon:empty,
    .bp3-input-group > .bp3-icon:empty{
      font-family:"Icons16", sans-serif;
      font-size:16px;
      font-style:normal;
      font-weight:400;
      line-height:1;
      -moz-osx-font-smoothing:grayscale;
      -webkit-font-smoothing:antialiased; }
  .bp3-input-group > .bp3-input-left-container > .bp3-icon,
  .bp3-input-group > .bp3-icon,
  .bp3-input-group .bp3-input-action > .bp3-spinner{
    margin:7px; }
  .bp3-input-group .bp3-tag{
    margin:5px; }
  .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus),
  .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){
    color:#5c7080; }
    .bp3-dark .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus), .bp3-dark
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){
      color:#a7b6c2; }
    .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large{
      color:#5c7080; }
  .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled,
  .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled{
    color:rgba(92, 112, 128, 0.6) !important; }
    .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon-large,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-standard,
    .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-large{
      color:rgba(92, 112, 128, 0.6) !important; }
  .bp3-input-group.bp3-disabled{
    cursor:not-allowed; }
    .bp3-input-group.bp3-disabled .bp3-icon{
      color:rgba(92, 112, 128, 0.6); }
  .bp3-input-group.bp3-large .bp3-button{
    min-height:30px;
    min-width:30px;
    margin:5px; }
  .bp3-input-group.bp3-large > .bp3-input-left-container > .bp3-icon,
  .bp3-input-group.bp3-large > .bp3-icon,
  .bp3-input-group.bp3-large .bp3-input-action > .bp3-spinner{
    margin:12px; }
  .bp3-input-group.bp3-large .bp3-input{
    font-size:16px;
    height:40px;
    line-height:40px; }
    .bp3-input-group.bp3-large .bp3-input[type="search"], .bp3-input-group.bp3-large .bp3-input.bp3-round{
      padding:0 15px; }
    .bp3-input-group.bp3-large .bp3-input:not(:first-child){
      padding-left:40px; }
    .bp3-input-group.bp3-large .bp3-input:not(:last-child){
      padding-right:40px; }
  .bp3-input-group.bp3-small .bp3-button{
    min-height:20px;
    min-width:20px;
    margin:2px; }
  .bp3-input-group.bp3-small .bp3-tag{
    min-height:20px;
    min-width:20px;
    margin:2px; }
  .bp3-input-group.bp3-small > .bp3-input-left-container > .bp3-icon,
  .bp3-input-group.bp3-small > .bp3-icon,
  .bp3-input-group.bp3-small .bp3-input-action > .bp3-spinner{
    margin:4px; }
  .bp3-input-group.bp3-small .bp3-input{
    font-size:12px;
    height:24px;
    line-height:24px;
    padding-left:8px;
    padding-right:8px; }
    .bp3-input-group.bp3-small .bp3-input[type="search"], .bp3-input-group.bp3-small .bp3-input.bp3-round{
      padding:0 12px; }
    .bp3-input-group.bp3-small .bp3-input:not(:first-child){
      padding-left:24px; }
    .bp3-input-group.bp3-small .bp3-input:not(:last-child){
      padding-right:24px; }
  .bp3-input-group.bp3-fill{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    width:100%; }
  .bp3-input-group.bp3-round .bp3-button,
  .bp3-input-group.bp3-round .bp3-input,
  .bp3-input-group.bp3-round .bp3-tag{
    border-radius:30px; }
  .bp3-dark .bp3-input-group .bp3-icon{
    color:#a7b6c2; }
  .bp3-dark .bp3-input-group.bp3-disabled .bp3-icon{
    color:rgba(167, 182, 194, 0.6); }
  .bp3-input-group.bp3-intent-primary .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-primary .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-primary .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #137cbd;
              box-shadow:inset 0 0 0 1px #137cbd; }
    .bp3-input-group.bp3-intent-primary .bp3-input:disabled, .bp3-input-group.bp3-intent-primary .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-primary > .bp3-icon{
    color:#106ba3; }
    .bp3-dark .bp3-input-group.bp3-intent-primary > .bp3-icon{
      color:#48aff0; }
  .bp3-input-group.bp3-intent-success .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-success .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-success .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #0f9960;
              box-shadow:inset 0 0 0 1px #0f9960; }
    .bp3-input-group.bp3-intent-success .bp3-input:disabled, .bp3-input-group.bp3-intent-success .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-success > .bp3-icon{
    color:#0d8050; }
    .bp3-dark .bp3-input-group.bp3-intent-success > .bp3-icon{
      color:#3dcc91; }
  .bp3-input-group.bp3-intent-warning .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-warning .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-warning .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #d9822b;
              box-shadow:inset 0 0 0 1px #d9822b; }
    .bp3-input-group.bp3-intent-warning .bp3-input:disabled, .bp3-input-group.bp3-intent-warning .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-warning > .bp3-icon{
    color:#bf7326; }
    .bp3-dark .bp3-input-group.bp3-intent-warning > .bp3-icon{
      color:#ffb366; }
  .bp3-input-group.bp3-intent-danger .bp3-input{
    -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-danger .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input-group.bp3-intent-danger .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #db3737;
              box-shadow:inset 0 0 0 1px #db3737; }
    .bp3-input-group.bp3-intent-danger .bp3-input:disabled, .bp3-input-group.bp3-intent-danger .bp3-input.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-input-group.bp3-intent-danger > .bp3-icon{
    color:#c23030; }
    .bp3-dark .bp3-input-group.bp3-intent-danger > .bp3-icon{
      color:#ff7373; }
.bp3-input{
  -webkit-appearance:none;
     -moz-appearance:none;
          appearance:none;
  background:#ffffff;
  border:none;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
  color:#182026;
  font-size:14px;
  font-weight:400;
  height:30px;
  line-height:30px;
  outline:none;
  padding:0 10px;
  -webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);
  vertical-align:middle; }
  .bp3-input::-webkit-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input::-moz-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input:-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input::-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input::placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input:focus, .bp3-input.bp3-active{
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-input[type="search"], .bp3-input.bp3-round{
    border-radius:30px;
    -webkit-box-sizing:border-box;
            box-sizing:border-box;
    padding-left:10px; }
  .bp3-input[readonly]{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); }
  .bp3-input:disabled, .bp3-input.bp3-disabled{
    background:rgba(206, 217, 224, 0.5);
    -webkit-box-shadow:none;
            box-shadow:none;
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed;
    resize:none; }
  .bp3-input.bp3-large{
    font-size:16px;
    height:40px;
    line-height:40px; }
    .bp3-input.bp3-large[type="search"], .bp3-input.bp3-large.bp3-round{
      padding:0 15px; }
  .bp3-input.bp3-small{
    font-size:12px;
    height:24px;
    line-height:24px;
    padding-left:8px;
    padding-right:8px; }
    .bp3-input.bp3-small[type="search"], .bp3-input.bp3-small.bp3-round{
      padding:0 12px; }
  .bp3-input.bp3-fill{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    width:100%; }
  .bp3-dark .bp3-input{
    background:rgba(16, 22, 26, 0.3);
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    color:#f5f8fa; }
    .bp3-dark .bp3-input::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input::placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-input:disabled, .bp3-dark .bp3-input.bp3-disabled{
      background:rgba(57, 75, 89, 0.5);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(167, 182, 194, 0.6); }
  .bp3-input.bp3-intent-primary{
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-primary:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-primary[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #137cbd;
              box-shadow:inset 0 0 0 1px #137cbd; }
    .bp3-input.bp3-intent-primary:disabled, .bp3-input.bp3-intent-primary.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-primary{
      -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-primary:focus{
        -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-primary[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #137cbd;
                box-shadow:inset 0 0 0 1px #137cbd; }
      .bp3-dark .bp3-input.bp3-intent-primary:disabled, .bp3-dark .bp3-input.bp3-intent-primary.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input.bp3-intent-success{
    -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-success:focus{
      -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-success[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #0f9960;
              box-shadow:inset 0 0 0 1px #0f9960; }
    .bp3-input.bp3-intent-success:disabled, .bp3-input.bp3-intent-success.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-success{
      -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-success:focus{
        -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #0f9960, 0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-success[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #0f9960;
                box-shadow:inset 0 0 0 1px #0f9960; }
      .bp3-dark .bp3-input.bp3-intent-success:disabled, .bp3-dark .bp3-input.bp3-intent-success.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input.bp3-intent-warning{
    -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-warning:focus{
      -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-warning[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #d9822b;
              box-shadow:inset 0 0 0 1px #d9822b; }
    .bp3-input.bp3-intent-warning:disabled, .bp3-input.bp3-intent-warning.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-warning{
      -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-warning:focus{
        -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #d9822b, 0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-warning[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #d9822b;
                box-shadow:inset 0 0 0 1px #d9822b; }
      .bp3-dark .bp3-input.bp3-intent-warning:disabled, .bp3-dark .bp3-input.bp3-intent-warning.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input.bp3-intent-danger{
    -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-danger:focus{
      -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-input.bp3-intent-danger[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px #db3737;
              box-shadow:inset 0 0 0 1px #db3737; }
    .bp3-input.bp3-intent-danger:disabled, .bp3-input.bp3-intent-danger.bp3-disabled{
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-input.bp3-intent-danger{
      -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-danger:focus{
        -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
                box-shadow:0 0 0 1px #db3737, 0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
      .bp3-dark .bp3-input.bp3-intent-danger[readonly]{
        -webkit-box-shadow:inset 0 0 0 1px #db3737;
                box-shadow:inset 0 0 0 1px #db3737; }
      .bp3-dark .bp3-input.bp3-intent-danger:disabled, .bp3-dark .bp3-input.bp3-intent-danger.bp3-disabled{
        -webkit-box-shadow:none;
                box-shadow:none; }
  .bp3-input::-ms-clear{
    display:none; }
textarea.bp3-input{
  max-width:100%;
  padding:10px; }
  textarea.bp3-input, textarea.bp3-input.bp3-large, textarea.bp3-input.bp3-small{
    height:auto;
    line-height:inherit; }
  textarea.bp3-input.bp3-small{
    padding:8px; }
  .bp3-dark textarea.bp3-input{
    background:rgba(16, 22, 26, 0.3);
    -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
    color:#f5f8fa; }
    .bp3-dark textarea.bp3-input::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input::placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark textarea.bp3-input:focus{
      -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark textarea.bp3-input[readonly]{
      -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark textarea.bp3-input:disabled, .bp3-dark textarea.bp3-input.bp3-disabled{
      background:rgba(57, 75, 89, 0.5);
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(167, 182, 194, 0.6); }
label.bp3-label{
  display:block;
  margin-bottom:15px;
  margin-top:0; }
  label.bp3-label .bp3-html-select,
  label.bp3-label .bp3-input,
  label.bp3-label .bp3-select,
  label.bp3-label .bp3-slider,
  label.bp3-label .bp3-popover-wrapper{
    display:block;
    margin-top:5px;
    text-transform:none; }
  label.bp3-label .bp3-button-group{
    margin-top:5px; }
  label.bp3-label .bp3-select select,
  label.bp3-label .bp3-html-select select{
    font-weight:400;
    vertical-align:top;
    width:100%; }
  label.bp3-label.bp3-disabled,
  label.bp3-label.bp3-disabled .bp3-text-muted{
    color:rgba(92, 112, 128, 0.6); }
  label.bp3-label.bp3-inline{
    line-height:30px; }
    label.bp3-label.bp3-inline .bp3-html-select,
    label.bp3-label.bp3-inline .bp3-input,
    label.bp3-label.bp3-inline .bp3-input-group,
    label.bp3-label.bp3-inline .bp3-select,
    label.bp3-label.bp3-inline .bp3-popover-wrapper{
      display:inline-block;
      margin:0 0 0 5px;
      vertical-align:top; }
    label.bp3-label.bp3-inline .bp3-button-group{
      margin:0 0 0 5px; }
    label.bp3-label.bp3-inline .bp3-input-group .bp3-input{
      margin-left:0; }
    label.bp3-label.bp3-inline.bp3-large{
      line-height:40px; }
  label.bp3-label:not(.bp3-inline) .bp3-popover-target{
    display:block; }
  .bp3-dark label.bp3-label{
    color:#f5f8fa; }
    .bp3-dark label.bp3-label.bp3-disabled,
    .bp3-dark label.bp3-label.bp3-disabled .bp3-text-muted{
      color:rgba(167, 182, 194, 0.6); }
.bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button{
  -webkit-box-flex:1;
      -ms-flex:1 1 14px;
          flex:1 1 14px;
  min-height:0;
  padding:0;
  width:30px; }
  .bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button:first-child{
    border-radius:0 3px 0 0; }
  .bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button:last-child{
    border-radius:0 0 3px 0; }

.bp3-numeric-input .bp3-button-group.bp3-vertical:first-child > .bp3-button:first-child{
  border-radius:3px 0 0 0; }

.bp3-numeric-input .bp3-button-group.bp3-vertical:first-child > .bp3-button:last-child{
  border-radius:0 0 0 3px; }

.bp3-numeric-input.bp3-large .bp3-button-group.bp3-vertical > .bp3-button{
  width:40px; }

form{
  display:block; }
.bp3-html-select select,
.bp3-select select{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  border:none;
  border-radius:3px;
  cursor:pointer;
  font-size:14px;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  padding:5px 10px;
  text-align:left;
  vertical-align:middle;
  background-color:#f5f8fa;
  background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
  background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
  color:#182026;
  -moz-appearance:none;
  -webkit-appearance:none;
  border-radius:3px;
  height:30px;
  padding:0 25px 0 10px;
  width:100%; }
  .bp3-html-select select > *, .bp3-select select > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-html-select select > .bp3-fill, .bp3-select select > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-html-select select::before,
  .bp3-select select::before, .bp3-html-select select > *, .bp3-select select > *{
    margin-right:7px; }
  .bp3-html-select select:empty::before,
  .bp3-select select:empty::before,
  .bp3-html-select select > :last-child,
  .bp3-select select > :last-child{
    margin-right:0; }
  .bp3-html-select select:hover,
  .bp3-select select:hover{
    background-clip:padding-box;
    background-color:#ebf1f5;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); }
  .bp3-html-select select:active,
  .bp3-select select:active, .bp3-html-select select.bp3-active,
  .bp3-select select.bp3-active{
    background-color:#d8e1e8;
    background-image:none;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-html-select select:disabled,
  .bp3-select select:disabled, .bp3-html-select select.bp3-disabled,
  .bp3-select select.bp3-disabled{
    background-color:rgba(206, 217, 224, 0.5);
    background-image:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed;
    outline:none; }
    .bp3-html-select select:disabled.bp3-active,
    .bp3-select select:disabled.bp3-active, .bp3-html-select select:disabled.bp3-active:hover,
    .bp3-select select:disabled.bp3-active:hover, .bp3-html-select select.bp3-disabled.bp3-active,
    .bp3-select select.bp3-disabled.bp3-active, .bp3-html-select select.bp3-disabled.bp3-active:hover,
    .bp3-select select.bp3-disabled.bp3-active:hover{
      background:rgba(206, 217, 224, 0.7); }

.bp3-html-select.bp3-minimal select,
.bp3-select.bp3-minimal select{
  background:none;
  -webkit-box-shadow:none;
          box-shadow:none; }
  .bp3-html-select.bp3-minimal select:hover,
  .bp3-select.bp3-minimal select:hover{
    background:rgba(167, 182, 194, 0.3);
    -webkit-box-shadow:none;
            box-shadow:none;
    color:#182026;
    text-decoration:none; }
  .bp3-html-select.bp3-minimal select:active,
  .bp3-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal select.bp3-active,
  .bp3-select.bp3-minimal select.bp3-active{
    background:rgba(115, 134, 148, 0.3);
    -webkit-box-shadow:none;
            box-shadow:none;
    color:#182026; }
  .bp3-html-select.bp3-minimal select:disabled,
  .bp3-select.bp3-minimal select:disabled, .bp3-html-select.bp3-minimal select:disabled:hover,
  .bp3-select.bp3-minimal select:disabled:hover, .bp3-html-select.bp3-minimal select.bp3-disabled,
  .bp3-select.bp3-minimal select.bp3-disabled, .bp3-html-select.bp3-minimal select.bp3-disabled:hover,
  .bp3-select.bp3-minimal select.bp3-disabled:hover{
    background:none;
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed; }
    .bp3-html-select.bp3-minimal select:disabled.bp3-active,
    .bp3-select.bp3-minimal select:disabled.bp3-active, .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active,
    .bp3-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active,
    .bp3-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active,
    .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active{
      background:rgba(115, 134, 148, 0.3); }
  .bp3-dark .bp3-html-select.bp3-minimal select, .bp3-html-select.bp3-minimal .bp3-dark select,
  .bp3-dark .bp3-select.bp3-minimal select, .bp3-select.bp3-minimal .bp3-dark select{
    background:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    color:inherit; }
    .bp3-dark .bp3-html-select.bp3-minimal select:hover, .bp3-html-select.bp3-minimal .bp3-dark select:hover,
    .bp3-dark .bp3-select.bp3-minimal select:hover, .bp3-select.bp3-minimal .bp3-dark select:hover, .bp3-dark .bp3-html-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal .bp3-dark select:active,
    .bp3-dark .bp3-select.bp3-minimal select:active, .bp3-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-active{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none; }
    .bp3-dark .bp3-html-select.bp3-minimal select:hover, .bp3-html-select.bp3-minimal .bp3-dark select:hover,
    .bp3-dark .bp3-select.bp3-minimal select:hover, .bp3-select.bp3-minimal .bp3-dark select:hover{
      background:rgba(138, 155, 168, 0.15); }
    .bp3-dark .bp3-html-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal .bp3-dark select:active,
    .bp3-dark .bp3-select.bp3-minimal select:active, .bp3-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-active{
      background:rgba(138, 155, 168, 0.3);
      color:#f5f8fa; }
    .bp3-dark .bp3-html-select.bp3-minimal select:disabled, .bp3-html-select.bp3-minimal .bp3-dark select:disabled,
    .bp3-dark .bp3-select.bp3-minimal select:disabled, .bp3-select.bp3-minimal .bp3-dark select:disabled, .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover, .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover,
    .bp3-dark .bp3-select.bp3-minimal select:disabled:hover, .bp3-select.bp3-minimal .bp3-dark select:disabled:hover, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover{
      background:none;
      color:rgba(167, 182, 194, 0.6);
      cursor:not-allowed; }
      .bp3-dark .bp3-html-select.bp3-minimal select:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select:disabled.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active{
        background:rgba(138, 155, 168, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-primary,
  .bp3-select.bp3-minimal select.bp3-intent-primary{
    color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,
    .bp3-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,
    .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,
    .bp3-select.bp3-minimal select.bp3-intent-primary:hover{
      background:rgba(19, 124, 189, 0.15);
      color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,
    .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{
      background:rgba(19, 124, 189, 0.3);
      color:#106ba3; }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled{
      background:none;
      color:rgba(16, 107, 163, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active{
        background:rgba(19, 124, 189, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{
      stroke:#106ba3; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary{
      color:#48aff0; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover{
        background:rgba(19, 124, 189, 0.2);
        color:#48aff0; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active{
        background:rgba(19, 124, 189, 0.3);
        color:#48aff0; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled{
        background:none;
        color:rgba(72, 175, 240, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active{
          background:rgba(19, 124, 189, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-success,
  .bp3-select.bp3-minimal select.bp3-intent-success{
    color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,
    .bp3-select.bp3-minimal select.bp3-intent-success:hover, .bp3-html-select.bp3-minimal select.bp3-intent-success:active,
    .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,
    .bp3-select.bp3-minimal select.bp3-intent-success:hover{
      background:rgba(15, 153, 96, 0.15);
      color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:active,
    .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{
      background:rgba(15, 153, 96, 0.3);
      color:#0d8050; }
    .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled{
      background:none;
      color:rgba(13, 128, 80, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active{
        background:rgba(15, 153, 96, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{
      stroke:#0d8050; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success{
      color:#3dcc91; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover{
        background:rgba(15, 153, 96, 0.2);
        color:#3dcc91; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active{
        background:rgba(15, 153, 96, 0.3);
        color:#3dcc91; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled{
        background:none;
        color:rgba(61, 204, 145, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active{
          background:rgba(15, 153, 96, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-warning,
  .bp3-select.bp3-minimal select.bp3-intent-warning{
    color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,
    .bp3-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,
    .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,
    .bp3-select.bp3-minimal select.bp3-intent-warning:hover{
      background:rgba(217, 130, 43, 0.15);
      color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,
    .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{
      background:rgba(217, 130, 43, 0.3);
      color:#bf7326; }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled{
      background:none;
      color:rgba(191, 115, 38, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active{
        background:rgba(217, 130, 43, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{
      stroke:#bf7326; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning{
      color:#ffb366; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover{
        background:rgba(217, 130, 43, 0.2);
        color:#ffb366; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active{
        background:rgba(217, 130, 43, 0.3);
        color:#ffb366; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled{
        background:none;
        color:rgba(255, 179, 102, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active{
          background:rgba(217, 130, 43, 0.3); }
  .bp3-html-select.bp3-minimal select.bp3-intent-danger,
  .bp3-select.bp3-minimal select.bp3-intent-danger{
    color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,
    .bp3-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,
    .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{
      background:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,
    .bp3-select.bp3-minimal select.bp3-intent-danger:hover{
      background:rgba(219, 55, 55, 0.15);
      color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,
    .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,
    .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{
      background:rgba(219, 55, 55, 0.3);
      color:#c23030; }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled,
    .bp3-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,
    .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled{
      background:none;
      color:rgba(194, 48, 48, 0.5); }
      .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,
      .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active{
        background:rgba(219, 55, 55, 0.3); }
    .bp3-html-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{
      stroke:#c23030; }
    .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger,
    .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger{
      color:#ff7373; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover{
        background:rgba(219, 55, 55, 0.2);
        color:#ff7373; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active{
        background:rgba(219, 55, 55, 0.3);
        color:#ff7373; }
      .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled,
      .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled{
        background:none;
        color:rgba(255, 115, 115, 0.5); }
        .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active,
        .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active{
          background:rgba(219, 55, 55, 0.3); }

.bp3-html-select.bp3-large select,
.bp3-select.bp3-large select{
  font-size:16px;
  height:40px;
  padding-right:35px; }

.bp3-dark .bp3-html-select select, .bp3-dark .bp3-select select{
  background-color:#394b59;
  background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
  background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
  color:#f5f8fa; }
  .bp3-dark .bp3-html-select select:hover, .bp3-dark .bp3-select select:hover, .bp3-dark .bp3-html-select select:active, .bp3-dark .bp3-select select:active, .bp3-dark .bp3-html-select select.bp3-active, .bp3-dark .bp3-select select.bp3-active{
    color:#f5f8fa; }
  .bp3-dark .bp3-html-select select:hover, .bp3-dark .bp3-select select:hover{
    background-color:#30404d;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-html-select select:active, .bp3-dark .bp3-select select:active, .bp3-dark .bp3-html-select select.bp3-active, .bp3-dark .bp3-select select.bp3-active{
    background-color:#202b33;
    background-image:none;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-html-select select:disabled, .bp3-dark .bp3-select select:disabled, .bp3-dark .bp3-html-select select.bp3-disabled, .bp3-dark .bp3-select select.bp3-disabled{
    background-color:rgba(57, 75, 89, 0.5);
    background-image:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-html-select select:disabled.bp3-active, .bp3-dark .bp3-select select:disabled.bp3-active, .bp3-dark .bp3-html-select select.bp3-disabled.bp3-active, .bp3-dark .bp3-select select.bp3-disabled.bp3-active{
      background:rgba(57, 75, 89, 0.7); }
  .bp3-dark .bp3-html-select select .bp3-button-spinner .bp3-spinner-head, .bp3-dark .bp3-select select .bp3-button-spinner .bp3-spinner-head{
    background:rgba(16, 22, 26, 0.5);
    stroke:#8a9ba8; }

.bp3-html-select select:disabled,
.bp3-select select:disabled{
  background-color:rgba(206, 217, 224, 0.5);
  -webkit-box-shadow:none;
          box-shadow:none;
  color:rgba(92, 112, 128, 0.6);
  cursor:not-allowed; }

.bp3-html-select .bp3-icon,
.bp3-select .bp3-icon, .bp3-select::after{
  color:#5c7080;
  pointer-events:none;
  position:absolute;
  right:7px;
  top:7px; }
  .bp3-html-select .bp3-disabled.bp3-icon,
  .bp3-select .bp3-disabled.bp3-icon, .bp3-disabled.bp3-select::after{
    color:rgba(92, 112, 128, 0.6); }
.bp3-html-select,
.bp3-select{
  display:inline-block;
  letter-spacing:normal;
  position:relative;
  vertical-align:middle; }
  .bp3-html-select select::-ms-expand,
  .bp3-select select::-ms-expand{
    display:none; }
  .bp3-html-select .bp3-icon,
  .bp3-select .bp3-icon{
    color:#5c7080; }
    .bp3-html-select .bp3-icon:hover,
    .bp3-select .bp3-icon:hover{
      color:#182026; }
    .bp3-dark .bp3-html-select .bp3-icon, .bp3-dark
    .bp3-select .bp3-icon{
      color:#a7b6c2; }
      .bp3-dark .bp3-html-select .bp3-icon:hover, .bp3-dark
      .bp3-select .bp3-icon:hover{
        color:#f5f8fa; }
  .bp3-html-select.bp3-large::after,
  .bp3-html-select.bp3-large .bp3-icon,
  .bp3-select.bp3-large::after,
  .bp3-select.bp3-large .bp3-icon{
    right:12px;
    top:12px; }
  .bp3-html-select.bp3-fill,
  .bp3-html-select.bp3-fill select,
  .bp3-select.bp3-fill,
  .bp3-select.bp3-fill select{
    width:100%; }
  .bp3-dark .bp3-html-select option, .bp3-dark
  .bp3-select option{
    background-color:#30404d;
    color:#f5f8fa; }
  .bp3-dark .bp3-html-select option:disabled, .bp3-dark
  .bp3-select option:disabled{
    color:rgba(167, 182, 194, 0.6); }
  .bp3-dark .bp3-html-select::after, .bp3-dark
  .bp3-select::after{
    color:#a7b6c2; }

.bp3-select::after{
  font-family:"Icons16", sans-serif;
  font-size:16px;
  font-style:normal;
  font-weight:400;
  line-height:1;
  -moz-osx-font-smoothing:grayscale;
  -webkit-font-smoothing:antialiased;
  content:""; }
.bp3-running-text table, table.bp3-html-table{
  border-spacing:0;
  font-size:14px; }
  .bp3-running-text table th, table.bp3-html-table th,
  .bp3-running-text table td,
  table.bp3-html-table td{
    padding:11px;
    text-align:left;
    vertical-align:top; }
  .bp3-running-text table th, table.bp3-html-table th{
    color:#182026;
    font-weight:600; }
  
  .bp3-running-text table td,
  table.bp3-html-table td{
    color:#182026; }
  .bp3-running-text table tbody tr:first-child th, table.bp3-html-table tbody tr:first-child th,
  .bp3-running-text table tbody tr:first-child td,
  table.bp3-html-table tbody tr:first-child td{
    -webkit-box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); }
  .bp3-dark .bp3-running-text table th, .bp3-running-text .bp3-dark table th, .bp3-dark table.bp3-html-table th{
    color:#f5f8fa; }
  .bp3-dark .bp3-running-text table td, .bp3-running-text .bp3-dark table td, .bp3-dark table.bp3-html-table td{
    color:#f5f8fa; }
  .bp3-dark .bp3-running-text table tbody tr:first-child th, .bp3-running-text .bp3-dark table tbody tr:first-child th, .bp3-dark table.bp3-html-table tbody tr:first-child th,
  .bp3-dark .bp3-running-text table tbody tr:first-child td,
  .bp3-running-text .bp3-dark table tbody tr:first-child td,
  .bp3-dark table.bp3-html-table tbody tr:first-child td{
    -webkit-box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15);
            box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); }

table.bp3-html-table.bp3-html-table-condensed th,
table.bp3-html-table.bp3-html-table-condensed td, table.bp3-html-table.bp3-small th,
table.bp3-html-table.bp3-small td{
  padding-bottom:6px;
  padding-top:6px; }

table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{
  background:rgba(191, 204, 214, 0.15); }

table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){
  -webkit-box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15);
          box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); }

table.bp3-html-table.bp3-html-table-bordered tbody tr td{
  -webkit-box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15);
          box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); }
  table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){
    -webkit-box-shadow:inset 1px 1px 0 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 1px 1px 0 0 rgba(16, 22, 26, 0.15); }

table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{
  -webkit-box-shadow:none;
          box-shadow:none; }
  table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:not(:first-child){
    -webkit-box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); }

table.bp3-html-table.bp3-interactive tbody tr:hover td{
  background-color:rgba(191, 204, 214, 0.3);
  cursor:pointer; }

table.bp3-html-table.bp3-interactive tbody tr:active td{
  background-color:rgba(191, 204, 214, 0.4); }

.bp3-dark table.bp3-html-table{ }
  .bp3-dark table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{
    background:rgba(92, 112, 128, 0.15); }
  .bp3-dark table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){
    -webkit-box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15);
            box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); }
  .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td{
    -webkit-box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15);
            box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); }
    .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){
      -webkit-box-shadow:inset 1px 1px 0 0 rgba(255, 255, 255, 0.15);
              box-shadow:inset 1px 1px 0 0 rgba(255, 255, 255, 0.15); }
  .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{
    -webkit-box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15);
            box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); }
    .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:first-child{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:hover td{
    background-color:rgba(92, 112, 128, 0.3);
    cursor:pointer; }
  .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:active td{
    background-color:rgba(92, 112, 128, 0.4); }

.bp3-key-combo{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center; }
  .bp3-key-combo > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-key-combo > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-key-combo::before,
  .bp3-key-combo > *{
    margin-right:5px; }
  .bp3-key-combo:empty::before,
  .bp3-key-combo > :last-child{
    margin-right:0; }

.bp3-hotkey-dialog{
  padding-bottom:0;
  top:40px; }
  .bp3-hotkey-dialog .bp3-dialog-body{
    margin:0;
    padding:0; }
  .bp3-hotkey-dialog .bp3-hotkey-label{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1; }

.bp3-hotkey-column{
  margin:auto;
  max-height:80vh;
  overflow-y:auto;
  padding:30px; }
  .bp3-hotkey-column .bp3-heading{
    margin-bottom:20px; }
    .bp3-hotkey-column .bp3-heading:not(:first-child){
      margin-top:40px; }

.bp3-hotkey{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-pack:justify;
      -ms-flex-pack:justify;
          justify-content:space-between;
  margin-left:0;
  margin-right:0; }
  .bp3-hotkey:not(:last-child){
    margin-bottom:10px; }
.bp3-icon{
  display:inline-block;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  vertical-align:text-bottom; }
  .bp3-icon:not(:empty)::before{
    content:"" !important;
    content:unset !important; }
  .bp3-icon > svg{
    display:block; }
    .bp3-icon > svg:not([fill]){
      fill:currentColor; }

.bp3-icon.bp3-intent-primary, .bp3-icon-standard.bp3-intent-primary, .bp3-icon-large.bp3-intent-primary{
  color:#106ba3; }
  .bp3-dark .bp3-icon.bp3-intent-primary, .bp3-dark .bp3-icon-standard.bp3-intent-primary, .bp3-dark .bp3-icon-large.bp3-intent-primary{
    color:#48aff0; }

.bp3-icon.bp3-intent-success, .bp3-icon-standard.bp3-intent-success, .bp3-icon-large.bp3-intent-success{
  color:#0d8050; }
  .bp3-dark .bp3-icon.bp3-intent-success, .bp3-dark .bp3-icon-standard.bp3-intent-success, .bp3-dark .bp3-icon-large.bp3-intent-success{
    color:#3dcc91; }

.bp3-icon.bp3-intent-warning, .bp3-icon-standard.bp3-intent-warning, .bp3-icon-large.bp3-intent-warning{
  color:#bf7326; }
  .bp3-dark .bp3-icon.bp3-intent-warning, .bp3-dark .bp3-icon-standard.bp3-intent-warning, .bp3-dark .bp3-icon-large.bp3-intent-warning{
    color:#ffb366; }

.bp3-icon.bp3-intent-danger, .bp3-icon-standard.bp3-intent-danger, .bp3-icon-large.bp3-intent-danger{
  color:#c23030; }
  .bp3-dark .bp3-icon.bp3-intent-danger, .bp3-dark .bp3-icon-standard.bp3-intent-danger, .bp3-dark .bp3-icon-large.bp3-intent-danger{
    color:#ff7373; }

span.bp3-icon-standard{
  font-family:"Icons16", sans-serif;
  font-size:16px;
  font-style:normal;
  font-weight:400;
  line-height:1;
  -moz-osx-font-smoothing:grayscale;
  -webkit-font-smoothing:antialiased;
  display:inline-block; }

span.bp3-icon-large{
  font-family:"Icons20", sans-serif;
  font-size:20px;
  font-style:normal;
  font-weight:400;
  line-height:1;
  -moz-osx-font-smoothing:grayscale;
  -webkit-font-smoothing:antialiased;
  display:inline-block; }

span.bp3-icon:empty{
  font-family:"Icons20";
  font-size:inherit;
  font-style:normal;
  font-weight:400;
  line-height:1; }
  span.bp3-icon:empty::before{
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased; }

.bp3-icon-add::before{
  content:""; }

.bp3-icon-add-column-left::before{
  content:""; }

.bp3-icon-add-column-right::before{
  content:""; }

.bp3-icon-add-row-bottom::before{
  content:""; }

.bp3-icon-add-row-top::before{
  content:""; }

.bp3-icon-add-to-artifact::before{
  content:""; }

.bp3-icon-add-to-folder::before{
  content:""; }

.bp3-icon-airplane::before{
  content:""; }

.bp3-icon-align-center::before{
  content:""; }

.bp3-icon-align-justify::before{
  content:""; }

.bp3-icon-align-left::before{
  content:""; }

.bp3-icon-align-right::before{
  content:""; }

.bp3-icon-alignment-bottom::before{
  content:""; }

.bp3-icon-alignment-horizontal-center::before{
  content:""; }

.bp3-icon-alignment-left::before{
  content:""; }

.bp3-icon-alignment-right::before{
  content:""; }

.bp3-icon-alignment-top::before{
  content:""; }

.bp3-icon-alignment-vertical-center::before{
  content:""; }

.bp3-icon-annotation::before{
  content:""; }

.bp3-icon-application::before{
  content:""; }

.bp3-icon-applications::before{
  content:""; }

.bp3-icon-archive::before{
  content:""; }

.bp3-icon-arrow-bottom-left::before{
  content:"↙"; }

.bp3-icon-arrow-bottom-right::before{
  content:"↘"; }

.bp3-icon-arrow-down::before{
  content:"↓"; }

.bp3-icon-arrow-left::before{
  content:"←"; }

.bp3-icon-arrow-right::before{
  content:"→"; }

.bp3-icon-arrow-top-left::before{
  content:"↖"; }

.bp3-icon-arrow-top-right::before{
  content:"↗"; }

.bp3-icon-arrow-up::before{
  content:"↑"; }

.bp3-icon-arrows-horizontal::before{
  content:"↔"; }

.bp3-icon-arrows-vertical::before{
  content:"↕"; }

.bp3-icon-asterisk::before{
  content:"*"; }

.bp3-icon-automatic-updates::before{
  content:""; }

.bp3-icon-badge::before{
  content:""; }

.bp3-icon-ban-circle::before{
  content:""; }

.bp3-icon-bank-account::before{
  content:""; }

.bp3-icon-barcode::before{
  content:""; }

.bp3-icon-blank::before{
  content:""; }

.bp3-icon-blocked-person::before{
  content:""; }

.bp3-icon-bold::before{
  content:""; }

.bp3-icon-book::before{
  content:""; }

.bp3-icon-bookmark::before{
  content:""; }

.bp3-icon-box::before{
  content:""; }

.bp3-icon-briefcase::before{
  content:""; }

.bp3-icon-bring-data::before{
  content:""; }

.bp3-icon-build::before{
  content:""; }

.bp3-icon-calculator::before{
  content:""; }

.bp3-icon-calendar::before{
  content:""; }

.bp3-icon-camera::before{
  content:""; }

.bp3-icon-caret-down::before{
  content:"⌄"; }

.bp3-icon-caret-left::before{
  content:"〈"; }

.bp3-icon-caret-right::before{
  content:"〉"; }

.bp3-icon-caret-up::before{
  content:"⌃"; }

.bp3-icon-cell-tower::before{
  content:""; }

.bp3-icon-changes::before{
  content:""; }

.bp3-icon-chart::before{
  content:""; }

.bp3-icon-chat::before{
  content:""; }

.bp3-icon-chevron-backward::before{
  content:""; }

.bp3-icon-chevron-down::before{
  content:""; }

.bp3-icon-chevron-forward::before{
  content:""; }

.bp3-icon-chevron-left::before{
  content:""; }

.bp3-icon-chevron-right::before{
  content:""; }

.bp3-icon-chevron-up::before{
  content:""; }

.bp3-icon-circle::before{
  content:""; }

.bp3-icon-circle-arrow-down::before{
  content:""; }

.bp3-icon-circle-arrow-left::before{
  content:""; }

.bp3-icon-circle-arrow-right::before{
  content:""; }

.bp3-icon-circle-arrow-up::before{
  content:""; }

.bp3-icon-citation::before{
  content:""; }

.bp3-icon-clean::before{
  content:""; }

.bp3-icon-clipboard::before{
  content:""; }

.bp3-icon-cloud::before{
  content:"☁"; }

.bp3-icon-cloud-download::before{
  content:""; }

.bp3-icon-cloud-upload::before{
  content:""; }

.bp3-icon-code::before{
  content:""; }

.bp3-icon-code-block::before{
  content:""; }

.bp3-icon-cog::before{
  content:""; }

.bp3-icon-collapse-all::before{
  content:""; }

.bp3-icon-column-layout::before{
  content:""; }

.bp3-icon-comment::before{
  content:""; }

.bp3-icon-comparison::before{
  content:""; }

.bp3-icon-compass::before{
  content:""; }

.bp3-icon-compressed::before{
  content:""; }

.bp3-icon-confirm::before{
  content:""; }

.bp3-icon-console::before{
  content:""; }

.bp3-icon-contrast::before{
  content:""; }

.bp3-icon-control::before{
  content:""; }

.bp3-icon-credit-card::before{
  content:""; }

.bp3-icon-cross::before{
  content:"✗"; }

.bp3-icon-crown::before{
  content:""; }

.bp3-icon-cube::before{
  content:""; }

.bp3-icon-cube-add::before{
  content:""; }

.bp3-icon-cube-remove::before{
  content:""; }

.bp3-icon-curved-range-chart::before{
  content:""; }

.bp3-icon-cut::before{
  content:""; }

.bp3-icon-dashboard::before{
  content:""; }

.bp3-icon-data-lineage::before{
  content:""; }

.bp3-icon-database::before{
  content:""; }

.bp3-icon-delete::before{
  content:""; }

.bp3-icon-delta::before{
  content:"Δ"; }

.bp3-icon-derive-column::before{
  content:""; }

.bp3-icon-desktop::before{
  content:""; }

.bp3-icon-diagnosis::before{
  content:""; }

.bp3-icon-diagram-tree::before{
  content:""; }

.bp3-icon-direction-left::before{
  content:""; }

.bp3-icon-direction-right::before{
  content:""; }

.bp3-icon-disable::before{
  content:""; }

.bp3-icon-document::before{
  content:""; }

.bp3-icon-document-open::before{
  content:""; }

.bp3-icon-document-share::before{
  content:""; }

.bp3-icon-dollar::before{
  content:"$"; }

.bp3-icon-dot::before{
  content:"•"; }

.bp3-icon-double-caret-horizontal::before{
  content:""; }

.bp3-icon-double-caret-vertical::before{
  content:""; }

.bp3-icon-double-chevron-down::before{
  content:""; }

.bp3-icon-double-chevron-left::before{
  content:""; }

.bp3-icon-double-chevron-right::before{
  content:""; }

.bp3-icon-double-chevron-up::before{
  content:""; }

.bp3-icon-doughnut-chart::before{
  content:""; }

.bp3-icon-download::before{
  content:""; }

.bp3-icon-drag-handle-horizontal::before{
  content:""; }

.bp3-icon-drag-handle-vertical::before{
  content:""; }

.bp3-icon-draw::before{
  content:""; }

.bp3-icon-drive-time::before{
  content:""; }

.bp3-icon-duplicate::before{
  content:""; }

.bp3-icon-edit::before{
  content:"✎"; }

.bp3-icon-eject::before{
  content:"⏏"; }

.bp3-icon-endorsed::before{
  content:""; }

.bp3-icon-envelope::before{
  content:"✉"; }

.bp3-icon-equals::before{
  content:""; }

.bp3-icon-eraser::before{
  content:""; }

.bp3-icon-error::before{
  content:""; }

.bp3-icon-euro::before{
  content:"€"; }

.bp3-icon-exchange::before{
  content:""; }

.bp3-icon-exclude-row::before{
  content:""; }

.bp3-icon-expand-all::before{
  content:""; }

.bp3-icon-export::before{
  content:""; }

.bp3-icon-eye-off::before{
  content:""; }

.bp3-icon-eye-on::before{
  content:""; }

.bp3-icon-eye-open::before{
  content:""; }

.bp3-icon-fast-backward::before{
  content:""; }

.bp3-icon-fast-forward::before{
  content:""; }

.bp3-icon-feed::before{
  content:""; }

.bp3-icon-feed-subscribed::before{
  content:""; }

.bp3-icon-film::before{
  content:""; }

.bp3-icon-filter::before{
  content:""; }

.bp3-icon-filter-keep::before{
  content:""; }

.bp3-icon-filter-list::before{
  content:""; }

.bp3-icon-filter-open::before{
  content:""; }

.bp3-icon-filter-remove::before{
  content:""; }

.bp3-icon-flag::before{
  content:"⚑"; }

.bp3-icon-flame::before{
  content:""; }

.bp3-icon-flash::before{
  content:""; }

.bp3-icon-floppy-disk::before{
  content:""; }

.bp3-icon-flow-branch::before{
  content:""; }

.bp3-icon-flow-end::before{
  content:""; }

.bp3-icon-flow-linear::before{
  content:""; }

.bp3-icon-flow-review::before{
  content:""; }

.bp3-icon-flow-review-branch::before{
  content:""; }

.bp3-icon-flows::before{
  content:""; }

.bp3-icon-folder-close::before{
  content:""; }

.bp3-icon-folder-new::before{
  content:""; }

.bp3-icon-folder-open::before{
  content:""; }

.bp3-icon-folder-shared::before{
  content:""; }

.bp3-icon-folder-shared-open::before{
  content:""; }

.bp3-icon-follower::before{
  content:""; }

.bp3-icon-following::before{
  content:""; }

.bp3-icon-font::before{
  content:""; }

.bp3-icon-fork::before{
  content:""; }

.bp3-icon-form::before{
  content:""; }

.bp3-icon-full-circle::before{
  content:""; }

.bp3-icon-full-stacked-chart::before{
  content:""; }

.bp3-icon-fullscreen::before{
  content:""; }

.bp3-icon-function::before{
  content:""; }

.bp3-icon-gantt-chart::before{
  content:""; }

.bp3-icon-geolocation::before{
  content:""; }

.bp3-icon-geosearch::before{
  content:""; }

.bp3-icon-git-branch::before{
  content:""; }

.bp3-icon-git-commit::before{
  content:""; }

.bp3-icon-git-merge::before{
  content:""; }

.bp3-icon-git-new-branch::before{
  content:""; }

.bp3-icon-git-pull::before{
  content:""; }

.bp3-icon-git-push::before{
  content:""; }

.bp3-icon-git-repo::before{
  content:""; }

.bp3-icon-glass::before{
  content:""; }

.bp3-icon-globe::before{
  content:""; }

.bp3-icon-globe-network::before{
  content:""; }

.bp3-icon-graph::before{
  content:""; }

.bp3-icon-graph-remove::before{
  content:""; }

.bp3-icon-greater-than::before{
  content:""; }

.bp3-icon-greater-than-or-equal-to::before{
  content:""; }

.bp3-icon-grid::before{
  content:""; }

.bp3-icon-grid-view::before{
  content:""; }

.bp3-icon-group-objects::before{
  content:""; }

.bp3-icon-grouped-bar-chart::before{
  content:""; }

.bp3-icon-hand::before{
  content:""; }

.bp3-icon-hand-down::before{
  content:""; }

.bp3-icon-hand-left::before{
  content:""; }

.bp3-icon-hand-right::before{
  content:""; }

.bp3-icon-hand-up::before{
  content:""; }

.bp3-icon-header::before{
  content:""; }

.bp3-icon-header-one::before{
  content:""; }

.bp3-icon-header-two::before{
  content:""; }

.bp3-icon-headset::before{
  content:""; }

.bp3-icon-heart::before{
  content:"♥"; }

.bp3-icon-heart-broken::before{
  content:""; }

.bp3-icon-heat-grid::before{
  content:""; }

.bp3-icon-heatmap::before{
  content:""; }

.bp3-icon-help::before{
  content:"?"; }

.bp3-icon-helper-management::before{
  content:""; }

.bp3-icon-highlight::before{
  content:""; }

.bp3-icon-history::before{
  content:""; }

.bp3-icon-home::before{
  content:"⌂"; }

.bp3-icon-horizontal-bar-chart::before{
  content:""; }

.bp3-icon-horizontal-bar-chart-asc::before{
  content:""; }

.bp3-icon-horizontal-bar-chart-desc::before{
  content:""; }

.bp3-icon-horizontal-distribution::before{
  content:""; }

.bp3-icon-id-number::before{
  content:""; }

.bp3-icon-image-rotate-left::before{
  content:""; }

.bp3-icon-image-rotate-right::before{
  content:""; }

.bp3-icon-import::before{
  content:""; }

.bp3-icon-inbox::before{
  content:""; }

.bp3-icon-inbox-filtered::before{
  content:""; }

.bp3-icon-inbox-geo::before{
  content:""; }

.bp3-icon-inbox-search::before{
  content:""; }

.bp3-icon-inbox-update::before{
  content:""; }

.bp3-icon-info-sign::before{
  content:"ℹ"; }

.bp3-icon-inheritance::before{
  content:""; }

.bp3-icon-inner-join::before{
  content:""; }

.bp3-icon-insert::before{
  content:""; }

.bp3-icon-intersection::before{
  content:""; }

.bp3-icon-ip-address::before{
  content:""; }

.bp3-icon-issue::before{
  content:""; }

.bp3-icon-issue-closed::before{
  content:""; }

.bp3-icon-issue-new::before{
  content:""; }

.bp3-icon-italic::before{
  content:""; }

.bp3-icon-join-table::before{
  content:""; }

.bp3-icon-key::before{
  content:""; }

.bp3-icon-key-backspace::before{
  content:""; }

.bp3-icon-key-command::before{
  content:""; }

.bp3-icon-key-control::before{
  content:""; }

.bp3-icon-key-delete::before{
  content:""; }

.bp3-icon-key-enter::before{
  content:""; }

.bp3-icon-key-escape::before{
  content:""; }

.bp3-icon-key-option::before{
  content:""; }

.bp3-icon-key-shift::before{
  content:""; }

.bp3-icon-key-tab::before{
  content:""; }

.bp3-icon-known-vehicle::before{
  content:""; }

.bp3-icon-lab-test::before{
  content:""; }

.bp3-icon-label::before{
  content:""; }

.bp3-icon-layer::before{
  content:""; }

.bp3-icon-layers::before{
  content:""; }

.bp3-icon-layout::before{
  content:""; }

.bp3-icon-layout-auto::before{
  content:""; }

.bp3-icon-layout-balloon::before{
  content:""; }

.bp3-icon-layout-circle::before{
  content:""; }

.bp3-icon-layout-grid::before{
  content:""; }

.bp3-icon-layout-group-by::before{
  content:""; }

.bp3-icon-layout-hierarchy::before{
  content:""; }

.bp3-icon-layout-linear::before{
  content:""; }

.bp3-icon-layout-skew-grid::before{
  content:""; }

.bp3-icon-layout-sorted-clusters::before{
  content:""; }

.bp3-icon-learning::before{
  content:""; }

.bp3-icon-left-join::before{
  content:""; }

.bp3-icon-less-than::before{
  content:""; }

.bp3-icon-less-than-or-equal-to::before{
  content:""; }

.bp3-icon-lifesaver::before{
  content:""; }

.bp3-icon-lightbulb::before{
  content:""; }

.bp3-icon-link::before{
  content:""; }

.bp3-icon-list::before{
  content:"☰"; }

.bp3-icon-list-columns::before{
  content:""; }

.bp3-icon-list-detail-view::before{
  content:""; }

.bp3-icon-locate::before{
  content:""; }

.bp3-icon-lock::before{
  content:""; }

.bp3-icon-log-in::before{
  content:""; }

.bp3-icon-log-out::before{
  content:""; }

.bp3-icon-manual::before{
  content:""; }

.bp3-icon-manually-entered-data::before{
  content:""; }

.bp3-icon-map::before{
  content:""; }

.bp3-icon-map-create::before{
  content:""; }

.bp3-icon-map-marker::before{
  content:""; }

.bp3-icon-maximize::before{
  content:""; }

.bp3-icon-media::before{
  content:""; }

.bp3-icon-menu::before{
  content:""; }

.bp3-icon-menu-closed::before{
  content:""; }

.bp3-icon-menu-open::before{
  content:""; }

.bp3-icon-merge-columns::before{
  content:""; }

.bp3-icon-merge-links::before{
  content:""; }

.bp3-icon-minimize::before{
  content:""; }

.bp3-icon-minus::before{
  content:"−"; }

.bp3-icon-mobile-phone::before{
  content:""; }

.bp3-icon-mobile-video::before{
  content:""; }

.bp3-icon-moon::before{
  content:""; }

.bp3-icon-more::before{
  content:""; }

.bp3-icon-mountain::before{
  content:""; }

.bp3-icon-move::before{
  content:""; }

.bp3-icon-mugshot::before{
  content:""; }

.bp3-icon-multi-select::before{
  content:""; }

.bp3-icon-music::before{
  content:""; }

.bp3-icon-new-drawing::before{
  content:""; }

.bp3-icon-new-grid-item::before{
  content:""; }

.bp3-icon-new-layer::before{
  content:""; }

.bp3-icon-new-layers::before{
  content:""; }

.bp3-icon-new-link::before{
  content:""; }

.bp3-icon-new-object::before{
  content:""; }

.bp3-icon-new-person::before{
  content:""; }

.bp3-icon-new-prescription::before{
  content:""; }

.bp3-icon-new-text-box::before{
  content:""; }

.bp3-icon-ninja::before{
  content:""; }

.bp3-icon-not-equal-to::before{
  content:""; }

.bp3-icon-notifications::before{
  content:""; }

.bp3-icon-notifications-updated::before{
  content:""; }

.bp3-icon-numbered-list::before{
  content:""; }

.bp3-icon-numerical::before{
  content:""; }

.bp3-icon-office::before{
  content:""; }

.bp3-icon-offline::before{
  content:""; }

.bp3-icon-oil-field::before{
  content:""; }

.bp3-icon-one-column::before{
  content:""; }

.bp3-icon-outdated::before{
  content:""; }

.bp3-icon-page-layout::before{
  content:""; }

.bp3-icon-panel-stats::before{
  content:""; }

.bp3-icon-panel-table::before{
  content:""; }

.bp3-icon-paperclip::before{
  content:""; }

.bp3-icon-paragraph::before{
  content:""; }

.bp3-icon-path::before{
  content:""; }

.bp3-icon-path-search::before{
  content:""; }

.bp3-icon-pause::before{
  content:""; }

.bp3-icon-people::before{
  content:""; }

.bp3-icon-percentage::before{
  content:""; }

.bp3-icon-person::before{
  content:""; }

.bp3-icon-phone::before{
  content:"☎"; }

.bp3-icon-pie-chart::before{
  content:""; }

.bp3-icon-pin::before{
  content:""; }

.bp3-icon-pivot::before{
  content:""; }

.bp3-icon-pivot-table::before{
  content:""; }

.bp3-icon-play::before{
  content:""; }

.bp3-icon-plus::before{
  content:"+"; }

.bp3-icon-polygon-filter::before{
  content:""; }

.bp3-icon-power::before{
  content:""; }

.bp3-icon-predictive-analysis::before{
  content:""; }

.bp3-icon-prescription::before{
  content:""; }

.bp3-icon-presentation::before{
  content:""; }

.bp3-icon-print::before{
  content:"⎙"; }

.bp3-icon-projects::before{
  content:""; }

.bp3-icon-properties::before{
  content:""; }

.bp3-icon-property::before{
  content:""; }

.bp3-icon-publish-function::before{
  content:""; }

.bp3-icon-pulse::before{
  content:""; }

.bp3-icon-random::before{
  content:""; }

.bp3-icon-record::before{
  content:""; }

.bp3-icon-redo::before{
  content:""; }

.bp3-icon-refresh::before{
  content:""; }

.bp3-icon-regression-chart::before{
  content:""; }

.bp3-icon-remove::before{
  content:""; }

.bp3-icon-remove-column::before{
  content:""; }

.bp3-icon-remove-column-left::before{
  content:""; }

.bp3-icon-remove-column-right::before{
  content:""; }

.bp3-icon-remove-row-bottom::before{
  content:""; }

.bp3-icon-remove-row-top::before{
  content:""; }

.bp3-icon-repeat::before{
  content:""; }

.bp3-icon-reset::before{
  content:""; }

.bp3-icon-resolve::before{
  content:""; }

.bp3-icon-rig::before{
  content:""; }

.bp3-icon-right-join::before{
  content:""; }

.bp3-icon-ring::before{
  content:""; }

.bp3-icon-rotate-document::before{
  content:""; }

.bp3-icon-rotate-page::before{
  content:""; }

.bp3-icon-satellite::before{
  content:""; }

.bp3-icon-saved::before{
  content:""; }

.bp3-icon-scatter-plot::before{
  content:""; }

.bp3-icon-search::before{
  content:""; }

.bp3-icon-search-around::before{
  content:""; }

.bp3-icon-search-template::before{
  content:""; }

.bp3-icon-search-text::before{
  content:""; }

.bp3-icon-segmented-control::before{
  content:""; }

.bp3-icon-select::before{
  content:""; }

.bp3-icon-selection::before{
  content:"⦿"; }

.bp3-icon-send-to::before{
  content:""; }

.bp3-icon-send-to-graph::before{
  content:""; }

.bp3-icon-send-to-map::before{
  content:""; }

.bp3-icon-series-add::before{
  content:""; }

.bp3-icon-series-configuration::before{
  content:""; }

.bp3-icon-series-derived::before{
  content:""; }

.bp3-icon-series-filtered::before{
  content:""; }

.bp3-icon-series-search::before{
  content:""; }

.bp3-icon-settings::before{
  content:""; }

.bp3-icon-share::before{
  content:""; }

.bp3-icon-shield::before{
  content:""; }

.bp3-icon-shop::before{
  content:""; }

.bp3-icon-shopping-cart::before{
  content:""; }

.bp3-icon-signal-search::before{
  content:""; }

.bp3-icon-sim-card::before{
  content:""; }

.bp3-icon-slash::before{
  content:""; }

.bp3-icon-small-cross::before{
  content:""; }

.bp3-icon-small-minus::before{
  content:""; }

.bp3-icon-small-plus::before{
  content:""; }

.bp3-icon-small-tick::before{
  content:""; }

.bp3-icon-snowflake::before{
  content:""; }

.bp3-icon-social-media::before{
  content:""; }

.bp3-icon-sort::before{
  content:""; }

.bp3-icon-sort-alphabetical::before{
  content:""; }

.bp3-icon-sort-alphabetical-desc::before{
  content:""; }

.bp3-icon-sort-asc::before{
  content:""; }

.bp3-icon-sort-desc::before{
  content:""; }

.bp3-icon-sort-numerical::before{
  content:""; }

.bp3-icon-sort-numerical-desc::before{
  content:""; }

.bp3-icon-split-columns::before{
  content:""; }

.bp3-icon-square::before{
  content:""; }

.bp3-icon-stacked-chart::before{
  content:""; }

.bp3-icon-star::before{
  content:"★"; }

.bp3-icon-star-empty::before{
  content:"☆"; }

.bp3-icon-step-backward::before{
  content:""; }

.bp3-icon-step-chart::before{
  content:""; }

.bp3-icon-step-forward::before{
  content:""; }

.bp3-icon-stop::before{
  content:""; }

.bp3-icon-stopwatch::before{
  content:""; }

.bp3-icon-strikethrough::before{
  content:""; }

.bp3-icon-style::before{
  content:""; }

.bp3-icon-swap-horizontal::before{
  content:""; }

.bp3-icon-swap-vertical::before{
  content:""; }

.bp3-icon-symbol-circle::before{
  content:""; }

.bp3-icon-symbol-cross::before{
  content:""; }

.bp3-icon-symbol-diamond::before{
  content:""; }

.bp3-icon-symbol-square::before{
  content:""; }

.bp3-icon-symbol-triangle-down::before{
  content:""; }

.bp3-icon-symbol-triangle-up::before{
  content:""; }

.bp3-icon-tag::before{
  content:""; }

.bp3-icon-take-action::before{
  content:""; }

.bp3-icon-taxi::before{
  content:""; }

.bp3-icon-text-highlight::before{
  content:""; }

.bp3-icon-th::before{
  content:""; }

.bp3-icon-th-derived::before{
  content:""; }

.bp3-icon-th-disconnect::before{
  content:""; }

.bp3-icon-th-filtered::before{
  content:""; }

.bp3-icon-th-list::before{
  content:""; }

.bp3-icon-thumbs-down::before{
  content:""; }

.bp3-icon-thumbs-up::before{
  content:""; }

.bp3-icon-tick::before{
  content:"✓"; }

.bp3-icon-tick-circle::before{
  content:""; }

.bp3-icon-time::before{
  content:"⏲"; }

.bp3-icon-timeline-area-chart::before{
  content:""; }

.bp3-icon-timeline-bar-chart::before{
  content:""; }

.bp3-icon-timeline-events::before{
  content:""; }

.bp3-icon-timeline-line-chart::before{
  content:""; }

.bp3-icon-tint::before{
  content:""; }

.bp3-icon-torch::before{
  content:""; }

.bp3-icon-tractor::before{
  content:""; }

.bp3-icon-train::before{
  content:""; }

.bp3-icon-translate::before{
  content:""; }

.bp3-icon-trash::before{
  content:""; }

.bp3-icon-tree::before{
  content:""; }

.bp3-icon-trending-down::before{
  content:""; }

.bp3-icon-trending-up::before{
  content:""; }

.bp3-icon-truck::before{
  content:""; }

.bp3-icon-two-columns::before{
  content:""; }

.bp3-icon-unarchive::before{
  content:""; }

.bp3-icon-underline::before{
  content:"⎁"; }

.bp3-icon-undo::before{
  content:"⎌"; }

.bp3-icon-ungroup-objects::before{
  content:""; }

.bp3-icon-unknown-vehicle::before{
  content:""; }

.bp3-icon-unlock::before{
  content:""; }

.bp3-icon-unpin::before{
  content:""; }

.bp3-icon-unresolve::before{
  content:""; }

.bp3-icon-updated::before{
  content:""; }

.bp3-icon-upload::before{
  content:""; }

.bp3-icon-user::before{
  content:""; }

.bp3-icon-variable::before{
  content:""; }

.bp3-icon-vertical-bar-chart-asc::before{
  content:""; }

.bp3-icon-vertical-bar-chart-desc::before{
  content:""; }

.bp3-icon-vertical-distribution::before{
  content:""; }

.bp3-icon-video::before{
  content:""; }

.bp3-icon-volume-down::before{
  content:""; }

.bp3-icon-volume-off::before{
  content:""; }

.bp3-icon-volume-up::before{
  content:""; }

.bp3-icon-walk::before{
  content:""; }

.bp3-icon-warning-sign::before{
  content:""; }

.bp3-icon-waterfall-chart::before{
  content:""; }

.bp3-icon-widget::before{
  content:""; }

.bp3-icon-widget-button::before{
  content:""; }

.bp3-icon-widget-footer::before{
  content:""; }

.bp3-icon-widget-header::before{
  content:""; }

.bp3-icon-wrench::before{
  content:""; }

.bp3-icon-zoom-in::before{
  content:""; }

.bp3-icon-zoom-out::before{
  content:""; }

.bp3-icon-zoom-to-fit::before{
  content:""; }
.bp3-submenu > .bp3-popover-wrapper{
  display:block; }

.bp3-submenu .bp3-popover-target{
  display:block; }
  .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item{ }

.bp3-submenu.bp3-popover{
  -webkit-box-shadow:none;
          box-shadow:none;
  padding:0 5px; }
  .bp3-submenu.bp3-popover > .bp3-popover-content{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-submenu.bp3-popover, .bp3-submenu.bp3-popover.bp3-dark{
    -webkit-box-shadow:none;
            box-shadow:none; }
    .bp3-dark .bp3-submenu.bp3-popover > .bp3-popover-content, .bp3-submenu.bp3-popover.bp3-dark > .bp3-popover-content{
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }
.bp3-menu{
  background:#ffffff;
  border-radius:3px;
  color:#182026;
  list-style:none;
  margin:0;
  min-width:180px;
  padding:5px;
  text-align:left; }

.bp3-menu-divider{
  border-top:1px solid rgba(16, 22, 26, 0.15);
  display:block;
  margin:5px; }
  .bp3-dark .bp3-menu-divider{
    border-top-color:rgba(255, 255, 255, 0.15); }

.bp3-menu-item{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:start;
      -ms-flex-align:start;
          align-items:flex-start;
  border-radius:2px;
  color:inherit;
  line-height:20px;
  padding:5px 7px;
  text-decoration:none;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-menu-item > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-menu-item > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-menu-item::before,
  .bp3-menu-item > *{
    margin-right:7px; }
  .bp3-menu-item:empty::before,
  .bp3-menu-item > :last-child{
    margin-right:0; }
  .bp3-menu-item > .bp3-fill{
    word-break:break-word; }
  .bp3-menu-item:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item{
    background-color:rgba(167, 182, 194, 0.3);
    cursor:pointer;
    text-decoration:none; }
  .bp3-menu-item.bp3-disabled{
    background-color:inherit;
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed; }
  .bp3-dark .bp3-menu-item{
    color:inherit; }
    .bp3-dark .bp3-menu-item:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-menu-item{
      background-color:rgba(138, 155, 168, 0.15);
      color:inherit; }
    .bp3-dark .bp3-menu-item.bp3-disabled{
      background-color:inherit;
      color:rgba(167, 182, 194, 0.6); }
  .bp3-menu-item.bp3-intent-primary{
    color:#106ba3; }
    .bp3-menu-item.bp3-intent-primary .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-primary::before, .bp3-menu-item.bp3-intent-primary::after,
    .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{
      color:#106ba3; }
    .bp3-menu-item.bp3-intent-primary:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-menu-item.bp3-intent-primary.bp3-active{
      background-color:#137cbd; }
    .bp3-menu-item.bp3-intent-primary:active{
      background-color:#106ba3; }
    .bp3-menu-item.bp3-intent-primary:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-menu-item.bp3-intent-primary:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-menu-item.bp3-intent-primary:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-primary:active, .bp3-menu-item.bp3-intent-primary:active::before, .bp3-menu-item.bp3-intent-primary:active::after,
    .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-primary.bp3-active, .bp3-menu-item.bp3-intent-primary.bp3-active::before, .bp3-menu-item.bp3-intent-primary.bp3-active::after,
    .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item.bp3-intent-success{
    color:#0d8050; }
    .bp3-menu-item.bp3-intent-success .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-success::before, .bp3-menu-item.bp3-intent-success::after,
    .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{
      color:#0d8050; }
    .bp3-menu-item.bp3-intent-success:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-menu-item.bp3-intent-success.bp3-active{
      background-color:#0f9960; }
    .bp3-menu-item.bp3-intent-success:active{
      background-color:#0d8050; }
    .bp3-menu-item.bp3-intent-success:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-menu-item.bp3-intent-success:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-menu-item.bp3-intent-success:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-success:active, .bp3-menu-item.bp3-intent-success:active::before, .bp3-menu-item.bp3-intent-success:active::after,
    .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-success.bp3-active, .bp3-menu-item.bp3-intent-success.bp3-active::before, .bp3-menu-item.bp3-intent-success.bp3-active::after,
    .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item.bp3-intent-warning{
    color:#bf7326; }
    .bp3-menu-item.bp3-intent-warning .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-warning::before, .bp3-menu-item.bp3-intent-warning::after,
    .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{
      color:#bf7326; }
    .bp3-menu-item.bp3-intent-warning:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-menu-item.bp3-intent-warning.bp3-active{
      background-color:#d9822b; }
    .bp3-menu-item.bp3-intent-warning:active{
      background-color:#bf7326; }
    .bp3-menu-item.bp3-intent-warning:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-menu-item.bp3-intent-warning:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-menu-item.bp3-intent-warning:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-warning:active, .bp3-menu-item.bp3-intent-warning:active::before, .bp3-menu-item.bp3-intent-warning:active::after,
    .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-warning.bp3-active, .bp3-menu-item.bp3-intent-warning.bp3-active::before, .bp3-menu-item.bp3-intent-warning.bp3-active::after,
    .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item.bp3-intent-danger{
    color:#c23030; }
    .bp3-menu-item.bp3-intent-danger .bp3-icon{
      color:inherit; }
    .bp3-menu-item.bp3-intent-danger::before, .bp3-menu-item.bp3-intent-danger::after,
    .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{
      color:#c23030; }
    .bp3-menu-item.bp3-intent-danger:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-menu-item.bp3-intent-danger.bp3-active{
      background-color:#db3737; }
    .bp3-menu-item.bp3-intent-danger:active{
      background-color:#c23030; }
    .bp3-menu-item.bp3-intent-danger:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-menu-item.bp3-intent-danger:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-menu-item.bp3-intent-danger:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after,
    .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,
    .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-danger:active, .bp3-menu-item.bp3-intent-danger:active::before, .bp3-menu-item.bp3-intent-danger:active::after,
    .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-danger.bp3-active, .bp3-menu-item.bp3-intent-danger.bp3-active::before, .bp3-menu-item.bp3-intent-danger.bp3-active::after,
    .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-menu-item::before{
    font-family:"Icons16", sans-serif;
    font-size:16px;
    font-style:normal;
    font-weight:400;
    line-height:1;
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased;
    margin-right:7px; }
  .bp3-menu-item::before,
  .bp3-menu-item > .bp3-icon{
    color:#5c7080;
    margin-top:2px; }
  .bp3-menu-item .bp3-menu-item-label{
    color:#5c7080; }
  .bp3-menu-item:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item{
    color:inherit; }
  .bp3-menu-item.bp3-active, .bp3-menu-item:active{
    background-color:rgba(115, 134, 148, 0.3); }
  .bp3-menu-item.bp3-disabled{
    background-color:inherit !important;
    color:rgba(92, 112, 128, 0.6) !important;
    cursor:not-allowed !important;
    outline:none !important; }
    .bp3-menu-item.bp3-disabled::before,
    .bp3-menu-item.bp3-disabled > .bp3-icon,
    .bp3-menu-item.bp3-disabled .bp3-menu-item-label{
      color:rgba(92, 112, 128, 0.6) !important; }
  .bp3-large .bp3-menu-item{
    font-size:16px;
    line-height:22px;
    padding:9px 7px; }
    .bp3-large .bp3-menu-item .bp3-icon{
      margin-top:3px; }
    .bp3-large .bp3-menu-item::before{
      font-family:"Icons20", sans-serif;
      font-size:20px;
      font-style:normal;
      font-weight:400;
      line-height:1;
      -moz-osx-font-smoothing:grayscale;
      -webkit-font-smoothing:antialiased;
      margin-right:10px;
      margin-top:1px; }

button.bp3-menu-item{
  background:none;
  border:none;
  text-align:left;
  width:100%; }
.bp3-menu-header{
  border-top:1px solid rgba(16, 22, 26, 0.15);
  display:block;
  margin:5px;
  cursor:default;
  padding-left:2px; }
  .bp3-dark .bp3-menu-header{
    border-top-color:rgba(255, 255, 255, 0.15); }
  .bp3-menu-header:first-of-type{
    border-top:none; }
  .bp3-menu-header > h6{
    color:#182026;
    font-weight:600;
    overflow:hidden;
    text-overflow:ellipsis;
    white-space:nowrap;
    word-wrap:normal;
    line-height:17px;
    margin:0;
    padding:10px 7px 0 1px; }
    .bp3-dark .bp3-menu-header > h6{
      color:#f5f8fa; }
  .bp3-menu-header:first-of-type > h6{
    padding-top:0; }
  .bp3-large .bp3-menu-header > h6{
    font-size:18px;
    padding-bottom:5px;
    padding-top:15px; }
  .bp3-large .bp3-menu-header:first-of-type > h6{
    padding-top:0; }

.bp3-dark .bp3-menu{
  background:#30404d;
  color:#f5f8fa; }

.bp3-dark .bp3-menu-item{ }
  .bp3-dark .bp3-menu-item.bp3-intent-primary{
    color:#48aff0; }
    .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-icon{
      color:inherit; }
    .bp3-dark .bp3-menu-item.bp3-intent-primary::before, .bp3-dark .bp3-menu-item.bp3-intent-primary::after,
    .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{
      color:#48aff0; }
    .bp3-dark .bp3-menu-item.bp3-intent-primary:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active{
      background-color:#137cbd; }
    .bp3-dark .bp3-menu-item.bp3-intent-primary:active{
      background-color:#106ba3; }
    .bp3-dark .bp3-menu-item.bp3-intent-primary:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after,
    .bp3-dark .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,
    .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,
    .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-primary:active, .bp3-dark .bp3-menu-item.bp3-intent-primary:active::before, .bp3-dark .bp3-menu-item.bp3-intent-primary:active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-dark .bp3-menu-item.bp3-intent-success{
    color:#3dcc91; }
    .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-icon{
      color:inherit; }
    .bp3-dark .bp3-menu-item.bp3-intent-success::before, .bp3-dark .bp3-menu-item.bp3-intent-success::after,
    .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{
      color:#3dcc91; }
    .bp3-dark .bp3-menu-item.bp3-intent-success:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active{
      background-color:#0f9960; }
    .bp3-dark .bp3-menu-item.bp3-intent-success:active{
      background-color:#0d8050; }
    .bp3-dark .bp3-menu-item.bp3-intent-success:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-success:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-success:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after,
    .bp3-dark .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,
    .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label,
    .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-success:active, .bp3-dark .bp3-menu-item.bp3-intent-success:active::before, .bp3-dark .bp3-menu-item.bp3-intent-success:active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-dark .bp3-menu-item.bp3-intent-warning{
    color:#ffb366; }
    .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-icon{
      color:inherit; }
    .bp3-dark .bp3-menu-item.bp3-intent-warning::before, .bp3-dark .bp3-menu-item.bp3-intent-warning::after,
    .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{
      color:#ffb366; }
    .bp3-dark .bp3-menu-item.bp3-intent-warning:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active{
      background-color:#d9822b; }
    .bp3-dark .bp3-menu-item.bp3-intent-warning:active{
      background-color:#bf7326; }
    .bp3-dark .bp3-menu-item.bp3-intent-warning:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after,
    .bp3-dark .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,
    .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,
    .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-warning:active, .bp3-dark .bp3-menu-item.bp3-intent-warning:active::before, .bp3-dark .bp3-menu-item.bp3-intent-warning:active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-dark .bp3-menu-item.bp3-intent-danger{
    color:#ff7373; }
    .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-icon{
      color:inherit; }
    .bp3-dark .bp3-menu-item.bp3-intent-danger::before, .bp3-dark .bp3-menu-item.bp3-intent-danger::after,
    .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{
      color:#ff7373; }
    .bp3-dark .bp3-menu-item.bp3-intent-danger:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active{
      background-color:#db3737; }
    .bp3-dark .bp3-menu-item.bp3-intent-danger:active{
      background-color:#c23030; }
    .bp3-dark .bp3-menu-item.bp3-intent-danger:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after,
    .bp3-dark .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,
    .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,
    .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-danger:active, .bp3-dark .bp3-menu-item.bp3-intent-danger:active::before, .bp3-dark .bp3-menu-item.bp3-intent-danger:active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::after,
    .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{
      color:#ffffff; }
  .bp3-dark .bp3-menu-item::before,
  .bp3-dark .bp3-menu-item > .bp3-icon{
    color:#a7b6c2; }
  .bp3-dark .bp3-menu-item .bp3-menu-item-label{
    color:#a7b6c2; }
  .bp3-dark .bp3-menu-item.bp3-active, .bp3-dark .bp3-menu-item:active{
    background-color:rgba(138, 155, 168, 0.3); }
  .bp3-dark .bp3-menu-item.bp3-disabled{
    color:rgba(167, 182, 194, 0.6) !important; }
    .bp3-dark .bp3-menu-item.bp3-disabled::before,
    .bp3-dark .bp3-menu-item.bp3-disabled > .bp3-icon,
    .bp3-dark .bp3-menu-item.bp3-disabled .bp3-menu-item-label{
      color:rgba(167, 182, 194, 0.6) !important; }

.bp3-dark .bp3-menu-divider,
.bp3-dark .bp3-menu-header{
  border-color:rgba(255, 255, 255, 0.15); }

.bp3-dark .bp3-menu-header > h6{
  color:#f5f8fa; }

.bp3-label .bp3-menu{
  margin-top:5px; }
.bp3-navbar{
  background-color:#ffffff;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2);
  height:50px;
  padding:0 15px;
  position:relative;
  width:100%;
  z-index:10; }
  .bp3-navbar.bp3-dark,
  .bp3-dark .bp3-navbar{
    background-color:#394b59; }
  .bp3-navbar.bp3-dark{
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-dark .bp3-navbar{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); }
  .bp3-navbar.bp3-fixed-top{
    left:0;
    position:fixed;
    right:0;
    top:0; }

.bp3-navbar-heading{
  font-size:16px;
  margin-right:15px; }

.bp3-navbar-group{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  height:50px; }
  .bp3-navbar-group.bp3-align-left{
    float:left; }
  .bp3-navbar-group.bp3-align-right{
    float:right; }

.bp3-navbar-divider{
  border-left:1px solid rgba(16, 22, 26, 0.15);
  height:20px;
  margin:0 10px; }
  .bp3-dark .bp3-navbar-divider{
    border-left-color:rgba(255, 255, 255, 0.15); }
.bp3-non-ideal-state{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  height:100%;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  text-align:center;
  width:100%; }
  .bp3-non-ideal-state > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-non-ideal-state > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-non-ideal-state::before,
  .bp3-non-ideal-state > *{
    margin-bottom:20px; }
  .bp3-non-ideal-state:empty::before,
  .bp3-non-ideal-state > :last-child{
    margin-bottom:0; }
  .bp3-non-ideal-state > *{
    max-width:400px; }

.bp3-non-ideal-state-visual{
  color:rgba(92, 112, 128, 0.6);
  font-size:60px; }
  .bp3-dark .bp3-non-ideal-state-visual{
    color:rgba(167, 182, 194, 0.6); }

.bp3-overflow-list{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -ms-flex-wrap:nowrap;
      flex-wrap:nowrap;
  min-width:0; }

.bp3-overflow-list-spacer{
  -ms-flex-negative:1;
      flex-shrink:1;
  width:1px; }

body.bp3-overlay-open{
  overflow:hidden; }

.bp3-overlay{
  bottom:0;
  left:0;
  position:static;
  right:0;
  top:0;
  z-index:20; }
  .bp3-overlay:not(.bp3-overlay-open){
    pointer-events:none; }
  .bp3-overlay.bp3-overlay-container{
    overflow:hidden;
    position:fixed; }
    .bp3-overlay.bp3-overlay-container.bp3-overlay-inline{
      position:absolute; }
  .bp3-overlay.bp3-overlay-scroll-container{
    overflow:auto;
    position:fixed; }
    .bp3-overlay.bp3-overlay-scroll-container.bp3-overlay-inline{
      position:absolute; }
  .bp3-overlay.bp3-overlay-inline{
    display:inline;
    overflow:visible; }

.bp3-overlay-content{
  position:fixed;
  z-index:20; }
  .bp3-overlay-inline .bp3-overlay-content,
  .bp3-overlay-scroll-container .bp3-overlay-content{
    position:absolute; }

.bp3-overlay-backdrop{
  bottom:0;
  left:0;
  position:fixed;
  right:0;
  top:0;
  opacity:1;
  background-color:rgba(16, 22, 26, 0.7);
  overflow:auto;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none;
  z-index:20; }
  .bp3-overlay-backdrop.bp3-overlay-enter, .bp3-overlay-backdrop.bp3-overlay-appear{
    opacity:0; }
  .bp3-overlay-backdrop.bp3-overlay-enter-active, .bp3-overlay-backdrop.bp3-overlay-appear-active{
    opacity:1;
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-overlay-backdrop.bp3-overlay-exit{
    opacity:1; }
  .bp3-overlay-backdrop.bp3-overlay-exit-active{
    opacity:0;
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-overlay-backdrop:focus{
    outline:none; }
  .bp3-overlay-inline .bp3-overlay-backdrop{
    position:absolute; }
.bp3-panel-stack{
  overflow:hidden;
  position:relative; }

.bp3-panel-stack-header{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  -webkit-box-shadow:0 1px rgba(16, 22, 26, 0.15);
          box-shadow:0 1px rgba(16, 22, 26, 0.15);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -ms-flex-negative:0;
      flex-shrink:0;
  height:30px;
  z-index:1; }
  .bp3-dark .bp3-panel-stack-header{
    -webkit-box-shadow:0 1px rgba(255, 255, 255, 0.15);
            box-shadow:0 1px rgba(255, 255, 255, 0.15); }
  .bp3-panel-stack-header > span{
    -webkit-box-align:stretch;
        -ms-flex-align:stretch;
            align-items:stretch;
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-flex:1;
        -ms-flex:1;
            flex:1; }
  .bp3-panel-stack-header .bp3-heading{
    margin:0 5px; }

.bp3-button.bp3-panel-stack-header-back{
  margin-left:5px;
  padding-left:0;
  white-space:nowrap; }
  .bp3-button.bp3-panel-stack-header-back .bp3-icon{
    margin:0 2px; }

.bp3-panel-stack-view{
  bottom:0;
  left:0;
  position:absolute;
  right:0;
  top:0;
  background-color:#ffffff;
  border-right:1px solid rgba(16, 22, 26, 0.15);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  margin-right:-1px;
  overflow-y:auto;
  z-index:1; }
  .bp3-dark .bp3-panel-stack-view{
    background-color:#30404d; }
  .bp3-panel-stack-view:nth-last-child(n + 4){
    display:none; }

.bp3-panel-stack-push .bp3-panel-stack-enter, .bp3-panel-stack-push .bp3-panel-stack-appear{
  -webkit-transform:translateX(100%);
          transform:translateX(100%);
  opacity:0; }

.bp3-panel-stack-push .bp3-panel-stack-enter-active, .bp3-panel-stack-push .bp3-panel-stack-appear-active{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1;
  -webkit-transition-delay:0;
          transition-delay:0;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease; }

.bp3-panel-stack-push .bp3-panel-stack-exit{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1; }

.bp3-panel-stack-push .bp3-panel-stack-exit-active{
  -webkit-transform:translateX(-50%);
          transform:translateX(-50%);
  opacity:0;
  -webkit-transition-delay:0;
          transition-delay:0;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease; }

.bp3-panel-stack-pop .bp3-panel-stack-enter, .bp3-panel-stack-pop .bp3-panel-stack-appear{
  -webkit-transform:translateX(-50%);
          transform:translateX(-50%);
  opacity:0; }

.bp3-panel-stack-pop .bp3-panel-stack-enter-active, .bp3-panel-stack-pop .bp3-panel-stack-appear-active{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1;
  -webkit-transition-delay:0;
          transition-delay:0;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease; }

.bp3-panel-stack-pop .bp3-panel-stack-exit{
  -webkit-transform:translate(0%);
          transform:translate(0%);
  opacity:1; }

.bp3-panel-stack-pop .bp3-panel-stack-exit-active{
  -webkit-transform:translateX(100%);
          transform:translateX(100%);
  opacity:0;
  -webkit-transition-delay:0;
          transition-delay:0;
  -webkit-transition-duration:400ms;
          transition-duration:400ms;
  -webkit-transition-property:opacity, -webkit-transform;
  transition-property:opacity, -webkit-transform;
  transition-property:transform, opacity;
  transition-property:transform, opacity, -webkit-transform;
  -webkit-transition-timing-function:ease;
          transition-timing-function:ease; }
.bp3-popover{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  -webkit-transform:scale(1);
          transform:scale(1);
  border-radius:3px;
  display:inline-block;
  z-index:20; }
  .bp3-popover .bp3-popover-arrow{
    height:30px;
    position:absolute;
    width:30px; }
    .bp3-popover .bp3-popover-arrow::before{
      height:20px;
      margin:5px;
      width:20px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover{
    margin-bottom:17px;
    margin-top:-17px; }
    .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow{
      bottom:-11px; }
      .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(-90deg);
                transform:rotate(-90deg); }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover{
    margin-left:17px; }
    .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow{
      left:-11px; }
      .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(0);
                transform:rotate(0); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover{
    margin-top:17px; }
    .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow{
      top:-11px; }
      .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(90deg);
                transform:rotate(90deg); }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover{
    margin-left:-17px;
    margin-right:17px; }
    .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow{
      right:-11px; }
      .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow svg{
        -webkit-transform:rotate(180deg);
                transform:rotate(180deg); }
  .bp3-tether-element-attached-middle > .bp3-popover > .bp3-popover-arrow{
    top:50%;
    -webkit-transform:translateY(-50%);
            transform:translateY(-50%); }
  .bp3-tether-element-attached-center > .bp3-popover > .bp3-popover-arrow{
    right:50%;
    -webkit-transform:translateX(50%);
            transform:translateX(50%); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow{
    top:-0.3934px; }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow{
    right:-0.3934px; }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow{
    left:-0.3934px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow{
    bottom:-0.3934px; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-left > .bp3-popover{
    -webkit-transform-origin:top left;
            transform-origin:top left; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-center > .bp3-popover{
    -webkit-transform-origin:top center;
            transform-origin:top center; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-right > .bp3-popover{
    -webkit-transform-origin:top right;
            transform-origin:top right; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-left > .bp3-popover{
    -webkit-transform-origin:center left;
            transform-origin:center left; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-center > .bp3-popover{
    -webkit-transform-origin:center center;
            transform-origin:center center; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-right > .bp3-popover{
    -webkit-transform-origin:center right;
            transform-origin:center right; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left > .bp3-popover{
    -webkit-transform-origin:bottom left;
            transform-origin:bottom left; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center > .bp3-popover{
    -webkit-transform-origin:bottom center;
            transform-origin:bottom center; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right > .bp3-popover{
    -webkit-transform-origin:bottom right;
            transform-origin:bottom right; }
  .bp3-popover .bp3-popover-content{
    background:#ffffff;
    color:inherit; }
  .bp3-popover .bp3-popover-arrow::before{
    -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2);
            box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); }
  .bp3-popover .bp3-popover-arrow-border{
    fill:#10161a;
    fill-opacity:0.1; }
  .bp3-popover .bp3-popover-arrow-fill{
    fill:#ffffff; }
  .bp3-popover-enter > .bp3-popover, .bp3-popover-appear > .bp3-popover{
    -webkit-transform:scale(0.3);
            transform:scale(0.3); }
  .bp3-popover-enter-active > .bp3-popover, .bp3-popover-appear-active > .bp3-popover{
    -webkit-transform:scale(1);
            transform:scale(1);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); }
  .bp3-popover-exit > .bp3-popover{
    -webkit-transform:scale(1);
            transform:scale(1); }
  .bp3-popover-exit-active > .bp3-popover{
    -webkit-transform:scale(0.3);
            transform:scale(0.3);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); }
  .bp3-popover .bp3-popover-content{
    border-radius:3px;
    position:relative; }
  .bp3-popover.bp3-popover-content-sizing .bp3-popover-content{
    max-width:350px;
    padding:20px; }
  .bp3-popover-target + .bp3-overlay .bp3-popover.bp3-popover-content-sizing{
    width:350px; }
  .bp3-popover.bp3-minimal{
    margin:0 !important; }
    .bp3-popover.bp3-minimal .bp3-popover-arrow{
      display:none; }
    .bp3-popover.bp3-minimal.bp3-popover{
      -webkit-transform:scale(1);
              transform:scale(1); }
      .bp3-popover-enter > .bp3-popover.bp3-minimal.bp3-popover, .bp3-popover-appear > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1); }
      .bp3-popover-enter-active > .bp3-popover.bp3-minimal.bp3-popover, .bp3-popover-appear-active > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1);
        -webkit-transition-delay:0;
                transition-delay:0;
        -webkit-transition-duration:100ms;
                transition-duration:100ms;
        -webkit-transition-property:-webkit-transform;
        transition-property:-webkit-transform;
        transition-property:transform;
        transition-property:transform, -webkit-transform;
        -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
                transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
      .bp3-popover-exit > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1); }
      .bp3-popover-exit-active > .bp3-popover.bp3-minimal.bp3-popover{
        -webkit-transform:scale(1);
                transform:scale(1);
        -webkit-transition-delay:0;
                transition-delay:0;
        -webkit-transition-duration:100ms;
                transition-duration:100ms;
        -webkit-transition-property:-webkit-transform;
        transition-property:-webkit-transform;
        transition-property:transform;
        transition-property:transform, -webkit-transform;
        -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
                transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-popover.bp3-dark,
  .bp3-dark .bp3-popover{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }
    .bp3-popover.bp3-dark .bp3-popover-content,
    .bp3-dark .bp3-popover .bp3-popover-content{
      background:#30404d;
      color:inherit; }
    .bp3-popover.bp3-dark .bp3-popover-arrow::before,
    .bp3-dark .bp3-popover .bp3-popover-arrow::before{
      -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4);
              box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); }
    .bp3-popover.bp3-dark .bp3-popover-arrow-border,
    .bp3-dark .bp3-popover .bp3-popover-arrow-border{
      fill:#10161a;
      fill-opacity:0.2; }
    .bp3-popover.bp3-dark .bp3-popover-arrow-fill,
    .bp3-dark .bp3-popover .bp3-popover-arrow-fill{
      fill:#30404d; }

.bp3-popover-arrow::before{
  border-radius:2px;
  content:"";
  display:block;
  position:absolute;
  -webkit-transform:rotate(45deg);
          transform:rotate(45deg); }

.bp3-tether-pinned .bp3-popover-arrow{
  display:none; }

.bp3-popover-backdrop{
  background:rgba(255, 255, 255, 0); }

.bp3-transition-container{
  opacity:1;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  z-index:20; }
  .bp3-transition-container.bp3-popover-enter, .bp3-transition-container.bp3-popover-appear{
    opacity:0; }
  .bp3-transition-container.bp3-popover-enter-active, .bp3-transition-container.bp3-popover-appear-active{
    opacity:1;
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-transition-container.bp3-popover-exit{
    opacity:1; }
  .bp3-transition-container.bp3-popover-exit-active{
    opacity:0;
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-property:opacity;
    transition-property:opacity;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-transition-container:focus{
    outline:none; }
  .bp3-transition-container.bp3-popover-leave .bp3-popover-content{
    pointer-events:none; }
  .bp3-transition-container[data-x-out-of-boundaries]{
    display:none; }

span.bp3-popover-target{
  display:inline-block; }

.bp3-popover-wrapper.bp3-fill{
  width:100%; }

.bp3-portal{
  left:0;
  position:absolute;
  right:0;
  top:0; }
@-webkit-keyframes linear-progress-bar-stripes{
  from{
    background-position:0 0; }
  to{
    background-position:30px 0; } }
@keyframes linear-progress-bar-stripes{
  from{
    background-position:0 0; }
  to{
    background-position:30px 0; } }

.bp3-progress-bar{
  background:rgba(92, 112, 128, 0.2);
  border-radius:40px;
  display:block;
  height:8px;
  overflow:hidden;
  position:relative;
  width:100%; }
  .bp3-progress-bar .bp3-progress-meter{
    background:linear-gradient(-45deg, rgba(255, 255, 255, 0.2) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.2) 50%, rgba(255, 255, 255, 0.2) 75%, transparent 75%);
    background-color:rgba(92, 112, 128, 0.8);
    background-size:30px 30px;
    border-radius:40px;
    height:100%;
    position:absolute;
    -webkit-transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    width:100%; }
  .bp3-progress-bar:not(.bp3-no-animation):not(.bp3-no-stripes) .bp3-progress-meter{
    animation:linear-progress-bar-stripes 300ms linear infinite reverse; }
  .bp3-progress-bar.bp3-no-stripes .bp3-progress-meter{
    background-image:none; }

.bp3-dark .bp3-progress-bar{
  background:rgba(16, 22, 26, 0.5); }
  .bp3-dark .bp3-progress-bar .bp3-progress-meter{
    background-color:#8a9ba8; }

.bp3-progress-bar.bp3-intent-primary .bp3-progress-meter{
  background-color:#137cbd; }

.bp3-progress-bar.bp3-intent-success .bp3-progress-meter{
  background-color:#0f9960; }

.bp3-progress-bar.bp3-intent-warning .bp3-progress-meter{
  background-color:#d9822b; }

.bp3-progress-bar.bp3-intent-danger .bp3-progress-meter{
  background-color:#db3737; }
@-webkit-keyframes skeleton-glow{
  from{
    background:rgba(206, 217, 224, 0.2);
    border-color:rgba(206, 217, 224, 0.2); }
  to{
    background:rgba(92, 112, 128, 0.2);
    border-color:rgba(92, 112, 128, 0.2); } }
@keyframes skeleton-glow{
  from{
    background:rgba(206, 217, 224, 0.2);
    border-color:rgba(206, 217, 224, 0.2); }
  to{
    background:rgba(92, 112, 128, 0.2);
    border-color:rgba(92, 112, 128, 0.2); } }
.bp3-skeleton{
  -webkit-animation:1000ms linear infinite alternate skeleton-glow;
          animation:1000ms linear infinite alternate skeleton-glow;
  background:rgba(206, 217, 224, 0.2);
  background-clip:padding-box !important;
  border-color:rgba(206, 217, 224, 0.2) !important;
  border-radius:2px;
  -webkit-box-shadow:none !important;
          box-shadow:none !important;
  color:transparent !important;
  cursor:default;
  pointer-events:none;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-skeleton::before, .bp3-skeleton::after,
  .bp3-skeleton *{
    visibility:hidden !important; }
.bp3-slider{
  height:40px;
  min-width:150px;
  width:100%;
  cursor:default;
  outline:none;
  position:relative;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-slider:hover{
    cursor:pointer; }
  .bp3-slider:active{
    cursor:-webkit-grabbing;
    cursor:grabbing; }
  .bp3-slider.bp3-disabled{
    cursor:not-allowed;
    opacity:0.5; }
  .bp3-slider.bp3-slider-unlabeled{
    height:16px; }

.bp3-slider-track,
.bp3-slider-progress{
  height:6px;
  left:0;
  right:0;
  top:5px;
  position:absolute; }

.bp3-slider-track{
  border-radius:3px;
  overflow:hidden; }

.bp3-slider-progress{
  background:rgba(92, 112, 128, 0.2); }
  .bp3-dark .bp3-slider-progress{
    background:rgba(16, 22, 26, 0.5); }
  .bp3-slider-progress.bp3-intent-primary{
    background-color:#137cbd; }
  .bp3-slider-progress.bp3-intent-success{
    background-color:#0f9960; }
  .bp3-slider-progress.bp3-intent-warning{
    background-color:#d9822b; }
  .bp3-slider-progress.bp3-intent-danger{
    background-color:#db3737; }

.bp3-slider-handle{
  background-color:#f5f8fa;
  background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));
  background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));
  -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
          box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
  color:#182026;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
  cursor:pointer;
  height:16px;
  left:0;
  position:absolute;
  top:0;
  width:16px; }
  .bp3-slider-handle:hover{
    background-clip:padding-box;
    background-color:#ebf1f5;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); }
  .bp3-slider-handle:active, .bp3-slider-handle.bp3-active{
    background-color:#d8e1e8;
    background-image:none;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
  .bp3-slider-handle:disabled, .bp3-slider-handle.bp3-disabled{
    background-color:rgba(206, 217, 224, 0.5);
    background-image:none;
    -webkit-box-shadow:none;
            box-shadow:none;
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed;
    outline:none; }
    .bp3-slider-handle:disabled.bp3-active, .bp3-slider-handle:disabled.bp3-active:hover, .bp3-slider-handle.bp3-disabled.bp3-active, .bp3-slider-handle.bp3-disabled.bp3-active:hover{
      background:rgba(206, 217, 224, 0.7); }
  .bp3-slider-handle:focus{
    z-index:1; }
  .bp3-slider-handle:hover{
    background-clip:padding-box;
    background-color:#ebf1f5;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1);
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2);
    cursor:-webkit-grab;
    cursor:grab;
    z-index:2; }
  .bp3-slider-handle.bp3-active{
    background-color:#d8e1e8;
    background-image:none;
    -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
            box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2);
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 1px rgba(16, 22, 26, 0.1);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 1px rgba(16, 22, 26, 0.1);
    cursor:-webkit-grabbing;
    cursor:grabbing; }
  .bp3-disabled .bp3-slider-handle{
    background:#bfccd6;
    -webkit-box-shadow:none;
            box-shadow:none;
    pointer-events:none; }
  .bp3-dark .bp3-slider-handle{
    background-color:#394b59;
    background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));
    background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
    color:#f5f8fa; }
    .bp3-dark .bp3-slider-handle:hover, .bp3-dark .bp3-slider-handle:active, .bp3-dark .bp3-slider-handle.bp3-active{
      color:#f5f8fa; }
    .bp3-dark .bp3-slider-handle:hover{
      background-color:#30404d;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-slider-handle:active, .bp3-dark .bp3-slider-handle.bp3-active{
      background-color:#202b33;
      background-image:none;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); }
    .bp3-dark .bp3-slider-handle:disabled, .bp3-dark .bp3-slider-handle.bp3-disabled{
      background-color:rgba(57, 75, 89, 0.5);
      background-image:none;
      -webkit-box-shadow:none;
              box-shadow:none;
      color:rgba(167, 182, 194, 0.6); }
      .bp3-dark .bp3-slider-handle:disabled.bp3-active, .bp3-dark .bp3-slider-handle.bp3-disabled.bp3-active{
        background:rgba(57, 75, 89, 0.7); }
    .bp3-dark .bp3-slider-handle .bp3-button-spinner .bp3-spinner-head{
      background:rgba(16, 22, 26, 0.5);
      stroke:#8a9ba8; }
    .bp3-dark .bp3-slider-handle, .bp3-dark .bp3-slider-handle:hover{
      background-color:#394b59; }
    .bp3-dark .bp3-slider-handle.bp3-active{
      background-color:#293742; }
  .bp3-dark .bp3-disabled .bp3-slider-handle{
    background:#5c7080;
    border-color:#5c7080;
    -webkit-box-shadow:none;
            box-shadow:none; }
  .bp3-slider-handle .bp3-slider-label{
    background:#394b59;
    border-radius:3px;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
    color:#f5f8fa;
    margin-left:8px; }
    .bp3-dark .bp3-slider-handle .bp3-slider-label{
      background:#e1e8ed;
      -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
      color:#394b59; }
    .bp3-disabled .bp3-slider-handle .bp3-slider-label{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-slider-handle.bp3-start, .bp3-slider-handle.bp3-end{
    width:8px; }
  .bp3-slider-handle.bp3-start{
    border-bottom-right-radius:0;
    border-top-right-radius:0; }
  .bp3-slider-handle.bp3-end{
    border-bottom-left-radius:0;
    border-top-left-radius:0;
    margin-left:8px; }
    .bp3-slider-handle.bp3-end .bp3-slider-label{
      margin-left:0; }

.bp3-slider-label{
  -webkit-transform:translate(-50%, 20px);
          transform:translate(-50%, 20px);
  display:inline-block;
  font-size:12px;
  line-height:1;
  padding:2px 5px;
  position:absolute;
  vertical-align:top; }

.bp3-slider.bp3-vertical{
  height:150px;
  min-width:40px;
  width:40px; }
  .bp3-slider.bp3-vertical .bp3-slider-track,
  .bp3-slider.bp3-vertical .bp3-slider-progress{
    bottom:0;
    height:auto;
    left:5px;
    top:0;
    width:6px; }
  .bp3-slider.bp3-vertical .bp3-slider-progress{
    top:auto; }
  .bp3-slider.bp3-vertical .bp3-slider-label{
    -webkit-transform:translate(20px, 50%);
            transform:translate(20px, 50%); }
  .bp3-slider.bp3-vertical .bp3-slider-handle{
    top:auto; }
    .bp3-slider.bp3-vertical .bp3-slider-handle .bp3-slider-label{
      margin-left:0;
      margin-top:-8px; }
    .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end, .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{
      height:8px;
      margin-left:0;
      width:16px; }
    .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{
      border-bottom-right-radius:3px;
      border-top-left-radius:0; }
      .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start .bp3-slider-label{
        -webkit-transform:translate(20px);
                transform:translate(20px); }
    .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end{
      border-bottom-left-radius:0;
      border-bottom-right-radius:0;
      border-top-left-radius:3px;
      margin-bottom:8px; }

@-webkit-keyframes pt-spinner-animation{
  from{
    -webkit-transform:rotate(0deg);
            transform:rotate(0deg); }
  to{
    -webkit-transform:rotate(360deg);
            transform:rotate(360deg); } }

@keyframes pt-spinner-animation{
  from{
    -webkit-transform:rotate(0deg);
            transform:rotate(0deg); }
  to{
    -webkit-transform:rotate(360deg);
            transform:rotate(360deg); } }

.bp3-spinner{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-pack:center;
      -ms-flex-pack:center;
          justify-content:center;
  overflow:visible;
  vertical-align:middle; }
  .bp3-spinner svg{
    display:block; }
  .bp3-spinner path{
    fill-opacity:0; }
  .bp3-spinner .bp3-spinner-head{
    stroke:rgba(92, 112, 128, 0.8);
    stroke-linecap:round;
    -webkit-transform-origin:center;
            transform-origin:center;
    -webkit-transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
    transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-spinner .bp3-spinner-track{
    stroke:rgba(92, 112, 128, 0.2); }

.bp3-spinner-animation{
  -webkit-animation:pt-spinner-animation 500ms linear infinite;
          animation:pt-spinner-animation 500ms linear infinite; }
  .bp3-no-spin > .bp3-spinner-animation{
    -webkit-animation:none;
            animation:none; }

.bp3-dark .bp3-spinner .bp3-spinner-head{
  stroke:#8a9ba8; }

.bp3-dark .bp3-spinner .bp3-spinner-track{
  stroke:rgba(16, 22, 26, 0.5); }

.bp3-spinner.bp3-intent-primary .bp3-spinner-head{
  stroke:#137cbd; }

.bp3-spinner.bp3-intent-success .bp3-spinner-head{
  stroke:#0f9960; }

.bp3-spinner.bp3-intent-warning .bp3-spinner-head{
  stroke:#d9822b; }

.bp3-spinner.bp3-intent-danger .bp3-spinner-head{
  stroke:#db3737; }
.bp3-tabs.bp3-vertical{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex; }
  .bp3-tabs.bp3-vertical > .bp3-tab-list{
    -webkit-box-align:start;
        -ms-flex-align:start;
            align-items:flex-start;
    -webkit-box-orient:vertical;
    -webkit-box-direction:normal;
        -ms-flex-direction:column;
            flex-direction:column; }
    .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab{
      border-radius:3px;
      padding:0 10px;
      width:100%; }
      .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab[aria-selected="true"]{
        background-color:rgba(19, 124, 189, 0.2);
        -webkit-box-shadow:none;
                box-shadow:none; }
    .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab-indicator-wrapper .bp3-tab-indicator{
      background-color:rgba(19, 124, 189, 0.2);
      border-radius:3px;
      bottom:0;
      height:auto;
      left:0;
      right:0;
      top:0; }
  .bp3-tabs.bp3-vertical > .bp3-tab-panel{
    margin-top:0;
    padding-left:20px; }

.bp3-tab-list{
  -webkit-box-align:end;
      -ms-flex-align:end;
          align-items:flex-end;
  border:none;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  list-style:none;
  margin:0;
  padding:0;
  position:relative; }
  .bp3-tab-list > *:not(:last-child){
    margin-right:20px; }

.bp3-tab{
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal;
  color:#182026;
  cursor:pointer;
  -webkit-box-flex:0;
      -ms-flex:0 0 auto;
          flex:0 0 auto;
  font-size:14px;
  line-height:30px;
  max-width:100%;
  position:relative;
  vertical-align:top; }
  .bp3-tab a{
    color:inherit;
    display:block;
    text-decoration:none; }
  .bp3-tab-indicator-wrapper ~ .bp3-tab{
    background-color:transparent !important;
    -webkit-box-shadow:none !important;
            box-shadow:none !important; }
  .bp3-tab[aria-disabled="true"]{
    color:rgba(92, 112, 128, 0.6);
    cursor:not-allowed; }
  .bp3-tab[aria-selected="true"]{
    border-radius:0;
    -webkit-box-shadow:inset 0 -3px 0 #106ba3;
            box-shadow:inset 0 -3px 0 #106ba3; }
  .bp3-tab[aria-selected="true"], .bp3-tab:not([aria-disabled="true"]):hover{
    color:#106ba3; }
  .bp3-tab:focus{
    -moz-outline-radius:0; }
  .bp3-large > .bp3-tab{
    font-size:16px;
    line-height:40px; }

.bp3-tab-panel{
  margin-top:20px; }
  .bp3-tab-panel[aria-hidden="true"]{
    display:none; }

.bp3-tab-indicator-wrapper{
  left:0;
  pointer-events:none;
  position:absolute;
  top:0;
  -webkit-transform:translateX(0), translateY(0);
          transform:translateX(0), translateY(0);
  -webkit-transition:height, width, -webkit-transform;
  transition:height, width, -webkit-transform;
  transition:height, transform, width;
  transition:height, transform, width, -webkit-transform;
  -webkit-transition-duration:200ms;
          transition-duration:200ms;
  -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
          transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-tab-indicator-wrapper .bp3-tab-indicator{
    background-color:#106ba3;
    bottom:0;
    height:3px;
    left:0;
    position:absolute;
    right:0; }
  .bp3-tab-indicator-wrapper.bp3-no-animation{
    -webkit-transition:none;
    transition:none; }

.bp3-dark .bp3-tab{
  color:#f5f8fa; }
  .bp3-dark .bp3-tab[aria-disabled="true"]{
    color:rgba(167, 182, 194, 0.6); }
  .bp3-dark .bp3-tab[aria-selected="true"]{
    -webkit-box-shadow:inset 0 -3px 0 #48aff0;
            box-shadow:inset 0 -3px 0 #48aff0; }
  .bp3-dark .bp3-tab[aria-selected="true"], .bp3-dark .bp3-tab:not([aria-disabled="true"]):hover{
    color:#48aff0; }

.bp3-dark .bp3-tab-indicator{
  background-color:#48aff0; }

.bp3-flex-expander{
  -webkit-box-flex:1;
      -ms-flex:1 1;
          flex:1 1; }
.bp3-tag{
  display:-webkit-inline-box;
  display:-ms-inline-flexbox;
  display:inline-flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  background-color:#5c7080;
  border:none;
  border-radius:3px;
  -webkit-box-shadow:none;
          box-shadow:none;
  color:#f5f8fa;
  font-size:12px;
  line-height:16px;
  max-width:100%;
  min-height:20px;
  min-width:20px;
  padding:2px 6px;
  position:relative; }
  .bp3-tag.bp3-interactive{
    cursor:pointer; }
    .bp3-tag.bp3-interactive:hover{
      background-color:rgba(92, 112, 128, 0.85); }
    .bp3-tag.bp3-interactive.bp3-active, .bp3-tag.bp3-interactive:active{
      background-color:rgba(92, 112, 128, 0.7); }
  .bp3-tag > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-tag > .bp3-fill{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-tag::before,
  .bp3-tag > *{
    margin-right:4px; }
  .bp3-tag:empty::before,
  .bp3-tag > :last-child{
    margin-right:0; }
  .bp3-tag:focus{
    outline:rgba(19, 124, 189, 0.6) auto 2px;
    outline-offset:0;
    -moz-outline-radius:6px; }
  .bp3-tag.bp3-round{
    border-radius:30px;
    padding-left:8px;
    padding-right:8px; }
  .bp3-dark .bp3-tag{
    background-color:#bfccd6;
    color:#182026; }
    .bp3-dark .bp3-tag.bp3-interactive{
      cursor:pointer; }
      .bp3-dark .bp3-tag.bp3-interactive:hover{
        background-color:rgba(191, 204, 214, 0.85); }
      .bp3-dark .bp3-tag.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-interactive:active{
        background-color:rgba(191, 204, 214, 0.7); }
    .bp3-dark .bp3-tag > .bp3-icon, .bp3-dark .bp3-tag .bp3-icon-standard, .bp3-dark .bp3-tag .bp3-icon-large{
      fill:currentColor; }
  .bp3-tag > .bp3-icon, .bp3-tag .bp3-icon-standard, .bp3-tag .bp3-icon-large{
    fill:#ffffff; }
  .bp3-tag.bp3-large,
  .bp3-large .bp3-tag{
    font-size:14px;
    line-height:20px;
    min-height:30px;
    min-width:30px;
    padding:5px 10px; }
    .bp3-tag.bp3-large::before,
    .bp3-tag.bp3-large > *,
    .bp3-large .bp3-tag::before,
    .bp3-large .bp3-tag > *{
      margin-right:7px; }
    .bp3-tag.bp3-large:empty::before,
    .bp3-tag.bp3-large > :last-child,
    .bp3-large .bp3-tag:empty::before,
    .bp3-large .bp3-tag > :last-child{
      margin-right:0; }
    .bp3-tag.bp3-large.bp3-round,
    .bp3-large .bp3-tag.bp3-round{
      padding-left:12px;
      padding-right:12px; }
  .bp3-tag.bp3-intent-primary{
    background:#137cbd;
    color:#ffffff; }
    .bp3-tag.bp3-intent-primary.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-primary.bp3-interactive:hover{
        background-color:rgba(19, 124, 189, 0.85); }
      .bp3-tag.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-primary.bp3-interactive:active{
        background-color:rgba(19, 124, 189, 0.7); }
  .bp3-tag.bp3-intent-success{
    background:#0f9960;
    color:#ffffff; }
    .bp3-tag.bp3-intent-success.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-success.bp3-interactive:hover{
        background-color:rgba(15, 153, 96, 0.85); }
      .bp3-tag.bp3-intent-success.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-success.bp3-interactive:active{
        background-color:rgba(15, 153, 96, 0.7); }
  .bp3-tag.bp3-intent-warning{
    background:#d9822b;
    color:#ffffff; }
    .bp3-tag.bp3-intent-warning.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-warning.bp3-interactive:hover{
        background-color:rgba(217, 130, 43, 0.85); }
      .bp3-tag.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-warning.bp3-interactive:active{
        background-color:rgba(217, 130, 43, 0.7); }
  .bp3-tag.bp3-intent-danger{
    background:#db3737;
    color:#ffffff; }
    .bp3-tag.bp3-intent-danger.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-intent-danger.bp3-interactive:hover{
        background-color:rgba(219, 55, 55, 0.85); }
      .bp3-tag.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-danger.bp3-interactive:active{
        background-color:rgba(219, 55, 55, 0.7); }
  .bp3-tag.bp3-fill{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    width:100%; }
  .bp3-tag.bp3-minimal > .bp3-icon, .bp3-tag.bp3-minimal .bp3-icon-standard, .bp3-tag.bp3-minimal .bp3-icon-large{
    fill:#5c7080; }
  .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]){
    background-color:rgba(138, 155, 168, 0.2);
    color:#182026; }
    .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:hover{
        background-color:rgba(92, 112, 128, 0.3); }
      .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive.bp3-active, .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:active{
        background-color:rgba(92, 112, 128, 0.4); }
    .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]){
      color:#f5f8fa; }
      .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:hover{
          background-color:rgba(191, 204, 214, 0.3); }
        .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]).bp3-interactive:active{
          background-color:rgba(191, 204, 214, 0.4); }
      .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]) > .bp3-icon, .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]) .bp3-icon-standard, .bp3-dark .bp3-tag.bp3-minimal:not([class*="bp3-intent-"]) .bp3-icon-large{
        fill:#a7b6c2; }
  .bp3-tag.bp3-minimal.bp3-intent-primary{
    background-color:rgba(19, 124, 189, 0.15);
    color:#106ba3; }
    .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{
        background-color:rgba(19, 124, 189, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{
        background-color:rgba(19, 124, 189, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-primary > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-large{
      fill:#137cbd; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary{
      background-color:rgba(19, 124, 189, 0.25);
      color:#48aff0; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{
          background-color:rgba(19, 124, 189, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{
          background-color:rgba(19, 124, 189, 0.45); }
  .bp3-tag.bp3-minimal.bp3-intent-success{
    background-color:rgba(15, 153, 96, 0.15);
    color:#0d8050; }
    .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{
        background-color:rgba(15, 153, 96, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{
        background-color:rgba(15, 153, 96, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-success > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-large{
      fill:#0f9960; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success{
      background-color:rgba(15, 153, 96, 0.25);
      color:#3dcc91; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{
          background-color:rgba(15, 153, 96, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{
          background-color:rgba(15, 153, 96, 0.45); }
  .bp3-tag.bp3-minimal.bp3-intent-warning{
    background-color:rgba(217, 130, 43, 0.15);
    color:#bf7326; }
    .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{
        background-color:rgba(217, 130, 43, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{
        background-color:rgba(217, 130, 43, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-warning > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-large{
      fill:#d9822b; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning{
      background-color:rgba(217, 130, 43, 0.25);
      color:#ffb366; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{
          background-color:rgba(217, 130, 43, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{
          background-color:rgba(217, 130, 43, 0.45); }
  .bp3-tag.bp3-minimal.bp3-intent-danger{
    background-color:rgba(219, 55, 55, 0.15);
    color:#c23030; }
    .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{
      cursor:pointer; }
      .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{
        background-color:rgba(219, 55, 55, 0.25); }
      .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{
        background-color:rgba(219, 55, 55, 0.35); }
    .bp3-tag.bp3-minimal.bp3-intent-danger > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-large{
      fill:#db3737; }
    .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger{
      background-color:rgba(219, 55, 55, 0.25);
      color:#ff7373; }
      .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{
        cursor:pointer; }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{
          background-color:rgba(219, 55, 55, 0.35); }
        .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{
          background-color:rgba(219, 55, 55, 0.45); }

.bp3-tag-remove{
  background:none;
  border:none;
  color:inherit;
  cursor:pointer;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  margin-bottom:-2px;
  margin-right:-6px !important;
  margin-top:-2px;
  opacity:0.5;
  padding:2px;
  padding-left:0; }
  .bp3-tag-remove:hover{
    background:none;
    opacity:0.8;
    text-decoration:none; }
  .bp3-tag-remove:active{
    opacity:1; }
  .bp3-tag-remove:empty::before{
    font-family:"Icons16", sans-serif;
    font-size:16px;
    font-style:normal;
    font-weight:400;
    line-height:1;
    -moz-osx-font-smoothing:grayscale;
    -webkit-font-smoothing:antialiased;
    content:""; }
  .bp3-large .bp3-tag-remove{
    margin-right:-10px !important;
    padding:0 5px 0 0; }
    .bp3-large .bp3-tag-remove:empty::before{
      font-family:"Icons20", sans-serif;
      font-size:20px;
      font-style:normal;
      font-weight:400;
      line-height:1; }
.bp3-tag-input{
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  -webkit-box-orient:horizontal;
  -webkit-box-direction:normal;
      -ms-flex-direction:row;
          flex-direction:row;
  -webkit-box-align:start;
      -ms-flex-align:start;
          align-items:flex-start;
  cursor:text;
  height:auto;
  line-height:inherit;
  min-height:30px;
  padding-left:5px;
  padding-right:0; }
  .bp3-tag-input > *{
    -webkit-box-flex:0;
        -ms-flex-positive:0;
            flex-grow:0;
    -ms-flex-negative:0;
        flex-shrink:0; }
  .bp3-tag-input > .bp3-tag-input-values{
    -webkit-box-flex:1;
        -ms-flex-positive:1;
            flex-grow:1;
    -ms-flex-negative:1;
        flex-shrink:1; }
  .bp3-tag-input .bp3-tag-input-icon{
    color:#5c7080;
    margin-left:2px;
    margin-right:7px;
    margin-top:7px; }
  .bp3-tag-input .bp3-tag-input-values{
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex;
    -webkit-box-orient:horizontal;
    -webkit-box-direction:normal;
        -ms-flex-direction:row;
            flex-direction:row;
    -webkit-box-align:center;
        -ms-flex-align:center;
            align-items:center;
    -ms-flex-item-align:stretch;
        align-self:stretch;
    -ms-flex-wrap:wrap;
        flex-wrap:wrap;
    margin-right:7px;
    margin-top:5px;
    min-width:0; }
    .bp3-tag-input .bp3-tag-input-values > *{
      -webkit-box-flex:0;
          -ms-flex-positive:0;
              flex-grow:0;
      -ms-flex-negative:0;
          flex-shrink:0; }
    .bp3-tag-input .bp3-tag-input-values > .bp3-fill{
      -webkit-box-flex:1;
          -ms-flex-positive:1;
              flex-grow:1;
      -ms-flex-negative:1;
          flex-shrink:1; }
    .bp3-tag-input .bp3-tag-input-values::before,
    .bp3-tag-input .bp3-tag-input-values > *{
      margin-right:5px; }
    .bp3-tag-input .bp3-tag-input-values:empty::before,
    .bp3-tag-input .bp3-tag-input-values > :last-child{
      margin-right:0; }
    .bp3-tag-input .bp3-tag-input-values:first-child .bp3-input-ghost:first-child{
      padding-left:5px; }
    .bp3-tag-input .bp3-tag-input-values > *{
      margin-bottom:5px; }
  .bp3-tag-input .bp3-tag{
    overflow-wrap:break-word; }
    .bp3-tag-input .bp3-tag.bp3-active{
      outline:rgba(19, 124, 189, 0.6) auto 2px;
      outline-offset:0;
      -moz-outline-radius:6px; }
  .bp3-tag-input .bp3-input-ghost{
    -webkit-box-flex:1;
        -ms-flex:1 1 auto;
            flex:1 1 auto;
    line-height:20px;
    width:80px; }
    .bp3-tag-input .bp3-input-ghost:disabled, .bp3-tag-input .bp3-input-ghost.bp3-disabled{
      cursor:not-allowed; }
  .bp3-tag-input .bp3-button,
  .bp3-tag-input .bp3-spinner{
    margin:3px;
    margin-left:0; }
  .bp3-tag-input .bp3-button{
    min-height:24px;
    min-width:24px;
    padding:0 7px; }
  .bp3-tag-input.bp3-large{
    height:auto;
    min-height:40px; }
    .bp3-tag-input.bp3-large::before,
    .bp3-tag-input.bp3-large > *{
      margin-right:10px; }
    .bp3-tag-input.bp3-large:empty::before,
    .bp3-tag-input.bp3-large > :last-child{
      margin-right:0; }
    .bp3-tag-input.bp3-large .bp3-tag-input-icon{
      margin-left:5px;
      margin-top:10px; }
    .bp3-tag-input.bp3-large .bp3-input-ghost{
      line-height:30px; }
    .bp3-tag-input.bp3-large .bp3-button{
      min-height:30px;
      min-width:30px;
      padding:5px 10px;
      margin:5px;
      margin-left:0; }
    .bp3-tag-input.bp3-large .bp3-spinner{
      margin:8px;
      margin-left:0; }
  .bp3-tag-input.bp3-active{
    background-color:#ffffff;
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-tag-input.bp3-active.bp3-intent-primary{
      -webkit-box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-tag-input.bp3-active.bp3-intent-success{
      -webkit-box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-tag-input.bp3-active.bp3-intent-warning{
      -webkit-box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
    .bp3-tag-input.bp3-active.bp3-intent-danger{
      -webkit-box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2);
              box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); }
  .bp3-dark .bp3-tag-input .bp3-tag-input-icon, .bp3-tag-input.bp3-dark .bp3-tag-input-icon{
    color:#a7b6c2; }
  .bp3-dark .bp3-tag-input .bp3-input-ghost, .bp3-tag-input.bp3-dark .bp3-input-ghost{
    color:#f5f8fa; }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::-webkit-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-webkit-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::-moz-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-moz-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost:-ms-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost:-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::-ms-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-ms-input-placeholder{
      color:rgba(167, 182, 194, 0.6); }
    .bp3-dark .bp3-tag-input .bp3-input-ghost::placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::placeholder{
      color:rgba(167, 182, 194, 0.6); }
  .bp3-dark .bp3-tag-input.bp3-active, .bp3-tag-input.bp3-dark.bp3-active{
    background-color:rgba(16, 22, 26, 0.3);
    -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-primary, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-primary{
      -webkit-box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-success, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-success{
      -webkit-box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-warning, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-warning{
      -webkit-box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }
    .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-danger, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-danger{
      -webkit-box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4);
              box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); }

.bp3-input-ghost{
  background:none;
  border:none;
  -webkit-box-shadow:none;
          box-shadow:none;
  padding:0; }
  .bp3-input-ghost::-webkit-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input-ghost::-moz-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input-ghost:-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input-ghost::-ms-input-placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input-ghost::placeholder{
    color:rgba(92, 112, 128, 0.6);
    opacity:1; }
  .bp3-input-ghost:focus{
    outline:none !important; }
.bp3-toast{
  -webkit-box-align:start;
      -ms-flex-align:start;
          align-items:flex-start;
  background-color:#ffffff;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  margin:20px 0 0;
  max-width:500px;
  min-width:300px;
  pointer-events:all;
  position:relative !important; }
  .bp3-toast.bp3-toast-enter, .bp3-toast.bp3-toast-appear{
    -webkit-transform:translateY(-40px);
            transform:translateY(-40px); }
  .bp3-toast.bp3-toast-enter-active, .bp3-toast.bp3-toast-appear-active{
    -webkit-transform:translateY(0);
            transform:translateY(0);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); }
  .bp3-toast.bp3-toast-enter ~ .bp3-toast, .bp3-toast.bp3-toast-appear ~ .bp3-toast{
    -webkit-transform:translateY(-40px);
            transform:translateY(-40px); }
  .bp3-toast.bp3-toast-enter-active ~ .bp3-toast, .bp3-toast.bp3-toast-appear-active ~ .bp3-toast{
    -webkit-transform:translateY(0);
            transform:translateY(0);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);
            transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); }
  .bp3-toast.bp3-toast-exit{
    opacity:1;
    -webkit-filter:blur(0);
            filter:blur(0); }
  .bp3-toast.bp3-toast-exit-active{
    opacity:0;
    -webkit-filter:blur(10px);
            filter:blur(10px);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:300ms;
            transition-duration:300ms;
    -webkit-transition-property:opacity, -webkit-filter;
    transition-property:opacity, -webkit-filter;
    transition-property:opacity, filter;
    transition-property:opacity, filter, -webkit-filter;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-toast.bp3-toast-exit ~ .bp3-toast{
    -webkit-transform:translateY(0);
            transform:translateY(0); }
  .bp3-toast.bp3-toast-exit-active ~ .bp3-toast{
    -webkit-transform:translateY(-40px);
            transform:translateY(-40px);
    -webkit-transition-delay:50ms;
            transition-delay:50ms;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-toast .bp3-button-group{
    -webkit-box-flex:0;
        -ms-flex:0 0 auto;
            flex:0 0 auto;
    padding:5px;
    padding-left:0; }
  .bp3-toast > .bp3-icon{
    color:#5c7080;
    margin:12px;
    margin-right:0; }
  .bp3-toast.bp3-dark,
  .bp3-dark .bp3-toast{
    background-color:#394b59;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }
    .bp3-toast.bp3-dark > .bp3-icon,
    .bp3-dark .bp3-toast > .bp3-icon{
      color:#a7b6c2; }
  .bp3-toast[class*="bp3-intent-"] a{
    color:rgba(255, 255, 255, 0.7); }
    .bp3-toast[class*="bp3-intent-"] a:hover{
      color:#ffffff; }
  .bp3-toast[class*="bp3-intent-"] > .bp3-icon{
    color:#ffffff; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button, .bp3-toast[class*="bp3-intent-"] .bp3-button::before,
  .bp3-toast[class*="bp3-intent-"] .bp3-button .bp3-icon, .bp3-toast[class*="bp3-intent-"] .bp3-button:active{
    color:rgba(255, 255, 255, 0.7) !important; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button:focus{
    outline-color:rgba(255, 255, 255, 0.5); }
  .bp3-toast[class*="bp3-intent-"] .bp3-button:hover{
    background-color:rgba(255, 255, 255, 0.15) !important;
    color:#ffffff !important; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button:active{
    background-color:rgba(255, 255, 255, 0.3) !important;
    color:#ffffff !important; }
  .bp3-toast[class*="bp3-intent-"] .bp3-button::after{
    background:rgba(255, 255, 255, 0.3) !important; }
  .bp3-toast.bp3-intent-primary{
    background-color:#137cbd;
    color:#ffffff; }
  .bp3-toast.bp3-intent-success{
    background-color:#0f9960;
    color:#ffffff; }
  .bp3-toast.bp3-intent-warning{
    background-color:#d9822b;
    color:#ffffff; }
  .bp3-toast.bp3-intent-danger{
    background-color:#db3737;
    color:#ffffff; }

.bp3-toast-message{
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  padding:11px;
  word-break:break-word; }

.bp3-toast-container{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  display:-webkit-box !important;
  display:-ms-flexbox !important;
  display:flex !important;
  -webkit-box-orient:vertical;
  -webkit-box-direction:normal;
      -ms-flex-direction:column;
          flex-direction:column;
  left:0;
  overflow:hidden;
  padding:0 20px 20px;
  pointer-events:none;
  position:fixed;
  right:0;
  z-index:40; }
  .bp3-toast-container.bp3-toast-container-top{
    top:0; }
  .bp3-toast-container.bp3-toast-container-bottom{
    bottom:0;
    -webkit-box-orient:vertical;
    -webkit-box-direction:reverse;
        -ms-flex-direction:column-reverse;
            flex-direction:column-reverse;
    top:auto; }
  .bp3-toast-container.bp3-toast-container-left{
    -webkit-box-align:start;
        -ms-flex-align:start;
            align-items:flex-start; }
  .bp3-toast-container.bp3-toast-container-right{
    -webkit-box-align:end;
        -ms-flex-align:end;
            align-items:flex-end; }

.bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active),
.bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active) ~ .bp3-toast, .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active),
.bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active) ~ .bp3-toast,
.bp3-toast-container-bottom .bp3-toast.bp3-toast-exit-active ~ .bp3-toast,
.bp3-toast-container-bottom .bp3-toast.bp3-toast-leave-active ~ .bp3-toast{
  -webkit-transform:translateY(60px);
          transform:translateY(60px); }
.bp3-tooltip{
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2);
  -webkit-transform:scale(1);
          transform:scale(1); }
  .bp3-tooltip .bp3-popover-arrow{
    height:22px;
    position:absolute;
    width:22px; }
    .bp3-tooltip .bp3-popover-arrow::before{
      height:14px;
      margin:4px;
      width:14px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip{
    margin-bottom:11px;
    margin-top:-11px; }
    .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow{
      bottom:-8px; }
      .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(-90deg);
                transform:rotate(-90deg); }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip{
    margin-left:11px; }
    .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow{
      left:-8px; }
      .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(0);
                transform:rotate(0); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip{
    margin-top:11px; }
    .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow{
      top:-8px; }
      .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(90deg);
                transform:rotate(90deg); }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip{
    margin-left:-11px;
    margin-right:11px; }
    .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow{
      right:-8px; }
      .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow svg{
        -webkit-transform:rotate(180deg);
                transform:rotate(180deg); }
  .bp3-tether-element-attached-middle > .bp3-tooltip > .bp3-popover-arrow{
    top:50%;
    -webkit-transform:translateY(-50%);
            transform:translateY(-50%); }
  .bp3-tether-element-attached-center > .bp3-tooltip > .bp3-popover-arrow{
    right:50%;
    -webkit-transform:translateX(50%);
            transform:translateX(50%); }
  .bp3-tether-element-attached-top.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow{
    top:-0.22183px; }
  .bp3-tether-element-attached-right.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow{
    right:-0.22183px; }
  .bp3-tether-element-attached-left.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow{
    left:-0.22183px; }
  .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow{
    bottom:-0.22183px; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-left > .bp3-tooltip{
    -webkit-transform-origin:top left;
            transform-origin:top left; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-center > .bp3-tooltip{
    -webkit-transform-origin:top center;
            transform-origin:top center; }
  .bp3-tether-element-attached-top.bp3-tether-element-attached-right > .bp3-tooltip{
    -webkit-transform-origin:top right;
            transform-origin:top right; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-left > .bp3-tooltip{
    -webkit-transform-origin:center left;
            transform-origin:center left; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-center > .bp3-tooltip{
    -webkit-transform-origin:center center;
            transform-origin:center center; }
  .bp3-tether-element-attached-middle.bp3-tether-element-attached-right > .bp3-tooltip{
    -webkit-transform-origin:center right;
            transform-origin:center right; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left > .bp3-tooltip{
    -webkit-transform-origin:bottom left;
            transform-origin:bottom left; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center > .bp3-tooltip{
    -webkit-transform-origin:bottom center;
            transform-origin:bottom center; }
  .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right > .bp3-tooltip{
    -webkit-transform-origin:bottom right;
            transform-origin:bottom right; }
  .bp3-tooltip .bp3-popover-content{
    background:#394b59;
    color:#f5f8fa; }
  .bp3-tooltip .bp3-popover-arrow::before{
    -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2);
            box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); }
  .bp3-tooltip .bp3-popover-arrow-border{
    fill:#10161a;
    fill-opacity:0.1; }
  .bp3-tooltip .bp3-popover-arrow-fill{
    fill:#394b59; }
  .bp3-popover-enter > .bp3-tooltip, .bp3-popover-appear > .bp3-tooltip{
    -webkit-transform:scale(0.8);
            transform:scale(0.8); }
  .bp3-popover-enter-active > .bp3-tooltip, .bp3-popover-appear-active > .bp3-tooltip{
    -webkit-transform:scale(1);
            transform:scale(1);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-popover-exit > .bp3-tooltip{
    -webkit-transform:scale(1);
            transform:scale(1); }
  .bp3-popover-exit-active > .bp3-tooltip{
    -webkit-transform:scale(0.8);
            transform:scale(0.8);
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:100ms;
            transition-duration:100ms;
    -webkit-transition-property:-webkit-transform;
    transition-property:-webkit-transform;
    transition-property:transform;
    transition-property:transform, -webkit-transform;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-tooltip .bp3-popover-content{
    padding:10px 12px; }
  .bp3-tooltip.bp3-dark,
  .bp3-dark .bp3-tooltip{
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); }
    .bp3-tooltip.bp3-dark .bp3-popover-content,
    .bp3-dark .bp3-tooltip .bp3-popover-content{
      background:#e1e8ed;
      color:#394b59; }
    .bp3-tooltip.bp3-dark .bp3-popover-arrow::before,
    .bp3-dark .bp3-tooltip .bp3-popover-arrow::before{
      -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4);
              box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); }
    .bp3-tooltip.bp3-dark .bp3-popover-arrow-border,
    .bp3-dark .bp3-tooltip .bp3-popover-arrow-border{
      fill:#10161a;
      fill-opacity:0.2; }
    .bp3-tooltip.bp3-dark .bp3-popover-arrow-fill,
    .bp3-dark .bp3-tooltip .bp3-popover-arrow-fill{
      fill:#e1e8ed; }
  .bp3-tooltip.bp3-intent-primary .bp3-popover-content{
    background:#137cbd;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-primary .bp3-popover-arrow-fill{
    fill:#137cbd; }
  .bp3-tooltip.bp3-intent-success .bp3-popover-content{
    background:#0f9960;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-success .bp3-popover-arrow-fill{
    fill:#0f9960; }
  .bp3-tooltip.bp3-intent-warning .bp3-popover-content{
    background:#d9822b;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-warning .bp3-popover-arrow-fill{
    fill:#d9822b; }
  .bp3-tooltip.bp3-intent-danger .bp3-popover-content{
    background:#db3737;
    color:#ffffff; }
  .bp3-tooltip.bp3-intent-danger .bp3-popover-arrow-fill{
    fill:#db3737; }

.bp3-tooltip-indicator{
  border-bottom:dotted 1px;
  cursor:help; }
.bp3-tree .bp3-icon, .bp3-tree .bp3-icon-standard, .bp3-tree .bp3-icon-large{
  color:#5c7080; }
  .bp3-tree .bp3-icon.bp3-intent-primary, .bp3-tree .bp3-icon-standard.bp3-intent-primary, .bp3-tree .bp3-icon-large.bp3-intent-primary{
    color:#137cbd; }
  .bp3-tree .bp3-icon.bp3-intent-success, .bp3-tree .bp3-icon-standard.bp3-intent-success, .bp3-tree .bp3-icon-large.bp3-intent-success{
    color:#0f9960; }
  .bp3-tree .bp3-icon.bp3-intent-warning, .bp3-tree .bp3-icon-standard.bp3-intent-warning, .bp3-tree .bp3-icon-large.bp3-intent-warning{
    color:#d9822b; }
  .bp3-tree .bp3-icon.bp3-intent-danger, .bp3-tree .bp3-icon-standard.bp3-intent-danger, .bp3-tree .bp3-icon-large.bp3-intent-danger{
    color:#db3737; }

.bp3-tree-node-list{
  list-style:none;
  margin:0;
  padding-left:0; }

.bp3-tree-root{
  background-color:transparent;
  cursor:default;
  padding-left:0;
  position:relative; }

.bp3-tree-node-content-0{
  padding-left:0px; }

.bp3-tree-node-content-1{
  padding-left:23px; }

.bp3-tree-node-content-2{
  padding-left:46px; }

.bp3-tree-node-content-3{
  padding-left:69px; }

.bp3-tree-node-content-4{
  padding-left:92px; }

.bp3-tree-node-content-5{
  padding-left:115px; }

.bp3-tree-node-content-6{
  padding-left:138px; }

.bp3-tree-node-content-7{
  padding-left:161px; }

.bp3-tree-node-content-8{
  padding-left:184px; }

.bp3-tree-node-content-9{
  padding-left:207px; }

.bp3-tree-node-content-10{
  padding-left:230px; }

.bp3-tree-node-content-11{
  padding-left:253px; }

.bp3-tree-node-content-12{
  padding-left:276px; }

.bp3-tree-node-content-13{
  padding-left:299px; }

.bp3-tree-node-content-14{
  padding-left:322px; }

.bp3-tree-node-content-15{
  padding-left:345px; }

.bp3-tree-node-content-16{
  padding-left:368px; }

.bp3-tree-node-content-17{
  padding-left:391px; }

.bp3-tree-node-content-18{
  padding-left:414px; }

.bp3-tree-node-content-19{
  padding-left:437px; }

.bp3-tree-node-content-20{
  padding-left:460px; }

.bp3-tree-node-content{
  -webkit-box-align:center;
      -ms-flex-align:center;
          align-items:center;
  display:-webkit-box;
  display:-ms-flexbox;
  display:flex;
  height:30px;
  padding-right:5px;
  width:100%; }
  .bp3-tree-node-content:hover{
    background-color:rgba(191, 204, 214, 0.4); }

.bp3-tree-node-caret,
.bp3-tree-node-caret-none{
  min-width:30px; }

.bp3-tree-node-caret{
  color:#5c7080;
  cursor:pointer;
  padding:7px;
  -webkit-transform:rotate(0deg);
          transform:rotate(0deg);
  -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);
  transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-tree-node-caret:hover{
    color:#182026; }
  .bp3-dark .bp3-tree-node-caret{
    color:#a7b6c2; }
    .bp3-dark .bp3-tree-node-caret:hover{
      color:#f5f8fa; }
  .bp3-tree-node-caret.bp3-tree-node-caret-open{
    -webkit-transform:rotate(90deg);
            transform:rotate(90deg); }
  .bp3-tree-node-caret.bp3-icon-standard::before{
    content:""; }

.bp3-tree-node-icon{
  margin-right:7px;
  position:relative; }

.bp3-tree-node-label{
  overflow:hidden;
  text-overflow:ellipsis;
  white-space:nowrap;
  word-wrap:normal;
  -webkit-box-flex:1;
      -ms-flex:1 1 auto;
          flex:1 1 auto;
  position:relative;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-tree-node-label span{
    display:inline; }

.bp3-tree-node-secondary-label{
  padding:0 5px;
  -webkit-user-select:none;
     -moz-user-select:none;
      -ms-user-select:none;
          user-select:none; }
  .bp3-tree-node-secondary-label .bp3-popover-wrapper,
  .bp3-tree-node-secondary-label .bp3-popover-target{
    -webkit-box-align:center;
        -ms-flex-align:center;
            align-items:center;
    display:-webkit-box;
    display:-ms-flexbox;
    display:flex; }

.bp3-tree-node.bp3-disabled .bp3-tree-node-content{
  background-color:inherit;
  color:rgba(92, 112, 128, 0.6);
  cursor:not-allowed; }

.bp3-tree-node.bp3-disabled .bp3-tree-node-caret,
.bp3-tree-node.bp3-disabled .bp3-tree-node-icon{
  color:rgba(92, 112, 128, 0.6);
  cursor:not-allowed; }

.bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content{
  background-color:#137cbd; }
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content,
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon, .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon-standard, .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon-large{
    color:#ffffff; }
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-tree-node-caret::before{
    color:rgba(255, 255, 255, 0.7); }
  .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-tree-node-caret:hover::before{
    color:#ffffff; }

.bp3-dark .bp3-tree-node-content:hover{
  background-color:rgba(92, 112, 128, 0.3); }

.bp3-dark .bp3-tree .bp3-icon, .bp3-dark .bp3-tree .bp3-icon-standard, .bp3-dark .bp3-tree .bp3-icon-large{
  color:#a7b6c2; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-primary, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-primary, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-primary{
    color:#137cbd; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-success, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-success, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-success{
    color:#0f9960; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-warning, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-warning, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-warning{
    color:#d9822b; }
  .bp3-dark .bp3-tree .bp3-icon.bp3-intent-danger, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-danger, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-danger{
    color:#db3737; }

.bp3-dark .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content{
  background-color:#137cbd; }
.bp3-omnibar{
  -webkit-filter:blur(0);
          filter:blur(0);
  opacity:1;
  background-color:#ffffff;
  border-radius:3px;
  -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
          box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2);
  left:calc(50% - 250px);
  top:20vh;
  width:500px;
  z-index:21; }
  .bp3-omnibar.bp3-overlay-enter, .bp3-omnibar.bp3-overlay-appear{
    -webkit-filter:blur(20px);
            filter:blur(20px);
    opacity:0.2; }
  .bp3-omnibar.bp3-overlay-enter-active, .bp3-omnibar.bp3-overlay-appear-active{
    -webkit-filter:blur(0);
            filter:blur(0);
    opacity:1;
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-property:opacity, -webkit-filter;
    transition-property:opacity, -webkit-filter;
    transition-property:filter, opacity;
    transition-property:filter, opacity, -webkit-filter;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-omnibar.bp3-overlay-exit{
    -webkit-filter:blur(0);
            filter:blur(0);
    opacity:1; }
  .bp3-omnibar.bp3-overlay-exit-active{
    -webkit-filter:blur(20px);
            filter:blur(20px);
    opacity:0.2;
    -webkit-transition-delay:0;
            transition-delay:0;
    -webkit-transition-duration:200ms;
            transition-duration:200ms;
    -webkit-transition-property:opacity, -webkit-filter;
    transition-property:opacity, -webkit-filter;
    transition-property:filter, opacity;
    transition-property:filter, opacity, -webkit-filter;
    -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);
            transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); }
  .bp3-omnibar .bp3-input{
    background-color:transparent;
    border-radius:0; }
    .bp3-omnibar .bp3-input, .bp3-omnibar .bp3-input:focus{
      -webkit-box-shadow:none;
              box-shadow:none; }
  .bp3-omnibar .bp3-menu{
    background-color:transparent;
    border-radius:0;
    -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
            box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15);
    max-height:calc(60vh - 40px);
    overflow:auto; }
    .bp3-omnibar .bp3-menu:empty{
      display:none; }
  .bp3-dark .bp3-omnibar, .bp3-omnibar.bp3-dark{
    background-color:#30404d;
    -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4);
            box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); }

.bp3-omnibar-overlay .bp3-overlay-backdrop{
  background-color:rgba(16, 22, 26, 0.2); }

.bp3-select-popover .bp3-popover-content{
  padding:5px; }

.bp3-select-popover .bp3-input-group{
  margin-bottom:0; }

.bp3-select-popover .bp3-menu{
  max-height:300px;
  max-width:400px;
  overflow:auto;
  padding:0; }
  .bp3-select-popover .bp3-menu:not(:first-child){
    padding-top:5px; }

.bp3-multi-select{
  min-width:150px; }

.bp3-multi-select-popover .bp3-menu{
  max-height:300px;
  max-width:400px;
  overflow:auto; }

.bp3-select-popover .bp3-popover-content{
  padding:5px; }

.bp3-select-popover .bp3-input-group{
  margin-bottom:0; }

.bp3-select-popover .bp3-menu{
  max-height:300px;
  max-width:400px;
  overflow:auto;
  padding:0; }
  .bp3-select-popover .bp3-menu:not(:first-child){
    padding-top:5px; }
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDhoLTIuODFjLS40NS0uNzgtMS4wNy0xLjQ1LTEuODItMS45NkwxNyA0LjQxIDE1LjU5IDNsLTIuMTcgMi4xN0MxMi45NiA1LjA2IDEyLjQ5IDUgMTIgNWMtLjQ5IDAtLjk2LjA2LTEuNDEuMTdMOC40MSAzIDcgNC40MWwxLjYyIDEuNjNDNy44OCA2LjU1IDcuMjYgNy4yMiA2LjgxIDhINHYyaDIuMDljLS4wNS4zMy0uMDkuNjYtLjA5IDF2MUg0djJoMnYxYzAgLjM0LjA0LjY3LjA5IDFINHYyaDIuODFjMS4wNCAxLjc5IDIuOTcgMyA1LjE5IDNzNC4xNS0xLjIxIDUuMTktM0gyMHYtMmgtMi4wOWMuMDUtLjMzLjA5LS42Ni4wOS0xdi0xaDJ2LTJoLTJ2LTFjMC0uMzQtLjA0LS42Ny0uMDktMUgyMFY4em0tNiA4aC00di0yaDR2MnptMC00aC00di0yaDR2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-build: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE0LjkgMTcuNDVDMTYuMjUgMTcuNDUgMTcuMzUgMTYuMzUgMTcuMzUgMTVDMTcuMzUgMTMuNjUgMTYuMjUgMTIuNTUgMTQuOSAxMi41NUMxMy41NCAxMi41NSAxMi40NSAxMy42NSAxMi40NSAxNUMxMi40NSAxNi4zNSAxMy41NCAxNy40NSAxNC45IDE3LjQ1Wk0yMC4xIDE1LjY4TDIxLjU4IDE2Ljg0QzIxLjcxIDE2Ljk1IDIxLjc1IDE3LjEzIDIxLjY2IDE3LjI5TDIwLjI2IDE5LjcxQzIwLjE3IDE5Ljg2IDIwIDE5LjkyIDE5LjgzIDE5Ljg2TDE4LjA5IDE5LjE2QzE3LjczIDE5LjQ0IDE3LjMzIDE5LjY3IDE2LjkxIDE5Ljg1TDE2LjY0IDIxLjdDMTYuNjIgMjEuODcgMTYuNDcgMjIgMTYuMyAyMkgxMy41QzEzLjMyIDIyIDEzLjE4IDIxLjg3IDEzLjE1IDIxLjdMMTIuODkgMTkuODVDMTIuNDYgMTkuNjcgMTIuMDcgMTkuNDQgMTEuNzEgMTkuMTZMOS45NjAwMiAxOS44NkM5LjgxMDAyIDE5LjkyIDkuNjIwMDIgMTkuODYgOS41NDAwMiAxOS43MUw4LjE0MDAyIDE3LjI5QzguMDUwMDIgMTcuMTMgOC4wOTAwMiAxNi45NSA4LjIyMDAyIDE2Ljg0TDkuNzAwMDIgMTUuNjhMOS42NTAwMSAxNUw5LjcwMDAyIDE0LjMxTDguMjIwMDIgMTMuMTZDOC4wOTAwMiAxMy4wNSA4LjA1MDAyIDEyLjg2IDguMTQwMDIgMTIuNzFMOS41NDAwMiAxMC4yOUM5LjYyMDAyIDEwLjEzIDkuODEwMDIgMTAuMDcgOS45NjAwMiAxMC4xM0wxMS43MSAxMC44NEMxMi4wNyAxMC41NiAxMi40NiAxMC4zMiAxMi44OSAxMC4xNUwxMy4xNSA4LjI4OTk4QzEzLjE4IDguMTI5OTggMTMuMzIgNy45OTk5OCAxMy41IDcuOTk5OThIMTYuM0MxNi40NyA3Ljk5OTk4IDE2LjYyIDguMTI5OTggMTYuNjQgOC4yODk5OEwxNi45MSAxMC4xNUMxNy4zMyAxMC4zMiAxNy43MyAxMC41NiAxOC4wOSAxMC44NEwxOS44MyAxMC4xM0MyMCAxMC4wNyAyMC4xNyAxMC4xMyAyMC4yNiAxMC4yOUwyMS42NiAxMi43MUMyMS43NSAxMi44NiAyMS43MSAxMy4wNSAyMS41OCAxMy4xNkwyMC4xIDE0LjMxTDIwLjE1IDE1TDIwLjEgMTUuNjhaIi8+CiAgICA8cGF0aCBkPSJNNy4zMjk2NiA3LjQ0NDU0QzguMDgzMSA3LjAwOTU0IDguMzM5MzIgNi4wNTMzMiA3LjkwNDMyIDUuMjk5ODhDNy40NjkzMiA0LjU0NjQzIDYuNTA4MSA0LjI4MTU2IDUuNzU0NjYgNC43MTY1NkM1LjM5MTc2IDQuOTI2MDggNS4xMjY5NSA1LjI3MTE4IDUuMDE4NDkgNS42NzU5NEM0LjkxMDA0IDYuMDgwNzEgNC45NjY4MiA2LjUxMTk4IDUuMTc2MzQgNi44NzQ4OEM1LjYxMTM0IDcuNjI4MzIgNi41NzYyMiA3Ljg3OTU0IDcuMzI5NjYgNy40NDQ1NFpNOS42NTcxOCA0Ljc5NTkzTDEwLjg2NzIgNC45NTE3OUMxMC45NjI4IDQuOTc3NDEgMTEuMDQwMiA1LjA3MTMzIDExLjAzODIgNS4xODc5M0wxMS4wMzg4IDYuOTg4OTNDMTEuMDQ1NSA3LjEwMDU0IDEwLjk2MTYgNy4xOTUxOCAxMC44NTUgNy4yMTA1NEw5LjY2MDAxIDcuMzgwODNMOS4yMzkxNSA4LjEzMTg4TDkuNjY5NjEgOS4yNTc0NUM5LjcwNzI5IDkuMzYyNzEgOS42NjkzNCA5LjQ3Njk5IDkuNTc0MDggOS41MzE5OUw4LjAxNTIzIDEwLjQzMkM3LjkxMTMxIDEwLjQ5MiA3Ljc5MzM3IDEwLjQ2NzcgNy43MjEwNSAxMC4zODI0TDYuOTg3NDggOS40MzE4OEw2LjEwOTMxIDkuNDMwODNMNS4zNDcwNCAxMC4zOTA1QzUuMjg5MDkgMTAuNDcwMiA1LjE3MzgzIDEwLjQ5MDUgNS4wNzE4NyAxMC40MzM5TDMuNTEyNDUgOS41MzI5M0MzLjQxMDQ5IDkuNDc2MzMgMy4zNzY0NyA5LjM1NzQxIDMuNDEwNzUgOS4yNTY3OUwzLjg2MzQ3IDguMTQwOTNMMy42MTc0OSA3Ljc3NDg4TDMuNDIzNDcgNy4zNzg4M0wyLjIzMDc1IDcuMjEyOTdDMi4xMjY0NyA3LjE5MjM1IDIuMDQwNDkgNy4xMDM0MiAyLjA0MjQ1IDYuOTg2ODJMMi4wNDE4NyA1LjE4NTgyQzIuMDQzODMgNS4wNjkyMiAyLjExOTA5IDQuOTc5NTggMi4yMTcwNCA0Ljk2OTIyTDMuNDIwNjUgNC43OTM5M0wzLjg2NzQ5IDQuMDI3ODhMMy40MTEwNSAyLjkxNzMxQzMuMzczMzcgMi44MTIwNCAzLjQxMTMxIDIuNjk3NzYgMy41MTUyMyAyLjYzNzc2TDUuMDc0MDggMS43Mzc3NkM1LjE2OTM0IDEuNjgyNzYgNS4yODcyOSAxLjcwNzA0IDUuMzU5NjEgMS43OTIzMUw2LjExOTE1IDIuNzI3ODhMNi45ODAwMSAyLjczODkzTDcuNzI0OTYgMS43ODkyMkM3Ljc5MTU2IDEuNzA0NTggNy45MTU0OCAxLjY3OTIyIDguMDA4NzkgMS43NDA4Mkw5LjU2ODIxIDIuNjQxODJDOS42NzAxNyAyLjY5ODQyIDkuNzEyODUgMi44MTIzNCA5LjY4NzIzIDIuOTA3OTdMOS4yMTcxOCA0LjAzMzgzTDkuNDYzMTYgNC4zOTk4OEw5LjY1NzE4IDQuNzk1OTNaIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTYuMTdMNC44MyAxMmwtMS40MiAxLjQxTDkgMTkgMjEgN2wtMS40MS0xLjQxeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1pY29uLWJyYW5kMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNmZmYiPgogICAgPHBhdGggZD0iTTEwNSAxMjcuM2g0MHYxMi44aC00MHpNNTEuMSA3N0w3NCA5OS45bC0yMy4zIDIzLjMgMTAuNSAxMC41IDIzLjMtMjMuM0w5NSA5OS45IDg0LjUgODkuNCA2MS42IDY2LjV6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNGOUE4MjUiPgogICAgPHBhdGggZD0iTTIwLjIgMTEuOGMtMS42IDAtMS43LjUtMS43IDEgMCAuNC4xLjkuMSAxLjMuMS41LjEuOS4xIDEuMyAwIDEuNy0xLjQgMi4zLTMuNSAyLjNoLS45di0xLjloLjVjMS4xIDAgMS40IDAgMS40LS44IDAtLjMgMC0uNi0uMS0xIDAtLjQtLjEtLjgtLjEtMS4yIDAtMS4zIDAtMS44IDEuMy0yLTEuMy0uMi0xLjMtLjctMS4zLTIgMC0uNC4xLS44LjEtMS4yLjEtLjQuMS0uNy4xLTEgMC0uOC0uNC0uNy0xLjQtLjhoLS41VjQuMWguOWMyLjIgMCAzLjUuNyAzLjUgMi4zIDAgLjQtLjEuOS0uMSAxLjMtLjEuNS0uMS45LS4xIDEuMyAwIC41LjIgMSAxLjcgMXYxLjh6TTEuOCAxMC4xYzEuNiAwIDEuNy0uNSAxLjctMSAwLS40LS4xLS45LS4xLTEuMy0uMS0uNS0uMS0uOS0uMS0xLjMgMC0xLjYgMS40LTIuMyAzLjUtMi4zaC45djEuOWgtLjVjLTEgMC0xLjQgMC0xLjQuOCAwIC4zIDAgLjYuMSAxIDAgLjIuMS42LjEgMSAwIDEuMyAwIDEuOC0xLjMgMkM2IDExLjIgNiAxMS43IDYgMTNjMCAuNC0uMS44LS4xIDEuMi0uMS4zLS4xLjctLjEgMSAwIC44LjMuOCAxLjQuOGguNXYxLjloLS45Yy0yLjEgMC0zLjUtLjYtMy41LTIuMyAwLS40LjEtLjkuMS0xLjMuMS0uNS4xLS45LjEtMS4zIDAtLjUtLjItMS0xLjctMXYtMS45eiIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSIxMy44IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY3g9IjExIiBjeT0iOC4yIiByPSIyLjEiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMzkiIGhlaWdodD0iNTEiIHZpZXdCb3g9IjAgMCAzOSA1MSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMTYzOCAtMjI4MSkiPgogICAgPGcgY2xhc3M9ImpwLWljb24td2FybjAiIGZpbGw9IiNGMzc3MjYiPgogICAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxNjM5Ljc0IDIzMTEuOTgpIiBkPSJNIDE4LjI2NDYgNy4xMzQxMUMgMTAuNDE0NSA3LjEzNDExIDMuNTU4NzIgNC4yNTc2IDAgMEMgMS4zMjUzOSAzLjgyMDQgMy43OTU1NiA3LjEzMDgxIDcuMDY4NiA5LjQ3MzAzQyAxMC4zNDE3IDExLjgxNTIgMTQuMjU1NyAxMy4wNzM0IDE4LjI2OSAxMy4wNzM0QyAyMi4yODIzIDEzLjA3MzQgMjYuMTk2MyAxMS44MTUyIDI5LjQ2OTQgOS40NzMwM0MgMzIuNzQyNCA3LjEzMDgxIDM1LjIxMjYgMy44MjA0IDM2LjUzOCAwQyAzMi45NzA1IDQuMjU3NiAyNi4xMTQ4IDcuMTM0MTEgMTguMjY0NiA3LjEzNDExWiIvPgogICAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxNjM5LjczIDIyODUuNDgpIiBkPSJNIDE4LjI3MzMgNS45MzkzMUMgMjYuMTIzNSA1LjkzOTMxIDMyLjk3OTMgOC44MTU4MyAzNi41MzggMTMuMDczNEMgMzUuMjEyNiA5LjI1MzAzIDMyLjc0MjQgNS45NDI2MiAyOS40Njk0IDMuNjAwNEMgMjYuMTk2MyAxLjI1ODE4IDIyLjI4MjMgMCAxOC4yNjkgMEMgMTQuMjU1NyAwIDEwLjM0MTcgMS4yNTgxOCA3LjA2ODYgMy42MDA0QyAzLjc5NTU2IDUuOTQyNjIgMS4zMjUzOSA5LjI1MzAzIDAgMTMuMDczNEMgMy41Njc0NSA4LjgyNDYzIDEwLjQyMzIgNS45MzkzMSAxOC4yNzMzIDUuOTM5MzFaIi8+CiAgICA8L2c+CiAgICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxNjY5LjMgMjI4MS4zMSkiIGQ9Ik0gNS44OTM1MyAyLjg0NEMgNS45MTg4OSAzLjQzMTY1IDUuNzcwODUgNC4wMTM2NyA1LjQ2ODE1IDQuNTE2NDVDIDUuMTY1NDUgNS4wMTkyMiA0LjcyMTY4IDUuNDIwMTUgNC4xOTI5OSA1LjY2ODUxQyAzLjY2NDMgNS45MTY4OCAzLjA3NDQ0IDYuMDAxNTEgMi40OTgwNSA1LjkxMTcxQyAxLjkyMTY2IDUuODIxOSAxLjM4NDYzIDUuNTYxNyAwLjk1NDg5OCA1LjE2NDAxQyAwLjUyNTE3IDQuNzY2MzMgMC4yMjIwNTYgNC4yNDkwMyAwLjA4MzkwMzcgMy42Nzc1N0MgLTAuMDU0MjQ4MyAzLjEwNjExIC0wLjAyMTIzIDIuNTA2MTcgMC4xNzg3ODEgMS45NTM2NEMgMC4zNzg3OTMgMS40MDExIDAuNzM2ODA5IDAuOTIwODE3IDEuMjA3NTQgMC41NzM1MzhDIDEuNjc4MjYgMC4yMjYyNTkgMi4yNDA1NSAwLjAyNzU5MTkgMi44MjMyNiAwLjAwMjY3MjI5QyAzLjYwMzg5IC0wLjAzMDcxMTUgNC4zNjU3MyAwLjI0OTc4OSA0Ljk0MTQyIDAuNzgyNTUxQyA1LjUxNzExIDEuMzE1MzEgNS44NTk1NiAyLjA1Njc2IDUuODkzNTMgMi44NDRaIi8+CiAgICAgIDxwYXRoIHRyYW5zZm9ybT0idHJhbnNsYXRlKDE2MzkuOCAyMzIzLjgxKSIgZD0iTSA3LjQyNzg5IDMuNTgzMzhDIDcuNDYwMDggNC4zMjQzIDcuMjczNTUgNS4wNTgxOSA2Ljg5MTkzIDUuNjkyMTNDIDYuNTEwMzEgNi4zMjYwNyA1Ljk1MDc1IDYuODMxNTYgNS4yODQxMSA3LjE0NDZDIDQuNjE3NDcgNy40NTc2MyAzLjg3MzcxIDcuNTY0MTQgMy4xNDcwMiA3LjQ1MDYzQyAyLjQyMDMyIDcuMzM3MTIgMS43NDMzNiA3LjAwODcgMS4yMDE4NCA2LjUwNjk1QyAwLjY2MDMyOCA2LjAwNTIgMC4yNzg2MSA1LjM1MjY4IDAuMTA1MDE3IDQuNjMyMDJDIC0wLjA2ODU3NTcgMy45MTEzNSAtMC4wMjYyMzYxIDMuMTU0OTQgMC4yMjY2NzUgMi40NTg1NkMgMC40Nzk1ODcgMS43NjIxNyAwLjkzMTY5NyAxLjE1NzEzIDEuNTI1NzYgMC43MjAwMzNDIDIuMTE5ODMgMC4yODI5MzUgMi44MjkxNCAwLjAzMzQzOTUgMy41NjM4OSAwLjAwMzEzMzQ0QyA0LjU0NjY3IC0wLjAzNzQwMzMgNS41MDUyOSAwLjMxNjcwNiA2LjIyOTYxIDAuOTg3ODM1QyA2Ljk1MzkzIDEuNjU4OTYgNy4zODQ4NCAyLjU5MjM1IDcuNDI3ODkgMy41ODMzOEwgNy40Mjc4OSAzLjU4MzM4WiIvPgogICAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxNjM4LjM2IDIyODYuMDYpIiBkPSJNIDIuMjc0NzEgNC4zOTYyOUMgMS44NDM2MyA0LjQxNTA4IDEuNDE2NzEgNC4zMDQ0NSAxLjA0Nzk5IDQuMDc4NDNDIDAuNjc5MjY4IDMuODUyNCAwLjM4NTMyOCAzLjUyMTE0IDAuMjAzMzcxIDMuMTI2NTZDIDAuMDIxNDEzNiAyLjczMTk4IC0wLjA0MDM3OTggMi4yOTE4MyAwLjAyNTgxMTYgMS44NjE4MUMgMC4wOTIwMDMxIDEuNDMxOCAwLjI4MzIwNCAxLjAzMTI2IDAuNTc1MjEzIDAuNzEwODgzQyAwLjg2NzIyMiAwLjM5MDUxIDEuMjQ2OTEgMC4xNjQ3MDggMS42NjYyMiAwLjA2MjA1OTJDIDIuMDg1NTMgLTAuMDQwNTg5NyAyLjUyNTYxIC0wLjAxNTQ3MTQgMi45MzA3NiAwLjEzNDIzNUMgMy4zMzU5MSAwLjI4Mzk0MSAzLjY4NzkyIDAuNTUxNTA1IDMuOTQyMjIgMC45MDMwNkMgNC4xOTY1MiAxLjI1NDYyIDQuMzQxNjkgMS42NzQzNiA0LjM1OTM1IDIuMTA5MTZDIDQuMzgyOTkgMi42OTEwNyA0LjE3Njc4IDMuMjU4NjkgMy43ODU5NyAzLjY4NzQ2QyAzLjM5NTE2IDQuMTE2MjQgMi44NTE2NiA0LjM3MTE2IDIuMjc0NzEgNC4zOTYyOUwgMi4yNzQ3MSA0LjM5NjI5WiIvPgogICAgPC9nPgogIDwvZz4+Cjwvc3ZnPgo=);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4=);
  --jp-icon-listings-info: url(data:image/svg+xml;base64,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);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMEQ0N0ExIj4KICAgIDxwYXRoIGQ9Ik0xMS4xIDYuOVY1LjhINi45YzAtLjUgMC0xLjMuMi0xLjYuNC0uNy44LTEuMSAxLjctMS40IDEuNy0uMyAyLjUtLjMgMy45LS4xIDEgLjEgMS45LjkgMS45IDEuOXY0LjJjMCAuNS0uOSAxLjYtMiAxLjZIOC44Yy0xLjUgMC0yLjQgMS40LTIuNCAyLjh2Mi4ySDQuN0MzLjUgMTUuMSAzIDE0IDMgMTMuMVY5Yy0uMS0xIC42LTIgMS44LTIgMS41LS4xIDYuMy0uMSA2LjMtLjF6Ii8+CiAgICA8cGF0aCBkPSJNMTAuOSAxNS4xdjEuMWg0LjJjMCAuNSAwIDEuMy0uMiAxLjYtLjQuNy0uOCAxLjEtMS43IDEuNC0xLjcuMy0yLjUuMy0zLjkuMS0xLS4xLTEuOS0uOS0xLjktMS45di00LjJjMC0uNS45LTEuNiAyLTEuNmgzLjhjMS41IDAgMi40LTEuNCAyLjQtMi44VjYuNmgxLjdDMTguNSA2LjkgMTkgOCAxOSA4LjlWMTNjMCAxLS43IDIuMS0xLjkgMi4xaC02LjJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4=);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTUgMTVIM3YyaDEydi0yem0wLThIM3YyaDEyVjd6TTMgMTNoMTh2LTJIM3Yyem0wIDhoMTh2LTJIM3Yyek0zIDN2MmgxOFYzSDN6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2ZXJzaW9uPSIxLjEiIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgoJPHBhdGggZD0iTTcsNUgyMVY3SDdWNU03LDEzVjExSDIxVjEzSDdNNCw0LjVBMS41LDEuNSAwIDAsMSA1LjUsNkExLjUsMS41IDAgMCwxIDQsNy41QTEuNSwxLjUgMCAwLDEgMi41LDZBMS41LDEuNSAwIDAsMSA0LDQuNU00LDEwLjVBMS41LDEuNSAwIDAsMSA1LjUsMTJBMS41LDEuNSAwIDAsMSA0LDEzLjVBMS41LDEuNSAwIDAsMSAyLjUsMTJBMS41LDEuNSAwIDAsMSA0LDEwLjVNNywxOVYxN0gyMVYxOUg3TTQsMTYuNUExLjUsMS41IDAgMCwxIDUuNSwxOEExLjUsMS41IDAgMCwxIDQsMTkuNUExLjUsMS41IDAgMCwxIDIuNSwxOEExLjUsMS41IDAgMCwxIDQsMTYuNVoiIC8+Cjwvc3ZnPgo=);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4=);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}
.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}
.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}
.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}
.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}
.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}
.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}
.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}
.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}
.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}
.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}
.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}
.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}
.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}
.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}
.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}
.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}
.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}
.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}
.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}
.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}
.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}
.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}
.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}
.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}
.jp-FileIcon {
  background-image: var(--jp-icon-file);
}
.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}
.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}
.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}
.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}
.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}
.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}
.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}
.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}
.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}
.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}
.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}
.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}
.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}
.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}
.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}
.jp-ListIcon {
  background-image: var(--jp-icon-list);
}
.jp-ListingsInfoIcon {
  background-image: var(--jp-icon-listings-info);
}
.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}
.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}
.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}
.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}
.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}
.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}
.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}
.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}
.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}
.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}
.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}
.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}
.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}
.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}
.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}
.jp-RunIcon {
  background-image: var(--jp-icon-run);
}
.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}
.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}
.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}
.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}
.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}
.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}
.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}
.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}
.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}
.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}
.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}
.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}
.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}
.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}
.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}
.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}
.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

:root {
  --jp-icon-search-white: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
}

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}
/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}
/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}
/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}
.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}
.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}
.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}
.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}
.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}
.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}
.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}
.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}
/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}
.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}
.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}
.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}
.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}
.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}
.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}
/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}
.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}
.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}
.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}
.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}
.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}
.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

/* CSS for icons in selected items in the settings editor */
#setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}
#setting-editor
  .jp-PluginList
  .jp-mod-selected
  .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* CSS for icons in selected tabs in the sidebar tab manager */
#tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable[fill] {
  fill: #fff;
}

#tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}
#tab-manager
  .lm-TabBar-tab.jp-mod-active
  .jp-icon-hover
  :hover
  .jp-icon-selectable[fill] {
  fill: var(--jp-brand-color1);
}

#tab-manager
  .lm-TabBar-tab.jp-mod-active
  .jp-icon-hover
  :hover
  .jp-icon-selectable-inverse[fill] {
  fill: #fff;
}

/**
 * TODO: come up with non css-hack solution for showing the busy icon on top
 *  of the close icon
 * CSS for complex behavior of close icon of tabs in the sidebar tab manager
 */
#tab-manager
  .lm-TabBar-tab.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}
#tab-manager
  .lm-TabBar-tab.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

#tab-manager
  .lm-TabBar-tab.jp-mod-dirty.jp-mod-active
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: #fff;
}

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}
/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) svg {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-border-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0px;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-warn-color0);
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

/* Override Blueprint's _reset.scss styles */
html {
  box-sizing: unset;
}

*,
*::before,
*::after {
  box-sizing: unset;
}

body {
  color: unset;
  font-family: var(--jp-ui-font-family);
}

p {
  margin-top: unset;
  margin-bottom: unset;
}

small {
  font-size: unset;
}

strong {
  font-weight: unset;
}

/* Override Blueprint's _typography.scss styles */
a {
  text-decoration: unset;
  color: unset;
}
a:hover {
  text-decoration: unset;
  color: unset;
}

/* Override Blueprint's _accessibility.scss styles */
:focus {
  outline: unset;
  outline-offset: unset;
  -moz-outline-radius: unset;
}

/* Styles for ui-components */
.jp-Button {
  border-radius: var(--jp-border-radius);
  padding: 0px 12px;
  font-size: var(--jp-ui-font-size1);
}

/* Use our own theme for hover styles */
button.jp-Button.bp3-button.bp3-minimal:hover {
  background-color: var(--jp-layout-color2);
}
.jp-Button.minimal {
  color: unset !important;
}

.jp-Button.jp-ToolbarButtonComponent {
  text-transform: none;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color3);
}

.jp-BPIcon {
  display: inline-block;
  vertical-align: middle;
  margin: auto;
}

/* Stop blueprint futzing with our icon fills */
.bp3-icon.jp-BPIcon > svg:not([fill]) {
  fill: var(--jp-inverse-layout-color3);
}

.jp-InputGroupAction {
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

/* Use our own theme for hover and option styles */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}
select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-top: 1px solid var(--jp-border-color2);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-Collapse-header {
  padding: 1px 12px;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size2);
}

.jp-Collapse-header:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Collapse-contents {
  padding: 0px 12px 0px 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0px;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0px 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.lm-CommandPalette-wrapper::after {
  content: ' ';
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  height: 30px;
  width: 10px;
  padding: 0px 10px;
  background-image: var(--jp-icon-search-white);
  background-size: 20px;
  background-repeat: no-repeat;
  background-position: center;
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color3);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0px;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color3);
}

.lm-CommandPalette-item.lm-mod-active {
  background: var(--jp-layout-color3);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  background: var(--jp-layout-color4);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color3);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.4;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty:after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0px 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0px;
  left: 0px;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px;
  padding-bottom: 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);
  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0px;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0px 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

.jp-HoverBox.jp-mod-outofview {
  display: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `p-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame:before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;

  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;

  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #aa00ff;

  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;

  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;

  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;

  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;

  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;

  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;

  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;

  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;

  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;

  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ffff00;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;

  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;

  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;

  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;

  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;

  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eeeeee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;

  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent:before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent:after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }
  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }
  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0px 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  height: 28px;
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  background-color: var(--jp-layout-color1);
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0px 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  height: 32px;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 1;
  overflow-x: hidden;
}

.jp-Toolbar:hover {
  overflow-x: auto;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0px;
  margin: 0px;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0px 6px;
  margin: 0px;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent span {
  padding: 0px;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/


/* <DEPRECATED> */ body.p-mod-override-cursor *, /* </DEPRECATED> */
body.lm-mod-override-cursor * {
  cursor: inherit !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0px;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/* BASICS */

.CodeMirror {
  /* Set height, width, borders, and global font properties here */
  font-family: monospace;
  height: 300px;
  color: black;
  direction: ltr;
}

/* PADDING */

.CodeMirror-lines {
  padding: 4px 0; /* Vertical padding around content */
}
.CodeMirror pre.CodeMirror-line,
.CodeMirror pre.CodeMirror-line-like {
  padding: 0 4px; /* Horizontal padding of content */
}

.CodeMirror-scrollbar-filler, .CodeMirror-gutter-filler {
  background-color: white; /* The little square between H and V scrollbars */
}

/* GUTTER */

.CodeMirror-gutters {
  border-right: 1px solid #ddd;
  background-color: #f7f7f7;
  white-space: nowrap;
}
.CodeMirror-linenumbers {}
.CodeMirror-linenumber {
  padding: 0 3px 0 5px;
  min-width: 20px;
  text-align: right;
  color: #999;
  white-space: nowrap;
}

.CodeMirror-guttermarker { color: black; }
.CodeMirror-guttermarker-subtle { color: #999; }

/* CURSOR */

.CodeMirror-cursor {
  border-left: 1px solid black;
  border-right: none;
  width: 0;
}
/* Shown when moving in bi-directional text */
.CodeMirror div.CodeMirror-secondarycursor {
  border-left: 1px solid silver;
}
.cm-fat-cursor .CodeMirror-cursor {
  width: auto;
  border: 0 !important;
  background: #7e7;
}
.cm-fat-cursor div.CodeMirror-cursors {
  z-index: 1;
}
.cm-fat-cursor-mark {
  background-color: rgba(20, 255, 20, 0.5);
  -webkit-animation: blink 1.06s steps(1) infinite;
  -moz-animation: blink 1.06s steps(1) infinite;
  animation: blink 1.06s steps(1) infinite;
}
.cm-animate-fat-cursor {
  width: auto;
  border: 0;
  -webkit-animation: blink 1.06s steps(1) infinite;
  -moz-animation: blink 1.06s steps(1) infinite;
  animation: blink 1.06s steps(1) infinite;
  background-color: #7e7;
}
@-moz-keyframes blink {
  0% {}
  50% { background-color: transparent; }
  100% {}
}
@-webkit-keyframes blink {
  0% {}
  50% { background-color: transparent; }
  100% {}
}
@keyframes blink {
  0% {}
  50% { background-color: transparent; }
  100% {}
}

/* Can style cursor different in overwrite (non-insert) mode */
.CodeMirror-overwrite .CodeMirror-cursor {}

.cm-tab { display: inline-block; text-decoration: inherit; }

.CodeMirror-rulers {
  position: absolute;
  left: 0; right: 0; top: -50px; bottom: 0;
  overflow: hidden;
}
.CodeMirror-ruler {
  border-left: 1px solid #ccc;
  top: 0; bottom: 0;
  position: absolute;
}

/* DEFAULT THEME */

.cm-s-default .cm-header {color: blue;}
.cm-s-default .cm-quote {color: #090;}
.cm-negative {color: #d44;}
.cm-positive {color: #292;}
.cm-header, .cm-strong {font-weight: bold;}
.cm-em {font-style: italic;}
.cm-link {text-decoration: underline;}
.cm-strikethrough {text-decoration: line-through;}

.cm-s-default .cm-keyword {color: #708;}
.cm-s-default .cm-atom {color: #219;}
.cm-s-default .cm-number {color: #164;}
.cm-s-default .cm-def {color: #00f;}
.cm-s-default .cm-variable,
.cm-s-default .cm-punctuation,
.cm-s-default .cm-property,
.cm-s-default .cm-operator {}
.cm-s-default .cm-variable-2 {color: #05a;}
.cm-s-default .cm-variable-3, .cm-s-default .cm-type {color: #085;}
.cm-s-default .cm-comment {color: #a50;}
.cm-s-default .cm-string {color: #a11;}
.cm-s-default .cm-string-2 {color: #f50;}
.cm-s-default .cm-meta {color: #555;}
.cm-s-default .cm-qualifier {color: #555;}
.cm-s-default .cm-builtin {color: #30a;}
.cm-s-default .cm-bracket {color: #997;}
.cm-s-default .cm-tag {color: #170;}
.cm-s-default .cm-attribute {color: #00c;}
.cm-s-default .cm-hr {color: #999;}
.cm-s-default .cm-link {color: #00c;}

.cm-s-default .cm-error {color: #f00;}
.cm-invalidchar {color: #f00;}

.CodeMirror-composing { border-bottom: 2px solid; }

/* Default styles for common addons */

div.CodeMirror span.CodeMirror-matchingbracket {color: #0b0;}
div.CodeMirror span.CodeMirror-nonmatchingbracket {color: #a22;}
.CodeMirror-matchingtag { background: rgba(255, 150, 0, .3); }
.CodeMirror-activeline-background {background: #e8f2ff;}

/* STOP */

/* The rest of this file contains styles related to the mechanics of
   the editor. You probably shouldn't touch them. */

.CodeMirror {
  position: relative;
  overflow: hidden;
  background: white;
}

.CodeMirror-scroll {
  overflow: scroll !important; /* Things will break if this is overridden */
  /* 50px is the magic margin used to hide the element's real scrollbars */
  /* See overflow: hidden in .CodeMirror */
  margin-bottom: -50px; margin-right: -50px;
  padding-bottom: 50px;
  height: 100%;
  outline: none; /* Prevent dragging from highlighting the element */
  position: relative;
}
.CodeMirror-sizer {
  position: relative;
  border-right: 50px solid transparent;
}

/* The fake, visible scrollbars. Used to force redraw during scrolling
   before actual scrolling happens, thus preventing shaking and
   flickering artifacts. */
.CodeMirror-vscrollbar, .CodeMirror-hscrollbar, .CodeMirror-scrollbar-filler, .CodeMirror-gutter-filler {
  position: absolute;
  z-index: 6;
  display: none;
}
.CodeMirror-vscrollbar {
  right: 0; top: 0;
  overflow-x: hidden;
  overflow-y: scroll;
}
.CodeMirror-hscrollbar {
  bottom: 0; left: 0;
  overflow-y: hidden;
  overflow-x: scroll;
}
.CodeMirror-scrollbar-filler {
  right: 0; bottom: 0;
}
.CodeMirror-gutter-filler {
  left: 0; bottom: 0;
}

.CodeMirror-gutters {
  position: absolute; left: 0; top: 0;
  min-height: 100%;
  z-index: 3;
}
.CodeMirror-gutter {
  white-space: normal;
  height: 100%;
  display: inline-block;
  vertical-align: top;
  margin-bottom: -50px;
}
.CodeMirror-gutter-wrapper {
  position: absolute;
  z-index: 4;
  background: none !important;
  border: none !important;
}
.CodeMirror-gutter-background {
  position: absolute;
  top: 0; bottom: 0;
  z-index: 4;
}
.CodeMirror-gutter-elt {
  position: absolute;
  cursor: default;
  z-index: 4;
}
.CodeMirror-gutter-wrapper ::selection { background-color: transparent }
.CodeMirror-gutter-wrapper ::-moz-selection { background-color: transparent }

.CodeMirror-lines {
  cursor: text;
  min-height: 1px; /* prevents collapsing before first draw */
}
.CodeMirror pre.CodeMirror-line,
.CodeMirror pre.CodeMirror-line-like {
  /* Reset some styles that the rest of the page might have set */
  -moz-border-radius: 0; -webkit-border-radius: 0; border-radius: 0;
  border-width: 0;
  background: transparent;
  font-family: inherit;
  font-size: inherit;
  margin: 0;
  white-space: pre;
  word-wrap: normal;
  line-height: inherit;
  color: inherit;
  z-index: 2;
  position: relative;
  overflow: visible;
  -webkit-tap-highlight-color: transparent;
  -webkit-font-variant-ligatures: contextual;
  font-variant-ligatures: contextual;
}
.CodeMirror-wrap pre.CodeMirror-line,
.CodeMirror-wrap pre.CodeMirror-line-like {
  word-wrap: break-word;
  white-space: pre-wrap;
  word-break: normal;
}

.CodeMirror-linebackground {
  position: absolute;
  left: 0; right: 0; top: 0; bottom: 0;
  z-index: 0;
}

.CodeMirror-linewidget {
  position: relative;
  z-index: 2;
  padding: 0.1px; /* Force widget margins to stay inside of the container */
}

.CodeMirror-widget {}

.CodeMirror-rtl pre { direction: rtl; }

.CodeMirror-code {
  outline: none;
}

/* Force content-box sizing for the elements where we expect it */
.CodeMirror-scroll,
.CodeMirror-sizer,
.CodeMirror-gutter,
.CodeMirror-gutters,
.CodeMirror-linenumber {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
}

.CodeMirror-measure {
  position: absolute;
  width: 100%;
  height: 0;
  overflow: hidden;
  visibility: hidden;
}

.CodeMirror-cursor {
  position: absolute;
  pointer-events: none;
}
.CodeMirror-measure pre { position: static; }

div.CodeMirror-cursors {
  visibility: hidden;
  position: relative;
  z-index: 3;
}
div.CodeMirror-dragcursors {
  visibility: visible;
}

.CodeMirror-focused div.CodeMirror-cursors {
  visibility: visible;
}

.CodeMirror-selected { background: #d9d9d9; }
.CodeMirror-focused .CodeMirror-selected { background: #d7d4f0; }
.CodeMirror-crosshair { cursor: crosshair; }
.CodeMirror-line::selection, .CodeMirror-line > span::selection, .CodeMirror-line > span > span::selection { background: #d7d4f0; }
.CodeMirror-line::-moz-selection, .CodeMirror-line > span::-moz-selection, .CodeMirror-line > span > span::-moz-selection { background: #d7d4f0; }

.cm-searching {
  background-color: #ffa;
  background-color: rgba(255, 255, 0, .4);
}

/* Used to force a border model for a node */
.cm-force-border { padding-right: .1px; }

@media print {
  /* Hide the cursor when printing */
  .CodeMirror div.CodeMirror-cursors {
    visibility: hidden;
  }
}

/* See issue #2901 */
.cm-tab-wrap-hack:after { content: ''; }

/* Help users use markselection to safely style text background */
span.CodeMirror-selectedtext { background: none; }

.CodeMirror-dialog {
  position: absolute;
  left: 0; right: 0;
  background: inherit;
  z-index: 15;
  padding: .1em .8em;
  overflow: hidden;
  color: inherit;
}

.CodeMirror-dialog-top {
  border-bottom: 1px solid #eee;
  top: 0;
}

.CodeMirror-dialog-bottom {
  border-top: 1px solid #eee;
  bottom: 0;
}

.CodeMirror-dialog input {
  border: none;
  outline: none;
  background: transparent;
  width: 20em;
  color: inherit;
  font-family: monospace;
}

.CodeMirror-dialog button {
  font-size: 70%;
}

.CodeMirror-foldmarker {
  color: blue;
  text-shadow: #b9f 1px 1px 2px, #b9f -1px -1px 2px, #b9f 1px -1px 2px, #b9f -1px 1px 2px;
  font-family: arial;
  line-height: .3;
  cursor: pointer;
}
.CodeMirror-foldgutter {
  width: .7em;
}
.CodeMirror-foldgutter-open,
.CodeMirror-foldgutter-folded {
  cursor: pointer;
}
.CodeMirror-foldgutter-open:after {
  content: "\25BE";
}
.CodeMirror-foldgutter-folded:after {
  content: "\25B8";
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.CodeMirror {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;
  /* Changed to auto to autogrow */
}

.CodeMirror pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

/* This causes https://github.com/jupyter/jupyterlab/issues/522 */
/* May not cause it not because we changed it! */
.CodeMirror-lines {
  padding: var(--jp-code-padding) 0;
}

.CodeMirror-linenumber {
  padding: 0 8px;
}

.jp-CodeMirrorEditor {
  cursor: text;
}

.jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor {
  border-left: var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color);
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.CodeMirror.jp-mod-readOnly .CodeMirror-cursor {
  display: none;
}

.CodeMirror-gutters {
  border-right: 1px solid var(--jp-border-color2);
  background-color: var(--jp-layout-color0);
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.CodeMirror-selectedtext.cm-searching {
  background-color: var(--jp-search-selected-match-background-color) !important;
  color: var(--jp-search-selected-match-color) !important;
}

.cm-searching {
  background-color: var(
    --jp-search-unselected-match-background-color
  ) !important;
  color: var(--jp-search-unselected-match-color) !important;
}

.CodeMirror-focused .CodeMirror-selected {
  background-color: var(--jp-editor-selected-focused-background);
}

.CodeMirror-selected {
  background-color: var(--jp-editor-selected-background);
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/**
 * Here is our jupyter theme for CodeMirror syntax highlighting
 * This is used in our marked.js syntax highlighting and CodeMirror itself
 * The string "jupyter" is set in ../codemirror/widget.DEFAULT_CODEMIRROR_THEME
 * This came from the classic notebook, which came form highlight.js/GitHub
 */

/**
 * CodeMirror themes are handling the background/color in this way. This works
 * fine for CodeMirror editors outside the notebook, but the notebook styles
 * these things differently.
 */
.CodeMirror.cm-s-jupyter {
  background: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

/* In the notebook, we want this styling to be handled by its container */
.jp-CodeConsole .CodeMirror.cm-s-jupyter,
.jp-Notebook .CodeMirror.cm-s-jupyter {
  background: transparent;
}

.cm-s-jupyter .CodeMirror-cursor {
  border-left: var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color);
}
.cm-s-jupyter span.cm-keyword {
  color: var(--jp-mirror-editor-keyword-color);
  font-weight: bold;
}
.cm-s-jupyter span.cm-atom {
  color: var(--jp-mirror-editor-atom-color);
}
.cm-s-jupyter span.cm-number {
  color: var(--jp-mirror-editor-number-color);
}
.cm-s-jupyter span.cm-def {
  color: var(--jp-mirror-editor-def-color);
}
.cm-s-jupyter span.cm-variable {
  color: var(--jp-mirror-editor-variable-color);
}
.cm-s-jupyter span.cm-variable-2 {
  color: var(--jp-mirror-editor-variable-2-color);
}
.cm-s-jupyter span.cm-variable-3 {
  color: var(--jp-mirror-editor-variable-3-color);
}
.cm-s-jupyter span.cm-punctuation {
  color: var(--jp-mirror-editor-punctuation-color);
}
.cm-s-jupyter span.cm-property {
  color: var(--jp-mirror-editor-property-color);
}
.cm-s-jupyter span.cm-operator {
  color: var(--jp-mirror-editor-operator-color);
  font-weight: bold;
}
.cm-s-jupyter span.cm-comment {
  color: var(--jp-mirror-editor-comment-color);
  font-style: italic;
}
.cm-s-jupyter span.cm-string {
  color: var(--jp-mirror-editor-string-color);
}
.cm-s-jupyter span.cm-string-2 {
  color: var(--jp-mirror-editor-string-2-color);
}
.cm-s-jupyter span.cm-meta {
  color: var(--jp-mirror-editor-meta-color);
}
.cm-s-jupyter span.cm-qualifier {
  color: var(--jp-mirror-editor-qualifier-color);
}
.cm-s-jupyter span.cm-builtin {
  color: var(--jp-mirror-editor-builtin-color);
}
.cm-s-jupyter span.cm-bracket {
  color: var(--jp-mirror-editor-bracket-color);
}
.cm-s-jupyter span.cm-tag {
  color: var(--jp-mirror-editor-tag-color);
}
.cm-s-jupyter span.cm-attribute {
  color: var(--jp-mirror-editor-attribute-color);
}
.cm-s-jupyter span.cm-header {
  color: var(--jp-mirror-editor-header-color);
}
.cm-s-jupyter span.cm-quote {
  color: var(--jp-mirror-editor-quote-color);
}
.cm-s-jupyter span.cm-link {
  color: var(--jp-mirror-editor-link-color);
}
.cm-s-jupyter span.cm-error {
  color: var(--jp-mirror-editor-error-color);
}
.cm-s-jupyter span.cm-hr {
  color: #999;
}

.cm-s-jupyter span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}

.cm-s-jupyter .CodeMirror-activeline-background,
.cm-s-jupyter .CodeMirror-gutter {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0px;
  padding: 0px;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}
.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}
.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}
.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}
.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}
.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}
.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}
.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}
.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}
.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}
.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}
.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}
.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}
.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}
.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}
.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}
.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}
.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}
.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}
.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0em;
}

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: 12px;
  table-layout: fixed;
  margin-left: auto;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon table {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0px;
}

.jp-RenderedHTMLCommon p {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}
[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}
/* ...or leave it untouched if they don't */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-dark-background {
}
[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-light-background {
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}
.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}
.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}
.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}
.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}
.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}
.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}
.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}
.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: 0.8em;
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser {
  display: flex;
  flex-direction: column;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  border-bottom: none;
  height: auto;
  margin: var(--jp-toolbar-header-margin);
  box-shadow: none;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0px 2px;
  padding: 0px 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0px;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar.jp-Toolbar {
  padding: 0px;
  margin: 8px 12px 0px 12px;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  justify-content: flex-start;
}

.jp-FileBrowser-toolbar.jp-Toolbar .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0px;
  padding-right: 2px;
}

.jp-FileBrowser-toolbar.jp-Toolbar .jp-ToolbarButtonComponent {
  width: 40px;
}

.jp-FileBrowser-toolbar.jp-Toolbar
  .jp-Toolbar-item:first-child
  .jp-ToolbarButtonComponent {
  width: 72px;
  background: var(--jp-brand-color1);
}

.jp-FileBrowser-toolbar.jp-Toolbar
  .jp-Toolbar-item:first-child
  .jp-ToolbarButtonComponent
  .jp-icon3 {
  fill: white;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: red;
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileBrowser-filterBox {
  padding: 0px;
  flex: 0 0 auto;
  margin: 8px 12px 0px 12px;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px 12px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: white;
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon:before {
  color: limegreen;
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0px;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-DirListing-deadSpace {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
}

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: flex;
  flex-direction: row;
}

.jp-OutputPrompt {
  flex: 0 0 var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);
  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-output {
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea-child .jp-OutputArea-output {
  flex-grow: 1;
  flex-shrink: 1;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `p-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated:before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0px;
  padding: 0px;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0px;
  flex: 1 1 auto;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-OutputArea-stdin {
  line-height: var(--jp-code-line-height);
  padding-top: var(--jp-code-padding);
  display: flex;
}

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0px;
  margin: 0px;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;
  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0px;
  bottom: 0px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0px;
  width: 100%;
  padding: 0px;
  margin: 0px;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: flex;
  flex-direction: row;
  overflow: hidden;
}

.jp-InputArea-editor {
  flex: 1 1 auto;
  overflow: hidden;
}

.jp-InputArea-editor {
  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0px;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  flex: 0 0 var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);
  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: flex;
  flex-direction: row;
  flex: 1 1 auto;
}

.jp-Placeholder-prompt {
  box-sizing: border-box;
}

.jp-Placeholder-content {
  flex: 1 1 auto;
  border: none;
  background: transparent;
  height: 20px;
  box-sizing: border-box;
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0px;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0px;
  margin: 0px;
  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 200px;
  box-shadow: inset 0 0 6px 2px rgba(0, 0, 0, 0.3);
  margin-left: var(--jp-private-cell-scrolling-output-offset);
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  flex: 0 0
    calc(
      var(--jp-cell-prompt-width) -
        var(--jp-private-cell-scrolling-output-offset)
    );
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  flex: 1 1 auto;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: 2px;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: flex;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0px rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0px;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0px rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-NotebookTools-tool {
  padding: 0px 12px 0 12px;
}

.jp-ActiveCellTool {
  padding: 12px;
  background-color: var(--jp-layout-color1);
  border-top: none !important;
}

.jp-ActiveCellTool .jp-InputArea-prompt {
  flex: 0 0 auto;
  padding-left: 0px;
}

.jp-ActiveCellTool .jp-InputArea-editor {
  flex: 1 1 auto;
  background: var(--jp-cell-editor-background);
  border-color: var(--jp-cell-editor-border-color);
}

.jp-ActiveCellTool .jp-InputArea-editor .CodeMirror {
  background: transparent;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0px 12px 0px;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

</style>

    <style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color),
    0px 1px 1px 0px var(--jp-shadow-penumbra-color),
    0px 1px 3px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color),
    0px 2px 2px 0px var(--jp-shadow-penumbra-color),
    0px 1px 5px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color),
    0px 4px 5px 0px var(--jp-shadow-penumbra-color),
    0px 1px 10px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color),
    0px 6px 10px 0px var(--jp-shadow-penumbra-color),
    0px 1px 18px 0px var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color),
    0px 8px 10px 1px var(--jp-shadow-penumbra-color),
    0px 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color),
    0px 12px 17px 2px var(--jp-shadow-penumbra-color),
    0px 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color),
    0px 16px 24px 2px var(--jp-shadow-penumbra-color),
    0px 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color),
    0px 20px 31px 3px var(--jp-shadow-penumbra-color),
    0px 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color),
    0px 24px 38px 3px var(--jp-shadow-penumbra-color),
    0px 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;

  --jp-ui-font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica,
    Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;

  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);

  --jp-content-link-color: var(--md-blue-700);

  --jp-content-font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI',
    Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: Menlo, Consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-700);
  --jp-brand-color1: var(--md-blue-500);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);

  --jp-accent-color0: var(--md-green-700);
  --jp-accent-color1: var(--md-green-500);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-700);
  --jp-warn-color1: var(--md-orange-500);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);

  --jp-error-color0: var(--md-red-700);
  --jp-error-color1: var(--md-red-500);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);

  --jp-success-color0: var(--md-green-700);
  --jp-success-color1: var(--md-green-500);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);

  --jp-info-color0: var(--md-cyan-700);
  --jp-info-color1: var(--md-cyan-500);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;

  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;

  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);

  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0px;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);
  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;
  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0px 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-border-color1);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: #05a;
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #aa22ff;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #aa22ff;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);
}
</style>

<style type="text/css">
a.anchor-link {
   display: none;
}
.highlight  {
    margin: 0.4em;
}

/* Input area styling */
.jp-InputArea {
    overflow: hidden;
}

.jp-InputArea-editor {
    overflow: hidden;
}

@media print {
  body {
    margin: 0;
  }
}
</style>

<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML-full,Safe"> </script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: { 
                    automatic: true 
                    }
                },
                "HTML-CSS": {
                    linebreaks: { 
                    automatic: true 
                    }
                }
            });
        
            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
    <!-- End of mathjax configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">metrics</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">imblearn.under_sampling</span> <span class="kn">import</span> <span class="n">RandomUnderSampler</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">RandomOverSampler</span>
<span class="n">undersample</span> <span class="o">=</span> <span class="n">RandomUnderSampler</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">oversample</span> <span class="o">=</span> <span class="n">RandomOverSampler</span><span class="p">(</span><span class="n">sampling_strategy</span><span class="o">=</span><span class="s1">&#39;minority&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">zscore</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span>  <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span> 
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span> 
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">array</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">LeaveOneOut</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">resample</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">import</span> <span class="nn">pickle</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="1.-Import-and-understand-the-data.">1. Import and understand the data.<a class="anchor-link" href="#1.-Import-and-understand-the-data.">&#182;</a></h1>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>1.A. Import ‘signal-data.csv’ as DataFrame.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;signal-data.csv&#39;</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[3]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>(1567, 592)</pre>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>1.B. Print 5 point summary and share at least 2 observations.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s2">&quot;display.max_rows&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[5]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Time</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>...</th>
      <th>581</th>
      <th>582</th>
      <th>583</th>
      <th>584</th>
      <th>585</th>
      <th>586</th>
      <th>587</th>
      <th>588</th>
      <th>589</th>
      <th>Pass/Fail</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2008-07-19 11:55:00</td>
      <td>3030.93</td>
      <td>2564.00</td>
      <td>2187.7333</td>
      <td>1411.1265</td>
      <td>1.3602</td>
      <td>100.0</td>
      <td>97.6133</td>
      <td>0.1242</td>
      <td>1.5005</td>
      <td>...</td>
      <td>NaN</td>
      <td>0.5005</td>
      <td>0.0118</td>
      <td>0.0035</td>
      <td>2.3630</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2008-07-19 12:32:00</td>
      <td>3095.78</td>
      <td>2465.14</td>
      <td>2230.4222</td>
      <td>1463.6606</td>
      <td>0.8294</td>
      <td>100.0</td>
      <td>102.3433</td>
      <td>0.1247</td>
      <td>1.4966</td>
      <td>...</td>
      <td>208.2045</td>
      <td>0.5019</td>
      <td>0.0223</td>
      <td>0.0055</td>
      <td>4.4447</td>
      <td>0.0096</td>
      <td>0.0201</td>
      <td>0.0060</td>
      <td>208.2045</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2008-07-19 13:17:00</td>
      <td>2932.61</td>
      <td>2559.94</td>
      <td>2186.4111</td>
      <td>1698.0172</td>
      <td>1.5102</td>
      <td>100.0</td>
      <td>95.4878</td>
      <td>0.1241</td>
      <td>1.4436</td>
      <td>...</td>
      <td>82.8602</td>
      <td>0.4958</td>
      <td>0.0157</td>
      <td>0.0039</td>
      <td>3.1745</td>
      <td>0.0584</td>
      <td>0.0484</td>
      <td>0.0148</td>
      <td>82.8602</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2008-07-19 14:43:00</td>
      <td>2988.72</td>
      <td>2479.90</td>
      <td>2199.0333</td>
      <td>909.7926</td>
      <td>1.3204</td>
      <td>100.0</td>
      <td>104.2367</td>
      <td>0.1217</td>
      <td>1.4882</td>
      <td>...</td>
      <td>73.8432</td>
      <td>0.4990</td>
      <td>0.0103</td>
      <td>0.0025</td>
      <td>2.0544</td>
      <td>0.0202</td>
      <td>0.0149</td>
      <td>0.0044</td>
      <td>73.8432</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2008-07-19 15:22:00</td>
      <td>3032.24</td>
      <td>2502.87</td>
      <td>2233.3667</td>
      <td>1326.5200</td>
      <td>1.5334</td>
      <td>100.0</td>
      <td>100.3967</td>
      <td>0.1235</td>
      <td>1.5031</td>
      <td>...</td>
      <td>NaN</td>
      <td>0.4800</td>
      <td>0.4766</td>
      <td>0.1045</td>
      <td>99.3032</td>
      <td>0.0202</td>
      <td>0.0149</td>
      <td>0.0044</td>
      <td>73.8432</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2008-07-19 17:53:00</td>
      <td>2946.25</td>
      <td>2432.84</td>
      <td>2233.3667</td>
      <td>1326.5200</td>
      <td>1.5334</td>
      <td>100.0</td>
      <td>100.3967</td>
      <td>0.1235</td>
      <td>1.5287</td>
      <td>...</td>
      <td>44.0077</td>
      <td>0.4949</td>
      <td>0.0189</td>
      <td>0.0044</td>
      <td>3.8276</td>
      <td>0.0342</td>
      <td>0.0151</td>
      <td>0.0052</td>
      <td>44.0077</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2008-07-19 19:44:00</td>
      <td>3030.27</td>
      <td>2430.12</td>
      <td>2230.4222</td>
      <td>1463.6606</td>
      <td>0.8294</td>
      <td>100.0</td>
      <td>102.3433</td>
      <td>0.1247</td>
      <td>1.5816</td>
      <td>...</td>
      <td>NaN</td>
      <td>0.5010</td>
      <td>0.0143</td>
      <td>0.0042</td>
      <td>2.8515</td>
      <td>0.0342</td>
      <td>0.0151</td>
      <td>0.0052</td>
      <td>44.0077</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2008-07-19 19:45:00</td>
      <td>3058.88</td>
      <td>2690.15</td>
      <td>2248.9000</td>
      <td>1004.4692</td>
      <td>0.7884</td>
      <td>100.0</td>
      <td>106.2400</td>
      <td>0.1185</td>
      <td>1.5153</td>
      <td>...</td>
      <td>95.0310</td>
      <td>0.4984</td>
      <td>0.0106</td>
      <td>0.0034</td>
      <td>2.1261</td>
      <td>0.0204</td>
      <td>0.0194</td>
      <td>0.0063</td>
      <td>95.0310</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2008-07-19 20:24:00</td>
      <td>2967.68</td>
      <td>2600.47</td>
      <td>2248.9000</td>
      <td>1004.4692</td>
      <td>0.7884</td>
      <td>100.0</td>
      <td>106.2400</td>
      <td>0.1185</td>
      <td>1.5358</td>
      <td>...</td>
      <td>111.6525</td>
      <td>0.4993</td>
      <td>0.0172</td>
      <td>0.0046</td>
      <td>3.4456</td>
      <td>0.0111</td>
      <td>0.0124</td>
      <td>0.0045</td>
      <td>111.6525</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>2008-07-19 21:35:00</td>
      <td>3016.11</td>
      <td>2428.37</td>
      <td>2248.9000</td>
      <td>1004.4692</td>
      <td>0.7884</td>
      <td>100.0</td>
      <td>106.2400</td>
      <td>0.1185</td>
      <td>1.5381</td>
      <td>...</td>
      <td>90.2294</td>
      <td>0.4967</td>
      <td>0.0152</td>
      <td>0.0038</td>
      <td>3.0687</td>
      <td>0.0212</td>
      <td>0.0191</td>
      <td>0.0073</td>
      <td>90.2294</td>
      <td>-1</td>
    </tr>
  </tbody>
</table>
<p>10 rows × 592 columns</p>
</div>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[6]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span> 
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[6]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1561.0</td>
      <td>3014.452896</td>
      <td>73.621787</td>
      <td>2743.2400</td>
      <td>2966.260000</td>
      <td>3011.49000</td>
      <td>3056.650000</td>
      <td>3356.3500</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1560.0</td>
      <td>2495.850231</td>
      <td>80.407705</td>
      <td>2158.7500</td>
      <td>2452.247500</td>
      <td>2499.40500</td>
      <td>2538.822500</td>
      <td>2846.4400</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1553.0</td>
      <td>2200.547318</td>
      <td>29.513152</td>
      <td>2060.6600</td>
      <td>2181.044400</td>
      <td>2201.06670</td>
      <td>2218.055500</td>
      <td>2315.2667</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1553.0</td>
      <td>1396.376627</td>
      <td>441.691640</td>
      <td>0.0000</td>
      <td>1081.875800</td>
      <td>1285.21440</td>
      <td>1591.223500</td>
      <td>3715.0417</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1553.0</td>
      <td>4.197013</td>
      <td>56.355540</td>
      <td>0.6815</td>
      <td>1.017700</td>
      <td>1.31680</td>
      <td>1.525700</td>
      <td>1114.5366</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1553.0</td>
      <td>100.000000</td>
      <td>0.000000</td>
      <td>100.0000</td>
      <td>100.000000</td>
      <td>100.00000</td>
      <td>100.000000</td>
      <td>100.0000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1553.0</td>
      <td>101.112908</td>
      <td>6.237214</td>
      <td>82.1311</td>
      <td>97.920000</td>
      <td>101.51220</td>
      <td>104.586700</td>
      <td>129.2522</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1558.0</td>
      <td>0.121822</td>
      <td>0.008961</td>
      <td>0.0000</td>
      <td>0.121100</td>
      <td>0.12240</td>
      <td>0.123800</td>
      <td>0.1286</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1565.0</td>
      <td>1.462862</td>
      <td>0.073897</td>
      <td>1.1910</td>
      <td>1.411200</td>
      <td>1.46160</td>
      <td>1.516900</td>
      <td>1.6564</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1565.0</td>
      <td>-0.000841</td>
      <td>0.015116</td>
      <td>-0.0534</td>
      <td>-0.010800</td>
      <td>-0.00130</td>
      <td>0.008400</td>
      <td>0.0749</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1565.0</td>
      <td>0.000146</td>
      <td>0.009302</td>
      <td>-0.0349</td>
      <td>-0.005600</td>
      <td>0.00040</td>
      <td>0.005900</td>
      <td>0.0530</td>
    </tr>
    <tr>
      <th>11</th>
      <td>1565.0</td>
      <td>0.964353</td>
      <td>0.012452</td>
      <td>0.6554</td>
      <td>0.958100</td>
      <td>0.96580</td>
      <td>0.971300</td>
      <td>0.9848</td>
    </tr>
    <tr>
      <th>12</th>
      <td>1565.0</td>
      <td>199.956809</td>
      <td>3.257276</td>
      <td>182.0940</td>
      <td>198.130700</td>
      <td>199.53560</td>
      <td>202.007100</td>
      <td>272.0451</td>
    </tr>
    <tr>
      <th>13</th>
      <td>1564.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>14</th>
      <td>1564.0</td>
      <td>9.005371</td>
      <td>2.796596</td>
      <td>2.2493</td>
      <td>7.094875</td>
      <td>8.96700</td>
      <td>10.861875</td>
      <td>19.5465</td>
    </tr>
    <tr>
      <th>15</th>
      <td>1564.0</td>
      <td>413.086035</td>
      <td>17.221095</td>
      <td>333.4486</td>
      <td>406.127400</td>
      <td>412.21910</td>
      <td>419.089275</td>
      <td>824.9271</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1564.0</td>
      <td>9.907603</td>
      <td>2.403867</td>
      <td>4.4696</td>
      <td>9.567625</td>
      <td>9.85175</td>
      <td>10.128175</td>
      <td>102.8677</td>
    </tr>
    <tr>
      <th>17</th>
      <td>1564.0</td>
      <td>0.971444</td>
      <td>0.012062</td>
      <td>0.5794</td>
      <td>0.968200</td>
      <td>0.97260</td>
      <td>0.976800</td>
      <td>0.9848</td>
    </tr>
    <tr>
      <th>18</th>
      <td>1564.0</td>
      <td>190.047354</td>
      <td>2.781041</td>
      <td>169.1774</td>
      <td>188.299825</td>
      <td>189.66420</td>
      <td>192.189375</td>
      <td>215.5977</td>
    </tr>
    <tr>
      <th>19</th>
      <td>1557.0</td>
      <td>12.481034</td>
      <td>0.217965</td>
      <td>9.8773</td>
      <td>12.460000</td>
      <td>12.49960</td>
      <td>12.547100</td>
      <td>12.9898</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1567.0</td>
      <td>1.405054</td>
      <td>0.016737</td>
      <td>1.1797</td>
      <td>1.396500</td>
      <td>1.40600</td>
      <td>1.415000</td>
      <td>1.4534</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1565.0</td>
      <td>-5618.393610</td>
      <td>626.822178</td>
      <td>-7150.2500</td>
      <td>-5933.250000</td>
      <td>-5523.25000</td>
      <td>-5356.250000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>22</th>
      <td>1565.0</td>
      <td>2699.378435</td>
      <td>295.498535</td>
      <td>0.0000</td>
      <td>2578.000000</td>
      <td>2664.00000</td>
      <td>2841.750000</td>
      <td>3656.2500</td>
    </tr>
    <tr>
      <th>23</th>
      <td>1565.0</td>
      <td>-3806.299734</td>
      <td>1380.162148</td>
      <td>-9986.7500</td>
      <td>-4371.750000</td>
      <td>-3820.75000</td>
      <td>-3352.750000</td>
      <td>2363.0000</td>
    </tr>
    <tr>
      <th>24</th>
      <td>1565.0</td>
      <td>-298.598136</td>
      <td>2902.690117</td>
      <td>-14804.5000</td>
      <td>-1476.000000</td>
      <td>-78.75000</td>
      <td>1377.250000</td>
      <td>14106.0000</td>
    </tr>
    <tr>
      <th>25</th>
      <td>1565.0</td>
      <td>1.203845</td>
      <td>0.177600</td>
      <td>0.0000</td>
      <td>1.094800</td>
      <td>1.28300</td>
      <td>1.304300</td>
      <td>1.3828</td>
    </tr>
    <tr>
      <th>26</th>
      <td>1565.0</td>
      <td>1.938477</td>
      <td>0.189495</td>
      <td>0.0000</td>
      <td>1.906500</td>
      <td>1.98650</td>
      <td>2.003200</td>
      <td>2.0528</td>
    </tr>
    <tr>
      <th>27</th>
      <td>1565.0</td>
      <td>6.638628</td>
      <td>1.244249</td>
      <td>0.0000</td>
      <td>5.263700</td>
      <td>7.26470</td>
      <td>7.329700</td>
      <td>7.6588</td>
    </tr>
    <tr>
      <th>28</th>
      <td>1565.0</td>
      <td>69.499532</td>
      <td>3.461181</td>
      <td>59.4000</td>
      <td>67.377800</td>
      <td>69.15560</td>
      <td>72.266700</td>
      <td>77.9000</td>
    </tr>
    <tr>
      <th>29</th>
      <td>1565.0</td>
      <td>2.366197</td>
      <td>0.408694</td>
      <td>0.6667</td>
      <td>2.088900</td>
      <td>2.37780</td>
      <td>2.655600</td>
      <td>3.5111</td>
    </tr>
    <tr>
      <th>30</th>
      <td>1565.0</td>
      <td>0.184159</td>
      <td>0.032944</td>
      <td>0.0341</td>
      <td>0.161700</td>
      <td>0.18670</td>
      <td>0.207100</td>
      <td>0.2851</td>
    </tr>
    <tr>
      <th>31</th>
      <td>1565.0</td>
      <td>3.673189</td>
      <td>0.535322</td>
      <td>2.0698</td>
      <td>3.362700</td>
      <td>3.43100</td>
      <td>3.531300</td>
      <td>4.8044</td>
    </tr>
    <tr>
      <th>32</th>
      <td>1566.0</td>
      <td>85.337469</td>
      <td>2.026549</td>
      <td>83.1829</td>
      <td>84.490500</td>
      <td>85.13545</td>
      <td>85.741900</td>
      <td>105.6038</td>
    </tr>
    <tr>
      <th>33</th>
      <td>1566.0</td>
      <td>8.960279</td>
      <td>1.344456</td>
      <td>7.6032</td>
      <td>8.580000</td>
      <td>8.76980</td>
      <td>9.060600</td>
      <td>23.3453</td>
    </tr>
    <tr>
      <th>34</th>
      <td>1566.0</td>
      <td>50.582639</td>
      <td>1.182618</td>
      <td>49.8348</td>
      <td>50.252350</td>
      <td>50.39640</td>
      <td>50.578800</td>
      <td>59.7711</td>
    </tr>
    <tr>
      <th>35</th>
      <td>1566.0</td>
      <td>64.555787</td>
      <td>2.574749</td>
      <td>63.6774</td>
      <td>64.024800</td>
      <td>64.16580</td>
      <td>64.344700</td>
      <td>94.2641</td>
    </tr>
    <tr>
      <th>36</th>
      <td>1566.0</td>
      <td>49.417370</td>
      <td>1.182619</td>
      <td>40.2289</td>
      <td>49.421200</td>
      <td>49.60360</td>
      <td>49.747650</td>
      <td>50.1652</td>
    </tr>
    <tr>
      <th>37</th>
      <td>1566.0</td>
      <td>66.221274</td>
      <td>0.304141</td>
      <td>64.9193</td>
      <td>66.040650</td>
      <td>66.23180</td>
      <td>66.343275</td>
      <td>67.9586</td>
    </tr>
    <tr>
      <th>38</th>
      <td>1566.0</td>
      <td>86.836577</td>
      <td>0.446756</td>
      <td>84.7327</td>
      <td>86.578300</td>
      <td>86.82070</td>
      <td>87.002400</td>
      <td>88.4188</td>
    </tr>
    <tr>
      <th>39</th>
      <td>1566.0</td>
      <td>118.679554</td>
      <td>1.807221</td>
      <td>111.7128</td>
      <td>118.015600</td>
      <td>118.39930</td>
      <td>118.939600</td>
      <td>133.3898</td>
    </tr>
    <tr>
      <th>40</th>
      <td>1543.0</td>
      <td>67.904909</td>
      <td>24.062943</td>
      <td>1.4340</td>
      <td>74.800000</td>
      <td>78.29000</td>
      <td>80.200000</td>
      <td>86.1200</td>
    </tr>
    <tr>
      <th>41</th>
      <td>1543.0</td>
      <td>3.353066</td>
      <td>2.360425</td>
      <td>-0.0759</td>
      <td>2.690000</td>
      <td>3.07400</td>
      <td>3.521000</td>
      <td>37.8800</td>
    </tr>
    <tr>
      <th>42</th>
      <td>1566.0</td>
      <td>70.000000</td>
      <td>0.000000</td>
      <td>70.0000</td>
      <td>70.000000</td>
      <td>70.00000</td>
      <td>70.000000</td>
      <td>70.0000</td>
    </tr>
    <tr>
      <th>43</th>
      <td>1566.0</td>
      <td>355.538904</td>
      <td>6.234706</td>
      <td>342.7545</td>
      <td>350.801575</td>
      <td>353.72090</td>
      <td>360.772250</td>
      <td>377.2973</td>
    </tr>
    <tr>
      <th>44</th>
      <td>1566.0</td>
      <td>10.031165</td>
      <td>0.175038</td>
      <td>9.4640</td>
      <td>9.925425</td>
      <td>10.03485</td>
      <td>10.152475</td>
      <td>11.0530</td>
    </tr>
    <tr>
      <th>45</th>
      <td>1566.0</td>
      <td>136.743060</td>
      <td>7.849247</td>
      <td>108.8464</td>
      <td>130.728875</td>
      <td>136.40000</td>
      <td>142.098225</td>
      <td>176.3136</td>
    </tr>
    <tr>
      <th>46</th>
      <td>1566.0</td>
      <td>733.672811</td>
      <td>12.170315</td>
      <td>699.8139</td>
      <td>724.442300</td>
      <td>733.45000</td>
      <td>741.454500</td>
      <td>789.7523</td>
    </tr>
    <tr>
      <th>47</th>
      <td>1566.0</td>
      <td>1.177958</td>
      <td>0.189637</td>
      <td>0.4967</td>
      <td>0.985000</td>
      <td>1.25105</td>
      <td>1.340350</td>
      <td>1.5111</td>
    </tr>
    <tr>
      <th>48</th>
      <td>1566.0</td>
      <td>139.972231</td>
      <td>4.524251</td>
      <td>125.7982</td>
      <td>136.926800</td>
      <td>140.00775</td>
      <td>143.195700</td>
      <td>163.2509</td>
    </tr>
    <tr>
      <th>49</th>
      <td>1566.0</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.0000</td>
      <td>1.000000</td>
      <td>1.00000</td>
      <td>1.000000</td>
      <td>1.0000</td>
    </tr>
    <tr>
      <th>50</th>
      <td>1566.0</td>
      <td>632.254197</td>
      <td>8.643985</td>
      <td>607.3927</td>
      <td>625.928425</td>
      <td>631.37090</td>
      <td>638.136325</td>
      <td>667.7418</td>
    </tr>
    <tr>
      <th>51</th>
      <td>1566.0</td>
      <td>157.420991</td>
      <td>60.925108</td>
      <td>40.2614</td>
      <td>115.508975</td>
      <td>183.31815</td>
      <td>206.977150</td>
      <td>258.5432</td>
    </tr>
    <tr>
      <th>52</th>
      <td>1566.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>53</th>
      <td>1563.0</td>
      <td>4.592971</td>
      <td>0.054950</td>
      <td>3.7060</td>
      <td>4.574000</td>
      <td>4.59600</td>
      <td>4.617000</td>
      <td>4.7640</td>
    </tr>
    <tr>
      <th>54</th>
      <td>1563.0</td>
      <td>4.838523</td>
      <td>0.059581</td>
      <td>3.9320</td>
      <td>4.816000</td>
      <td>4.84300</td>
      <td>4.869000</td>
      <td>5.0110</td>
    </tr>
    <tr>
      <th>55</th>
      <td>1563.0</td>
      <td>2856.172105</td>
      <td>25.749317</td>
      <td>2801.0000</td>
      <td>2836.000000</td>
      <td>2854.00000</td>
      <td>2874.000000</td>
      <td>2936.0000</td>
    </tr>
    <tr>
      <th>56</th>
      <td>1563.0</td>
      <td>0.928849</td>
      <td>0.006807</td>
      <td>0.8755</td>
      <td>0.925450</td>
      <td>0.93100</td>
      <td>0.933100</td>
      <td>0.9378</td>
    </tr>
    <tr>
      <th>57</th>
      <td>1563.0</td>
      <td>0.949215</td>
      <td>0.004176</td>
      <td>0.9319</td>
      <td>0.946650</td>
      <td>0.94930</td>
      <td>0.952000</td>
      <td>0.9598</td>
    </tr>
    <tr>
      <th>58</th>
      <td>1563.0</td>
      <td>4.593312</td>
      <td>0.085095</td>
      <td>4.2199</td>
      <td>4.531900</td>
      <td>4.57270</td>
      <td>4.668600</td>
      <td>4.8475</td>
    </tr>
    <tr>
      <th>59</th>
      <td>1560.0</td>
      <td>2.960241</td>
      <td>9.532220</td>
      <td>-28.9882</td>
      <td>-1.871575</td>
      <td>0.94725</td>
      <td>4.385225</td>
      <td>168.1455</td>
    </tr>
    <tr>
      <th>60</th>
      <td>1561.0</td>
      <td>355.159094</td>
      <td>6.027889</td>
      <td>324.7145</td>
      <td>350.596400</td>
      <td>353.79910</td>
      <td>359.673600</td>
      <td>373.8664</td>
    </tr>
    <tr>
      <th>61</th>
      <td>1561.0</td>
      <td>10.423143</td>
      <td>0.274877</td>
      <td>9.4611</td>
      <td>10.283000</td>
      <td>10.43670</td>
      <td>10.591600</td>
      <td>11.7849</td>
    </tr>
    <tr>
      <th>62</th>
      <td>1561.0</td>
      <td>116.502329</td>
      <td>8.629022</td>
      <td>81.4900</td>
      <td>112.022700</td>
      <td>116.21180</td>
      <td>120.927300</td>
      <td>287.1509</td>
    </tr>
    <tr>
      <th>63</th>
      <td>1560.0</td>
      <td>13.989927</td>
      <td>7.119863</td>
      <td>1.6591</td>
      <td>10.364300</td>
      <td>13.24605</td>
      <td>16.376100</td>
      <td>188.0923</td>
    </tr>
    <tr>
      <th>64</th>
      <td>1560.0</td>
      <td>20.542109</td>
      <td>4.977467</td>
      <td>6.4482</td>
      <td>17.364800</td>
      <td>20.02135</td>
      <td>22.813625</td>
      <td>48.9882</td>
    </tr>
    <tr>
      <th>65</th>
      <td>1560.0</td>
      <td>27.131816</td>
      <td>7.121703</td>
      <td>4.3080</td>
      <td>23.056425</td>
      <td>26.26145</td>
      <td>29.914950</td>
      <td>118.0836</td>
    </tr>
    <tr>
      <th>66</th>
      <td>1561.0</td>
      <td>706.668523</td>
      <td>11.623078</td>
      <td>632.4226</td>
      <td>698.770200</td>
      <td>706.45360</td>
      <td>714.597000</td>
      <td>770.6084</td>
    </tr>
    <tr>
      <th>67</th>
      <td>1561.0</td>
      <td>16.715444</td>
      <td>307.502293</td>
      <td>0.4137</td>
      <td>0.890700</td>
      <td>0.97830</td>
      <td>1.065000</td>
      <td>7272.8283</td>
    </tr>
    <tr>
      <th>68</th>
      <td>1561.0</td>
      <td>147.437578</td>
      <td>4.240095</td>
      <td>87.0255</td>
      <td>145.237300</td>
      <td>147.59730</td>
      <td>149.959100</td>
      <td>167.8309</td>
    </tr>
    <tr>
      <th>69</th>
      <td>1561.0</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.0000</td>
      <td>1.000000</td>
      <td>1.00000</td>
      <td>1.000000</td>
      <td>1.0000</td>
    </tr>
    <tr>
      <th>70</th>
      <td>1561.0</td>
      <td>619.101687</td>
      <td>9.539190</td>
      <td>581.7773</td>
      <td>612.774500</td>
      <td>619.03270</td>
      <td>625.170000</td>
      <td>722.6018</td>
    </tr>
    <tr>
      <th>71</th>
      <td>1561.0</td>
      <td>104.329033</td>
      <td>31.651899</td>
      <td>21.4332</td>
      <td>87.484200</td>
      <td>102.60430</td>
      <td>115.498900</td>
      <td>238.4775</td>
    </tr>
    <tr>
      <th>72</th>
      <td>773.0</td>
      <td>150.361552</td>
      <td>18.388481</td>
      <td>-59.4777</td>
      <td>145.305300</td>
      <td>152.29720</td>
      <td>158.437800</td>
      <td>175.4132</td>
    </tr>
    <tr>
      <th>73</th>
      <td>773.0</td>
      <td>468.020404</td>
      <td>17.629886</td>
      <td>456.0447</td>
      <td>464.458100</td>
      <td>466.08170</td>
      <td>467.889900</td>
      <td>692.4256</td>
    </tr>
    <tr>
      <th>74</th>
      <td>1561.0</td>
      <td>0.002688</td>
      <td>0.106190</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>4.1955</td>
    </tr>
    <tr>
      <th>75</th>
      <td>1543.0</td>
      <td>-0.006903</td>
      <td>0.022292</td>
      <td>-0.1049</td>
      <td>-0.019550</td>
      <td>-0.00630</td>
      <td>0.007100</td>
      <td>0.2315</td>
    </tr>
    <tr>
      <th>76</th>
      <td>1543.0</td>
      <td>-0.029390</td>
      <td>0.033203</td>
      <td>-0.1862</td>
      <td>-0.051900</td>
      <td>-0.02890</td>
      <td>-0.006500</td>
      <td>0.0723</td>
    </tr>
    <tr>
      <th>77</th>
      <td>1543.0</td>
      <td>-0.007041</td>
      <td>0.031368</td>
      <td>-0.1046</td>
      <td>-0.029500</td>
      <td>-0.00990</td>
      <td>0.009250</td>
      <td>0.1331</td>
    </tr>
    <tr>
      <th>78</th>
      <td>1543.0</td>
      <td>-0.013643</td>
      <td>0.047872</td>
      <td>-0.3482</td>
      <td>-0.047600</td>
      <td>-0.01250</td>
      <td>0.012200</td>
      <td>0.2492</td>
    </tr>
    <tr>
      <th>79</th>
      <td>1543.0</td>
      <td>0.003458</td>
      <td>0.023080</td>
      <td>-0.0568</td>
      <td>-0.010800</td>
      <td>0.00060</td>
      <td>0.013200</td>
      <td>0.1013</td>
    </tr>
    <tr>
      <th>80</th>
      <td>1543.0</td>
      <td>-0.018531</td>
      <td>0.049226</td>
      <td>-0.1437</td>
      <td>-0.044500</td>
      <td>-0.00870</td>
      <td>0.009100</td>
      <td>0.1186</td>
    </tr>
    <tr>
      <th>81</th>
      <td>1543.0</td>
      <td>-0.021153</td>
      <td>0.017021</td>
      <td>-0.0982</td>
      <td>-0.027200</td>
      <td>-0.01960</td>
      <td>-0.012000</td>
      <td>0.0584</td>
    </tr>
    <tr>
      <th>82</th>
      <td>1543.0</td>
      <td>0.006055</td>
      <td>0.036074</td>
      <td>-0.2129</td>
      <td>-0.018000</td>
      <td>0.00760</td>
      <td>0.026900</td>
      <td>0.1437</td>
    </tr>
    <tr>
      <th>83</th>
      <td>1566.0</td>
      <td>7.452067</td>
      <td>0.516251</td>
      <td>5.8257</td>
      <td>7.104225</td>
      <td>7.46745</td>
      <td>7.807625</td>
      <td>8.9904</td>
    </tr>
    <tr>
      <th>84</th>
      <td>1555.0</td>
      <td>0.133108</td>
      <td>0.005051</td>
      <td>0.1174</td>
      <td>0.129800</td>
      <td>0.13300</td>
      <td>0.136300</td>
      <td>0.1505</td>
    </tr>
    <tr>
      <th>85</th>
      <td>226.0</td>
      <td>0.112783</td>
      <td>0.002928</td>
      <td>0.1053</td>
      <td>0.110725</td>
      <td>0.11355</td>
      <td>0.114900</td>
      <td>0.1184</td>
    </tr>
    <tr>
      <th>86</th>
      <td>1567.0</td>
      <td>2.401872</td>
      <td>0.037332</td>
      <td>2.2425</td>
      <td>2.376850</td>
      <td>2.40390</td>
      <td>2.428600</td>
      <td>2.5555</td>
    </tr>
    <tr>
      <th>87</th>
      <td>1567.0</td>
      <td>0.982420</td>
      <td>0.012848</td>
      <td>0.7749</td>
      <td>0.975800</td>
      <td>0.98740</td>
      <td>0.989700</td>
      <td>0.9935</td>
    </tr>
    <tr>
      <th>88</th>
      <td>1567.0</td>
      <td>1807.815021</td>
      <td>53.537262</td>
      <td>1627.4714</td>
      <td>1777.470300</td>
      <td>1809.24920</td>
      <td>1841.873000</td>
      <td>2105.1823</td>
    </tr>
    <tr>
      <th>89</th>
      <td>1516.0</td>
      <td>0.188703</td>
      <td>0.052373</td>
      <td>0.1113</td>
      <td>0.169375</td>
      <td>0.19010</td>
      <td>0.200425</td>
      <td>1.4727</td>
    </tr>
    <tr>
      <th>90</th>
      <td>1516.0</td>
      <td>8827.536865</td>
      <td>396.313662</td>
      <td>7397.3100</td>
      <td>8564.689975</td>
      <td>8825.43510</td>
      <td>9065.432400</td>
      <td>10746.6000</td>
    </tr>
    <tr>
      <th>91</th>
      <td>1561.0</td>
      <td>0.002440</td>
      <td>0.087683</td>
      <td>-0.3570</td>
      <td>-0.042900</td>
      <td>0.00000</td>
      <td>0.050700</td>
      <td>0.3627</td>
    </tr>
    <tr>
      <th>92</th>
      <td>1565.0</td>
      <td>0.000507</td>
      <td>0.003231</td>
      <td>-0.0126</td>
      <td>-0.001200</td>
      <td>0.00040</td>
      <td>0.002000</td>
      <td>0.0281</td>
    </tr>
    <tr>
      <th>93</th>
      <td>1565.0</td>
      <td>-0.000541</td>
      <td>0.003010</td>
      <td>-0.0171</td>
      <td>-0.001600</td>
      <td>-0.00020</td>
      <td>0.001000</td>
      <td>0.0133</td>
    </tr>
    <tr>
      <th>94</th>
      <td>1561.0</td>
      <td>-0.000029</td>
      <td>0.000174</td>
      <td>-0.0020</td>
      <td>-0.000100</td>
      <td>0.00000</td>
      <td>0.000100</td>
      <td>0.0011</td>
    </tr>
    <tr>
      <th>95</th>
      <td>1561.0</td>
      <td>0.000060</td>
      <td>0.000104</td>
      <td>-0.0009</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000100</td>
      <td>0.0009</td>
    </tr>
    <tr>
      <th>96</th>
      <td>1561.0</td>
      <td>0.017127</td>
      <td>0.219578</td>
      <td>-1.4803</td>
      <td>-0.088600</td>
      <td>0.00390</td>
      <td>0.122000</td>
      <td>2.5093</td>
    </tr>
    <tr>
      <th>97</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>98</th>
      <td>1561.0</td>
      <td>-0.018143</td>
      <td>0.427110</td>
      <td>-5.2717</td>
      <td>-0.218800</td>
      <td>0.00000</td>
      <td>0.189300</td>
      <td>2.5698</td>
    </tr>
    <tr>
      <th>99</th>
      <td>1561.0</td>
      <td>0.001540</td>
      <td>0.062740</td>
      <td>-0.5283</td>
      <td>-0.029800</td>
      <td>0.00000</td>
      <td>0.029800</td>
      <td>0.8854</td>
    </tr>
    <tr>
      <th>100</th>
      <td>1561.0</td>
      <td>-0.000021</td>
      <td>0.000356</td>
      <td>-0.0030</td>
      <td>-0.000200</td>
      <td>0.00000</td>
      <td>0.000200</td>
      <td>0.0023</td>
    </tr>
    <tr>
      <th>101</th>
      <td>1561.0</td>
      <td>-0.000007</td>
      <td>0.000221</td>
      <td>-0.0024</td>
      <td>-0.000100</td>
      <td>0.00000</td>
      <td>0.000100</td>
      <td>0.0017</td>
    </tr>
    <tr>
      <th>102</th>
      <td>1561.0</td>
      <td>0.001115</td>
      <td>0.062968</td>
      <td>-0.5353</td>
      <td>-0.035700</td>
      <td>0.00000</td>
      <td>0.033600</td>
      <td>0.2979</td>
    </tr>
    <tr>
      <th>103</th>
      <td>1565.0</td>
      <td>-0.009789</td>
      <td>0.003065</td>
      <td>-0.0329</td>
      <td>-0.011800</td>
      <td>-0.01010</td>
      <td>-0.008200</td>
      <td>0.0203</td>
    </tr>
    <tr>
      <th>104</th>
      <td>1565.0</td>
      <td>-0.000015</td>
      <td>0.000851</td>
      <td>-0.0119</td>
      <td>-0.000400</td>
      <td>0.00000</td>
      <td>0.000400</td>
      <td>0.0071</td>
    </tr>
    <tr>
      <th>105</th>
      <td>1561.0</td>
      <td>-0.000498</td>
      <td>0.003202</td>
      <td>-0.0281</td>
      <td>-0.001900</td>
      <td>-0.00020</td>
      <td>0.001100</td>
      <td>0.0127</td>
    </tr>
    <tr>
      <th>106</th>
      <td>1561.0</td>
      <td>0.000540</td>
      <td>0.002988</td>
      <td>-0.0133</td>
      <td>-0.001000</td>
      <td>0.00020</td>
      <td>0.001600</td>
      <td>0.0172</td>
    </tr>
    <tr>
      <th>107</th>
      <td>1561.0</td>
      <td>-0.001766</td>
      <td>0.087475</td>
      <td>-0.5226</td>
      <td>-0.048600</td>
      <td>0.00000</td>
      <td>0.049000</td>
      <td>0.4856</td>
    </tr>
    <tr>
      <th>108</th>
      <td>1561.0</td>
      <td>-0.010789</td>
      <td>0.086758</td>
      <td>-0.3454</td>
      <td>-0.064900</td>
      <td>-0.01120</td>
      <td>0.038000</td>
      <td>0.3938</td>
    </tr>
    <tr>
      <th>109</th>
      <td>549.0</td>
      <td>0.979993</td>
      <td>0.008695</td>
      <td>0.7848</td>
      <td>0.978800</td>
      <td>0.98100</td>
      <td>0.982300</td>
      <td>0.9842</td>
    </tr>
    <tr>
      <th>110</th>
      <td>549.0</td>
      <td>101.318253</td>
      <td>1.880087</td>
      <td>88.1938</td>
      <td>100.389000</td>
      <td>101.48170</td>
      <td>102.078100</td>
      <td>106.9227</td>
    </tr>
    <tr>
      <th>111</th>
      <td>549.0</td>
      <td>231.818898</td>
      <td>2.105318</td>
      <td>213.0083</td>
      <td>230.373800</td>
      <td>231.20120</td>
      <td>233.036100</td>
      <td>236.9546</td>
    </tr>
    <tr>
      <th>112</th>
      <td>852.0</td>
      <td>0.457538</td>
      <td>0.048939</td>
      <td>0.0000</td>
      <td>0.459300</td>
      <td>0.46285</td>
      <td>0.466425</td>
      <td>0.4885</td>
    </tr>
    <tr>
      <th>113</th>
      <td>1567.0</td>
      <td>0.945424</td>
      <td>0.012133</td>
      <td>0.8534</td>
      <td>0.938600</td>
      <td>0.94640</td>
      <td>0.952300</td>
      <td>0.9763</td>
    </tr>
    <tr>
      <th>114</th>
      <td>1567.0</td>
      <td>0.000123</td>
      <td>0.001668</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0414</td>
    </tr>
    <tr>
      <th>115</th>
      <td>1567.0</td>
      <td>747.383792</td>
      <td>48.949250</td>
      <td>544.0254</td>
      <td>721.023000</td>
      <td>750.86140</td>
      <td>776.781850</td>
      <td>924.5318</td>
    </tr>
    <tr>
      <th>116</th>
      <td>1567.0</td>
      <td>0.987130</td>
      <td>0.009497</td>
      <td>0.8900</td>
      <td>0.989500</td>
      <td>0.99050</td>
      <td>0.990900</td>
      <td>0.9924</td>
    </tr>
    <tr>
      <th>117</th>
      <td>1567.0</td>
      <td>58.625908</td>
      <td>6.485174</td>
      <td>52.8068</td>
      <td>57.978300</td>
      <td>58.54910</td>
      <td>59.133900</td>
      <td>311.7344</td>
    </tr>
    <tr>
      <th>118</th>
      <td>1543.0</td>
      <td>0.598412</td>
      <td>0.008102</td>
      <td>0.5274</td>
      <td>0.594100</td>
      <td>0.59900</td>
      <td>0.603400</td>
      <td>0.6245</td>
    </tr>
    <tr>
      <th>119</th>
      <td>1567.0</td>
      <td>0.970777</td>
      <td>0.008949</td>
      <td>0.8411</td>
      <td>0.964800</td>
      <td>0.96940</td>
      <td>0.978300</td>
      <td>0.9827</td>
    </tr>
    <tr>
      <th>120</th>
      <td>1567.0</td>
      <td>6.310863</td>
      <td>0.124304</td>
      <td>5.1259</td>
      <td>6.246400</td>
      <td>6.31360</td>
      <td>6.375850</td>
      <td>7.5220</td>
    </tr>
    <tr>
      <th>121</th>
      <td>1558.0</td>
      <td>15.796425</td>
      <td>0.099618</td>
      <td>15.4600</td>
      <td>15.730000</td>
      <td>15.79000</td>
      <td>15.860000</td>
      <td>16.0700</td>
    </tr>
    <tr>
      <th>122</th>
      <td>1558.0</td>
      <td>3.898390</td>
      <td>0.904120</td>
      <td>1.6710</td>
      <td>3.202000</td>
      <td>3.87700</td>
      <td>4.392000</td>
      <td>6.8890</td>
    </tr>
    <tr>
      <th>123</th>
      <td>1558.0</td>
      <td>15.829660</td>
      <td>0.108315</td>
      <td>15.1700</td>
      <td>15.762500</td>
      <td>15.83000</td>
      <td>15.900000</td>
      <td>16.1000</td>
    </tr>
    <tr>
      <th>124</th>
      <td>1558.0</td>
      <td>15.794705</td>
      <td>0.114144</td>
      <td>15.4300</td>
      <td>15.722500</td>
      <td>15.78000</td>
      <td>15.870000</td>
      <td>16.1000</td>
    </tr>
    <tr>
      <th>125</th>
      <td>1558.0</td>
      <td>1.184956</td>
      <td>0.280555</td>
      <td>0.3122</td>
      <td>0.974400</td>
      <td>1.14400</td>
      <td>1.338000</td>
      <td>2.4650</td>
    </tr>
    <tr>
      <th>126</th>
      <td>1558.0</td>
      <td>2.750728</td>
      <td>0.253471</td>
      <td>2.3400</td>
      <td>2.572000</td>
      <td>2.73500</td>
      <td>2.873000</td>
      <td>3.9910</td>
    </tr>
    <tr>
      <th>127</th>
      <td>1558.0</td>
      <td>0.648478</td>
      <td>0.135409</td>
      <td>0.3161</td>
      <td>0.548900</td>
      <td>0.65390</td>
      <td>0.713500</td>
      <td>1.1750</td>
    </tr>
    <tr>
      <th>128</th>
      <td>1558.0</td>
      <td>3.192182</td>
      <td>0.264175</td>
      <td>0.0000</td>
      <td>3.074000</td>
      <td>3.19500</td>
      <td>3.311000</td>
      <td>3.8950</td>
    </tr>
    <tr>
      <th>129</th>
      <td>1558.0</td>
      <td>-0.554228</td>
      <td>1.220479</td>
      <td>-3.7790</td>
      <td>-0.898800</td>
      <td>-0.14190</td>
      <td>0.047300</td>
      <td>2.4580</td>
    </tr>
    <tr>
      <th>130</th>
      <td>1558.0</td>
      <td>0.744976</td>
      <td>0.082531</td>
      <td>0.4199</td>
      <td>0.688700</td>
      <td>0.75875</td>
      <td>0.814500</td>
      <td>0.8884</td>
    </tr>
    <tr>
      <th>131</th>
      <td>1558.0</td>
      <td>0.997808</td>
      <td>0.002251</td>
      <td>0.9936</td>
      <td>0.996400</td>
      <td>0.99775</td>
      <td>0.998900</td>
      <td>1.0190</td>
    </tr>
    <tr>
      <th>132</th>
      <td>1559.0</td>
      <td>2.318545</td>
      <td>0.053181</td>
      <td>2.1911</td>
      <td>2.277300</td>
      <td>2.31240</td>
      <td>2.358300</td>
      <td>2.4723</td>
    </tr>
    <tr>
      <th>133</th>
      <td>1559.0</td>
      <td>1004.043093</td>
      <td>6.537701</td>
      <td>980.4510</td>
      <td>999.996100</td>
      <td>1004.05000</td>
      <td>1008.670600</td>
      <td>1020.9944</td>
    </tr>
    <tr>
      <th>134</th>
      <td>1559.0</td>
      <td>39.391979</td>
      <td>2.990476</td>
      <td>33.3658</td>
      <td>37.347250</td>
      <td>38.90260</td>
      <td>40.804600</td>
      <td>64.1287</td>
    </tr>
    <tr>
      <th>135</th>
      <td>1562.0</td>
      <td>117.960948</td>
      <td>57.544627</td>
      <td>58.0000</td>
      <td>92.000000</td>
      <td>109.00000</td>
      <td>127.000000</td>
      <td>994.0000</td>
    </tr>
    <tr>
      <th>136</th>
      <td>1561.0</td>
      <td>138.194747</td>
      <td>53.909792</td>
      <td>36.1000</td>
      <td>90.000000</td>
      <td>134.60000</td>
      <td>181.000000</td>
      <td>295.8000</td>
    </tr>
    <tr>
      <th>137</th>
      <td>1560.0</td>
      <td>122.692949</td>
      <td>52.253015</td>
      <td>19.2000</td>
      <td>81.300000</td>
      <td>117.70000</td>
      <td>161.600000</td>
      <td>334.7000</td>
    </tr>
    <tr>
      <th>138</th>
      <td>1553.0</td>
      <td>57.603025</td>
      <td>12.345358</td>
      <td>19.8000</td>
      <td>50.900100</td>
      <td>55.90010</td>
      <td>62.900100</td>
      <td>141.7998</td>
    </tr>
    <tr>
      <th>139</th>
      <td>1553.0</td>
      <td>416.766964</td>
      <td>263.300614</td>
      <td>0.0000</td>
      <td>243.786000</td>
      <td>339.56100</td>
      <td>502.205900</td>
      <td>1770.6909</td>
    </tr>
    <tr>
      <th>140</th>
      <td>1553.0</td>
      <td>26.077904</td>
      <td>506.922106</td>
      <td>0.0319</td>
      <td>0.131700</td>
      <td>0.23580</td>
      <td>0.439100</td>
      <td>9998.8944</td>
    </tr>
    <tr>
      <th>141</th>
      <td>1553.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>142</th>
      <td>1553.0</td>
      <td>6.641565</td>
      <td>3.552254</td>
      <td>1.7400</td>
      <td>5.110000</td>
      <td>6.26000</td>
      <td>7.500000</td>
      <td>103.3900</td>
    </tr>
    <tr>
      <th>143</th>
      <td>1558.0</td>
      <td>0.004169</td>
      <td>0.001282</td>
      <td>0.0000</td>
      <td>0.003300</td>
      <td>0.00390</td>
      <td>0.004900</td>
      <td>0.0121</td>
    </tr>
    <tr>
      <th>144</th>
      <td>1565.0</td>
      <td>0.120008</td>
      <td>0.061343</td>
      <td>0.0324</td>
      <td>0.083900</td>
      <td>0.10750</td>
      <td>0.132700</td>
      <td>0.6253</td>
    </tr>
    <tr>
      <th>145</th>
      <td>1565.0</td>
      <td>0.063621</td>
      <td>0.026541</td>
      <td>0.0214</td>
      <td>0.048000</td>
      <td>0.05860</td>
      <td>0.071800</td>
      <td>0.2507</td>
    </tr>
    <tr>
      <th>146</th>
      <td>1565.0</td>
      <td>0.055010</td>
      <td>0.021844</td>
      <td>0.0227</td>
      <td>0.042300</td>
      <td>0.05000</td>
      <td>0.061500</td>
      <td>0.2479</td>
    </tr>
    <tr>
      <th>147</th>
      <td>1565.0</td>
      <td>0.017411</td>
      <td>0.027123</td>
      <td>0.0043</td>
      <td>0.010000</td>
      <td>0.01590</td>
      <td>0.021300</td>
      <td>0.9783</td>
    </tr>
    <tr>
      <th>148</th>
      <td>1565.0</td>
      <td>8.471308</td>
      <td>18.740631</td>
      <td>1.4208</td>
      <td>6.359900</td>
      <td>7.91730</td>
      <td>9.585300</td>
      <td>742.9421</td>
    </tr>
    <tr>
      <th>149</th>
      <td>1564.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>150</th>
      <td>1564.0</td>
      <td>6.814268</td>
      <td>3.241843</td>
      <td>1.3370</td>
      <td>4.459250</td>
      <td>5.95100</td>
      <td>8.275000</td>
      <td>22.3180</td>
    </tr>
    <tr>
      <th>151</th>
      <td>1564.0</td>
      <td>14.047403</td>
      <td>31.002541</td>
      <td>2.0200</td>
      <td>8.089750</td>
      <td>10.99350</td>
      <td>14.347250</td>
      <td>536.5640</td>
    </tr>
    <tr>
      <th>152</th>
      <td>1564.0</td>
      <td>1.196733</td>
      <td>23.364063</td>
      <td>0.1544</td>
      <td>0.373750</td>
      <td>0.46870</td>
      <td>0.679925</td>
      <td>924.3780</td>
    </tr>
    <tr>
      <th>153</th>
      <td>1564.0</td>
      <td>0.011926</td>
      <td>0.009346</td>
      <td>0.0036</td>
      <td>0.007275</td>
      <td>0.01110</td>
      <td>0.014900</td>
      <td>0.2389</td>
    </tr>
    <tr>
      <th>154</th>
      <td>1564.0</td>
      <td>7.697971</td>
      <td>5.239219</td>
      <td>1.2438</td>
      <td>5.926950</td>
      <td>7.51270</td>
      <td>9.054675</td>
      <td>191.5478</td>
    </tr>
    <tr>
      <th>155</th>
      <td>1557.0</td>
      <td>0.507171</td>
      <td>1.122427</td>
      <td>0.1400</td>
      <td>0.240000</td>
      <td>0.32000</td>
      <td>0.450000</td>
      <td>12.7100</td>
    </tr>
    <tr>
      <th>156</th>
      <td>1567.0</td>
      <td>0.058089</td>
      <td>0.079174</td>
      <td>0.0111</td>
      <td>0.036250</td>
      <td>0.04870</td>
      <td>0.066700</td>
      <td>2.2016</td>
    </tr>
    <tr>
      <th>157</th>
      <td>138.0</td>
      <td>0.047104</td>
      <td>0.039538</td>
      <td>0.0118</td>
      <td>0.027050</td>
      <td>0.03545</td>
      <td>0.048875</td>
      <td>0.2876</td>
    </tr>
    <tr>
      <th>158</th>
      <td>138.0</td>
      <td>1039.650738</td>
      <td>406.848810</td>
      <td>234.0996</td>
      <td>721.675050</td>
      <td>1020.30005</td>
      <td>1277.750125</td>
      <td>2505.2998</td>
    </tr>
    <tr>
      <th>159</th>
      <td>1565.0</td>
      <td>882.680511</td>
      <td>983.043021</td>
      <td>0.0000</td>
      <td>411.000000</td>
      <td>623.00000</td>
      <td>966.000000</td>
      <td>7791.0000</td>
    </tr>
    <tr>
      <th>160</th>
      <td>1565.0</td>
      <td>555.346326</td>
      <td>574.808588</td>
      <td>0.0000</td>
      <td>295.000000</td>
      <td>438.00000</td>
      <td>625.000000</td>
      <td>4170.0000</td>
    </tr>
    <tr>
      <th>161</th>
      <td>1565.0</td>
      <td>4066.850479</td>
      <td>4239.245058</td>
      <td>0.0000</td>
      <td>1321.000000</td>
      <td>2614.00000</td>
      <td>5034.000000</td>
      <td>37943.0000</td>
    </tr>
    <tr>
      <th>162</th>
      <td>1565.0</td>
      <td>4797.154633</td>
      <td>6553.569317</td>
      <td>0.0000</td>
      <td>451.000000</td>
      <td>1784.00000</td>
      <td>6384.000000</td>
      <td>36871.0000</td>
    </tr>
    <tr>
      <th>163</th>
      <td>1565.0</td>
      <td>0.140204</td>
      <td>0.121989</td>
      <td>0.0000</td>
      <td>0.091000</td>
      <td>0.12000</td>
      <td>0.154000</td>
      <td>0.9570</td>
    </tr>
    <tr>
      <th>164</th>
      <td>1565.0</td>
      <td>0.127942</td>
      <td>0.242534</td>
      <td>0.0000</td>
      <td>0.068000</td>
      <td>0.08900</td>
      <td>0.116000</td>
      <td>1.8170</td>
    </tr>
    <tr>
      <th>165</th>
      <td>1565.0</td>
      <td>0.252026</td>
      <td>0.407329</td>
      <td>0.0000</td>
      <td>0.132000</td>
      <td>0.18400</td>
      <td>0.255000</td>
      <td>3.2860</td>
    </tr>
    <tr>
      <th>166</th>
      <td>1565.0</td>
      <td>2.788882</td>
      <td>1.119756</td>
      <td>0.8000</td>
      <td>2.100000</td>
      <td>2.60000</td>
      <td>3.200000</td>
      <td>21.1000</td>
    </tr>
    <tr>
      <th>167</th>
      <td>1565.0</td>
      <td>1.235783</td>
      <td>0.632767</td>
      <td>0.3000</td>
      <td>0.900000</td>
      <td>1.20000</td>
      <td>1.500000</td>
      <td>16.3000</td>
    </tr>
    <tr>
      <th>168</th>
      <td>1565.0</td>
      <td>0.124397</td>
      <td>0.047639</td>
      <td>0.0330</td>
      <td>0.090000</td>
      <td>0.11900</td>
      <td>0.151000</td>
      <td>0.7250</td>
    </tr>
    <tr>
      <th>169</th>
      <td>1565.0</td>
      <td>0.400454</td>
      <td>0.197918</td>
      <td>0.0460</td>
      <td>0.230000</td>
      <td>0.41200</td>
      <td>0.536000</td>
      <td>1.1430</td>
    </tr>
    <tr>
      <th>170</th>
      <td>1566.0</td>
      <td>0.684330</td>
      <td>0.157468</td>
      <td>0.2979</td>
      <td>0.575600</td>
      <td>0.68600</td>
      <td>0.797300</td>
      <td>1.1530</td>
    </tr>
    <tr>
      <th>171</th>
      <td>1566.0</td>
      <td>0.120064</td>
      <td>0.060785</td>
      <td>0.0089</td>
      <td>0.079800</td>
      <td>0.11250</td>
      <td>0.140300</td>
      <td>0.4940</td>
    </tr>
    <tr>
      <th>172</th>
      <td>1566.0</td>
      <td>0.320113</td>
      <td>0.071243</td>
      <td>0.1287</td>
      <td>0.276600</td>
      <td>0.32385</td>
      <td>0.370200</td>
      <td>0.5484</td>
    </tr>
    <tr>
      <th>173</th>
      <td>1566.0</td>
      <td>0.576192</td>
      <td>0.095734</td>
      <td>0.2538</td>
      <td>0.516800</td>
      <td>0.57760</td>
      <td>0.634500</td>
      <td>0.8643</td>
    </tr>
    <tr>
      <th>174</th>
      <td>1566.0</td>
      <td>0.320113</td>
      <td>0.071247</td>
      <td>0.1287</td>
      <td>0.276500</td>
      <td>0.32385</td>
      <td>0.370200</td>
      <td>0.5484</td>
    </tr>
    <tr>
      <th>175</th>
      <td>1566.0</td>
      <td>0.778044</td>
      <td>0.116322</td>
      <td>0.4616</td>
      <td>0.692200</td>
      <td>0.76820</td>
      <td>0.843900</td>
      <td>1.1720</td>
    </tr>
    <tr>
      <th>176</th>
      <td>1566.0</td>
      <td>0.244718</td>
      <td>0.074918</td>
      <td>0.0735</td>
      <td>0.196250</td>
      <td>0.24290</td>
      <td>0.293925</td>
      <td>0.4411</td>
    </tr>
    <tr>
      <th>177</th>
      <td>1566.0</td>
      <td>0.394760</td>
      <td>0.282903</td>
      <td>0.0470</td>
      <td>0.222000</td>
      <td>0.29900</td>
      <td>0.423000</td>
      <td>1.8580</td>
    </tr>
    <tr>
      <th>178</th>
      <td>1543.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>179</th>
      <td>1566.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>180</th>
      <td>1566.0</td>
      <td>19.013257</td>
      <td>3.311632</td>
      <td>9.4000</td>
      <td>16.850000</td>
      <td>18.69000</td>
      <td>20.972500</td>
      <td>48.6700</td>
    </tr>
    <tr>
      <th>181</th>
      <td>1566.0</td>
      <td>0.546770</td>
      <td>0.224402</td>
      <td>0.0930</td>
      <td>0.378000</td>
      <td>0.52400</td>
      <td>0.688750</td>
      <td>3.5730</td>
    </tr>
    <tr>
      <th>182</th>
      <td>1566.0</td>
      <td>10.780543</td>
      <td>4.164051</td>
      <td>3.1700</td>
      <td>7.732500</td>
      <td>10.17000</td>
      <td>13.337500</td>
      <td>55.0000</td>
    </tr>
    <tr>
      <th>183</th>
      <td>1566.0</td>
      <td>26.661170</td>
      <td>6.836101</td>
      <td>5.0140</td>
      <td>21.171500</td>
      <td>27.20050</td>
      <td>31.687000</td>
      <td>72.9470</td>
    </tr>
    <tr>
      <th>184</th>
      <td>1566.0</td>
      <td>0.144815</td>
      <td>0.110198</td>
      <td>0.0297</td>
      <td>0.102200</td>
      <td>0.13260</td>
      <td>0.169150</td>
      <td>3.2283</td>
    </tr>
    <tr>
      <th>185</th>
      <td>1566.0</td>
      <td>7.365741</td>
      <td>7.188720</td>
      <td>1.9400</td>
      <td>5.390000</td>
      <td>6.73500</td>
      <td>8.450000</td>
      <td>267.9100</td>
    </tr>
    <tr>
      <th>186</th>
      <td>1566.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>187</th>
      <td>1566.0</td>
      <td>17.936290</td>
      <td>8.609912</td>
      <td>6.2200</td>
      <td>14.505000</td>
      <td>17.86500</td>
      <td>20.860000</td>
      <td>307.9300</td>
    </tr>
    <tr>
      <th>188</th>
      <td>1566.0</td>
      <td>43.211418</td>
      <td>21.711876</td>
      <td>6.6130</td>
      <td>24.711000</td>
      <td>40.20950</td>
      <td>57.674750</td>
      <td>191.8300</td>
    </tr>
    <tr>
      <th>189</th>
      <td>1566.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>190</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>191</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>192</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>193</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>194</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>195</th>
      <td>1563.0</td>
      <td>0.287084</td>
      <td>0.395187</td>
      <td>0.0800</td>
      <td>0.218000</td>
      <td>0.25900</td>
      <td>0.296000</td>
      <td>4.8380</td>
    </tr>
    <tr>
      <th>196</th>
      <td>1560.0</td>
      <td>8.688487</td>
      <td>15.720926</td>
      <td>1.7500</td>
      <td>5.040000</td>
      <td>6.78000</td>
      <td>9.555000</td>
      <td>396.1100</td>
    </tr>
    <tr>
      <th>197</th>
      <td>1561.0</td>
      <td>20.092710</td>
      <td>10.552162</td>
      <td>9.2200</td>
      <td>17.130000</td>
      <td>19.37000</td>
      <td>21.460000</td>
      <td>252.8700</td>
    </tr>
    <tr>
      <th>198</th>
      <td>1561.0</td>
      <td>0.557359</td>
      <td>0.537705</td>
      <td>0.0900</td>
      <td>0.296000</td>
      <td>0.42400</td>
      <td>0.726000</td>
      <td>10.0170</td>
    </tr>
    <tr>
      <th>199</th>
      <td>1561.0</td>
      <td>11.532056</td>
      <td>16.445556</td>
      <td>2.7700</td>
      <td>6.740000</td>
      <td>8.57000</td>
      <td>11.460000</td>
      <td>390.1200</td>
    </tr>
    <tr>
      <th>200</th>
      <td>1560.0</td>
      <td>17.600192</td>
      <td>8.690718</td>
      <td>3.2100</td>
      <td>14.155000</td>
      <td>17.23500</td>
      <td>20.162500</td>
      <td>199.6200</td>
    </tr>
    <tr>
      <th>201</th>
      <td>1560.0</td>
      <td>7.839359</td>
      <td>5.104495</td>
      <td>0.0000</td>
      <td>5.020000</td>
      <td>6.76000</td>
      <td>9.490000</td>
      <td>126.5300</td>
    </tr>
    <tr>
      <th>202</th>
      <td>1560.0</td>
      <td>10.170463</td>
      <td>14.622904</td>
      <td>0.0000</td>
      <td>6.094000</td>
      <td>8.46200</td>
      <td>11.953000</td>
      <td>490.5610</td>
    </tr>
    <tr>
      <th>203</th>
      <td>1561.0</td>
      <td>30.073143</td>
      <td>17.461798</td>
      <td>7.7280</td>
      <td>24.653000</td>
      <td>30.09700</td>
      <td>33.506000</td>
      <td>500.3490</td>
    </tr>
    <tr>
      <th>204</th>
      <td>1561.0</td>
      <td>32.218169</td>
      <td>565.101239</td>
      <td>0.0429</td>
      <td>0.114300</td>
      <td>0.15820</td>
      <td>0.230700</td>
      <td>9998.4483</td>
    </tr>
    <tr>
      <th>205</th>
      <td>1561.0</td>
      <td>9.050122</td>
      <td>11.541083</td>
      <td>2.3000</td>
      <td>6.040000</td>
      <td>7.74000</td>
      <td>9.940000</td>
      <td>320.0500</td>
    </tr>
    <tr>
      <th>206</th>
      <td>1561.0</td>
      <td>0.001281</td>
      <td>0.050621</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>2.0000</td>
    </tr>
    <tr>
      <th>207</th>
      <td>1561.0</td>
      <td>20.376176</td>
      <td>17.497556</td>
      <td>4.0100</td>
      <td>16.350000</td>
      <td>19.72000</td>
      <td>22.370000</td>
      <td>457.6500</td>
    </tr>
    <tr>
      <th>208</th>
      <td>1561.0</td>
      <td>73.264316</td>
      <td>28.067143</td>
      <td>5.3590</td>
      <td>56.158000</td>
      <td>73.24800</td>
      <td>90.515000</td>
      <td>172.3490</td>
    </tr>
    <tr>
      <th>209</th>
      <td>1561.0</td>
      <td>0.029564</td>
      <td>1.168074</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>46.1500</td>
    </tr>
    <tr>
      <th>210</th>
      <td>1543.0</td>
      <td>0.088866</td>
      <td>0.042065</td>
      <td>0.0319</td>
      <td>0.065600</td>
      <td>0.07970</td>
      <td>0.099450</td>
      <td>0.5164</td>
    </tr>
    <tr>
      <th>211</th>
      <td>1543.0</td>
      <td>0.056755</td>
      <td>0.025005</td>
      <td>0.0022</td>
      <td>0.043800</td>
      <td>0.05320</td>
      <td>0.064200</td>
      <td>0.3227</td>
    </tr>
    <tr>
      <th>212</th>
      <td>1543.0</td>
      <td>0.051432</td>
      <td>0.031578</td>
      <td>0.0071</td>
      <td>0.032500</td>
      <td>0.04160</td>
      <td>0.062450</td>
      <td>0.5941</td>
    </tr>
    <tr>
      <th>213</th>
      <td>1543.0</td>
      <td>0.060346</td>
      <td>0.053030</td>
      <td>0.0037</td>
      <td>0.036400</td>
      <td>0.05600</td>
      <td>0.073700</td>
      <td>1.2837</td>
    </tr>
    <tr>
      <th>214</th>
      <td>1543.0</td>
      <td>0.083268</td>
      <td>0.056456</td>
      <td>0.0193</td>
      <td>0.056800</td>
      <td>0.07540</td>
      <td>0.093550</td>
      <td>0.7615</td>
    </tr>
    <tr>
      <th>215</th>
      <td>1543.0</td>
      <td>0.081076</td>
      <td>0.030437</td>
      <td>0.0059</td>
      <td>0.063200</td>
      <td>0.08250</td>
      <td>0.098300</td>
      <td>0.3429</td>
    </tr>
    <tr>
      <th>216</th>
      <td>1543.0</td>
      <td>0.083484</td>
      <td>0.025764</td>
      <td>0.0097</td>
      <td>0.069550</td>
      <td>0.08460</td>
      <td>0.097550</td>
      <td>0.2828</td>
    </tr>
    <tr>
      <th>217</th>
      <td>1543.0</td>
      <td>0.071635</td>
      <td>0.046283</td>
      <td>0.0079</td>
      <td>0.045800</td>
      <td>0.06170</td>
      <td>0.086350</td>
      <td>0.6744</td>
    </tr>
    <tr>
      <th>218</th>
      <td>1566.0</td>
      <td>3.771465</td>
      <td>1.170436</td>
      <td>1.0340</td>
      <td>2.946100</td>
      <td>3.63075</td>
      <td>4.404750</td>
      <td>8.8015</td>
    </tr>
    <tr>
      <th>219</th>
      <td>1555.0</td>
      <td>0.003254</td>
      <td>0.001646</td>
      <td>0.0007</td>
      <td>0.002300</td>
      <td>0.00300</td>
      <td>0.003800</td>
      <td>0.0163</td>
    </tr>
    <tr>
      <th>220</th>
      <td>226.0</td>
      <td>0.009213</td>
      <td>0.001989</td>
      <td>0.0057</td>
      <td>0.007800</td>
      <td>0.00895</td>
      <td>0.010300</td>
      <td>0.0240</td>
    </tr>
    <tr>
      <th>221</th>
      <td>1567.0</td>
      <td>0.060718</td>
      <td>0.023305</td>
      <td>0.0200</td>
      <td>0.040200</td>
      <td>0.06090</td>
      <td>0.076500</td>
      <td>0.2305</td>
    </tr>
    <tr>
      <th>222</th>
      <td>1567.0</td>
      <td>0.008821</td>
      <td>0.055937</td>
      <td>0.0003</td>
      <td>0.001400</td>
      <td>0.00230</td>
      <td>0.005500</td>
      <td>0.9911</td>
    </tr>
    <tr>
      <th>223</th>
      <td>1567.0</td>
      <td>122.846571</td>
      <td>55.156003</td>
      <td>32.2637</td>
      <td>95.147350</td>
      <td>119.43600</td>
      <td>144.502800</td>
      <td>1768.8802</td>
    </tr>
    <tr>
      <th>224</th>
      <td>1516.0</td>
      <td>0.059370</td>
      <td>0.071211</td>
      <td>0.0093</td>
      <td>0.029775</td>
      <td>0.03980</td>
      <td>0.061300</td>
      <td>1.4361</td>
    </tr>
    <tr>
      <th>225</th>
      <td>1516.0</td>
      <td>1041.056588</td>
      <td>433.170076</td>
      <td>168.7998</td>
      <td>718.725350</td>
      <td>967.29980</td>
      <td>1261.299800</td>
      <td>3601.2998</td>
    </tr>
    <tr>
      <th>226</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>227</th>
      <td>1565.0</td>
      <td>0.019125</td>
      <td>0.010756</td>
      <td>0.0062</td>
      <td>0.013200</td>
      <td>0.01650</td>
      <td>0.021200</td>
      <td>0.1541</td>
    </tr>
    <tr>
      <th>228</th>
      <td>1565.0</td>
      <td>0.017844</td>
      <td>0.010745</td>
      <td>0.0072</td>
      <td>0.012600</td>
      <td>0.01550</td>
      <td>0.020000</td>
      <td>0.2133</td>
    </tr>
    <tr>
      <th>229</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>230</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>231</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>232</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>233</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>234</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>235</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>236</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>237</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>238</th>
      <td>1565.0</td>
      <td>0.004791</td>
      <td>0.001698</td>
      <td>0.0013</td>
      <td>0.003700</td>
      <td>0.00460</td>
      <td>0.005700</td>
      <td>0.0244</td>
    </tr>
    <tr>
      <th>239</th>
      <td>1565.0</td>
      <td>0.004575</td>
      <td>0.001441</td>
      <td>0.0014</td>
      <td>0.003600</td>
      <td>0.00440</td>
      <td>0.005300</td>
      <td>0.0236</td>
    </tr>
    <tr>
      <th>240</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>241</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>242</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>243</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>244</th>
      <td>549.0</td>
      <td>0.005755</td>
      <td>0.084618</td>
      <td>0.0003</td>
      <td>0.001200</td>
      <td>0.00170</td>
      <td>0.002600</td>
      <td>1.9844</td>
    </tr>
    <tr>
      <th>245</th>
      <td>549.0</td>
      <td>1.729723</td>
      <td>4.335614</td>
      <td>0.2914</td>
      <td>0.911500</td>
      <td>1.18510</td>
      <td>1.761800</td>
      <td>99.9022</td>
    </tr>
    <tr>
      <th>246</th>
      <td>549.0</td>
      <td>4.148742</td>
      <td>10.045084</td>
      <td>1.1022</td>
      <td>2.725900</td>
      <td>3.67300</td>
      <td>4.479700</td>
      <td>237.1837</td>
    </tr>
    <tr>
      <th>247</th>
      <td>852.0</td>
      <td>0.053374</td>
      <td>0.066880</td>
      <td>0.0000</td>
      <td>0.019200</td>
      <td>0.02700</td>
      <td>0.051500</td>
      <td>0.4914</td>
    </tr>
    <tr>
      <th>248</th>
      <td>1567.0</td>
      <td>0.025171</td>
      <td>0.049235</td>
      <td>0.0030</td>
      <td>0.014700</td>
      <td>0.02100</td>
      <td>0.027300</td>
      <td>0.9732</td>
    </tr>
    <tr>
      <th>249</th>
      <td>1567.0</td>
      <td>0.001065</td>
      <td>0.015771</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.4138</td>
    </tr>
    <tr>
      <th>250</th>
      <td>1567.0</td>
      <td>109.650967</td>
      <td>54.597274</td>
      <td>21.0107</td>
      <td>76.132150</td>
      <td>103.09360</td>
      <td>131.758400</td>
      <td>1119.7042</td>
    </tr>
    <tr>
      <th>251</th>
      <td>1567.0</td>
      <td>0.004285</td>
      <td>0.037472</td>
      <td>0.0003</td>
      <td>0.000700</td>
      <td>0.00100</td>
      <td>0.001300</td>
      <td>0.9909</td>
    </tr>
    <tr>
      <th>252</th>
      <td>1567.0</td>
      <td>4.645115</td>
      <td>64.354756</td>
      <td>0.7673</td>
      <td>2.205650</td>
      <td>2.86460</td>
      <td>3.795050</td>
      <td>2549.9885</td>
    </tr>
    <tr>
      <th>253</th>
      <td>1543.0</td>
      <td>0.033216</td>
      <td>0.022425</td>
      <td>0.0094</td>
      <td>0.024500</td>
      <td>0.03080</td>
      <td>0.037900</td>
      <td>0.4517</td>
    </tr>
    <tr>
      <th>254</th>
      <td>1567.0</td>
      <td>0.013943</td>
      <td>0.009132</td>
      <td>0.0017</td>
      <td>0.004700</td>
      <td>0.01500</td>
      <td>0.021300</td>
      <td>0.0787</td>
    </tr>
    <tr>
      <th>255</th>
      <td>1567.0</td>
      <td>0.403848</td>
      <td>0.120334</td>
      <td>0.1269</td>
      <td>0.307600</td>
      <td>0.40510</td>
      <td>0.480950</td>
      <td>0.9255</td>
    </tr>
    <tr>
      <th>256</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>257</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>258</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>259</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>260</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>261</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>262</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>263</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>264</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>265</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>266</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>267</th>
      <td>1559.0</td>
      <td>0.070587</td>
      <td>0.029649</td>
      <td>0.0198</td>
      <td>0.044000</td>
      <td>0.07060</td>
      <td>0.091650</td>
      <td>0.1578</td>
    </tr>
    <tr>
      <th>268</th>
      <td>1559.0</td>
      <td>19.504677</td>
      <td>7.344404</td>
      <td>6.0980</td>
      <td>13.828000</td>
      <td>17.97700</td>
      <td>24.653000</td>
      <td>40.8550</td>
    </tr>
    <tr>
      <th>269</th>
      <td>1559.0</td>
      <td>3.777866</td>
      <td>1.152329</td>
      <td>1.3017</td>
      <td>2.956500</td>
      <td>3.70350</td>
      <td>4.379400</td>
      <td>10.1529</td>
    </tr>
    <tr>
      <th>270</th>
      <td>1562.0</td>
      <td>29.260291</td>
      <td>8.402013</td>
      <td>15.5471</td>
      <td>24.982300</td>
      <td>28.77350</td>
      <td>31.702200</td>
      <td>158.5260</td>
    </tr>
    <tr>
      <th>271</th>
      <td>1561.0</td>
      <td>46.056598</td>
      <td>17.866438</td>
      <td>10.4015</td>
      <td>30.013900</td>
      <td>45.67650</td>
      <td>59.594700</td>
      <td>132.6479</td>
    </tr>
    <tr>
      <th>272</th>
      <td>1560.0</td>
      <td>41.298147</td>
      <td>17.737513</td>
      <td>6.9431</td>
      <td>27.092725</td>
      <td>40.01925</td>
      <td>54.277325</td>
      <td>122.1174</td>
    </tr>
    <tr>
      <th>273</th>
      <td>1553.0</td>
      <td>20.181246</td>
      <td>3.830463</td>
      <td>8.6512</td>
      <td>18.247100</td>
      <td>19.58090</td>
      <td>22.097300</td>
      <td>43.5737</td>
    </tr>
    <tr>
      <th>274</th>
      <td>1553.0</td>
      <td>136.292426</td>
      <td>85.607784</td>
      <td>0.0000</td>
      <td>81.215600</td>
      <td>110.60140</td>
      <td>162.038200</td>
      <td>659.1696</td>
    </tr>
    <tr>
      <th>275</th>
      <td>1553.0</td>
      <td>8.693213</td>
      <td>168.949413</td>
      <td>0.0111</td>
      <td>0.044700</td>
      <td>0.07840</td>
      <td>0.144900</td>
      <td>3332.5964</td>
    </tr>
    <tr>
      <th>276</th>
      <td>1553.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>277</th>
      <td>1553.0</td>
      <td>2.210744</td>
      <td>1.196437</td>
      <td>0.5615</td>
      <td>1.697700</td>
      <td>2.08310</td>
      <td>2.514300</td>
      <td>32.1709</td>
    </tr>
    <tr>
      <th>278</th>
      <td>1558.0</td>
      <td>0.001117</td>
      <td>0.000340</td>
      <td>0.0000</td>
      <td>0.000900</td>
      <td>0.00110</td>
      <td>0.001300</td>
      <td>0.0034</td>
    </tr>
    <tr>
      <th>279</th>
      <td>1565.0</td>
      <td>0.041057</td>
      <td>0.020289</td>
      <td>0.0107</td>
      <td>0.028300</td>
      <td>0.03720</td>
      <td>0.045800</td>
      <td>0.1884</td>
    </tr>
    <tr>
      <th>280</th>
      <td>1565.0</td>
      <td>0.018034</td>
      <td>0.006483</td>
      <td>0.0073</td>
      <td>0.014200</td>
      <td>0.01690</td>
      <td>0.020700</td>
      <td>0.0755</td>
    </tr>
    <tr>
      <th>281</th>
      <td>1565.0</td>
      <td>0.015094</td>
      <td>0.005545</td>
      <td>0.0069</td>
      <td>0.011900</td>
      <td>0.01390</td>
      <td>0.016600</td>
      <td>0.0597</td>
    </tr>
    <tr>
      <th>282</th>
      <td>1565.0</td>
      <td>0.005770</td>
      <td>0.008550</td>
      <td>0.0016</td>
      <td>0.003300</td>
      <td>0.00530</td>
      <td>0.007100</td>
      <td>0.3083</td>
    </tr>
    <tr>
      <th>283</th>
      <td>1565.0</td>
      <td>2.803984</td>
      <td>5.864324</td>
      <td>0.5050</td>
      <td>2.210400</td>
      <td>2.65800</td>
      <td>3.146200</td>
      <td>232.8049</td>
    </tr>
    <tr>
      <th>284</th>
      <td>1564.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>285</th>
      <td>1564.0</td>
      <td>2.119795</td>
      <td>0.962923</td>
      <td>0.4611</td>
      <td>1.438175</td>
      <td>1.87515</td>
      <td>2.606950</td>
      <td>6.8698</td>
    </tr>
    <tr>
      <th>286</th>
      <td>1564.0</td>
      <td>4.260018</td>
      <td>9.763829</td>
      <td>0.7280</td>
      <td>2.467200</td>
      <td>3.36005</td>
      <td>4.311425</td>
      <td>207.0161</td>
    </tr>
    <tr>
      <th>287</th>
      <td>1564.0</td>
      <td>0.367529</td>
      <td>7.386343</td>
      <td>0.0513</td>
      <td>0.114875</td>
      <td>0.13895</td>
      <td>0.198450</td>
      <td>292.2274</td>
    </tr>
    <tr>
      <th>288</th>
      <td>1564.0</td>
      <td>0.003924</td>
      <td>0.002936</td>
      <td>0.0012</td>
      <td>0.002400</td>
      <td>0.00360</td>
      <td>0.004900</td>
      <td>0.0749</td>
    </tr>
    <tr>
      <th>289</th>
      <td>1564.0</td>
      <td>2.578596</td>
      <td>1.616993</td>
      <td>0.3960</td>
      <td>2.092125</td>
      <td>2.54900</td>
      <td>3.024525</td>
      <td>59.5187</td>
    </tr>
    <tr>
      <th>290</th>
      <td>1557.0</td>
      <td>0.123427</td>
      <td>0.270987</td>
      <td>0.0416</td>
      <td>0.064900</td>
      <td>0.08330</td>
      <td>0.118100</td>
      <td>4.4203</td>
    </tr>
    <tr>
      <th>291</th>
      <td>1567.0</td>
      <td>0.019926</td>
      <td>0.025549</td>
      <td>0.0038</td>
      <td>0.012500</td>
      <td>0.01690</td>
      <td>0.023600</td>
      <td>0.6915</td>
    </tr>
    <tr>
      <th>292</th>
      <td>138.0</td>
      <td>0.014487</td>
      <td>0.011494</td>
      <td>0.0041</td>
      <td>0.008725</td>
      <td>0.01100</td>
      <td>0.014925</td>
      <td>0.0831</td>
    </tr>
    <tr>
      <th>293</th>
      <td>138.0</td>
      <td>335.551157</td>
      <td>137.692483</td>
      <td>82.3233</td>
      <td>229.809450</td>
      <td>317.86710</td>
      <td>403.989300</td>
      <td>879.2260</td>
    </tr>
    <tr>
      <th>294</th>
      <td>1565.0</td>
      <td>401.814750</td>
      <td>477.050076</td>
      <td>0.0000</td>
      <td>185.089800</td>
      <td>278.67190</td>
      <td>428.554500</td>
      <td>3933.7550</td>
    </tr>
    <tr>
      <th>295</th>
      <td>1565.0</td>
      <td>252.999118</td>
      <td>283.530702</td>
      <td>0.0000</td>
      <td>130.220300</td>
      <td>195.82560</td>
      <td>273.952600</td>
      <td>2005.8744</td>
    </tr>
    <tr>
      <th>296</th>
      <td>1565.0</td>
      <td>1879.228369</td>
      <td>1975.111365</td>
      <td>0.0000</td>
      <td>603.032900</td>
      <td>1202.41210</td>
      <td>2341.288700</td>
      <td>15559.9525</td>
    </tr>
    <tr>
      <th>297</th>
      <td>1565.0</td>
      <td>2342.826978</td>
      <td>3226.924298</td>
      <td>0.0000</td>
      <td>210.936600</td>
      <td>820.09880</td>
      <td>3190.616400</td>
      <td>18520.4683</td>
    </tr>
    <tr>
      <th>298</th>
      <td>1565.0</td>
      <td>0.063804</td>
      <td>0.064225</td>
      <td>0.0000</td>
      <td>0.040700</td>
      <td>0.05280</td>
      <td>0.069200</td>
      <td>0.5264</td>
    </tr>
    <tr>
      <th>299</th>
      <td>1565.0</td>
      <td>0.060267</td>
      <td>0.130825</td>
      <td>0.0000</td>
      <td>0.030200</td>
      <td>0.04000</td>
      <td>0.052000</td>
      <td>1.0312</td>
    </tr>
    <tr>
      <th>300</th>
      <td>1565.0</td>
      <td>0.118386</td>
      <td>0.219147</td>
      <td>0.0000</td>
      <td>0.058900</td>
      <td>0.08280</td>
      <td>0.115500</td>
      <td>1.8123</td>
    </tr>
    <tr>
      <th>301</th>
      <td>1565.0</td>
      <td>0.910146</td>
      <td>0.331982</td>
      <td>0.3100</td>
      <td>0.717200</td>
      <td>0.86040</td>
      <td>1.046400</td>
      <td>5.7110</td>
    </tr>
    <tr>
      <th>302</th>
      <td>1565.0</td>
      <td>0.403342</td>
      <td>0.197514</td>
      <td>0.1118</td>
      <td>0.295800</td>
      <td>0.38080</td>
      <td>0.477000</td>
      <td>5.1549</td>
    </tr>
    <tr>
      <th>303</th>
      <td>1565.0</td>
      <td>0.040344</td>
      <td>0.014511</td>
      <td>0.0108</td>
      <td>0.030000</td>
      <td>0.03880</td>
      <td>0.048600</td>
      <td>0.2258</td>
    </tr>
    <tr>
      <th>304</th>
      <td>1565.0</td>
      <td>0.132076</td>
      <td>0.064867</td>
      <td>0.0138</td>
      <td>0.072800</td>
      <td>0.13720</td>
      <td>0.178500</td>
      <td>0.3337</td>
    </tr>
    <tr>
      <th>305</th>
      <td>1566.0</td>
      <td>0.264917</td>
      <td>0.057387</td>
      <td>0.1171</td>
      <td>0.225000</td>
      <td>0.26430</td>
      <td>0.307500</td>
      <td>0.4750</td>
    </tr>
    <tr>
      <th>306</th>
      <td>1566.0</td>
      <td>0.048623</td>
      <td>0.025400</td>
      <td>0.0034</td>
      <td>0.033100</td>
      <td>0.04480</td>
      <td>0.055200</td>
      <td>0.2246</td>
    </tr>
    <tr>
      <th>307</th>
      <td>1566.0</td>
      <td>0.128921</td>
      <td>0.027468</td>
      <td>0.0549</td>
      <td>0.113700</td>
      <td>0.12950</td>
      <td>0.147600</td>
      <td>0.2112</td>
    </tr>
    <tr>
      <th>308</th>
      <td>1566.0</td>
      <td>0.218414</td>
      <td>0.033593</td>
      <td>0.0913</td>
      <td>0.197600</td>
      <td>0.21945</td>
      <td>0.237900</td>
      <td>0.3239</td>
    </tr>
    <tr>
      <th>309</th>
      <td>1566.0</td>
      <td>0.128921</td>
      <td>0.027470</td>
      <td>0.0549</td>
      <td>0.113700</td>
      <td>0.12950</td>
      <td>0.147600</td>
      <td>0.2112</td>
    </tr>
    <tr>
      <th>310</th>
      <td>1566.0</td>
      <td>0.304752</td>
      <td>0.043460</td>
      <td>0.1809</td>
      <td>0.278550</td>
      <td>0.30290</td>
      <td>0.331900</td>
      <td>0.4438</td>
    </tr>
    <tr>
      <th>311</th>
      <td>1566.0</td>
      <td>0.097344</td>
      <td>0.028796</td>
      <td>0.0328</td>
      <td>0.077600</td>
      <td>0.09770</td>
      <td>0.115900</td>
      <td>0.1784</td>
    </tr>
    <tr>
      <th>312</th>
      <td>1566.0</td>
      <td>0.160051</td>
      <td>0.117316</td>
      <td>0.0224</td>
      <td>0.091500</td>
      <td>0.12150</td>
      <td>0.160175</td>
      <td>0.7549</td>
    </tr>
    <tr>
      <th>313</th>
      <td>1543.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>314</th>
      <td>1543.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>315</th>
      <td>1566.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>316</th>
      <td>1566.0</td>
      <td>5.976977</td>
      <td>1.018629</td>
      <td>2.7882</td>
      <td>5.301525</td>
      <td>5.83150</td>
      <td>6.547800</td>
      <td>13.0958</td>
    </tr>
    <tr>
      <th>317</th>
      <td>1566.0</td>
      <td>0.172629</td>
      <td>0.072392</td>
      <td>0.0283</td>
      <td>0.117375</td>
      <td>0.16340</td>
      <td>0.218100</td>
      <td>1.0034</td>
    </tr>
    <tr>
      <th>318</th>
      <td>1566.0</td>
      <td>3.188770</td>
      <td>1.215930</td>
      <td>0.9848</td>
      <td>2.319725</td>
      <td>2.89890</td>
      <td>4.021250</td>
      <td>15.8934</td>
    </tr>
    <tr>
      <th>319</th>
      <td>1566.0</td>
      <td>7.916036</td>
      <td>2.179059</td>
      <td>1.6574</td>
      <td>6.245150</td>
      <td>8.38880</td>
      <td>9.481100</td>
      <td>20.0455</td>
    </tr>
    <tr>
      <th>320</th>
      <td>1566.0</td>
      <td>0.043105</td>
      <td>0.031885</td>
      <td>0.0084</td>
      <td>0.031200</td>
      <td>0.03985</td>
      <td>0.050200</td>
      <td>0.9474</td>
    </tr>
    <tr>
      <th>321</th>
      <td>1566.0</td>
      <td>2.263727</td>
      <td>2.116994</td>
      <td>0.6114</td>
      <td>1.670075</td>
      <td>2.07765</td>
      <td>2.633350</td>
      <td>79.1515</td>
    </tr>
    <tr>
      <th>322</th>
      <td>1566.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>323</th>
      <td>1566.0</td>
      <td>5.393420</td>
      <td>2.518859</td>
      <td>1.7101</td>
      <td>4.272950</td>
      <td>5.45880</td>
      <td>6.344875</td>
      <td>89.1917</td>
    </tr>
    <tr>
      <th>324</th>
      <td>1566.0</td>
      <td>13.332172</td>
      <td>6.615850</td>
      <td>2.2345</td>
      <td>7.578600</td>
      <td>12.50450</td>
      <td>17.925175</td>
      <td>51.8678</td>
    </tr>
    <tr>
      <th>325</th>
      <td>1566.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>326</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>327</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>328</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>329</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>330</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>331</th>
      <td>1563.0</td>
      <td>0.083232</td>
      <td>0.063425</td>
      <td>0.0224</td>
      <td>0.068800</td>
      <td>0.08480</td>
      <td>0.095600</td>
      <td>1.0959</td>
    </tr>
    <tr>
      <th>332</th>
      <td>1560.0</td>
      <td>2.593485</td>
      <td>5.645226</td>
      <td>0.5373</td>
      <td>1.546550</td>
      <td>2.06270</td>
      <td>2.790525</td>
      <td>174.8944</td>
    </tr>
    <tr>
      <th>333</th>
      <td>1561.0</td>
      <td>6.215866</td>
      <td>3.403447</td>
      <td>2.8372</td>
      <td>5.453900</td>
      <td>5.98010</td>
      <td>6.549500</td>
      <td>90.5159</td>
    </tr>
    <tr>
      <th>334</th>
      <td>1561.0</td>
      <td>0.168364</td>
      <td>0.172883</td>
      <td>0.0282</td>
      <td>0.089400</td>
      <td>0.12940</td>
      <td>0.210400</td>
      <td>3.4125</td>
    </tr>
    <tr>
      <th>335</th>
      <td>1561.0</td>
      <td>3.426925</td>
      <td>5.781558</td>
      <td>0.7899</td>
      <td>2.035700</td>
      <td>2.51350</td>
      <td>3.360400</td>
      <td>172.7119</td>
    </tr>
    <tr>
      <th>336</th>
      <td>1560.0</td>
      <td>9.736386</td>
      <td>7.556131</td>
      <td>5.2151</td>
      <td>8.288525</td>
      <td>9.07355</td>
      <td>10.041625</td>
      <td>214.8628</td>
    </tr>
    <tr>
      <th>337</th>
      <td>1560.0</td>
      <td>2.327482</td>
      <td>1.699435</td>
      <td>0.0000</td>
      <td>1.542850</td>
      <td>2.05445</td>
      <td>2.785475</td>
      <td>38.8995</td>
    </tr>
    <tr>
      <th>338</th>
      <td>1560.0</td>
      <td>3.037580</td>
      <td>5.645022</td>
      <td>0.0000</td>
      <td>1.901350</td>
      <td>2.56085</td>
      <td>3.405450</td>
      <td>196.6880</td>
    </tr>
    <tr>
      <th>339</th>
      <td>1561.0</td>
      <td>9.328958</td>
      <td>6.074702</td>
      <td>2.2001</td>
      <td>7.588900</td>
      <td>9.47420</td>
      <td>10.439900</td>
      <td>197.4988</td>
    </tr>
    <tr>
      <th>340</th>
      <td>1561.0</td>
      <td>14.673507</td>
      <td>261.738451</td>
      <td>0.0131</td>
      <td>0.034600</td>
      <td>0.04640</td>
      <td>0.066800</td>
      <td>5043.8789</td>
    </tr>
    <tr>
      <th>341</th>
      <td>1561.0</td>
      <td>2.732094</td>
      <td>3.667902</td>
      <td>0.5741</td>
      <td>1.911800</td>
      <td>2.37730</td>
      <td>2.985400</td>
      <td>97.7089</td>
    </tr>
    <tr>
      <th>342</th>
      <td>1561.0</td>
      <td>0.000286</td>
      <td>0.011319</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.4472</td>
    </tr>
    <tr>
      <th>343</th>
      <td>1561.0</td>
      <td>6.198508</td>
      <td>5.371825</td>
      <td>1.2565</td>
      <td>4.998900</td>
      <td>6.00560</td>
      <td>6.885200</td>
      <td>156.3360</td>
    </tr>
    <tr>
      <th>344</th>
      <td>1561.0</td>
      <td>23.217146</td>
      <td>8.895221</td>
      <td>2.0560</td>
      <td>17.860900</td>
      <td>23.21470</td>
      <td>28.873100</td>
      <td>59.3241</td>
    </tr>
    <tr>
      <th>345</th>
      <td>773.0</td>
      <td>7.958376</td>
      <td>17.512965</td>
      <td>1.7694</td>
      <td>4.440600</td>
      <td>5.56700</td>
      <td>6.825500</td>
      <td>257.0106</td>
    </tr>
    <tr>
      <th>346</th>
      <td>773.0</td>
      <td>5.770212</td>
      <td>17.077498</td>
      <td>1.0177</td>
      <td>2.532700</td>
      <td>3.04640</td>
      <td>4.085700</td>
      <td>187.7589</td>
    </tr>
    <tr>
      <th>347</th>
      <td>1561.0</td>
      <td>0.008914</td>
      <td>0.352186</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>13.9147</td>
    </tr>
    <tr>
      <th>348</th>
      <td>1543.0</td>
      <td>0.024706</td>
      <td>0.011862</td>
      <td>0.0103</td>
      <td>0.018000</td>
      <td>0.02260</td>
      <td>0.027300</td>
      <td>0.2200</td>
    </tr>
    <tr>
      <th>349</th>
      <td>1543.0</td>
      <td>0.025252</td>
      <td>0.010603</td>
      <td>0.0010</td>
      <td>0.019600</td>
      <td>0.02400</td>
      <td>0.028600</td>
      <td>0.1339</td>
    </tr>
    <tr>
      <th>350</th>
      <td>1543.0</td>
      <td>0.023202</td>
      <td>0.014326</td>
      <td>0.0029</td>
      <td>0.014600</td>
      <td>0.01880</td>
      <td>0.028500</td>
      <td>0.2914</td>
    </tr>
    <tr>
      <th>351</th>
      <td>1543.0</td>
      <td>0.027584</td>
      <td>0.024563</td>
      <td>0.0020</td>
      <td>0.016600</td>
      <td>0.02530</td>
      <td>0.033900</td>
      <td>0.6188</td>
    </tr>
    <tr>
      <th>352</th>
      <td>1543.0</td>
      <td>0.023356</td>
      <td>0.013157</td>
      <td>0.0056</td>
      <td>0.016000</td>
      <td>0.02200</td>
      <td>0.026900</td>
      <td>0.1429</td>
    </tr>
    <tr>
      <th>353</th>
      <td>1543.0</td>
      <td>0.040331</td>
      <td>0.015499</td>
      <td>0.0026</td>
      <td>0.030200</td>
      <td>0.04210</td>
      <td>0.050200</td>
      <td>0.1535</td>
    </tr>
    <tr>
      <th>354</th>
      <td>1543.0</td>
      <td>0.041921</td>
      <td>0.013068</td>
      <td>0.0040</td>
      <td>0.034850</td>
      <td>0.04420</td>
      <td>0.050000</td>
      <td>0.1344</td>
    </tr>
    <tr>
      <th>355</th>
      <td>1543.0</td>
      <td>0.034543</td>
      <td>0.022307</td>
      <td>0.0038</td>
      <td>0.021200</td>
      <td>0.02940</td>
      <td>0.042300</td>
      <td>0.2789</td>
    </tr>
    <tr>
      <th>356</th>
      <td>1566.0</td>
      <td>1.298629</td>
      <td>0.386918</td>
      <td>0.3796</td>
      <td>1.025475</td>
      <td>1.25530</td>
      <td>1.533325</td>
      <td>2.8348</td>
    </tr>
    <tr>
      <th>357</th>
      <td>1555.0</td>
      <td>0.000999</td>
      <td>0.000501</td>
      <td>0.0003</td>
      <td>0.000700</td>
      <td>0.00090</td>
      <td>0.001100</td>
      <td>0.0052</td>
    </tr>
    <tr>
      <th>358</th>
      <td>226.0</td>
      <td>0.002443</td>
      <td>0.000395</td>
      <td>0.0017</td>
      <td>0.002200</td>
      <td>0.00240</td>
      <td>0.002700</td>
      <td>0.0047</td>
    </tr>
    <tr>
      <th>359</th>
      <td>1567.0</td>
      <td>0.019840</td>
      <td>0.007136</td>
      <td>0.0076</td>
      <td>0.013800</td>
      <td>0.01960</td>
      <td>0.025000</td>
      <td>0.0888</td>
    </tr>
    <tr>
      <th>360</th>
      <td>1567.0</td>
      <td>0.002945</td>
      <td>0.020003</td>
      <td>0.0001</td>
      <td>0.000400</td>
      <td>0.00070</td>
      <td>0.001800</td>
      <td>0.4090</td>
    </tr>
    <tr>
      <th>361</th>
      <td>1567.0</td>
      <td>39.936406</td>
      <td>17.056304</td>
      <td>10.7204</td>
      <td>32.168700</td>
      <td>39.69610</td>
      <td>47.079200</td>
      <td>547.1722</td>
    </tr>
    <tr>
      <th>362</th>
      <td>1516.0</td>
      <td>0.018383</td>
      <td>0.021644</td>
      <td>0.0028</td>
      <td>0.009500</td>
      <td>0.01250</td>
      <td>0.018600</td>
      <td>0.4163</td>
    </tr>
    <tr>
      <th>363</th>
      <td>1516.0</td>
      <td>333.319601</td>
      <td>138.801928</td>
      <td>60.9882</td>
      <td>228.682525</td>
      <td>309.83165</td>
      <td>412.329775</td>
      <td>1072.2031</td>
    </tr>
    <tr>
      <th>364</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>365</th>
      <td>1565.0</td>
      <td>0.005199</td>
      <td>0.002656</td>
      <td>0.0017</td>
      <td>0.003800</td>
      <td>0.00460</td>
      <td>0.005800</td>
      <td>0.0368</td>
    </tr>
    <tr>
      <th>366</th>
      <td>1565.0</td>
      <td>0.004814</td>
      <td>0.002382</td>
      <td>0.0020</td>
      <td>0.003500</td>
      <td>0.00430</td>
      <td>0.005400</td>
      <td>0.0392</td>
    </tr>
    <tr>
      <th>367</th>
      <td>1561.0</td>
      <td>0.003773</td>
      <td>0.002699</td>
      <td>0.0000</td>
      <td>0.002600</td>
      <td>0.00320</td>
      <td>0.004200</td>
      <td>0.0357</td>
    </tr>
    <tr>
      <th>368</th>
      <td>1561.0</td>
      <td>0.003172</td>
      <td>0.002107</td>
      <td>0.0000</td>
      <td>0.002200</td>
      <td>0.00280</td>
      <td>0.003600</td>
      <td>0.0334</td>
    </tr>
    <tr>
      <th>369</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>370</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>371</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>372</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>373</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>374</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>375</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>376</th>
      <td>1565.0</td>
      <td>0.001601</td>
      <td>0.000534</td>
      <td>0.0004</td>
      <td>0.001300</td>
      <td>0.00160</td>
      <td>0.001900</td>
      <td>0.0082</td>
    </tr>
    <tr>
      <th>377</th>
      <td>1565.0</td>
      <td>0.001571</td>
      <td>0.000467</td>
      <td>0.0004</td>
      <td>0.001300</td>
      <td>0.00150</td>
      <td>0.001800</td>
      <td>0.0077</td>
    </tr>
    <tr>
      <th>378</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>379</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>380</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>381</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>382</th>
      <td>549.0</td>
      <td>0.001826</td>
      <td>0.026740</td>
      <td>0.0001</td>
      <td>0.000400</td>
      <td>0.00050</td>
      <td>0.000800</td>
      <td>0.6271</td>
    </tr>
    <tr>
      <th>383</th>
      <td>549.0</td>
      <td>0.541040</td>
      <td>1.341020</td>
      <td>0.0875</td>
      <td>0.295500</td>
      <td>0.37260</td>
      <td>0.541200</td>
      <td>30.9982</td>
    </tr>
    <tr>
      <th>384</th>
      <td>549.0</td>
      <td>1.285448</td>
      <td>3.168427</td>
      <td>0.3383</td>
      <td>0.842300</td>
      <td>1.10630</td>
      <td>1.386600</td>
      <td>74.8445</td>
    </tr>
    <tr>
      <th>385</th>
      <td>852.0</td>
      <td>0.011427</td>
      <td>0.014366</td>
      <td>0.0000</td>
      <td>0.005300</td>
      <td>0.00680</td>
      <td>0.011325</td>
      <td>0.2073</td>
    </tr>
    <tr>
      <th>386</th>
      <td>1567.0</td>
      <td>0.008281</td>
      <td>0.015488</td>
      <td>0.0008</td>
      <td>0.004800</td>
      <td>0.00680</td>
      <td>0.009300</td>
      <td>0.3068</td>
    </tr>
    <tr>
      <th>387</th>
      <td>1567.0</td>
      <td>0.000339</td>
      <td>0.004989</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.1309</td>
    </tr>
    <tr>
      <th>388</th>
      <td>1567.0</td>
      <td>35.155091</td>
      <td>17.227003</td>
      <td>6.3101</td>
      <td>24.386550</td>
      <td>32.53070</td>
      <td>42.652450</td>
      <td>348.8293</td>
    </tr>
    <tr>
      <th>389</th>
      <td>1567.0</td>
      <td>0.001338</td>
      <td>0.011816</td>
      <td>0.0001</td>
      <td>0.000200</td>
      <td>0.00030</td>
      <td>0.000400</td>
      <td>0.3127</td>
    </tr>
    <tr>
      <th>390</th>
      <td>1567.0</td>
      <td>1.431868</td>
      <td>20.326415</td>
      <td>0.3046</td>
      <td>0.675150</td>
      <td>0.87730</td>
      <td>1.148200</td>
      <td>805.3936</td>
    </tr>
    <tr>
      <th>391</th>
      <td>1543.0</td>
      <td>0.010956</td>
      <td>0.006738</td>
      <td>0.0031</td>
      <td>0.008300</td>
      <td>0.01020</td>
      <td>0.012400</td>
      <td>0.1375</td>
    </tr>
    <tr>
      <th>392</th>
      <td>1567.0</td>
      <td>0.004533</td>
      <td>0.002956</td>
      <td>0.0005</td>
      <td>0.001500</td>
      <td>0.00490</td>
      <td>0.006900</td>
      <td>0.0229</td>
    </tr>
    <tr>
      <th>393</th>
      <td>1567.0</td>
      <td>0.133990</td>
      <td>0.038408</td>
      <td>0.0342</td>
      <td>0.104400</td>
      <td>0.13390</td>
      <td>0.160400</td>
      <td>0.2994</td>
    </tr>
    <tr>
      <th>394</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>395</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>396</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>397</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>398</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>399</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>400</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>401</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>402</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>403</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>404</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>405</th>
      <td>1559.0</td>
      <td>0.024208</td>
      <td>0.010728</td>
      <td>0.0062</td>
      <td>0.014000</td>
      <td>0.02390</td>
      <td>0.032300</td>
      <td>0.0514</td>
    </tr>
    <tr>
      <th>406</th>
      <td>1559.0</td>
      <td>6.730615</td>
      <td>2.829583</td>
      <td>2.0545</td>
      <td>4.547600</td>
      <td>5.92010</td>
      <td>8.585200</td>
      <td>14.7277</td>
    </tr>
    <tr>
      <th>407</th>
      <td>1559.0</td>
      <td>1.231997</td>
      <td>0.364711</td>
      <td>0.4240</td>
      <td>0.966500</td>
      <td>1.23970</td>
      <td>1.416700</td>
      <td>3.3128</td>
    </tr>
    <tr>
      <th>408</th>
      <td>1562.0</td>
      <td>5.340932</td>
      <td>2.578118</td>
      <td>2.7378</td>
      <td>4.127800</td>
      <td>4.92245</td>
      <td>5.787100</td>
      <td>44.3100</td>
    </tr>
    <tr>
      <th>409</th>
      <td>1561.0</td>
      <td>4.580430</td>
      <td>1.776843</td>
      <td>1.2163</td>
      <td>3.012800</td>
      <td>4.48970</td>
      <td>5.936700</td>
      <td>9.5765</td>
    </tr>
    <tr>
      <th>410</th>
      <td>1560.0</td>
      <td>4.929344</td>
      <td>2.122978</td>
      <td>0.7342</td>
      <td>3.265075</td>
      <td>4.73275</td>
      <td>6.458300</td>
      <td>13.8071</td>
    </tr>
    <tr>
      <th>411</th>
      <td>1553.0</td>
      <td>2.616086</td>
      <td>0.551474</td>
      <td>0.9609</td>
      <td>2.321300</td>
      <td>2.54810</td>
      <td>2.853200</td>
      <td>6.2150</td>
    </tr>
    <tr>
      <th>412</th>
      <td>1553.0</td>
      <td>30.911316</td>
      <td>18.413622</td>
      <td>0.0000</td>
      <td>18.407900</td>
      <td>26.15690</td>
      <td>38.139700</td>
      <td>128.2816</td>
    </tr>
    <tr>
      <th>413</th>
      <td>1553.0</td>
      <td>25.612690</td>
      <td>47.308463</td>
      <td>4.0416</td>
      <td>11.375800</td>
      <td>20.25510</td>
      <td>29.307300</td>
      <td>899.1190</td>
    </tr>
    <tr>
      <th>414</th>
      <td>1553.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>415</th>
      <td>1553.0</td>
      <td>6.630616</td>
      <td>3.958371</td>
      <td>1.5340</td>
      <td>4.927400</td>
      <td>6.17660</td>
      <td>7.570700</td>
      <td>116.8615</td>
    </tr>
    <tr>
      <th>416</th>
      <td>1558.0</td>
      <td>3.404349</td>
      <td>1.035433</td>
      <td>0.0000</td>
      <td>2.660100</td>
      <td>3.23400</td>
      <td>4.010700</td>
      <td>9.6900</td>
    </tr>
    <tr>
      <th>417</th>
      <td>1565.0</td>
      <td>8.190905</td>
      <td>4.054515</td>
      <td>2.1531</td>
      <td>5.765500</td>
      <td>7.39560</td>
      <td>9.168800</td>
      <td>39.0376</td>
    </tr>
    <tr>
      <th>418</th>
      <td>1565.0</td>
      <td>320.259235</td>
      <td>287.704482</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>302.17760</td>
      <td>524.002200</td>
      <td>999.3160</td>
    </tr>
    <tr>
      <th>419</th>
      <td>1565.0</td>
      <td>309.061299</td>
      <td>325.448391</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>272.44870</td>
      <td>582.935200</td>
      <td>998.6813</td>
    </tr>
    <tr>
      <th>420</th>
      <td>1565.0</td>
      <td>1.821261</td>
      <td>3.057692</td>
      <td>0.4411</td>
      <td>1.030400</td>
      <td>1.64510</td>
      <td>2.214700</td>
      <td>111.4956</td>
    </tr>
    <tr>
      <th>421</th>
      <td>1565.0</td>
      <td>4.174524</td>
      <td>6.913855</td>
      <td>0.7217</td>
      <td>3.184200</td>
      <td>3.94310</td>
      <td>4.784300</td>
      <td>273.0952</td>
    </tr>
    <tr>
      <th>422</th>
      <td>1564.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>423</th>
      <td>1564.0</td>
      <td>77.660446</td>
      <td>32.596933</td>
      <td>23.0200</td>
      <td>55.976675</td>
      <td>69.90545</td>
      <td>92.911500</td>
      <td>424.2152</td>
    </tr>
    <tr>
      <th>424</th>
      <td>1564.0</td>
      <td>3.315469</td>
      <td>6.325365</td>
      <td>0.4866</td>
      <td>1.965250</td>
      <td>2.66710</td>
      <td>3.470975</td>
      <td>103.1809</td>
    </tr>
    <tr>
      <th>425</th>
      <td>1564.0</td>
      <td>6.796312</td>
      <td>23.257716</td>
      <td>1.4666</td>
      <td>3.766200</td>
      <td>4.76440</td>
      <td>6.883500</td>
      <td>898.6085</td>
    </tr>
    <tr>
      <th>426</th>
      <td>1564.0</td>
      <td>1.233858</td>
      <td>0.995620</td>
      <td>0.3632</td>
      <td>0.743425</td>
      <td>1.13530</td>
      <td>1.539500</td>
      <td>24.9904</td>
    </tr>
    <tr>
      <th>427</th>
      <td>1564.0</td>
      <td>4.058501</td>
      <td>3.042144</td>
      <td>0.6637</td>
      <td>3.113225</td>
      <td>3.94145</td>
      <td>4.768650</td>
      <td>113.2230</td>
    </tr>
    <tr>
      <th>428</th>
      <td>1557.0</td>
      <td>4.220747</td>
      <td>10.632730</td>
      <td>1.1198</td>
      <td>1.935500</td>
      <td>2.53410</td>
      <td>3.609000</td>
      <td>118.7533</td>
    </tr>
    <tr>
      <th>429</th>
      <td>1567.0</td>
      <td>4.171844</td>
      <td>6.435390</td>
      <td>0.7837</td>
      <td>2.571400</td>
      <td>3.45380</td>
      <td>4.755800</td>
      <td>186.6164</td>
    </tr>
    <tr>
      <th>430</th>
      <td>1565.0</td>
      <td>18.421600</td>
      <td>36.060084</td>
      <td>0.0000</td>
      <td>6.999700</td>
      <td>11.10560</td>
      <td>17.423100</td>
      <td>400.0000</td>
    </tr>
    <tr>
      <th>431</th>
      <td>1565.0</td>
      <td>22.358305</td>
      <td>36.395408</td>
      <td>0.0000</td>
      <td>11.059000</td>
      <td>16.38100</td>
      <td>21.765200</td>
      <td>400.0000</td>
    </tr>
    <tr>
      <th>432</th>
      <td>1565.0</td>
      <td>99.367633</td>
      <td>126.188715</td>
      <td>0.0000</td>
      <td>31.032400</td>
      <td>57.96930</td>
      <td>120.172900</td>
      <td>994.2857</td>
    </tr>
    <tr>
      <th>433</th>
      <td>1565.0</td>
      <td>205.519304</td>
      <td>225.778870</td>
      <td>0.0000</td>
      <td>10.027100</td>
      <td>151.11560</td>
      <td>305.026300</td>
      <td>995.7447</td>
    </tr>
    <tr>
      <th>434</th>
      <td>1565.0</td>
      <td>14.733945</td>
      <td>34.108854</td>
      <td>0.0000</td>
      <td>7.550700</td>
      <td>10.19770</td>
      <td>12.754200</td>
      <td>400.0000</td>
    </tr>
    <tr>
      <th>435</th>
      <td>1565.0</td>
      <td>9.370666</td>
      <td>34.369789</td>
      <td>0.0000</td>
      <td>3.494400</td>
      <td>4.55110</td>
      <td>5.822800</td>
      <td>400.0000</td>
    </tr>
    <tr>
      <th>436</th>
      <td>1565.0</td>
      <td>7.513266</td>
      <td>34.557804</td>
      <td>0.0000</td>
      <td>1.950900</td>
      <td>2.76430</td>
      <td>3.822200</td>
      <td>400.0000</td>
    </tr>
    <tr>
      <th>437</th>
      <td>1565.0</td>
      <td>4.016785</td>
      <td>1.611274</td>
      <td>1.1568</td>
      <td>3.070700</td>
      <td>3.78090</td>
      <td>4.678600</td>
      <td>32.2740</td>
    </tr>
    <tr>
      <th>438</th>
      <td>1565.0</td>
      <td>54.701052</td>
      <td>34.108051</td>
      <td>0.0000</td>
      <td>36.290300</td>
      <td>49.09090</td>
      <td>66.666700</td>
      <td>851.6129</td>
    </tr>
    <tr>
      <th>439</th>
      <td>1565.0</td>
      <td>70.643942</td>
      <td>38.376178</td>
      <td>14.1206</td>
      <td>48.173800</td>
      <td>65.43780</td>
      <td>84.973400</td>
      <td>657.7621</td>
    </tr>
    <tr>
      <th>440</th>
      <td>1565.0</td>
      <td>11.526617</td>
      <td>6.169471</td>
      <td>1.0973</td>
      <td>5.414100</td>
      <td>12.08590</td>
      <td>15.796400</td>
      <td>33.0580</td>
    </tr>
    <tr>
      <th>441</th>
      <td>1566.0</td>
      <td>0.802081</td>
      <td>0.184213</td>
      <td>0.3512</td>
      <td>0.679600</td>
      <td>0.80760</td>
      <td>0.927600</td>
      <td>1.2771</td>
    </tr>
    <tr>
      <th>442</th>
      <td>1566.0</td>
      <td>1.345259</td>
      <td>0.659195</td>
      <td>0.0974</td>
      <td>0.907650</td>
      <td>1.26455</td>
      <td>1.577825</td>
      <td>5.1317</td>
    </tr>
    <tr>
      <th>443</th>
      <td>1566.0</td>
      <td>0.633941</td>
      <td>0.143552</td>
      <td>0.2169</td>
      <td>0.550500</td>
      <td>0.64350</td>
      <td>0.733425</td>
      <td>1.0851</td>
    </tr>
    <tr>
      <th>444</th>
      <td>1566.0</td>
      <td>0.895043</td>
      <td>0.155522</td>
      <td>0.3336</td>
      <td>0.804800</td>
      <td>0.90270</td>
      <td>0.988800</td>
      <td>1.3511</td>
    </tr>
    <tr>
      <th>445</th>
      <td>1566.0</td>
      <td>0.647090</td>
      <td>0.141252</td>
      <td>0.3086</td>
      <td>0.555800</td>
      <td>0.65110</td>
      <td>0.748400</td>
      <td>1.1087</td>
    </tr>
    <tr>
      <th>446</th>
      <td>1566.0</td>
      <td>1.175003</td>
      <td>0.176158</td>
      <td>0.6968</td>
      <td>1.046800</td>
      <td>1.16380</td>
      <td>1.272300</td>
      <td>1.7639</td>
    </tr>
    <tr>
      <th>447</th>
      <td>1566.0</td>
      <td>0.281895</td>
      <td>0.086461</td>
      <td>0.0846</td>
      <td>0.226100</td>
      <td>0.27970</td>
      <td>0.338825</td>
      <td>0.5085</td>
    </tr>
    <tr>
      <th>448</th>
      <td>1566.0</td>
      <td>0.332270</td>
      <td>0.236275</td>
      <td>0.0399</td>
      <td>0.187700</td>
      <td>0.25120</td>
      <td>0.351100</td>
      <td>1.4754</td>
    </tr>
    <tr>
      <th>449</th>
      <td>1543.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>450</th>
      <td>1543.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>451</th>
      <td>1566.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>452</th>
      <td>1566.0</td>
      <td>5.346816</td>
      <td>0.919196</td>
      <td>2.6709</td>
      <td>4.764200</td>
      <td>5.27145</td>
      <td>5.913000</td>
      <td>13.9776</td>
    </tr>
    <tr>
      <th>453</th>
      <td>1566.0</td>
      <td>5.460971</td>
      <td>2.250804</td>
      <td>0.9037</td>
      <td>3.747875</td>
      <td>5.22710</td>
      <td>6.902475</td>
      <td>34.4902</td>
    </tr>
    <tr>
      <th>454</th>
      <td>1566.0</td>
      <td>7.883742</td>
      <td>3.059660</td>
      <td>2.3294</td>
      <td>5.806525</td>
      <td>7.42490</td>
      <td>9.576775</td>
      <td>42.0703</td>
    </tr>
    <tr>
      <th>455</th>
      <td>1566.0</td>
      <td>3.636633</td>
      <td>0.938372</td>
      <td>0.6948</td>
      <td>2.899675</td>
      <td>3.72450</td>
      <td>4.341925</td>
      <td>10.1840</td>
    </tr>
    <tr>
      <th>456</th>
      <td>1566.0</td>
      <td>12.325685</td>
      <td>8.125876</td>
      <td>3.0489</td>
      <td>8.816575</td>
      <td>11.35090</td>
      <td>14.387900</td>
      <td>232.1258</td>
    </tr>
    <tr>
      <th>457</th>
      <td>1566.0</td>
      <td>5.263666</td>
      <td>4.537737</td>
      <td>1.4428</td>
      <td>3.827525</td>
      <td>4.79335</td>
      <td>6.089450</td>
      <td>164.1093</td>
    </tr>
    <tr>
      <th>458</th>
      <td>1566.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>459</th>
      <td>1566.0</td>
      <td>2.838380</td>
      <td>1.345576</td>
      <td>0.9910</td>
      <td>2.291175</td>
      <td>2.83035</td>
      <td>3.309225</td>
      <td>47.7772</td>
    </tr>
    <tr>
      <th>460</th>
      <td>1566.0</td>
      <td>29.197414</td>
      <td>13.335189</td>
      <td>7.9534</td>
      <td>20.221850</td>
      <td>26.16785</td>
      <td>35.278800</td>
      <td>149.3851</td>
    </tr>
    <tr>
      <th>461</th>
      <td>1566.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>462</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>463</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>464</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>465</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>466</th>
      <td>1563.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>467</th>
      <td>1563.0</td>
      <td>6.252091</td>
      <td>8.673724</td>
      <td>1.7163</td>
      <td>4.697500</td>
      <td>5.64500</td>
      <td>6.386900</td>
      <td>109.0074</td>
    </tr>
    <tr>
      <th>468</th>
      <td>1560.0</td>
      <td>224.173047</td>
      <td>230.766915</td>
      <td>0.0000</td>
      <td>38.472775</td>
      <td>150.34010</td>
      <td>335.922400</td>
      <td>999.8770</td>
    </tr>
    <tr>
      <th>469</th>
      <td>1561.0</td>
      <td>5.662293</td>
      <td>3.151685</td>
      <td>2.6009</td>
      <td>4.847200</td>
      <td>5.47240</td>
      <td>6.005700</td>
      <td>77.8007</td>
    </tr>
    <tr>
      <th>470</th>
      <td>1561.0</td>
      <td>5.367752</td>
      <td>4.983367</td>
      <td>0.8325</td>
      <td>2.823300</td>
      <td>4.06110</td>
      <td>7.006800</td>
      <td>87.1347</td>
    </tr>
    <tr>
      <th>471</th>
      <td>1561.0</td>
      <td>9.638797</td>
      <td>10.174117</td>
      <td>2.4026</td>
      <td>5.807300</td>
      <td>7.39600</td>
      <td>9.720200</td>
      <td>212.6557</td>
    </tr>
    <tr>
      <th>472</th>
      <td>1560.0</td>
      <td>137.888406</td>
      <td>47.698041</td>
      <td>11.4997</td>
      <td>105.525150</td>
      <td>138.25515</td>
      <td>168.410125</td>
      <td>492.7718</td>
    </tr>
    <tr>
      <th>473</th>
      <td>1560.0</td>
      <td>39.426847</td>
      <td>22.457104</td>
      <td>0.0000</td>
      <td>24.900800</td>
      <td>34.24675</td>
      <td>47.727850</td>
      <td>358.9504</td>
    </tr>
    <tr>
      <th>474</th>
      <td>1560.0</td>
      <td>37.637050</td>
      <td>24.822918</td>
      <td>0.0000</td>
      <td>23.156500</td>
      <td>32.82005</td>
      <td>45.169475</td>
      <td>415.4355</td>
    </tr>
    <tr>
      <th>475</th>
      <td>1561.0</td>
      <td>4.262573</td>
      <td>2.611174</td>
      <td>1.1011</td>
      <td>3.494500</td>
      <td>4.27620</td>
      <td>4.741800</td>
      <td>79.1162</td>
    </tr>
    <tr>
      <th>476</th>
      <td>1561.0</td>
      <td>20.132155</td>
      <td>14.939590</td>
      <td>0.0000</td>
      <td>11.577100</td>
      <td>15.97380</td>
      <td>23.737200</td>
      <td>274.8871</td>
    </tr>
    <tr>
      <th>477</th>
      <td>1561.0</td>
      <td>6.257921</td>
      <td>10.185026</td>
      <td>1.6872</td>
      <td>4.105400</td>
      <td>5.24220</td>
      <td>6.703800</td>
      <td>289.8264</td>
    </tr>
    <tr>
      <th>478</th>
      <td>1561.0</td>
      <td>0.128123</td>
      <td>5.062075</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>200.0000</td>
    </tr>
    <tr>
      <th>479</th>
      <td>1561.0</td>
      <td>3.283394</td>
      <td>2.638608</td>
      <td>0.6459</td>
      <td>2.627700</td>
      <td>3.18450</td>
      <td>3.625300</td>
      <td>63.3336</td>
    </tr>
    <tr>
      <th>480</th>
      <td>1561.0</td>
      <td>75.538131</td>
      <td>35.752493</td>
      <td>8.8406</td>
      <td>52.894500</td>
      <td>70.43450</td>
      <td>93.119600</td>
      <td>221.9747</td>
    </tr>
    <tr>
      <th>481</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>482</th>
      <td>1543.0</td>
      <td>318.418448</td>
      <td>281.011323</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>293.51850</td>
      <td>514.585900</td>
      <td>999.4135</td>
    </tr>
    <tr>
      <th>483</th>
      <td>1543.0</td>
      <td>206.564196</td>
      <td>192.864413</td>
      <td>0.0000</td>
      <td>81.316150</td>
      <td>148.31750</td>
      <td>262.865250</td>
      <td>989.4737</td>
    </tr>
    <tr>
      <th>484</th>
      <td>1543.0</td>
      <td>215.288948</td>
      <td>213.126638</td>
      <td>0.0000</td>
      <td>76.455400</td>
      <td>138.77550</td>
      <td>294.667050</td>
      <td>996.8586</td>
    </tr>
    <tr>
      <th>485</th>
      <td>1543.0</td>
      <td>201.111728</td>
      <td>218.690015</td>
      <td>0.0000</td>
      <td>50.383550</td>
      <td>112.95340</td>
      <td>288.893450</td>
      <td>994.0000</td>
    </tr>
    <tr>
      <th>486</th>
      <td>1543.0</td>
      <td>302.506186</td>
      <td>287.364070</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>249.92700</td>
      <td>501.607450</td>
      <td>999.4911</td>
    </tr>
    <tr>
      <th>487</th>
      <td>1543.0</td>
      <td>239.455326</td>
      <td>263.837645</td>
      <td>0.0000</td>
      <td>55.555150</td>
      <td>112.27550</td>
      <td>397.506100</td>
      <td>995.7447</td>
    </tr>
    <tr>
      <th>488</th>
      <td>1543.0</td>
      <td>352.616477</td>
      <td>252.043751</td>
      <td>0.0000</td>
      <td>139.914350</td>
      <td>348.52940</td>
      <td>510.647150</td>
      <td>997.5186</td>
    </tr>
    <tr>
      <th>489</th>
      <td>1543.0</td>
      <td>272.169707</td>
      <td>228.046702</td>
      <td>0.0000</td>
      <td>112.859250</td>
      <td>219.48720</td>
      <td>377.144200</td>
      <td>994.0035</td>
    </tr>
    <tr>
      <th>490</th>
      <td>1566.0</td>
      <td>51.354045</td>
      <td>18.048612</td>
      <td>13.7225</td>
      <td>38.391100</td>
      <td>48.55745</td>
      <td>61.494725</td>
      <td>142.8436</td>
    </tr>
    <tr>
      <th>491</th>
      <td>1555.0</td>
      <td>2.442673</td>
      <td>1.224283</td>
      <td>0.5558</td>
      <td>1.747100</td>
      <td>2.25080</td>
      <td>2.839800</td>
      <td>12.7698</td>
    </tr>
    <tr>
      <th>492</th>
      <td>226.0</td>
      <td>8.170943</td>
      <td>1.759262</td>
      <td>4.8882</td>
      <td>6.924650</td>
      <td>8.00895</td>
      <td>9.078900</td>
      <td>21.0443</td>
    </tr>
    <tr>
      <th>493</th>
      <td>1567.0</td>
      <td>2.530046</td>
      <td>0.973948</td>
      <td>0.8330</td>
      <td>1.663750</td>
      <td>2.52910</td>
      <td>3.199100</td>
      <td>9.4024</td>
    </tr>
    <tr>
      <th>494</th>
      <td>1567.0</td>
      <td>0.956442</td>
      <td>6.615200</td>
      <td>0.0342</td>
      <td>0.139000</td>
      <td>0.23250</td>
      <td>0.563000</td>
      <td>127.5728</td>
    </tr>
    <tr>
      <th>495</th>
      <td>1567.0</td>
      <td>6.807826</td>
      <td>3.260019</td>
      <td>1.7720</td>
      <td>5.274600</td>
      <td>6.60790</td>
      <td>7.897200</td>
      <td>107.6926</td>
    </tr>
    <tr>
      <th>496</th>
      <td>1516.0</td>
      <td>29.865896</td>
      <td>24.621586</td>
      <td>4.8135</td>
      <td>16.342300</td>
      <td>22.03910</td>
      <td>32.438475</td>
      <td>219.6436</td>
    </tr>
    <tr>
      <th>497</th>
      <td>1516.0</td>
      <td>11.821030</td>
      <td>4.956647</td>
      <td>1.9496</td>
      <td>8.150350</td>
      <td>10.90655</td>
      <td>14.469050</td>
      <td>40.2818</td>
    </tr>
    <tr>
      <th>498</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>499</th>
      <td>1565.0</td>
      <td>263.195864</td>
      <td>324.771342</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>536.204600</td>
      <td>1000.0000</td>
    </tr>
    <tr>
      <th>500</th>
      <td>1565.0</td>
      <td>240.981377</td>
      <td>323.003410</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>505.401000</td>
      <td>999.2337</td>
    </tr>
    <tr>
      <th>501</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>502</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>503</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>504</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>505</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>506</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>507</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>508</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>509</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>510</th>
      <td>1565.0</td>
      <td>55.763508</td>
      <td>37.691736</td>
      <td>0.0000</td>
      <td>35.322200</td>
      <td>46.98610</td>
      <td>64.248700</td>
      <td>451.4851</td>
    </tr>
    <tr>
      <th>511</th>
      <td>1565.0</td>
      <td>275.979457</td>
      <td>329.664680</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>555.294100</td>
      <td>1000.0000</td>
    </tr>
    <tr>
      <th>512</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>513</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>514</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>515</th>
      <td>1561.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>516</th>
      <td>549.0</td>
      <td>0.678898</td>
      <td>10.783880</td>
      <td>0.0287</td>
      <td>0.121500</td>
      <td>0.17470</td>
      <td>0.264900</td>
      <td>252.8604</td>
    </tr>
    <tr>
      <th>517</th>
      <td>549.0</td>
      <td>1.738902</td>
      <td>4.890663</td>
      <td>0.2880</td>
      <td>0.890300</td>
      <td>1.15430</td>
      <td>1.759700</td>
      <td>113.2758</td>
    </tr>
    <tr>
      <th>518</th>
      <td>549.0</td>
      <td>1.806273</td>
      <td>4.715894</td>
      <td>0.4674</td>
      <td>1.171200</td>
      <td>1.58910</td>
      <td>1.932800</td>
      <td>111.3495</td>
    </tr>
    <tr>
      <th>519</th>
      <td>852.0</td>
      <td>11.728440</td>
      <td>15.814420</td>
      <td>0.0000</td>
      <td>4.160300</td>
      <td>5.83295</td>
      <td>10.971850</td>
      <td>184.3488</td>
    </tr>
    <tr>
      <th>520</th>
      <td>1567.0</td>
      <td>2.695999</td>
      <td>5.702366</td>
      <td>0.3121</td>
      <td>1.552150</td>
      <td>2.22100</td>
      <td>2.903700</td>
      <td>111.7365</td>
    </tr>
    <tr>
      <th>521</th>
      <td>1567.0</td>
      <td>11.610080</td>
      <td>103.122996</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>1000.0000</td>
    </tr>
    <tr>
      <th>522</th>
      <td>1567.0</td>
      <td>14.728866</td>
      <td>7.104435</td>
      <td>2.6811</td>
      <td>10.182800</td>
      <td>13.74260</td>
      <td>17.808950</td>
      <td>137.9838</td>
    </tr>
    <tr>
      <th>523</th>
      <td>1567.0</td>
      <td>0.453896</td>
      <td>4.147581</td>
      <td>0.0258</td>
      <td>0.073050</td>
      <td>0.10000</td>
      <td>0.133200</td>
      <td>111.3330</td>
    </tr>
    <tr>
      <th>524</th>
      <td>1567.0</td>
      <td>5.687782</td>
      <td>20.663414</td>
      <td>1.3104</td>
      <td>3.769650</td>
      <td>4.87710</td>
      <td>6.450650</td>
      <td>818.0005</td>
    </tr>
    <tr>
      <th>525</th>
      <td>1543.0</td>
      <td>5.560397</td>
      <td>3.920370</td>
      <td>1.5400</td>
      <td>4.101500</td>
      <td>5.13420</td>
      <td>6.329500</td>
      <td>80.0406</td>
    </tr>
    <tr>
      <th>526</th>
      <td>1567.0</td>
      <td>1.443457</td>
      <td>0.958428</td>
      <td>0.1705</td>
      <td>0.484200</td>
      <td>1.55010</td>
      <td>2.211650</td>
      <td>8.2037</td>
    </tr>
    <tr>
      <th>527</th>
      <td>1567.0</td>
      <td>6.395717</td>
      <td>1.888698</td>
      <td>2.1700</td>
      <td>4.895450</td>
      <td>6.41080</td>
      <td>7.594250</td>
      <td>14.4479</td>
    </tr>
    <tr>
      <th>528</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>529</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>530</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>531</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>532</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>533</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>534</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>535</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>536</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>537</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>538</th>
      <td>1558.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>539</th>
      <td>1559.0</td>
      <td>3.034235</td>
      <td>1.252913</td>
      <td>0.8516</td>
      <td>1.889900</td>
      <td>3.05480</td>
      <td>3.947000</td>
      <td>6.5803</td>
    </tr>
    <tr>
      <th>540</th>
      <td>1559.0</td>
      <td>1.942828</td>
      <td>0.731928</td>
      <td>0.6144</td>
      <td>1.385300</td>
      <td>1.78550</td>
      <td>2.458350</td>
      <td>4.0825</td>
    </tr>
    <tr>
      <th>541</th>
      <td>1559.0</td>
      <td>9.611628</td>
      <td>2.896376</td>
      <td>3.2761</td>
      <td>7.495750</td>
      <td>9.45930</td>
      <td>11.238400</td>
      <td>25.7792</td>
    </tr>
    <tr>
      <th>542</th>
      <td>1565.0</td>
      <td>0.111208</td>
      <td>0.002737</td>
      <td>0.1053</td>
      <td>0.109600</td>
      <td>0.10960</td>
      <td>0.113400</td>
      <td>0.1184</td>
    </tr>
    <tr>
      <th>543</th>
      <td>1565.0</td>
      <td>0.008471</td>
      <td>0.001534</td>
      <td>0.0051</td>
      <td>0.007800</td>
      <td>0.00780</td>
      <td>0.009000</td>
      <td>0.0240</td>
    </tr>
    <tr>
      <th>544</th>
      <td>1565.0</td>
      <td>0.002509</td>
      <td>0.000296</td>
      <td>0.0016</td>
      <td>0.002400</td>
      <td>0.00260</td>
      <td>0.002600</td>
      <td>0.0047</td>
    </tr>
    <tr>
      <th>545</th>
      <td>1565.0</td>
      <td>7.611403</td>
      <td>1.315544</td>
      <td>4.4294</td>
      <td>7.116000</td>
      <td>7.11600</td>
      <td>8.020700</td>
      <td>21.0443</td>
    </tr>
    <tr>
      <th>546</th>
      <td>1307.0</td>
      <td>1.039630</td>
      <td>0.389066</td>
      <td>0.4444</td>
      <td>0.797500</td>
      <td>0.91110</td>
      <td>1.285550</td>
      <td>3.9786</td>
    </tr>
    <tr>
      <th>547</th>
      <td>1307.0</td>
      <td>403.546477</td>
      <td>5.063887</td>
      <td>372.8220</td>
      <td>400.694000</td>
      <td>403.12200</td>
      <td>407.431000</td>
      <td>421.7020</td>
    </tr>
    <tr>
      <th>548</th>
      <td>1307.0</td>
      <td>75.679871</td>
      <td>3.390523</td>
      <td>71.0380</td>
      <td>73.254000</td>
      <td>74.08400</td>
      <td>78.397000</td>
      <td>83.7200</td>
    </tr>
    <tr>
      <th>549</th>
      <td>1307.0</td>
      <td>0.663256</td>
      <td>0.673346</td>
      <td>0.0446</td>
      <td>0.226250</td>
      <td>0.47100</td>
      <td>0.850350</td>
      <td>7.0656</td>
    </tr>
    <tr>
      <th>550</th>
      <td>1307.0</td>
      <td>17.013313</td>
      <td>4.966954</td>
      <td>6.1100</td>
      <td>14.530000</td>
      <td>16.34000</td>
      <td>19.035000</td>
      <td>131.6800</td>
    </tr>
    <tr>
      <th>551</th>
      <td>1307.0</td>
      <td>1.230712</td>
      <td>1.361117</td>
      <td>0.1200</td>
      <td>0.870000</td>
      <td>1.15000</td>
      <td>1.370000</td>
      <td>39.3300</td>
    </tr>
    <tr>
      <th>552</th>
      <td>1307.0</td>
      <td>0.276688</td>
      <td>0.276231</td>
      <td>0.0187</td>
      <td>0.094900</td>
      <td>0.19790</td>
      <td>0.358450</td>
      <td>2.7182</td>
    </tr>
    <tr>
      <th>553</th>
      <td>1307.0</td>
      <td>7.703874</td>
      <td>2.192647</td>
      <td>2.7860</td>
      <td>6.738100</td>
      <td>7.42790</td>
      <td>8.637150</td>
      <td>56.9303</td>
    </tr>
    <tr>
      <th>554</th>
      <td>1307.0</td>
      <td>0.503657</td>
      <td>0.598852</td>
      <td>0.0520</td>
      <td>0.343800</td>
      <td>0.47890</td>
      <td>0.562350</td>
      <td>17.4781</td>
    </tr>
    <tr>
      <th>555</th>
      <td>1307.0</td>
      <td>57.746537</td>
      <td>35.207552</td>
      <td>4.8269</td>
      <td>27.017600</td>
      <td>54.44170</td>
      <td>74.628700</td>
      <td>303.5500</td>
    </tr>
    <tr>
      <th>556</th>
      <td>1307.0</td>
      <td>4.216905</td>
      <td>1.280008</td>
      <td>1.4967</td>
      <td>3.625100</td>
      <td>4.06710</td>
      <td>4.702700</td>
      <td>35.3198</td>
    </tr>
    <tr>
      <th>557</th>
      <td>1307.0</td>
      <td>1.623070</td>
      <td>1.870433</td>
      <td>0.1646</td>
      <td>1.182900</td>
      <td>1.52980</td>
      <td>1.815600</td>
      <td>54.2917</td>
    </tr>
    <tr>
      <th>558</th>
      <td>1566.0</td>
      <td>0.995009</td>
      <td>0.083860</td>
      <td>0.8919</td>
      <td>0.955200</td>
      <td>0.97270</td>
      <td>1.000800</td>
      <td>1.5121</td>
    </tr>
    <tr>
      <th>559</th>
      <td>1566.0</td>
      <td>0.325708</td>
      <td>0.201392</td>
      <td>0.0699</td>
      <td>0.149825</td>
      <td>0.29090</td>
      <td>0.443600</td>
      <td>1.0737</td>
    </tr>
    <tr>
      <th>560</th>
      <td>1566.0</td>
      <td>0.072443</td>
      <td>0.051578</td>
      <td>0.0177</td>
      <td>0.036200</td>
      <td>0.05920</td>
      <td>0.089000</td>
      <td>0.4457</td>
    </tr>
    <tr>
      <th>561</th>
      <td>1566.0</td>
      <td>32.284956</td>
      <td>19.026081</td>
      <td>7.2369</td>
      <td>15.762450</td>
      <td>29.73115</td>
      <td>44.113400</td>
      <td>101.1146</td>
    </tr>
    <tr>
      <th>562</th>
      <td>1294.0</td>
      <td>262.729683</td>
      <td>7.630585</td>
      <td>242.2860</td>
      <td>259.972500</td>
      <td>264.27200</td>
      <td>265.707000</td>
      <td>311.4040</td>
    </tr>
    <tr>
      <th>563</th>
      <td>1294.0</td>
      <td>0.679641</td>
      <td>0.121758</td>
      <td>0.3049</td>
      <td>0.567100</td>
      <td>0.65100</td>
      <td>0.768875</td>
      <td>1.2988</td>
    </tr>
    <tr>
      <th>564</th>
      <td>1294.0</td>
      <td>6.444985</td>
      <td>2.633583</td>
      <td>0.9700</td>
      <td>4.980000</td>
      <td>5.16000</td>
      <td>7.800000</td>
      <td>32.5800</td>
    </tr>
    <tr>
      <th>565</th>
      <td>1294.0</td>
      <td>0.145610</td>
      <td>0.081122</td>
      <td>0.0224</td>
      <td>0.087700</td>
      <td>0.11955</td>
      <td>0.186150</td>
      <td>0.6892</td>
    </tr>
    <tr>
      <th>566</th>
      <td>1294.0</td>
      <td>2.610870</td>
      <td>1.032761</td>
      <td>0.4122</td>
      <td>2.090200</td>
      <td>2.15045</td>
      <td>3.098725</td>
      <td>14.0141</td>
    </tr>
    <tr>
      <th>567</th>
      <td>1294.0</td>
      <td>0.060086</td>
      <td>0.032761</td>
      <td>0.0091</td>
      <td>0.038200</td>
      <td>0.04865</td>
      <td>0.075275</td>
      <td>0.2932</td>
    </tr>
    <tr>
      <th>568</th>
      <td>1294.0</td>
      <td>2.452417</td>
      <td>0.996644</td>
      <td>0.3706</td>
      <td>1.884400</td>
      <td>1.99970</td>
      <td>2.970850</td>
      <td>12.7462</td>
    </tr>
    <tr>
      <th>569</th>
      <td>1294.0</td>
      <td>21.117674</td>
      <td>10.213294</td>
      <td>3.2504</td>
      <td>15.466200</td>
      <td>16.98835</td>
      <td>24.772175</td>
      <td>84.8024</td>
    </tr>
    <tr>
      <th>570</th>
      <td>1567.0</td>
      <td>530.523623</td>
      <td>17.499736</td>
      <td>317.1964</td>
      <td>530.702700</td>
      <td>532.39820</td>
      <td>534.356400</td>
      <td>589.5082</td>
    </tr>
    <tr>
      <th>571</th>
      <td>1567.0</td>
      <td>2.101836</td>
      <td>0.275112</td>
      <td>0.9802</td>
      <td>1.982900</td>
      <td>2.11860</td>
      <td>2.290650</td>
      <td>2.7395</td>
    </tr>
    <tr>
      <th>572</th>
      <td>1567.0</td>
      <td>28.450165</td>
      <td>86.304681</td>
      <td>3.5400</td>
      <td>7.500000</td>
      <td>8.65000</td>
      <td>10.130000</td>
      <td>454.5600</td>
    </tr>
    <tr>
      <th>573</th>
      <td>1567.0</td>
      <td>0.345636</td>
      <td>0.248478</td>
      <td>0.0667</td>
      <td>0.242250</td>
      <td>0.29340</td>
      <td>0.366900</td>
      <td>2.1967</td>
    </tr>
    <tr>
      <th>574</th>
      <td>1567.0</td>
      <td>9.162315</td>
      <td>26.920150</td>
      <td>1.0395</td>
      <td>2.567850</td>
      <td>2.97580</td>
      <td>3.492500</td>
      <td>170.0204</td>
    </tr>
    <tr>
      <th>575</th>
      <td>1567.0</td>
      <td>0.104729</td>
      <td>0.067791</td>
      <td>0.0230</td>
      <td>0.075100</td>
      <td>0.08950</td>
      <td>0.112150</td>
      <td>0.5502</td>
    </tr>
    <tr>
      <th>576</th>
      <td>1567.0</td>
      <td>5.563747</td>
      <td>16.921369</td>
      <td>0.6636</td>
      <td>1.408450</td>
      <td>1.62450</td>
      <td>1.902000</td>
      <td>90.4235</td>
    </tr>
    <tr>
      <th>577</th>
      <td>1567.0</td>
      <td>16.642363</td>
      <td>12.485267</td>
      <td>4.5820</td>
      <td>11.501550</td>
      <td>13.81790</td>
      <td>17.080900</td>
      <td>96.9601</td>
    </tr>
    <tr>
      <th>578</th>
      <td>618.0</td>
      <td>0.021615</td>
      <td>0.011730</td>
      <td>-0.0169</td>
      <td>0.013800</td>
      <td>0.02040</td>
      <td>0.027700</td>
      <td>0.1028</td>
    </tr>
    <tr>
      <th>579</th>
      <td>618.0</td>
      <td>0.016829</td>
      <td>0.009640</td>
      <td>0.0032</td>
      <td>0.010600</td>
      <td>0.01480</td>
      <td>0.020000</td>
      <td>0.0799</td>
    </tr>
    <tr>
      <th>580</th>
      <td>618.0</td>
      <td>0.005396</td>
      <td>0.003116</td>
      <td>0.0010</td>
      <td>0.003400</td>
      <td>0.00470</td>
      <td>0.006475</td>
      <td>0.0286</td>
    </tr>
    <tr>
      <th>581</th>
      <td>618.0</td>
      <td>97.934373</td>
      <td>87.520966</td>
      <td>0.0000</td>
      <td>46.184900</td>
      <td>72.28890</td>
      <td>116.539150</td>
      <td>737.3048</td>
    </tr>
    <tr>
      <th>582</th>
      <td>1566.0</td>
      <td>0.500096</td>
      <td>0.003404</td>
      <td>0.4778</td>
      <td>0.497900</td>
      <td>0.50020</td>
      <td>0.502375</td>
      <td>0.5098</td>
    </tr>
    <tr>
      <th>583</th>
      <td>1566.0</td>
      <td>0.015318</td>
      <td>0.017180</td>
      <td>0.0060</td>
      <td>0.011600</td>
      <td>0.01380</td>
      <td>0.016500</td>
      <td>0.4766</td>
    </tr>
    <tr>
      <th>584</th>
      <td>1566.0</td>
      <td>0.003847</td>
      <td>0.003720</td>
      <td>0.0017</td>
      <td>0.003100</td>
      <td>0.00360</td>
      <td>0.004100</td>
      <td>0.1045</td>
    </tr>
    <tr>
      <th>585</th>
      <td>1566.0</td>
      <td>3.067826</td>
      <td>3.578033</td>
      <td>1.1975</td>
      <td>2.306500</td>
      <td>2.75765</td>
      <td>3.295175</td>
      <td>99.3032</td>
    </tr>
    <tr>
      <th>586</th>
      <td>1566.0</td>
      <td>0.021458</td>
      <td>0.012358</td>
      <td>-0.0169</td>
      <td>0.013425</td>
      <td>0.02050</td>
      <td>0.027600</td>
      <td>0.1028</td>
    </tr>
    <tr>
      <th>587</th>
      <td>1566.0</td>
      <td>0.016475</td>
      <td>0.008808</td>
      <td>0.0032</td>
      <td>0.010600</td>
      <td>0.01480</td>
      <td>0.020300</td>
      <td>0.0799</td>
    </tr>
    <tr>
      <th>588</th>
      <td>1566.0</td>
      <td>0.005283</td>
      <td>0.002867</td>
      <td>0.0010</td>
      <td>0.003300</td>
      <td>0.00460</td>
      <td>0.006400</td>
      <td>0.0286</td>
    </tr>
    <tr>
      <th>589</th>
      <td>1566.0</td>
      <td>99.670066</td>
      <td>93.891919</td>
      <td>0.0000</td>
      <td>44.368600</td>
      <td>71.90050</td>
      <td>114.749700</td>
      <td>737.3048</td>
    </tr>
    <tr>
      <th>Pass/Fail</th>
      <td>1567.0</td>
      <td>-0.867262</td>
      <td>0.498010</td>
      <td>-1.0000</td>
      <td>-1.000000</td>
      <td>-1.00000</td>
      <td>-1.000000</td>
      <td>1.0000</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Observations:
    1.591 Featues having continous values 
    2.Few of features (( 5,13,42,49,etc...)having standard deviation as 0 which implies they have single value 
    3.Featue pass/Fail has only 2 values( 1, -1)</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="2.-Data-cleansing:">2. Data cleansing:<a class="anchor-link" href="#2.-Data-cleansing:">&#182;</a></h1>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>2.A. Write a for loop which will remove all the features with 20%+ Null values and impute rest with mean of the feature.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[7]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Function to remove Null value columns above threshold will be removed. </span>
<span class="c1">#Also remaining Null values will be imputed as median</span>
<span class="k">def</span> <span class="nf">null_feature_remove</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">threshold</span><span class="p">):</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">cols</span><span class="p">:</span>
        <span class="n">col_data</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span>
        <span class="n">missing_data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">100.00</span>
        <span class="k">if</span> <span class="p">(</span> <span class="n">missing_data</span> <span class="o">&gt;</span> <span class="n">threshold</span> <span class="p">):</span>
            <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">missing_data</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">col_median</span> <span class="o">=</span> <span class="n">col_data</span><span class="o">.</span><span class="n">median</span><span class="p">()</span>
            <span class="n">col_data</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">col_median</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">col_data</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[8]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">=</span><span class="n">null_feature_remove</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[9]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[9]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>(1567, 560)</pre>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>32 Features removed due to more than 20% Null</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>2.B. Identify and drop the features which are having same value for all the rows</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[10]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">low_card_cols_1</span> <span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[11]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">low_card_cols_1</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>[&#39;5&#39;, &#39;13&#39;, &#39;42&#39;, &#39;49&#39;, &#39;52&#39;, &#39;69&#39;, &#39;97&#39;, &#39;141&#39;, &#39;149&#39;, &#39;178&#39;, &#39;179&#39;, &#39;186&#39;, &#39;189&#39;, &#39;190&#39;, &#39;191&#39;, &#39;192&#39;, &#39;193&#39;, &#39;194&#39;, &#39;226&#39;, &#39;229&#39;, &#39;230&#39;, &#39;231&#39;, &#39;232&#39;, &#39;233&#39;, &#39;234&#39;, &#39;235&#39;, &#39;236&#39;, &#39;237&#39;, &#39;240&#39;, &#39;241&#39;, &#39;242&#39;, &#39;243&#39;, &#39;256&#39;, &#39;257&#39;, &#39;258&#39;, &#39;259&#39;, &#39;260&#39;, &#39;261&#39;, &#39;262&#39;, &#39;263&#39;, &#39;264&#39;, &#39;265&#39;, &#39;266&#39;, &#39;276&#39;, &#39;284&#39;, &#39;313&#39;, &#39;314&#39;, &#39;315&#39;, &#39;322&#39;, &#39;325&#39;, &#39;326&#39;, &#39;327&#39;, &#39;328&#39;, &#39;329&#39;, &#39;330&#39;, &#39;364&#39;, &#39;369&#39;, &#39;370&#39;, &#39;371&#39;, &#39;372&#39;, &#39;373&#39;, &#39;374&#39;, &#39;375&#39;, &#39;378&#39;, &#39;379&#39;, &#39;380&#39;, &#39;381&#39;, &#39;394&#39;, &#39;395&#39;, &#39;396&#39;, &#39;397&#39;, &#39;398&#39;, &#39;399&#39;, &#39;400&#39;, &#39;401&#39;, &#39;402&#39;, &#39;403&#39;, &#39;404&#39;, &#39;414&#39;, &#39;422&#39;, &#39;449&#39;, &#39;450&#39;, &#39;451&#39;, &#39;458&#39;, &#39;461&#39;, &#39;462&#39;, &#39;463&#39;, &#39;464&#39;, &#39;465&#39;, &#39;466&#39;, &#39;481&#39;, &#39;498&#39;, &#39;501&#39;, &#39;502&#39;, &#39;503&#39;, &#39;504&#39;, &#39;505&#39;, &#39;506&#39;, &#39;507&#39;, &#39;508&#39;, &#39;509&#39;, &#39;512&#39;, &#39;513&#39;, &#39;514&#39;, &#39;515&#39;, &#39;528&#39;, &#39;529&#39;, &#39;530&#39;, &#39;531&#39;, &#39;532&#39;, &#39;533&#39;, &#39;534&#39;, &#39;535&#39;, &#39;536&#39;, &#39;537&#39;, &#39;538&#39;]
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[12]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span> <span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">low_card_cols_1</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[13]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[13]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>(1567, 444)</pre>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>2.c.Drop other features if required using relevant functional knowledge. Clearly justify the same.</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Time feature has unique values for each row. So it won't create any impact on output</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[14]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[15]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[15]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>(1567, 443)</pre>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>2.D. Check for multi-collinearity in the data and take necessary action.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[16]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">remove_collinear_features</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="n">threshold</span> <span class="o">=</span><span class="mf">0.70</span>
    <span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
    <span class="n">iters</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">drop_cols</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">iters</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">item</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">j</span><span class="p">:(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">):(</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">)]</span>
            <span class="n">col</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">columns</span>
            <span class="n">row</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">index</span>
            <span class="n">val</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
                <span class="c1"># If correlation exceeds the threshold</span>
            <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">:</span>
                <span class="c1"># Print the correlated features and the correlation value</span>
                <span class="c1">#print(col.values[0], &quot;|&quot;, row.values[0], &quot;|&quot;, round(val[0][0], 2))</span>
                <span class="n">drop_cols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">col</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">drops</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">drop_cols</span><span class="p">)</span>
    <span class="n">features1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">drops</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">features1</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[17]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">=</span><span class="n">remove_collinear_features</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="mf">.70</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[18]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[18]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>(1567, 202)</pre>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>2.E. Make all relevant modifications on the data using both functional/logical reasoning/assumptions.</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>1.Features having unique value across rows removed.Becuase it has 0 Std which will not give impact on Algorithm
2.Time feature removed becuase it won't impact target prediction
3.Features which have multi-collinear removed</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="3.-Data-analysis-&amp;-visualisation">3. Data analysis &amp; visualisation<a class="anchor-link" href="#3.-Data-analysis-&amp;-visualisation">&#182;</a></h1>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>3A. Perform a detailed univariate Analysis with appropriate detailed comments after each analysis.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[19]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[19]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>&lt;AxesSubplot:ylabel=&#39;Density&#39;&gt;</pre>
</div>

</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>




<div class="jp-RenderedImage jp-OutputArea-output ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Observation: 
    Data is not Normal Curve.
    It is Positve Skewd 
    Data has lot of outliers</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[20]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">unique_vals</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Pass/Fail&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span> 
<span class="n">unique_vals</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[20]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>array([-1,  1])</pre>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[21]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Pass/Fail&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">val</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">unique_vals</span><span class="p">]</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[22]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">targets</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[22]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>[            0         1          2          3          4         6       7  \
 0     3030.93  2564.000  2187.7333  1411.1265     1.3602   97.6133  0.1242   
 1     3095.78  2465.140  2230.4222  1463.6606     0.8294  102.3433  0.1247   
 3     2988.72  2479.900  2199.0333   909.7926     1.3204  104.2367  0.1217   
 4     3032.24  2502.870  2233.3667  1326.5200     1.5334  100.3967  0.1235   
 5     2946.25  2432.840  2233.3667  1326.5200     1.5334  100.3967  0.1235   
 6     3030.27  2430.120  2230.4222  1463.6606     0.8294  102.3433  0.1247   
 7     3058.88  2690.150  2248.9000  1004.4692     0.7884  106.2400  0.1185   
 8     2967.68  2600.470  2248.9000  1004.4692     0.7884  106.2400  0.1185   
 9     3016.11  2428.370  2248.9000  1004.4692     0.7884  106.2400  0.1185   
 12    2920.07  2507.400  2195.1222  1046.1468     1.3204  103.3400  0.1223   
 13    3051.44  2529.270  2184.4333   877.6266     1.4668  107.8711  0.1240   
 15    2988.31  2546.260  2224.6222   947.7739     1.2924  104.8489  0.1197   
 16    3028.02  2560.870  2270.2556  1258.4558     1.3950  104.8078  0.1207   
 17    3032.73  2517.790  2270.2556  1258.4558     1.3950  104.8078  0.1207   
 18    3040.34  2501.160  2207.3889   962.5317     1.2043  104.0311  0.1210   
 19    2988.30  2519.050  2208.8556  1157.7224     1.5509  107.8022  0.1233   
 20    2987.32  2528.810  2201.0667  1285.2144     1.3168  101.5122  0.1195   
 21    3011.49  2481.850  2207.3889   962.5317     1.2043  104.0311  0.1210   
 22    3002.27  2497.450  2207.3889   962.5317     1.2043  104.0311  0.1210   
 24    3010.41  2632.800  2203.9000  1116.4129     1.2639  102.2733  0.1199   
 25    2979.74  2446.560  2257.1667  1437.9565     1.4918  106.3400  0.1203   
 26    3067.35  2456.330  2257.1667  1437.9565     1.4918  106.3400  0.1203   
 27    2988.99  2607.630  2223.0333  1533.9934     1.3548  109.7067  0.1211   
 28    2972.78  2431.570  2190.4889  1059.4390     0.8614  102.1178  0.1216   
 29    2981.85  2529.110  2180.3778  1208.7411     1.2998  100.2789  0.1209   
 30    2975.88  2489.700  2191.6667  1153.9011     1.2569  100.6767  0.1210   
 31    3058.61  2492.360  2180.3778  1208.7411     1.2998  100.2789  0.1209   
 32    3047.19  2524.180  2197.3111   969.8910     1.3015  105.3911  0.1201   
 33    2981.31  2566.350  2197.3111   969.8910     1.3015  105.3911  0.1201   
 34    2963.83  2457.640  2225.1777  1457.7934     1.2414  110.2789  0.1211   
 35    3040.72  2477.350  2191.6667  1153.9011     1.2569  100.6767  0.1210   
 36    2989.47  2445.440  2223.1667  1522.5535     1.1981  110.6333  0.1210   
 37    2981.54  2302.460  2221.8445  1416.8211     1.1758  111.6278  0.1210   
 39    3017.21  2530.670  2169.4667  1185.4449     1.2412  100.8444  0.1221   
 41    3017.10  2517.180  2180.3778  1208.7411     1.2998  100.2789  0.1209   
 42    3021.26  2503.460  2180.3778  1208.7411     1.2998  100.2789  0.1209   
 43    3000.36  2748.670  2174.8666  1039.2291     1.0455  103.6000  0.1221   
 44    3047.78  2490.710  2166.5222   907.0746     1.0647  104.5211  0.1221   
 46    3076.38  2475.530  2166.5222   907.0746     1.0647  104.5211  0.1221   
 47    3055.97  2788.400  2166.5222   907.0746     1.0647  104.5211  0.1221   
 51    2993.14  2520.210  2180.6778  1230.6762     1.4095  103.3778  0.1205   
 52    2938.41  2466.780  2166.5222   907.0746     1.0647  104.5211  0.1221   
 53    3058.98  2484.310  2172.9778  1222.6067     1.3658  101.8400  0.1220   
 54    2950.46  2398.440  2180.6778  1230.6762     1.4095  103.3778  0.1205   
 55    3060.03  2809.790  2166.5222   907.0746     1.0647  104.5211  0.1221   
 56    2983.09  2569.810  2206.5112  1244.1552     1.2691  101.6667  0.1229   
 59    3020.76  2481.280  2197.3111   969.8910     1.3015  105.3911  0.1201   
 60    3042.08  2694.310  2206.5112  1244.1552     1.2691  101.6667  0.1229   
 61    3000.56  2541.920  2220.5445  1192.8757     1.3872  106.2567  0.1218   
 63    3016.64  2492.800  2246.4889  1006.9548     1.0997  103.3222  0.1184   
 65    2847.81  2461.380  2202.7111  1010.4454     1.0032  104.3067  0.1225   
 66    3011.49  2544.520  2202.7111  1010.4454     1.0032  104.3067  0.1225   
 67    2975.64  2508.280  2202.7111  1010.4454     1.0032  104.3067  0.1225   
 68    3066.19  2702.640  2246.4889  1006.9548     1.0997  103.3222  0.1184   
 69    2873.35  2565.800  2220.5445  1192.8757     1.3872  106.2567  0.1218   
 70    3045.03  2315.760  2246.4889  1006.9548     1.0997  103.3222  0.1184   
 71    3090.18  2375.490  2191.6667  1107.4330     1.3529  103.4233  0.1206   
 72    2955.85  2623.100  2191.6667  1107.4330     1.3529  103.4233  0.1206   
 73    3006.23  2424.370  2220.5445  1192.8757     1.3872  106.2567  0.1218   
 74    2958.05  2450.250  2199.3334  1017.9399     1.2414  100.5544  0.1214   
 75    2999.72  2492.290  2191.6667  1107.4330     1.3529  103.4233  0.1206   
 76    2918.90  2500.410  2183.4333  1582.5646     1.3601   99.0267  0.1240   
 77    2926.07  2505.730  2191.6667  1107.4330     1.3529  103.4233  0.1206   
 78    3032.89  2500.810  2183.4333  1582.5646     1.3601   99.0267  0.1240   
 79    2865.31  2531.750  2246.4889  1006.9548     1.0997  103.3222  0.1184   
 80    2855.80  2537.350  2183.4333  1582.5646     1.3601   99.0267  0.1240   
 81    2930.22  2417.850  2246.4889  1006.9548     1.0997  103.3222  0.1184   
 83    2940.53  2459.870  2202.7111  1010.4454     1.0032  104.3067  0.1225   
 84    2916.41  2503.590  2246.4889  1006.9548     1.0997  103.3222  0.1184   
 85    3047.67  2565.030  2199.3334  1017.9399     1.2414  100.5544  0.1214   
 86    3082.45  2739.190  2199.3334  1017.9399     1.2414  100.5544  0.1214   
 87    3088.82  2655.370  2183.4556   955.9073     1.1048  102.6978  0.1223   
 88    3090.42  2531.980  2265.1889  1740.3297     1.4715  100.3889  0.1222   
 89    3031.90  2414.830  2191.9000  1936.7653     1.5291   98.1244  0.1254   
 90    3029.16  2519.680  2265.1889  1740.3297     1.4715  100.3889  0.1222   
 91    3040.63  2547.430  2174.8666  1039.2291     1.0455  103.6000  0.1221   
 92    3039.93  2447.890  2246.4889  1006.9548     1.0997  103.3222  0.1184   
 93    2922.94  2478.550  2193.1555  1172.6226     1.2752  101.1111  0.1209   
 94    3046.33  2545.340  2172.3666  1752.3206     1.3622   97.2633  0.1269   
 95    3081.07  2560.990  2180.1556  1822.5073     1.2579   98.1289  0.1261   
 97    3049.31  2453.820  2265.1889  1740.3297     1.4715  100.3889  0.1222   
 98    3011.49  2537.900  2183.4556   955.9073     1.1048  102.6978  0.1223   
 99    2913.15  2544.520  2232.5889  1717.2750     1.6700  104.1067  0.1223   
 100   3080.26  2526.820  2265.1889  1740.3297     1.4715  100.3889  0.1222   
 101   3070.50  2439.130  2209.9222  1693.5732     1.3918   99.0811  0.1250   
 102   2960.18  2459.620  2213.7667  1679.3498     1.0685   98.1978  0.1235   
 103   2960.65  2436.170  2265.1889  1740.3297     1.4715  100.3889  0.1222   
 104   3000.46  2523.080  2191.9000  1936.7653     1.5291   98.1244  0.1254   
 105   2946.86  2474.050  2220.7111  1064.8446     0.9747  103.0567  0.1193   
 106   2831.91  2433.960  2171.9000  1811.8799     1.3811   99.2200  0.1276   
 107   2872.40  2607.750  2172.3666  1752.3206     1.3622   97.2633  0.1269   
 108   3054.78  2479.170  2171.9000  1811.8799     1.3811   99.2200  0.1276   
 109   3086.88  2395.850  2171.9000  1811.8799     1.3811   99.2200  0.1276   
 110   2913.42  2437.690  2220.7111  1064.8446     0.9747  103.0567  0.1193   
 111   3039.57  2583.780  2197.8445  1907.8909     1.1799   99.6667  0.1247   
 112   2958.13  2485.570  2197.8667  1888.0388     1.1988  100.2811  0.1247   
 113   2928.16  2523.210  2210.6111  1184.6481     1.2577  102.9356  0.1201   
 114   2951.20  2418.830  2181.1555   988.7660     0.9454  101.3611  0.1215   
 116   3046.34  2307.240  2178.1444  1074.5145     1.2260  101.0956  0.1218   
 117   3011.49  2810.120  2232.5889  1717.2750     1.6700  104.1067  0.1223   
 118   2985.11  2496.730  2220.7111  1064.8446     0.9747  103.0567  0.1193   
 119   2943.58  2512.490  2171.9000  1811.8799     1.3811   99.2200  0.1276   
 120   2992.89  2481.430  2183.4556   955.9073     1.1048  102.6978  0.1223   
 121   2962.27  2495.350  2171.9000  1811.8799     1.3811   99.2200  0.1276   
 122   3043.67  2518.210  2222.3222  1678.9669     1.5099   99.6944  0.1248   
 123   3024.46  2468.330  2222.3333  1720.9033     1.3150  100.3989  0.1231   
 124   3032.81  2534.740  2239.4223  1997.3782     1.5397   98.3356  0.1229   
 125   3020.12  2416.730  2196.6555  1066.1908     1.2188  101.8900  0.1211   
 126   2955.30  2460.080  2191.9000  1936.7653     1.5291   98.1244  0.1254   
 127   3100.44  2690.990  2239.4223  1997.3782     1.5397   98.3356  0.1229   
 128   2976.48  2502.960  2232.5889  1717.2750     1.6700  104.1067  0.1223   
 129   3056.50  2481.090  2220.4444  1637.2946     1.3680  100.5189  0.1231   
 130   3017.50  2634.300  2183.4556   955.9073     1.1048  102.6978  0.1223   
 132   2924.23  2531.340  2196.6555  1066.1908     1.2188  101.8900  0.1211   
 133   3023.86  2432.850  2172.3666  1752.3206     1.3622   97.2633  0.1269   
 134   3020.46  2477.860  2242.2444  1695.7049     1.4215   99.9944  0.1215   
 135   3035.71  2567.290  2222.3222  1678.9669     1.5099   99.6944  0.1248   
 136   2917.86  2490.860  2178.1444  1074.5145     1.2260  101.0956  0.1218   
 137   3044.46  2497.580  2178.1444  1074.5145     1.2260  101.0956  0.1218   
 138   2959.65  2576.840  2171.9000  1811.8799     1.3811   99.2200  0.1276   
 139   2936.30  2473.270  2181.1555   988.7660     0.9454  101.3611  0.1215   
 140   3001.41  2581.070  2224.4778   994.9032     1.2834  101.8267  0.1191   
 141   3072.73  2587.600  2224.4778   994.9032     1.2834  101.8267  0.1191   
 142   2989.63  2364.730  2196.6555  1066.1908     1.2188  101.8900  0.1211   
 143   3027.10  2374.500  2222.3222  1678.9669     1.5099   99.6944  0.1248   
 144   2950.00  2539.370  2196.6555  1066.1908     1.2188  101.8900  0.1211   
 145   3095.50  2394.580  2196.6555  1066.1908     1.2188  101.8900  0.1211   
 146   3049.19  2406.220  2222.3222  1678.9669     1.5099   99.6944  0.1248   
 147   3050.47  2515.300  2242.2444  1695.7049     1.4215   99.9944  0.1215   
 148   2957.73  2581.930  2224.4778   994.9032     1.2834  101.8267  0.1191   
 149   2963.35  2537.400  2222.0000  1630.1128     1.3555  100.0144  0.1231   
 150   2983.27  2522.160  2219.8222  1032.5542     1.2795  102.3356  0.1195   
 151   3095.06  2494.550  2180.1556  1822.5073     1.2579   98.1289  0.1261   
 152   2999.18  2560.670  2222.3222  1678.9669     1.5099   99.6944  0.1248   
 153   2939.56  2557.940  2242.2444  1695.7049     1.4215   99.9944  0.1215   
 155   3052.82  2525.330  2186.2667  1088.7359     1.2723  103.5633  0.1212   
 156   2914.47  2492.140  2181.6111  1083.3937     1.2484  104.1478  0.1216   
 159   3045.70  2456.170  2184.8778   960.8486     1.0160  102.5333  0.1214   
 160   3057.03  2468.410  2184.8778   960.8486     1.0160  102.5333  0.1214   
 161   2969.52  2546.750  2221.9444  1551.6947     1.5296   99.2678  0.1222   
 162   2892.37  2517.090  2184.8778   960.8486     1.0160  102.5333  0.1214   
 163   3026.77  2474.320  2181.6111  1083.3937     1.2484  104.1478  0.1216   
 164   2999.01  2447.290  2217.7556  1349.3451     1.2793  105.3100  0.1237   
 165   3072.75  2581.180  2221.9444  1551.6947     1.5296   99.2678  0.1222   
 166   3090.97  2469.600  2218.7889  1622.3514     1.2043   98.2667  0.1224   
 168   2964.85  2493.320  2218.7889  1622.3514     1.2043   98.2667  0.1224   
 170   2908.06  2465.070  2221.9444  1551.6947     1.5296   99.2678  0.1222   
 171   2949.65  2335.060  2184.8778   960.8486     1.0160  102.5333  0.1214   
 172   2954.44  2576.940  2221.9444  1551.6947     1.5296   99.2678  0.1222   
 173   3018.00  2320.050  2248.4222  1418.5634     1.4905  102.6444  0.1212   
 174   2929.20  2486.590  2234.6111  1590.1699     1.2049   98.7567  0.1208   
 175   3071.09  2521.230  2248.4222  1418.5634     1.4905  102.6444  0.1212   
 176   3033.95  2505.400  2248.4222  1418.5634     1.4905  102.6444  0.1212   
 177   3075.87  2501.930  2248.4222  1418.5634     1.4905  102.6444  0.1212   
 178   2846.07  2514.660  2234.6111  1590.1699     1.2049   98.7567  0.1208   
 179   2916.54  2480.640  2256.1222  1226.2217     1.4656  106.3122  0.1209   
 181   2997.07  2543.110  2256.1222  1226.2217     1.4656  106.3122  0.1209   
 183   3028.65  2455.140  2228.0555  1816.9286     1.2244  100.2256  0.1231   
 184   2943.34  2485.000  2234.6111  1590.1699     1.2049   98.7567  0.1208   
 185   3022.71  2511.090  2220.7111  1064.8446     0.9747  103.0567  0.1193   
 187   3035.20  2662.190  2221.9444  1551.6947     1.5296   99.2678  0.1222   
 190   3024.74  2553.600  2171.8111  1007.2396     1.4598  104.7444  0.1252   
 191   3109.01  2487.160  2171.8111  1007.2396     1.4598  104.7444  0.1252   
 192   2956.64  2436.120  2139.2667  1123.3450     1.3699  102.2522  0.1254   
 193   3024.15  2521.240  2191.7889  1133.3967     1.3808  106.6711  0.1237   
 194   2997.41  2401.530  2139.2667  1123.3450     1.3699  102.2522  0.1254   
 195   3030.17  2559.120  2220.7111  1064.8446     0.9747  103.0567  0.1193   
 196   2967.40  2553.040  2304.2111  1857.8658     1.7719   96.9967  0.1183   
 197   3067.34  2496.560  2177.1667   969.6185     1.0125  104.9322  0.1243   
 198   3074.63  2538.750  2223.4000  1104.9568     1.2479  105.7289  0.1219   
 199   2992.92  2706.540  2146.8555   917.5352     1.0638  106.8833  0.1260   
 200   2896.63  2550.510  2165.6222   953.0040     1.3366  105.9356  0.1250   
 201   3054.13  2511.140  2192.6889  1130.9910     1.3923  102.1067  0.1220   
 202   3031.39  2632.880  2280.8222  1125.7334     0.6815  101.9111  0.1221   
 203   2981.95  2279.480  2205.2889  1630.3112     1.2733   98.8056  0.1218   
 204   2977.98  2384.660  2212.7111  1062.6288     1.3848  101.9300  0.1212   
 205   3072.35  2551.840  2212.7111  1062.6288     1.3848  101.9300  0.1212   
 206   2998.59  2559.070  2184.8889   982.8147     1.0124  103.1656  0.1245   
 207   2855.96  2539.780  2187.4444   980.2436     1.0997  104.1300  0.1233   
 208   2962.13  2540.950  2211.3889  1763.4015     1.6569  101.9133  0.1212   
 209   3051.32  2464.390  2205.2889  1630.3112     1.2733   98.8056  0.1218   
 210   3104.40  2506.270  2205.4667  1035.6241     1.1295  105.5600  0.1216   
 211   2892.99  2332.390  2216.9556   907.1863     1.4472  105.0367  0.1212   
 212   2995.76  2526.220  2230.0333  1668.6804     1.5739   99.0522  0.1204   
 213   3034.34  2631.470  2179.0445  2028.2208     1.5552   95.4256  0.1234   
 214   3075.76  2491.550  2185.9333  1659.6962     1.6290   98.6822  0.1227   
 215   3043.05  2466.360  2205.2889  1630.3112     1.2733   98.8056  0.1218   
 216   2940.50  2441.610  2205.2889  1630.3112     1.2733   98.8056  0.1218   
 217   3065.36  2162.870  2211.3889  1763.4015     1.6569  101.9133  0.1212   
 219   2912.76  2480.540  2233.7666  1772.4931     0.9501  100.7256  0.1215   
 220   3091.71  2548.950  2233.7666  1772.4931     0.9501  100.7256  0.1215   
 221   2907.96  2439.310  2225.5889  1890.2199     1.6000   97.2756  0.1202   
 223   2971.15  2474.950  2185.9333  1659.6962     1.6290   98.6822  0.1227   
 224   3076.52  2502.620  2197.6444  1247.0334     0.7865   99.9211  0.1203   
 225   2999.86  2287.900  2197.6444  1247.0334     0.7865   99.9211  0.1203   
 226   2992.52  2470.140  2197.6444  1247.0334     0.7865   99.9211  0.1203   
 227   3109.02  2472.130  2197.6444  1247.0334     0.7865   99.9211  0.1203   
 228   2954.10  2489.910  2179.0445  2028.2208     1.5552   95.4256  0.1234   
 229   3006.22  2257.810  2185.9333  1659.6962     1.6290   98.6822  0.1227   
 230   3088.73  2378.950  2185.9333  1659.6962     1.6290   98.6822  0.1227   
 232   2948.42  2444.550  2204.9223  1787.6757     1.5138  100.4322  0.1216   
 233   2958.39  2478.990  2231.9556  1185.0959     1.0208  102.5344  0.1192   
 234   3025.01  2604.260  2177.3222  1089.3655     1.3101  101.1478  0.1216   
 237   2908.04  2494.370  2185.9333  1659.6962     1.6290   98.6822  0.1227   
 239   3061.98  2344.910  2197.2667   911.2054     1.3257  104.1722  0.1208   
 242   2959.20  2491.600  2181.1555  1010.6897     1.3711  103.1456  0.1215   
 245   2981.08  2616.630  2177.3222  1089.3655     1.3101  101.1478  0.1224   
 246   2985.06  2509.800  2177.3222  1089.3655     1.3101  101.1478  0.1216   
 247   3043.17  2483.740  2195.7666  1015.3046     1.3663  101.4600  0.1207   
 248   3056.95  2549.730  2195.7666  1015.3046     1.3663  101.4600  0.1207   
 249   3050.29  2402.570  2185.8111   995.3928     1.3714  102.6422  0.1218   
 250   3079.90  2463.510  2178.7333  1039.3641     0.7367  101.4922  0.1219   
 251   3016.16  2382.030  2185.8111   995.3928     1.3714  102.6422  0.1218   
 252   2972.78  2513.680  2197.2667   911.2054     1.3257  104.1722  0.1208   
 253   2872.01  2570.130  2244.1111  1676.7316     0.9197  100.8067  0.1204   
 254   3008.98  2523.550  2197.6444  1247.0334     0.7865   99.9211  0.1203   
 255   2821.23  2549.890  2244.1111  1676.7316     0.9197  100.8067  0.1204   
 256   3073.67  2349.480  2244.1111  1676.7316     0.9197  100.8067  0.1204   
 257   3012.98  2498.280  2243.7778  1502.9221     1.8160  102.0978  0.1195   
 258   3072.31  2308.600  2176.6334  1272.4090     1.3828  100.4622  0.1216   
 259   2968.02  2494.250  2197.2667   911.2054     1.3257  104.1722  0.1208   
 260   3111.93  2460.940  2176.6334  1272.4090     1.3828  100.4622  0.1216   
 261   2957.97  2483.970  2243.7778  1502.9221     1.8160  102.0978  0.1195   
 262   2850.33  2570.420  2177.3222  1089.3655     1.3101  101.1478  0.1216   
 263   2927.46  2464.800  2243.7778  1502.9221     1.8160  102.0978  0.1195   
 264   3055.21  2377.270  2177.3222  1089.3655     1.3101  101.1478  0.1216   
 265   3045.21  2561.100  2243.7778  1502.9221     1.8160  102.0978  0.1195   
 266   3000.39  2581.670  2197.2667   911.2054     1.3257  104.1722  0.1208   
 267   2970.87  2610.780  2185.8111   995.3928     1.3714  102.6422  0.1218   
 268   3087.45  2503.320  2243.7778  1502.9221     1.8160  102.0978  0.1195   
 269   3054.45  2443.680  2243.7778  1502.9221     1.8160  102.0978  0.1195   
 270   2988.52  2291.920  2183.5777  1764.5386     1.7050  100.4478  0.1222   
 271   3033.29  2587.270  2244.1111  1676.7316     0.9197  100.8067  0.1204   
 272   3076.50  2554.290  2178.7333  1039.3641     0.7367  101.4922  0.1219   
 274   3038.25  2337.960  2244.1111  1676.7316     0.9197  100.8067  0.1204   
 275   2989.91  2575.710  2244.1111  1676.7316     0.9197  100.8067  0.1204   
 276   3061.24  2734.330  2243.7778  1502.9221     1.8160  102.0978  0.1195   
 278   3054.84  2488.500  2185.8111   995.3928     1.3714  102.6422  0.1218   
 279   3010.71  2513.050  2243.7778  1502.9221     1.8160  102.0978  0.1195   
 280   2935.73  2573.740  2201.5777   976.4791     0.7679   99.9956  0.1202   
 281   3018.60  2374.640  2243.7778  1502.9221     1.8160  102.0978  0.1195   
 283   2969.01  2537.880  2194.6444   999.4387     1.3259  101.9767  0.1205   
 284   2984.32  2539.730  2214.7111  1493.8781     1.5899  103.9367  0.1202   
 285   3017.94  2629.060  2194.6444   999.4387     1.3259  101.9767  0.1205   
 286   2988.66  2533.490  2194.6444   999.4387     1.3259  101.9767  0.1205   
 287   3002.46  2693.160  2214.7111  1493.8781     1.5899  103.9367  0.1202   
 288   2962.65  2535.190  2214.7111  1493.8781     1.5899  103.9367  0.1202   
 289   3021.41  2509.190  2218.5778  1632.2734     0.8396   98.1311  0.1211   
 290   3073.81  2786.970  2178.7333  1039.3641     0.7367  101.4922  0.1219   
 292   3029.19  2459.560  2214.7111  1493.8781     1.5899  103.9367  0.1202   
 293   3082.31  2457.920  2194.6444   999.4387     1.3259  101.9767  0.1205   
 295   3067.43  2467.110  2214.7111  1493.8781     1.5899  103.9367  0.1202   
 296   2864.05  2533.750  2224.0000  1510.0797     1.5611   99.8300  0.1199   
 297   3024.14  2505.030  2185.8111   995.3928     1.3714  102.6422  0.1218   
 298   3032.29  2356.940  2218.5778  1632.2734     0.8396   98.1311  0.1211   
 299   2968.06  2524.250  2218.5778  1632.2734     0.8396   98.1311  0.1211   
 300   2988.95  2611.210  2202.4333  1586.1015     1.6025  100.8067  0.1214   
 301   3052.43  2486.480  2201.5777   976.4791     0.7679   99.9956  0.1202   
 302   3031.44  2487.460  2195.5333  1561.7164     1.4292   99.2500  0.1223   
 303   2889.93  2499.405  2238.5444  1659.1424     0.9010   99.3100  0.1204   
 304   3020.25  2436.770  2224.0000  1510.0797     1.5611   99.8300  0.1199   
 305   3020.71  2483.520  2201.5777   976.4791     0.7679   99.9956  0.1202   
 306   2933.79  2417.700  2195.5333  1561.7164     1.4292   99.2500  0.1223   
 307   3067.25  2312.140  2214.7111  1493.8781     1.5899  103.9367  0.1202   
 308   2973.52  2414.480  2214.7111  1493.8781     1.5899  103.9367  0.1202   
 309   2999.81  2477.460  2178.7333  1039.3641     0.7367  101.4922  0.1219   
 310   3098.79  2453.540  2214.7111  1493.8781     1.5899  103.9367  0.1202   
 311   2995.26  2540.190  2214.7111  1493.8781     1.5899  103.9367  0.1202   
 312   3065.02  2515.550  2238.5444  1659.1424     0.9010   99.3100  0.1204   
 313   3091.11  2520.850  2238.5444  1659.1424     0.9010   99.3100  0.1204   
 314   2941.71  2529.420  2196.6889  1593.1220     1.5925   99.1133  0.1226   
 315   3019.05  2461.890  2196.6889  1593.1220     1.5925   99.1133  0.1226   
 316   2885.72  2575.710  2218.5778  1632.2734     0.8396   98.1311  0.1211   
 317   3054.70  2450.480  2201.5889   956.1617     1.3886  105.4122  0.1207   
 318   3033.33  2512.510  2224.0000  1510.0797     1.5611   99.8300  0.1199   
 319   2926.40  2421.450  2180.9666   998.4939     1.3990  106.3311  0.1216   
 320   3081.75  2442.260  2196.6889  1593.1220     1.5925   99.1133  0.1226   
 322   3093.48  2413.920  2238.5444  1659.1424     0.9010   99.3100  0.1204   
 324   2787.49  2584.150  2224.0000  1510.0797     1.5611   99.8300  0.1199   
 325   3039.66  2393.780  2194.2889  1631.5293     1.5119   99.6367  0.1222   
 328   2894.04  2490.060  2207.0444  1330.6718     1.3076  101.6778  0.1216   
 329   3042.90  2431.540  2205.2222  1427.3840     1.4633  106.8389  0.1215   
 330   2915.83  2526.290  2197.6778  1056.7817     1.3168  102.9611  0.1203   
 331   2967.04  2273.460  2208.4444  1089.1175     0.7665  103.7100  0.1201   
 332   3057.33  2496.080  2196.6889  1593.1220     1.5925   99.1133  0.1226   
 333   2904.17  2405.530  2204.2667  1475.7797     1.4053  101.2411  0.1221   
 334   3038.41  2475.670  2224.0000  1510.0797     1.5611   99.8300  0.1199   
 335   3035.75  2484.620  2207.0444  1330.6718     1.3076  101.6778  0.1216   
 337   2991.98  2412.410  2197.2333  1435.1460     0.9740  104.8733  0.1226   
 338   3065.83  2273.800  2197.2333  1435.1460     0.9740  104.8733  0.1226   
 339   3106.85  2453.470  2197.2333  1435.1460     0.9740  104.8733  0.1226   
 340   3033.82  2531.630  2180.9666   998.4939     1.3990  106.3311  0.1216   
 341   3059.46  2567.680  2197.6778  1056.7817     1.3168  102.9611  0.1203   
 342   2908.11  2512.990  2218.0555  1517.4371     0.8579  105.8133  0.1206   
 343   3111.09  2536.560  2180.9666   998.4939     1.3990  106.3311  0.1216   
 345   3084.00  2453.910  2194.2889  1631.5293     1.5119   99.6367  0.1222   
 346   2959.29  2478.620  2197.2333  1435.1460     0.9740  104.8733  0.1226   
 347   3047.54  2709.450  2238.5444  1659.1424     0.9010   99.3100  0.1204   
 348   2990.95  2379.960  2218.5778  1632.2734     0.8396   98.1311  0.1211   
 349   3038.81  2569.580  2198.7222  1534.2053     0.9374  104.1989  0.1224   
 350   3028.43  2517.540  2170.5444   921.0605     1.4390  104.5300  0.1226   
 352   3190.45  2391.470  2208.4444  1089.1175     0.7665  103.7100  0.1201   
 353   2893.56  2408.400  2208.4444  1089.1175     0.7665  103.7100  0.1201   
 354   3031.79  2479.690  2186.9889   949.2201     1.2981  103.3322  0.1219   
 355   3111.28  2503.690  2218.0555  1517.4371     0.8579  105.8133  0.1206   
 356   3100.44  2495.400  2218.0555  1517.4371     0.8579  105.8133  0.1206   
 357   3040.45  2226.990  2218.0555  1517.4371     0.8579  105.8133  0.1206   
 358   2942.16  2450.050  2224.0000  1510.0797     1.5611   99.8300  0.1199   
 359   3012.18  2590.800  2197.6778  1056.7817     1.3168  102.9611  0.1203   
 360   2973.57  2421.450  2218.5778  1632.2734     0.8396   98.1311  0.1211   
 361   2999.40  2270.680  2198.7222  1534.2053     0.9374  104.1989  0.1224   
 362   3018.64  2401.800  2224.0000  1510.0797     1.5611   99.8300  0.1199   
 363   3066.07  2547.840  2186.9889   949.2201     1.2981  103.3322  0.1219   
 364   3063.36  2333.960  2186.9889   949.2201     1.2981  103.3322  0.1219   
 365   2988.92  2460.910  2178.0778   941.9524     0.8039  104.0167  0.1229   
 366   3089.31  2546.270  2208.4444  1089.1175     0.7665  103.7100  0.1201   
 367   2939.94  2516.110  2186.9889   949.2201     1.2981  103.3322  0.1219   
 369   3206.18  2456.650  2198.7222  1534.2053     0.9374  104.1989  0.1224   
 370   3109.46  2510.080  2197.6778  1056.7817     1.3168  102.9611  0.1203   
 371   2998.23  2475.500  2238.5444  1659.1424     0.9010   99.3100  0.1204   
 372   3047.65  2605.260  2196.8000  1090.0084     1.3270   99.3944  0.1212   
 374   2993.48  2502.340  2218.0555  1517.4371     0.8579  105.8133  0.1206   
 375   3045.73  2479.930  2197.6778  1056.7817     1.3168  102.9611  0.1203   
 376   3067.14  2512.140  2205.2222  1427.3840     1.4633  106.8389  0.1215   
 377   2982.49  2288.050  2198.7222  1534.2053     0.9374  104.1989  0.1224   
 378   2927.75  2462.820  2207.0444  1330.6718     1.3076  101.6778  0.1216   
 379   3001.50  2523.600  2197.2333  1435.1460     0.9740  104.8733  0.1226   
 380   2875.43  2513.280  2197.2333  1435.1460     0.9740  104.8733  0.1226   
 381   2957.88  2560.910  2196.8000  1090.0084     1.3270   99.3944  0.1212   
 382   3047.00  2436.010  2213.7556  1113.5599     0.7217  104.1667  0.1211   
 383   3037.81  2305.520  2186.9889   949.2201     1.2981  103.3322  0.1219   
 384   3040.17  2375.550  2198.7222  1534.2053     0.9374  104.1989  0.1224   
 385   3067.89  2570.930  2196.8000  1090.0084     1.3270   99.3944  0.1212   
 386   3083.97  2311.420  2205.2222  1427.3840     1.4633  106.8389  0.1215   
 387   2912.24  2555.490  2206.4222  1113.3443     1.2678   97.7689  0.1215   
 388   3024.19  2474.410  2186.9889   949.2201     1.2981  103.3322  0.1219   
 389   3002.85  2578.760  2197.6778  1056.7817     1.3168  102.9611  0.1203   
 390   2989.82  2434.000  2180.0556  1031.0669     0.7565  104.7056  0.1226   
 391   2924.07  2566.220  2213.7556  1113.5599     0.7217  104.1667  0.1211   
 393   2950.24  2512.970  2205.2222  1427.3840     1.4633  106.8389  0.1215   
 394   2821.63  2491.130  2198.7222  1534.2053     0.9374  104.1989  0.1224   
 395   3245.00  2371.450  2196.8000  1090.0084     1.3270   99.3944  0.1212   
 396   3127.07  2478.970  2198.7222  1534.2053     0.9374  104.1989  0.1224   
 397   3034.08  2735.650  2213.7556  1113.5599     0.7217  104.1667  0.1211   
 398   2953.65  2310.290  2197.2333  1435.1460     0.9740  104.8733  0.1226   
 399   2996.53  2533.520  2202.1222  1034.5674     0.7760  104.6156  0.1219   
 400   2951.56  2441.220  2202.1222  1034.5674     0.7760  104.6156  0.1219   
 401   2986.34  2318.950  2213.7556  1113.5599     0.7217  104.1667  0.1211   
 402   3105.49  2587.300  2218.5778  1632.2734     0.8396   98.1311  0.1211   
 403   3090.09  2490.540  2213.7556  1113.5599     0.7217  104.1667  0.1211   
 404   2986.32  2490.920  2218.0555  1517.4371     0.8579  105.8133  0.1206   
 405   3052.97  2501.630  2218.0555  1517.4371     0.8579  105.8133  0.1206   
 407   2981.36  2502.060  2178.6889  1657.3518     1.6603  100.8022  0.1229   
 408   3017.11  2364.310  2197.2333  1435.1460     0.9740  104.8733  0.1226   
 409   2940.72  2419.830  2218.0555  1517.4371     0.8579  105.8133  0.1206   
 410   2940.17  2514.300  2186.9889   949.2201     1.2981  103.3322  0.1219   
 411   2933.88  2488.490  2178.6889  1657.3518     1.6603  100.8022  0.1229   
 412   2989.85  2501.880  2197.2333  1435.1460     0.9740  104.8733  0.1226   
 413   3083.49  2536.430  2213.7556  1113.5599     0.7217  104.1667  0.1211   
 414   2965.48  2467.940  2178.6889  1657.3518     1.6603  100.8022  0.1229   
 415   2951.41  2577.270  2207.8111  1202.4520     1.6219  108.7689  0.1212   
 416   3015.48  2532.880  2218.0555  1517.4371     0.8579  105.8133  0.1206   
 417   2981.47  2434.320  2180.0556  1031.0669     0.7565  104.7056  0.1226   
 418   2946.70  2535.820  2178.6889  1657.3518     1.6603  100.8022  0.1229   
 419   3015.67  2539.460  2204.2889  1616.0688     0.9830  102.9922  0.1226   
 420   3099.87  2675.580  2200.7000  1568.0936     0.9496   99.4889  0.1217   
 421   3048.73  2460.110  2205.2222  1427.3840     1.4633  106.8389  0.1215   
 422   3022.80  2436.690  2204.2889  1616.0688     0.9830  102.9922  0.1226   
 423   3018.81  2281.300  2197.2333  1435.1460     0.9740  104.8733  0.1226   
 425   3079.17  2405.560  2217.3777  1425.1041     1.7585  106.2556  0.1200   
 426   2911.37  2541.210  2207.8111  1202.4520     1.6219  108.7689  0.1212   
 427   3085.57  2364.780  2178.6889  1657.3518     1.6603  100.8022  0.1229   
 428   3053.49  2457.080  2172.5333  1351.9648     1.6377  103.8800  0.1243   
 429   3120.68  2396.400  2177.0222  1448.8499     1.5565  103.2567  0.1232   
 430   2956.84  2592.290  2217.3777  1425.1041     1.7585  106.2556  0.1200   
 431   3068.98  2439.000  2172.5333  1351.9648     1.6377  103.8800  0.1243   
 432   2999.89  2506.390  2178.6889  1657.3518     1.6603  100.8022  0.1229   
 433   3114.68  2605.480  2172.5333  1351.9648     1.6377  103.8800  0.1243   
 434   3266.04  2417.040  2217.3777  1425.1041     1.7585  106.2556  0.1200   
 435   3041.33  2526.730  2213.7556  1113.5599     0.7217  104.1667  0.1211   
 436   3071.58  2489.470  2217.3777  1425.1041     1.7585  106.2556  0.1200   
 437   2921.48  2496.540  2213.9333  1248.6209     1.8848   97.1856  0.1209   
 438   3059.20  2467.800  2196.0889  1277.8592     1.8246   95.6322  0.1224   
 439   3062.54  2370.500  2177.0222  1448.8499     1.5565  103.2567  0.1232   
 440   3067.70  2500.270  2177.0222  1448.8499     1.5565  103.2567  0.1232   
 442   2948.76  2644.320  2185.3334  1780.4149     1.7632   93.9844  0.1215   
 443   3079.77  2354.510  2207.0444  1269.6078     1.7571   97.0189  0.1221   
 444   3114.46  2656.840  2207.0444  1269.6078     1.7571   97.0189  0.1221   
 445   2882.17  2601.980  2213.9333  1248.6209     1.8848   97.1856  0.1209   
 446   3014.35  2653.230  2203.0445   996.1560     1.3660  104.6633  0.1209   
 447   2901.07  2505.830  2207.0444  1269.6078     1.7571   97.0189  0.1221   
 449   2882.76  2516.110  2172.5333  1351.9648     1.6377  103.8800  0.1243   
 450   3047.76  2446.960  2187.6667  1468.8963     0.9253  101.8611  0.1234   
 451   3133.43  2406.580  2173.4556  1433.6732     1.0304  110.5422  0.1245   
 452   3080.01  2481.040  2172.1223  1871.2454     1.5915  100.2533  0.1247   
 453   2967.57  2520.910  2172.1223  1871.2454     1.5915  100.2533  0.1247   
 454   3007.38  2474.970  2170.9667  1600.3858     1.0430  104.9756  0.1249   
 455   3012.56  2534.620  2180.7000  1159.3838     1.0177   98.9367  0.1222   
 456   2958.91  2460.190  2191.5667  1448.8757     0.9187  104.4511  0.1261   
 457   2940.56  2490.680  2170.9667  1600.3858     1.0430  104.9756  0.1249   
 458   3021.45  2611.980  2113.0778  1016.6750     0.9218   97.9200  0.1265   
 459   2996.66  2501.030  2235.0556  1302.6607     1.6347  109.9856  0.1230   
 460   3072.41  2705.160  2183.3111  1588.5090     1.6269  102.8467  0.1248   
 461   3200.36  2378.770  2163.5889  1448.3869     1.7014  104.8333  0.1256   
 462   3012.41  2481.590  2141.0667  1236.5212     0.9698   98.3344  0.1238   
 463   2961.36  2362.090  2163.5889  1448.3869     1.7014  104.8333  0.1256   
 464   3106.55  2673.410  2202.2889  1518.2905     0.9257   94.8922  0.1213   
 465   3043.23  2556.220  2230.7555  1281.7862     1.0038  111.5489  0.1235   
 466   3043.34  2473.490  2170.9667  1600.3858     1.0430  104.9756  0.1249   
 467   3044.88  2636.690  2230.7555  1281.7862     1.0038  111.5489  0.1235   
 468   2975.62  2465.160  2198.6889  1252.1130     1.8008   98.0211  0.1218   
 469   2997.15  2531.250  2183.1000  1175.1481     1.7881  111.8900  0.1256   
 470   2929.84  2504.500  2183.3111  1588.5090     1.6269  102.8467  0.1248   
 471   3036.93  2570.130  2230.7555  1281.7862     1.0038  111.5489  0.1235   
 472   3100.12  2412.530  2231.4889  1275.3021     1.6787   94.1256  0.1193   
 473   3038.93  2187.670  2219.1333  1167.9207     0.9002   97.9378  0.1203   
 474   3008.92  2566.940  2235.0556  1302.6607     1.6347  109.9856  0.1230   
 475   2981.50  2500.590  2173.6444  1458.7176     0.8811  101.5667  0.1258   
 476   3046.93  2459.210  2179.0778  1820.0613     1.6746   99.3300  0.1261   
 477   2957.63  2558.810  2230.7555  1281.7862     1.0038  111.5489  0.1235   
 478   3023.34  2493.120  2208.2334  1517.0152     1.0980  110.1900  0.1247   
 479   3042.78  2377.890  2173.4556  1433.6732     1.0304  110.5422  0.1245   
 480   3091.64  2340.050  2200.7333  1602.8427     0.9790   95.3311  0.1220   
 481   3008.02  2425.160  2230.7555  1281.7862     1.0038  111.5489  0.1235   
 482   2993.53  2441.120  2180.7000  1159.3838     1.0177   98.9367  0.1222   
 483   3003.08  2521.760  2183.3111  1588.5090     1.6269  102.8467  0.1248   
 484   3053.95  2459.390  2183.1000  1175.1481     1.7881  111.8900  0.1256   
 485   2933.89  2485.500  2189.6555  1200.5912     0.9898   98.0433  0.1215   
 486   2996.95  2446.920  2163.5889  1448.3869     1.7014  104.8333  0.1256   
 487   3070.54  2406.400  2183.3111  1588.5090     1.6269  102.8467  0.1248   
 488   3085.99  2550.520  2179.0778  1820.0613     1.6746   99.3300  0.1261   
 489   3094.34  2391.090  2207.0444  1269.6078     1.7571   97.0189  0.1221   
 490   2973.71  2487.450  2163.5778  1434.2764     1.6241  105.0833  0.1249   
 491   2934.01  2461.280  2168.7222  1212.1040     1.5175  106.7589  0.1233   
 492   3043.22  2484.780  2191.5667  1448.8757     0.9187  104.4511  0.1261   
 493   3037.38  2441.160  2146.1111  1792.7115     1.6513  100.7244  0.1248   
 494   2974.93  2473.340  2200.7333  1602.8427     0.9790   95.3311  0.1220   
 496   2964.74  2480.510  2163.5889  1448.3869     1.7014  104.8333  0.1256   
 497   3057.56  2471.010  2180.7000  1159.3838     1.0177   98.9367  0.1222   
 498   2954.41  2614.990  2208.2334  1517.0152     1.0980  110.1900  0.1247   
 499   2997.27  2619.880  2148.6223  1288.4151     1.4990  107.9122  0.1254   
 500   3069.02  2474.370  2151.2000  1089.5951     0.8860  100.5622  0.1247   
 501   2987.72  2550.520  2180.7000  1159.3838     1.0177   98.9367  0.1222   
 502   3057.06  2445.540  2183.3111  1588.5090     1.6269  102.8467  0.1248   
 503   2976.40  2534.910  2185.3334  1780.4149     1.7632   93.9844  0.1215   
 504   3120.81  2349.270  2126.6555  1015.0770     1.4381  102.4733  0.1255   
 505   2984.05  2600.070  2235.0556  1302.6607     1.6347  109.9856  0.1230   
 506   3071.29  2561.990  2219.1333  1167.9207     0.9002   97.9378  0.1203   
 507   2965.67  2606.000  2155.6333  1070.0439     0.8024  101.4333  0.1241   
 509   2970.71  2506.320  2180.8333  1435.0747     1.5082  107.3322  0.1261   
 510   3060.07  2693.200  2219.1333  1167.9207     0.9002   97.9378  0.1203   
 511   3108.24  2662.700  2183.3444  1111.4499     1.5548   97.5311  0.1236   
 512   3008.50  2526.160  2173.6444  1458.7176     0.8811  101.5667  0.1258   
 513   3034.90  2598.020  2208.2334  1517.0152     1.0980  110.1900  0.1247   
 514   2884.65  2523.360  2208.2667  1656.2210     0.9143  102.6267  0.1243   
 515   3028.50  2480.480  2234.5222  1590.2597     1.7889   93.1967  0.1188   
 516   2941.25  2492.810  2219.1333  1167.9207     0.9002   97.9378  0.1203   
 517   2980.11  2516.330  2201.8222  1288.0857     1.6769   95.9789  0.1209   
 519   2933.27  2473.540  2183.3111  1588.5090     1.6269  102.8467  0.1248   
 520   2792.24  2533.760  2180.7000  1159.3838     1.0177   98.9367  0.1222   
 521   2991.74  2575.390  2173.6444  1458.7176     0.8811  101.5667  0.1258   
 522   2960.22  2502.250  2183.3111  1588.5090     1.6269  102.8467  0.1248   
 523   3078.79  2720.380  2167.2000  1108.9456     1.4869   94.2600  0.1249   
 524   2953.55  2506.190  2146.1111  1792.7115     1.6513  100.7244  0.1248   
 525   2994.56  2505.350  2234.5222  1590.2597     1.7889   93.1967  0.1188   
 526   2946.11  2491.900  2167.2000  1108.9456     1.4869   94.2600  0.1249   
 527   2987.43  2508.220  2124.8444  1180.2820     0.8465  100.7978  0.1257   
 528   3100.96  2547.830  2167.2000  1108.9456     1.4869   94.2600  0.1249   
 529   3047.28  2186.060  2235.0556  1302.6607     1.6347  109.9856  0.1230   
 530   3076.81  2158.750  2208.2334  1517.0152     1.0980  110.1900  0.1247   
 531   2951.62  2511.920  2253.5111  1397.5060     0.9660  109.7611  0.1210   
 532   2930.42  2505.170  2235.0556  1302.6607     1.6347  109.9856  0.1230   
 533   2997.28  2357.990  2141.0667  1236.5212     0.9698   98.3344  0.1238   
 534   3025.10  2475.180  2235.0556  1302.6607     1.6347  109.9856  0.1230   
 535   2992.15  2538.050  2162.8445  1312.3198     0.8286  100.3633  0.1242   
 536   3055.87  2569.670  2235.0556  1302.6607     1.6347  109.9856  0.1230   
 537   2971.23  2626.130  2183.3444  1111.4499     1.5548   97.5311  0.1236   
 538   3011.49  2651.370  2180.7000  1159.3838     1.0177   98.9367  0.1222   
 539   3028.53  2480.420  2220.4778  1531.6980     1.7751  107.6856  0.1249   
 540   3072.67  2487.080  2202.2889  1518.2905     0.9257   94.8922  0.1213   
 541   3048.78  2442.470  2183.3444  1111.4499     1.5548   97.5311  0.1236   
 542   2962.79  2560.690  2155.6333  1070.0439     0.8024  101.4333  0.1241   
 543   3085.35  2503.860  2201.8222  1288.0857     1.6769   95.9789  0.1209   
 544   3037.82  2436.390  2167.2000  1108.9456     1.4869   94.2600  0.1249   
 545   3089.53  2695.570  2242.8555  1430.7270     0.8832  106.2100  0.1216   
 546   3106.03  2506.420  2216.4778  1242.2350     0.8379  105.1111  0.1233   
 547   3050.22  2563.580  2242.8555  1430.7270     0.8832  106.2100  0.1216   
 548   2929.60  2564.910  2216.4778  1242.2350     0.8379  105.1111  0.1233   
 549   3058.60  2568.350  2228.4444  1364.0563     0.8795  106.2756  0.1216   
 550   2961.59  2585.060  2167.2000  1108.9456     1.4869   94.2600  0.1249   
 551   3158.88  2462.640  2216.4778  1242.2350     0.8379  105.1111  0.1233   
 552   2945.19  2456.560  2216.4778  1242.2350     0.8379  105.1111  0.1233   
 553   3071.43  2370.720  2216.4778  1242.2350     0.8379  105.1111  0.1233   
 554   3026.87  2436.850  2163.8667  1106.0618     1.6273  100.8000  0.1251   
 555   2983.83  2413.650  2235.0556  1302.6607     1.6347  109.9856  0.1230   
 556   2923.26  2548.340  2216.4778  1242.2350     0.8379  105.1111  0.1233   
 557   2967.53  2545.770  2155.6333  1070.0439     0.8024  101.4333  0.1241   
 558   2996.16  2449.260  2180.7000  1159.3838     1.0177   98.9367  0.1222   
 559   3072.99  2471.620  2126.6555  1015.0770     1.4381  102.4733  0.1255   
 560   3017.28  2590.950  2124.8444  1180.2820     0.8465  100.7978  0.1257   
 561   3009.40  2436.380  2164.3000  1031.4032     1.3626  102.1322  0.1242   
 562   2979.95  2518.440  2164.3000  1031.4032     1.3626  102.1322  0.1242   
 563   2978.67  2559.320  2253.5111  1397.5060     0.9660  109.7611  0.1210   
 564   3108.50  2321.450  2162.8445  1312.3198     0.8286  100.3633  0.1242   
 565   3064.81  2538.430  2167.2000  1108.9456     1.4869   94.2600  0.1249   
 566   3023.07  2484.550  2242.8555  1430.7270     0.8832  106.2100  0.1216   
 567   3032.98  2403.470  2180.7000  1159.3838     1.0177   98.9367  0.1222   
 568   2972.12  2571.210  2167.2000  1108.9456     1.4869   94.2600  0.1249   
 569   3041.89  2458.120  2242.8555  1430.7270     0.8832  106.2100  0.1216   
 570   3006.39  2496.550  2220.4778  1531.6980     1.7751  107.6856  0.1249   
 571   2976.75  2508.100  2242.8555  1430.7270     0.8832  106.2100  0.1216   
 572   3009.65  2515.970  2216.4778  1242.2350     0.8379  105.1111  0.1233   
 573   3052.16  2576.190  2141.0667  1236.5212     0.9698   98.3344  0.1238   
 574   3033.91  2607.330  2164.3000  1031.4032     1.3626  102.1322  0.1242   
 575   2922.65  2560.920  2201.8222  1288.0857     1.6769   95.9789  0.1209   
 577   2973.53  2532.810  2228.4444  1364.0563     0.8795  106.2756  0.1216   
 578   2898.08  2573.360  2159.0556  1084.3779     0.8184  100.7056  0.1249   
 579   3042.36  2493.850  2124.8444  1180.2820     0.8465  100.7978  0.1257   
 580   2977.46  2497.090  2183.3444  1111.4499     1.5548   97.5311  0.1236   
 581   2994.64  2666.040  2159.0556  1084.3779     0.8184  100.7056  0.1249   
 582   2984.68  2497.490  2179.0778  1820.0613     1.6746   99.3300  0.1261   
 584   2968.78  2501.480  2171.8222  1010.0662     0.8599  103.6356  0.1237   
 585   3093.75  2440.820  2160.6000  1124.5821     1.5257   98.7122  0.1246   
 586   2973.88  2449.170  2159.0556  1084.3779     0.8184  100.7056  0.1249   
 587   2984.30  2481.200  2238.4556  1256.5186     1.3404  103.2589  0.1222   
 588   2931.26  2527.770  2242.8555  1430.7270     0.8832  106.2100  0.1216   
 589   2978.18  2484.780  2159.0556  1084.3779     0.8184  100.7056  0.1249   
 590   2890.67  2574.320  2159.0556  1084.3779     0.8184  100.7056  0.1249   
 591   2971.43  2568.930  2167.2000  1108.9456     1.4869   94.2600  0.1249   
 592   3022.02  2323.340  2159.0556  1084.3779     0.8184  100.7056  0.1249   
 593   2941.23  2505.380  2160.6000  1124.5821     1.5257   98.7122  0.1246   
 594   3042.70  2355.110  2242.8555  1430.7270     0.8832  106.2100  0.1216   
 595   3036.34  2468.910  2179.0778  1820.0613     1.6746   99.3300  0.1261   
 596   3045.38  2513.260  2124.8444  1180.2820     0.8465  100.7978  0.1257   
 597   2907.52  2547.130  2159.0556  1084.3779     0.8184  100.7056  0.1249   
 598   3033.29  2508.860  2124.8444  1180.2820     0.8465  100.7978  0.1257   
 599   2895.71  2573.590  2173.6444  1458.7176     0.8811  101.5667  0.1258   
 600   2960.54  2508.090  2164.3000  1031.4032     1.3626  102.1322  0.1242   
 602   2974.20  2449.160  2231.4889  1275.3021     1.6787   94.1256  0.1193   
 603   3114.92  2574.480  2200.0666  1012.6747     1.3954  103.0644  0.1212   
 604   2937.63  2499.720  2200.0666  1012.6747     1.3954  103.0644  0.1212   
 606   2985.38  2542.340  2200.0666  1012.6747     1.3954  103.0644  0.1212   
 607   3032.07  2477.720  2172.4000  1148.4101     1.2614  102.4522  0.1227   
 608   3065.45  2698.140  2200.0666  1012.6747     1.3954  103.0644  0.1212   
 609   2966.94  2509.220  2113.0778  1016.6750     0.9218   97.9200  0.1265   
 610   3010.40  2499.405  2172.4000  1148.4101     1.2614  102.4522  0.1227   
 611   2967.54  2573.090  2160.6000  1124.5821     1.5257   98.7122  0.1246   
 612   2891.17  2480.110  2171.8556   940.9917     1.2906  103.4733  0.1234   
 613   2992.61  2541.130  2171.8556   940.9917     1.2906  103.4733  0.1234   
 614   2924.96  2564.290  2171.8556   940.9917     1.2906  103.4733  0.1234   
 615   2956.18  2435.990  2171.8556   940.9917     1.2906  103.4733  0.1234   
 616   3020.40  2690.550  2171.8556   940.9917     1.2906  103.4733  0.1234   
 617   3059.01  2641.110  2171.8556   940.9917     1.2906  103.4733  0.1234   
 618   2993.11  2498.910  2171.8556   940.9917     1.2906  103.4733  0.1234   
 619   2987.75  2574.530  2183.5000  1099.0027     1.3593  104.4156  0.1220   
 620   3004.71  2329.660  2217.8667  1275.0917     1.5487  105.2933  0.1230   
 621   2931.40  2573.210  2171.8556   940.9917     1.2906  103.4733  0.1234   
 622   3108.56  2468.650  2201.0667   880.2317     1.4148  106.5478  0.1211   
 623   3071.80  2517.650  2200.0666  1012.6747     1.3954  103.0644  0.1212   
 624   2989.44  2487.660  2201.0667   880.2317     1.4148  106.5478  0.1211   
 625   2996.89  2492.400  2217.8667  1275.0917     1.5487  105.2933  0.1230   
 626   2990.85  2485.990  2167.9444   861.8041     1.4140  106.6033  0.1243   
 627   3059.43  2473.550  2214.9333  1663.7024     1.0203  100.4456  0.1247   
 628   3024.54  2420.250  2167.9444   861.8041     1.4140  106.6033  0.1243   
 629   3069.44  2459.500  2183.5000  1099.0027     1.3593  104.4156  0.1220   
 630   2989.37  2584.750  2167.9444   861.8041     1.4140  106.6033  0.1243   
 631   3147.74  2281.350  2171.8556   940.9917     1.2906  103.4733  0.1234   
 632   3031.65  2565.470  2184.8889   905.1501     1.3378  106.6900  0.1226   
 633   3007.85  2685.060  2217.8667  1275.0917     1.5487  105.2933  0.1230   
 635   3017.53  2524.090  2201.0667   880.2317     1.4148  106.5478  0.1211   
 636   2980.53  2221.100  2167.9444   861.8041     1.4140  106.6033  0.1243   
 637   2925.34  2521.850  2184.8889   905.1501     1.3378  106.6900  0.1226   
 638   3048.42  2691.960  2167.9444   861.8041     1.4140  106.6033  0.1243   
 639   2998.18  2615.960  2171.3222   966.5755     0.8066  107.1700  0.1242   
 640   3032.45  2455.320  2172.4000  1148.4101     1.2614  102.4522  0.1227   
 641   2986.50  2497.130  2236.0667  1680.1825     1.4834   98.6889  0.1221   
 642   2939.60  2541.120  2233.1556  1434.9983     1.5188  102.6611  0.1235   
 643   3041.23  2487.830  2183.5000  1099.0027     1.3593  104.4156  0.1220   
 644   3072.21  2610.620  2214.9333  1663.7024     1.0203  100.4456  0.1247   
 645   3017.87  2467.400  2217.8667  1275.0917     1.5487  105.2933  0.1230   
 646   3119.97  2373.030  2184.8889   905.1501     1.3378  106.6900  0.1226   
 647   2743.24  2614.540  2171.3222   966.5755     0.8066  107.1700  0.1242   
 648   3068.56  2363.520  2171.3222   966.5755     0.8066  107.1700  0.1242   
 649   3022.04  2482.880  2214.9333  1663.7024     1.0203  100.4456  0.1247   
 650   2954.46  2449.480  2236.0667  1680.1825     1.4834   98.6889  0.1221   
 651   2978.62  2478.810  2236.0667  1680.1825     1.4834   98.6889  0.1221   
 652   2938.27  2447.280  2201.0667   880.2317     1.4148  106.5478  0.1211   
 653   2976.24  2411.420  2233.1556  1434.9983     1.5188  102.6611  0.1235   
 654   2993.04  2504.660  2229.3333  1553.3158     1.5123  102.7800  0.1235   
 655   3083.33  2440.210  2171.8556   940.9917     1.2906  103.4733  0.1234   
 656   2992.39  2591.830  2233.1556  1434.9983     1.5188  102.6611  0.1235   
 657   3049.80  2512.790  2165.8666   847.7976     1.4274  108.6589  0.1236   
 658   3034.55  2473.390  2233.1556  1434.9983     1.5188  102.6611  0.1235   
 659   3068.38  2515.320  2165.8666   847.7976     1.4274  108.6589  0.1236   
 660   2961.59  2513.790  2229.3333  1553.3158     1.5123  102.7800  0.1235   
 661   3085.78  2428.950  2229.3333  1553.3158     1.5123  102.7800  0.1235   
 662   3020.29  2433.990  2217.8111  1744.7771     0.9618  100.1789  0.1218   
 663   3011.84  2604.990  2167.9444   861.8041     1.4140  106.6033  0.1243   
 664   2979.45  2546.780  2217.8111  1744.7771     0.9618  100.1789  0.1218   
 665   3098.04  2386.330  2236.0667  1680.1825     1.4834   98.6889  0.1221   
 666   3080.57  2428.250  2217.8667  1275.0917     1.5487  105.2933  0.1230   
 667   2938.11  2471.740  2233.1556  1434.9983     1.5188  102.6611  0.1235   
 668   3073.57  2419.180  2171.3222   966.5755     0.8066  107.1700  0.1242   
 669   3057.31  2481.530  2214.9333  1663.7024     1.0203  100.4456  0.1247   
 670   2979.86  2545.480  2229.3333  1553.3158     1.5123  102.7800  0.1235   
 671   3113.13  2459.140  2159.0556  1084.3779     0.8184  100.7056  0.1249   
 672   3094.95  2505.700  2201.0667   880.2317     1.4148  106.5478  0.1211   
 673   2978.89  2523.930  2217.8111  1744.7771     0.9618  100.1789  0.1218   
 674   3005.61  2393.560  2214.9333  1663.7024     1.0203  100.4456  0.1247   
 675   2965.50  2366.360  2184.8889   905.1501     1.3378  106.6900  0.1226   
 676   2933.05  2514.760  2229.3333  1553.3158     1.5123  102.7800  0.1235   
 677   2970.25  2432.020  2233.1556  1434.9983     1.5188  102.6611  0.1235   
 678   3005.80  2669.090  2233.1556  1434.9983     1.5188  102.6611  0.1235   
 679   3015.13  2605.970  2233.1556  1434.9983     1.5188  102.6611  0.1235   
 680   3110.23  2397.920  2184.8889   905.1501     1.3378  106.6900  0.1226   
 681   3018.34  2556.620  2217.8111  1744.7771     0.9618  100.1789  0.1218   
 682   3067.95  2461.410  2233.1556  1434.9983     1.5188  102.6611  0.1235   
 683   3046.22  2715.500  2171.3222   966.5755     0.8066  107.1700  0.1242   
 684   2960.65  2442.710  2159.0556  1084.3779     0.8184  100.7056  0.1249   
 685   3165.51  2380.330  2215.3889  1779.1892     0.8552  101.2178  0.1226   
 686   3024.24  2446.060  2165.8666   847.7976     1.4274  108.6589  0.1236   
 687   3011.84  2451.120  2215.3889  1779.1892     0.8552  101.2178  0.1226   
 688   2951.67  2521.410  2165.8666   847.7976     1.4274  108.6589  0.1236   
 689   3025.81  2314.350  2179.2000   875.7538     1.2708  106.9600  0.1213   
 690   2917.17  2379.310  2215.3889  1779.1892     0.8552  101.2178  0.1226   
 691   3122.43  2541.550  2233.1556  1434.9983     1.5188  102.6611  0.1235   
 692   2960.04  2292.400  2215.3889  1779.1892     0.8552  101.2178  0.1226   
 693   2991.62  2540.360  2172.2778  1085.3232     0.7955  105.7633  0.1221   
 694   3205.08  2286.450  2184.8889   905.1501     1.3378  106.6900  0.1226   
 695   2965.52  2539.470  2201.0667   880.2317     1.4148  106.5478  0.1211   
 696   3046.63  2440.050  2214.9333  1663.7024     1.0203  100.4456  0.1247   
 697   3110.14  2474.300  2242.4333  1543.8401     1.4994   98.1133  0.1218   
 698   3005.14  2443.390  2150.5889   868.5387     1.3597  105.6089  0.1236   
 699   3064.13  2526.220  2205.7222   906.9522     1.3443  105.6600  0.1200   
 700   3073.93  2472.680  2227.2444  1329.8933     0.9480  107.8778  0.1220   
 701   3010.92  2491.130  2179.2000   875.7538     1.2708  106.9600  0.1213   
 702   3040.91  2499.405  2205.7222   906.9522     1.3443  105.6600  0.1200   
 703   3004.63  2559.700  2220.4001  1389.2768     1.5976  100.9711  0.1249   
 704   2988.50  2560.750  2214.9333  1663.7024     1.0203  100.4456  0.1247   
 705   2965.28  2482.610  2205.7222   906.9522     1.3443  105.6600  0.1200   
 706   2978.87  2503.090  2171.3222   966.5755     0.8066  107.1700  0.1242   
 707   3045.88  2506.600  2229.3333  1553.3158     1.5123  102.7800  0.1235   
 708   3005.53  2479.240  2171.3222   966.5755     0.8066  107.1700  0.1242   
 710   2972.22  2519.870  2215.3889  1779.1892     0.8552  101.2178  0.1226   
 711   3043.49  2476.750  2205.7222   906.9522     1.3443  105.6600  0.1200   
 712   3219.60  2329.440  2201.0667   880.2317     1.4148  106.5478  0.1211   
 713   3096.81  2476.950  2205.7222   906.9522     1.3443  105.6600  0.1200   
 714   3101.28  2464.330  2216.5111   871.2526     1.2366  107.4867  0.1194   
 715   3070.07  2459.220  2216.5111   871.2526     1.2366  107.4867  0.1194   
 716   3068.81  2543.620  2221.5333  1738.1057     1.3844   99.7633  0.1228   
 717   2943.83  2445.170  2236.0667  1680.1825     1.4834   98.6889  0.1221   
 718   2992.33  2513.220  2229.3333  1553.3158     1.5123  102.7800  0.1235   
 719   2949.77  2522.320  2205.7222   906.9522     1.3443  105.6600  0.1200   
 720   3003.13  2498.300  2227.2444  1329.8933     0.9480  107.8778  0.1220   
 721   3000.68  2432.180  2198.2667   986.5558     0.8652  105.0589  0.1209   
 722   3056.40  2502.480  2173.6222   918.8631     1.2742  107.1889  0.1217   
 723   2964.83  2516.950  2210.9778  1572.4698     1.0204  106.2089  0.1222   
 724   2967.98  2512.650  2210.9778  1572.4698     1.0204  106.2089  0.1222   
 725   3011.49  2608.110  2216.5111   871.2526     1.2366  107.4867  0.1194   
 726   2994.75  2583.360  2210.9778  1572.4698     1.0204  106.2089  0.1222   
 727   2984.80  2508.030  2250.7445   996.4071     0.8572  106.2956  0.1172   
 728   3048.25  2523.940  2242.4333  1543.8401     1.4994   98.1133  0.1218   
 729   3063.44  2510.760  2216.6778  1567.4646     1.3617   96.5556  0.1230   
 730   3136.34  2442.450  2250.7445   996.4071     0.8572  106.2956  0.1172   
 731   3048.54  2353.210  2210.9778  1572.4698     1.0204  106.2089  0.1222   
 732   2992.37  2484.270  2210.9778  1572.4698     1.0204  106.2089  0.1222   
 733   3037.92  2361.500  2210.9778  1572.4698     1.0204  106.2089  0.1222   
 734   3069.73  2533.450  2218.6333   870.5620     1.3084  106.9089  0.1195   
 735   3027.92  2636.810  2189.2333   972.1465     1.3796  103.5789  0.1207   
 736   3028.97  2415.870  2136.4111  1172.5767     0.7809  106.6633  0.1245   
 737   2970.11  2437.820  2136.4111  1172.5767     0.7809  106.6633  0.1245   
 738   2957.59  2441.800  2189.8778  1158.0925     1.5291  107.8200  0.1219   
 739   3061.59  2483.440  2151.6889  1197.2643     1.5018  102.6711  0.1239   
 740   3090.55  2540.970  2136.4111  1172.5767     0.7809  106.6633  0.1245   
 741   3014.54  2539.590  2201.3556  1249.8445     1.4699  103.9978  0.1228   
 742   2988.93  2512.380  2189.8778  1158.0925     1.5291  107.8200  0.1219   
 743   2996.90  2448.070  2162.7556  1041.1557     0.8479  107.2622  0.1221   
 744   2957.41  2469.130  2203.4556  1441.1445     0.8264  104.6767  0.1232   
 745   2953.41  2531.050  2136.4111  1172.5767     0.7809  106.6633  0.1245   
 746   3102.49  2502.440  2189.8778  1158.0925     1.5291  107.8200  0.1219   
 747   3057.65  2540.350  2203.4556  1441.1445     0.8264  104.6767  0.1232   
 748   3083.62  2728.280  2203.4556  1441.1445     0.8264  104.6767  0.1232   
 749   3056.58  2505.620  2219.3445  1734.8435     0.9800  104.3867  0.1221   
 750   2971.93  2557.580  2189.8778  1158.0925     1.5291  107.8200  0.1219   
 751   3080.89  2590.450  2162.7556  1006.7789     0.8736  106.8744  0.1220   
 752   2951.59  2474.600  2251.8222  1104.9132     1.3881  104.9456  0.1206   
 753   3108.23  2423.570  2223.2778  1222.0834     1.5160  102.9600  0.1218   
 754   3059.82  2727.560  2146.9111  1063.8762     1.5377  104.3711  0.1232   
 755   2940.26  2525.290  2203.4556  1441.1445     0.8264  104.6767  0.1232   
 756   2959.36  2504.860  2210.9778  1572.4698     1.0204  106.2089  0.1222   
 757   3011.27  2477.470  2210.9778  1572.4698     1.0204  106.2089  0.1222   
 758   3046.97  2517.220  2151.6889  1197.2643     1.5018  102.6711  0.1239   
 759   3017.65  2542.640  2162.7556  1006.7789     0.8736  106.8744  0.1220   
 760   3001.36  2491.230  2155.3111   918.2161     1.2753  105.0478  0.1227   
 761   2981.92  2530.670  2151.6889  1197.2643     1.5018  102.6711  0.1239   
 762   2931.70  2487.010  2155.3111   918.2161     1.2753  105.0478  0.1227   
 763   3032.26  2455.050  2145.4555  1017.0577     1.4534  103.7878  0.1229   
 764   2922.05  2511.370  2155.3111   918.2161     1.2753  105.0478  0.1227   
 765   2939.40  2504.440  2210.9778  1572.4698     1.0204  106.2089  0.1222   
 766   3097.14  2463.060  2155.3111   918.2161     1.2753  105.0478  0.1227   
 767   2976.91  2583.600  2117.5889   894.0996     1.4330  106.4944  0.1253   
 768   3003.96  2560.610  2155.3111   918.2161     1.2753  105.0478  0.1227   
 769   2961.73  2368.340  2146.9111  1063.8762     1.5377  104.3711  0.1232   
 770   3005.04  2431.660  2136.4111  1172.5767     0.7809  106.6633  0.1245   
 771   3028.21  2529.080  2245.3667  1315.8241     1.5424  102.4089  0.1210   
 772   3086.08  2415.750  2190.1555  1298.8207     1.3947  105.7411  0.1242   
 773   2962.28  2452.280  2127.9555  1011.1924     1.0863  107.1522  0.1261   
 774   3070.63  2377.970  2130.0556   874.9165     1.2545  105.9011  0.1253   
 775   3055.13  2523.650  2181.0444   939.8456     1.3968  104.0133  0.1214   
 776   2899.68  2492.240  2127.9555  1011.1924     1.0863  107.1522  0.1261   
 777   2957.04  2525.160  2190.1555  1298.8207     1.3947  105.7411  0.1242   
 778   3015.41  2474.940  2174.7555  1206.3506     1.4202  104.2622  0.1246   
 779   3051.05  2582.080  2190.1555  1298.8207     1.3947  105.7411  0.1242   
 780   3055.66  2625.200  2211.7222  1335.4837     1.3922  100.9256  0.1232   
 781   3043.14  2517.250  2190.1555  1298.8207     1.3947  105.7411  0.1242   
 782   2982.59  2466.860  2117.5889   894.0996     1.4330  106.4944  0.1253   
 783   3068.42  2570.500  2190.1555  1298.8207     1.3947  105.7411  0.1242   
 784   3068.45  2575.330  2130.0556   874.9165     1.2545  105.9011  0.1253   
 785   2978.45  2504.940  2181.0444   939.8456     1.3968  104.0133  0.1214   
 786   2928.55  2579.840  2181.0444   939.8456     1.3968  104.0133  0.1214   
 787   3049.58  2432.090  2169.1111  1431.8740     1.0321  100.1744  0.1252   
 788   2999.24  2435.470  2175.9667  1539.5734     1.4139   98.5822  0.1254   
 789   3018.82  2470.210  2175.9667  1539.5734     1.4139   98.5822  0.1254   
 790   2982.16  2574.450  2181.0444   939.8456     1.3968  104.0133  0.1214   
 791   2958.83  2488.500  2197.5222  1373.0077     1.1369  106.0733  0.1240   
 792   2975.57  2566.790  2197.5222  1373.0077     1.1369  106.0733  0.1240   
 793   2925.62  2501.910  2197.5222  1373.0077     1.1369  106.0733  0.1240   
 794   3043.70  2535.710  2127.9555  1011.1924     1.0863  107.1522  0.1261   
 796   3090.47  2314.520  2169.1111  1431.8740     1.0321  100.1744  0.1252   
 798   3064.76  2510.720  2194.5889  1614.2776     1.2686   97.7622  0.1247   
 799   2977.39  2535.070  2194.5889  1614.2776     1.2686   97.7622  0.1247   
 800   3010.26  2508.800  2170.0666  1364.5157     1.5447   96.7700  0.1230   
 801   2947.39  2308.450  2218.6333   870.5620     1.3084  106.9089  0.1195   
 802   3036.64  2483.930  2194.5889  1614.2776     1.2686   97.7622  0.1247   
 803   3045.07  2618.250  2175.9667  1539.5734     1.4139   98.5822  0.1254   
 804   3085.93  2518.470  2204.5445  2076.6730     1.0961   95.9222  0.1251   
 805   2952.31  2453.530  2188.7222  1325.5230     1.1721  105.1700  0.1250   
 806   3047.06  2531.160  2204.5445  2076.6730     1.0961   95.9222  0.1251   
 807   2991.25  2519.260  2194.8000  1456.0760     1.5130   99.0344  0.1233   
 808   3045.33  2472.230  2194.5889  1614.2776     1.2686   97.7622  0.1247   
 809   3208.47  2294.010  2194.8000  1456.0760     1.5130   99.0344  0.1233   
 810   2939.93  2595.820  2204.5445  2076.6730     1.0961   95.9222  0.1251   
 811   3042.85  2403.210  2204.5445  2076.6730     1.0961   95.9222  0.1251   
 812   3039.04  2598.940  2187.8111  2228.7377     1.4990   90.4933  0.1240   
 813   3050.36  2846.440  2220.0445  2253.2847     1.7112   88.0444  0.1222   
 814   3012.09  2551.080  2216.7333  1748.0885     1.1127   97.5822  0.1242   
 815   2924.39  2470.950  2188.7222  1325.5230     1.1721  105.1700  0.1250   
 816   2988.81  2461.820  2187.8111  2228.7377     1.4990   90.4933  0.1240   
 817   2958.04  2502.890  2134.7445  1244.2899     1.1827   99.5133  0.1249   
 818   3087.39  2392.000  2188.6000  2115.2244     1.3174   93.1656  0.1255   
 819   3064.71  2478.510  2216.7333  1748.0885     1.1127   97.5822  0.1242   
 820   2946.62  2470.260  2187.8111  2228.7377     1.4990   90.4933  0.1240   
 821   3009.71  2565.530  2224.6778  1308.6479     1.3907  101.1333  0.1208   
 822   3105.30  2435.900  2194.5889  1614.2776     1.2686   97.7622  0.1247   
 823   3021.77  2544.100  2204.5445  2076.6730     1.0961   95.9222  0.1251   
 824   2927.54  2563.080  2188.7111  1115.4922     1.5156  102.0011  0.1225   
 825   2951.83  2484.440  2218.9445  1491.3268     1.4420   96.4389  0.1210   
 827   3073.73  2504.010  2218.8778  1188.2630     1.2557  100.9444  0.1212   
 828   3103.85  2256.120  2194.5889  1614.2776     1.2686   97.7622  0.1247   
 829   3002.22  2462.060  2202.1222  1833.3772     1.8005   95.8500  0.1242   
 830   2968.33  2476.580  2216.7333  1748.0885     1.1127   97.5822  0.1242   
 832   3072.03  2500.680  2205.7445  1363.1048     1.0518  101.8644  0.1220   
 833   3021.83  2419.830  2205.7445  1363.1048     1.0518  101.8644  0.1220   
 834   3006.95  2435.340  2189.8111  1084.6502     1.1993  104.8856  0.1234   
 835   3003.72  2537.660  2210.7778  2008.9216     1.1351   91.1078  0.1240   
 836   2953.59  2504.860  2224.6778  1308.6479     1.3907  101.1333  0.1208   
 837   3086.52  2360.040  2204.2333  2110.8288     1.6392   89.0356  0.1245   
 838   3048.76  2545.680  2224.6778  1308.6479     1.3907  101.1333  0.1208   
 839   2984.06  2619.600  2225.0222  1730.8480     1.5333   98.5978  0.1232   
 840   2947.87  2460.050  2204.2333  2110.8288     1.6392   89.0356  0.1245   
 841   3008.28  2504.210  2202.2556  1914.0689     1.6013   94.6922  0.1242   
 842   3084.81  2445.200  2224.6778  1308.6479     1.3907  101.1333  0.1208   
 843   3034.50  2431.350  2220.0445  2253.2847     1.7112   88.0444  0.1222   
 844   3001.26  2519.920  2224.6778  1308.6479     1.3907  101.1333  0.1208   
 845   3017.39  2544.640  2246.5778  1963.8016     1.1665   96.7089  0.1209   
 846   3018.80  2440.930  2195.6667  1333.7299     1.0772   98.9844  0.1223   
 847   2931.05  2528.740  2205.7000  1072.8058     1.2856  100.8511  0.1216   
 848   3004.08  2514.670  2200.2333  1173.8377     1.3281  101.6111  0.1211   
 849   2893.07  2596.630  2214.2889   988.2071     1.2513  101.7044  0.1209   
 850   3215.87  2453.970  2212.8667  1066.9539     0.8161  101.6156  0.1203   
 851   3004.39  2468.560  2200.9556  1126.8678     0.7860  100.3700  0.1215   
 852   2929.93  2517.500  2218.8778  1188.2630     1.2557  100.9444  0.1212   
 853   3089.10  2414.290  2200.9556  1126.8678     0.7860  100.3700  0.1215   
 854   2993.19  2577.230  2204.2333  2110.8288     1.6392   89.0356  0.1245   
 855   3048.39  2489.150  2205.7445  1363.1048     1.0518  101.8644  0.1220   
 856   2951.85  2525.000  2189.5777  1320.3197     1.3459  100.7744  0.1234   
 857   2889.67  2254.990  2199.6334  1242.8420     1.4083   99.2178  0.1221   
 858   2938.02  2499.680  2114.6667  1549.4874     1.3393   98.7844  0.1262   
 859   2919.71  2420.320  2134.7445  1244.2899     1.1827   99.5133  0.1249   
 860   3081.34  2453.780  2175.3445  1147.3421     1.1419  107.1422  0.1251   
 861   3069.72  2372.790  2127.9555  1011.1924     1.0863  107.1522  0.1261   
 862   2982.62  2474.880  2114.6667  1549.4874     1.3393   98.7844  0.1262   
 863   2997.94  2471.970  2214.1667  1705.2046     0.9113  100.4633  0.1231   
 864   2904.21  2530.630  2188.6000  2115.2244     1.3174   93.1656  0.1255   
 865   2949.75  2528.500  2246.5778  1963.8016     1.1665   96.7089  0.1209   
 866   2979.77  2493.030  2208.6556  1728.7576     1.5137   97.0367  0.1240   
 867   3006.33  2424.130  2189.3556  2363.6412     2.1415   83.4233  0.1246   
 868   3043.78  2522.760  2192.0000  2085.6871     1.1723   94.9300  0.1238   
 869   3019.38  2499.750  2200.9556  1126.8678     0.7860  100.3700  0.1215   
 870   3085.10  2500.130  2199.6334  1242.8420     1.4083   99.2178  0.1221   
 872   2982.07  2447.060  2199.6334  1242.8420     1.4083   99.2178  0.1221   
 873   3116.38  2404.130  2212.8667  1066.9539     0.8161  101.6156  0.1203   
 874   3123.02  2488.960  2240.8666  2048.2566     1.1620   86.3822  0.1233   
 875   3078.60  2441.680  2213.5111  1183.4356     0.7531  103.0911  0.1199   
 876   2921.73  2515.910  2189.3556  2363.6412     2.1415   83.4233  0.1246   
 877   3060.41  2453.750  2196.0000  1230.0293     0.7684   97.5578  0.1219   
 878   3030.23  2568.320  2196.0000  1230.0293     0.7684   97.5578  0.1219   
 879   2998.89  2532.660  2189.3556  2363.6412     2.1415   83.4233  0.1246   
 880   3073.25  2440.080  2240.8666  2048.2566     1.1620   86.3822  0.1233   
 881   3078.77  2533.040  2187.4111  1942.3069     1.1864   88.0911  0.1245   
 882   3007.47  2571.180  2187.4111  1942.3069     1.1864   88.0911  0.1245   
 883   3019.98  2515.630  2196.0000  1230.0293     0.7684   97.5578  0.1219   
 884   3056.28  2464.400  2211.7778  1177.4224     1.3377   98.9989  0.1200   
 885   3011.49  2532.450  2191.1333  2197.6570     1.1569   89.7222  0.1251   
 886   2993.79  2510.590  2211.7778  1177.4224     1.3377   98.9989  0.1200   
 887   3007.75  2535.140  2216.5000  1111.5436     0.8373   99.9867  0.1205   
 888   3083.14  2456.750  2216.5000  1111.5436     0.8373   99.9867  0.1205   
 889   3094.08  2664.520  2202.5556  1081.8043     1.2913  101.8922  0.1205   
 890   3071.05  2642.150  2200.9889  1054.5240     1.3830  100.1800  0.1201   
 891   3074.96  2448.820  2315.2667  2360.1325     1.1259   90.1144  0.1160   
 892   3001.95  2598.220  2213.2111  2070.7147     1.9705   87.7411  0.1232   
 893   3197.43  2405.620  2171.5000  1028.4440     0.7899  101.5122  0.1224   
 894   3001.60  2500.260  2171.5000  1028.4440     0.7899  101.5122  0.1224   
 895   2956.77  2518.500  2187.4111  1942.3069     1.1864   88.0911  0.1245   
 896   3013.86  2527.200  2195.6667  1333.7299     1.0772   98.9844  0.1223   
 897   2950.92  2439.590  2213.5111  1183.4356     0.7531  103.0911  0.1199   
 898   2948.09  2480.050  2200.9889  1054.5240     1.3830  100.1800  0.1201   
 899   3058.08  2524.600  2192.3778  1110.5453     0.8147   99.2922  0.1226   
 900   3066.38  2473.100  2171.5000  1028.4440     0.7899  101.5122  0.1224   
 901   3043.18  2545.530  2192.3778  1110.5453     0.8147   99.2922  0.1226   
 902   2978.57  2559.800  2180.5556  1165.1351     0.7892  101.4578  0.1226   
 903   2825.67  2286.090  2210.2778  2120.5760     1.0700   95.1089  0.1230   
 904   2993.98  2456.250  2191.1333  2197.6570     1.1569   89.7222  0.1251   
 905   2948.57  2526.190  2216.5000  1111.5436     0.8373   99.9867  0.1205   
 906   2966.39  2507.220  2171.5000  1028.4440     0.7899  101.5122  0.1224   
 907   3086.05  2417.340  2216.5000  1111.5436     0.8373   99.9867  0.1205   
 908   2972.52  2454.800  2180.5556  1165.1351     0.7892  101.4578  0.1226   
 909   2954.92  2412.760  2171.5000  1028.4440     0.7899  101.5122  0.1224   
 910   2986.28  2483.710  2200.9889  1054.5240     1.3830  100.1800  0.1201   
 911   3111.64  2387.230  2206.5777   978.7832     1.3316  100.6189  0.1203   
 912   3093.57  2492.270  2210.2778  2120.5760     1.0700   95.1089  0.1230   
 913   2980.66  2437.210  2200.9889  1054.5240     1.3830  100.1800  0.1201   
 915   3184.83  2400.400  2189.9667  1046.6212     0.8662  102.3622  0.1208   
 916   2998.01  2479.770  2189.9667  1046.6212     0.8662  102.3622  0.1208   
 917   2973.56  2536.680  2189.9667  1046.6212     0.8662  102.3622  0.1208   
 918   3221.21  2391.200  2189.9667  1046.6212     0.8662  102.3622  0.1208   
 919   3000.38  2593.090  2191.1333  2197.6570     1.1569   89.7222  0.1251   
 920   2964.63  2494.750  2201.0667  1285.2144     1.3168  101.5122  0.1224   
 921   2990.58  2506.560  2194.3778  1265.1715     0.7588  100.1833  0.1204   
 922   3007.00  2572.620  2213.2111  2070.7147     1.9705   87.7411  0.1232   
 923   2982.10  2572.780  2201.0667  1285.2144     1.3168  101.5122  0.1224   
 925   3013.66  2526.440  2185.2111  1141.6306     0.8447  100.5978  0.1217   
 927   3084.82  2387.420  2171.5000  1028.4440     0.7899  101.5122  0.1224   
 928   2955.87  2541.890  2201.0667  1285.2144     1.3168  101.5122  0.1224   
 930   3037.62  2431.930  2194.9556  2341.7833     2.3917   86.8100  0.1231   
 931   3097.63  2461.580  2194.9556  2341.7833     2.3917   86.8100  0.1231   
 932   3084.23  2433.670  2194.9556  2341.7833     2.3917   86.8100  0.1231   
 933   2919.84  2484.900  2213.2111  2070.7147     1.9705   87.7411  0.1232   
 934   3154.13  2368.120  2189.9667  1046.6212     0.8662  102.3622  0.1208   
 935   3054.81  2430.600  2060.6600  1410.3599     3.8894   96.6256  0.1181   
 936   2913.40  2496.010  2227.2444  1129.8739     1.2803  102.7956  0.1188   
 937   3150.13  2531.810  2211.4334  2130.9862     1.1351   86.7956  0.1237   
 938   3059.09  2555.690  2060.6600  1410.3599     3.8894   96.6256  0.1181   
 939   3076.81  2521.250  2195.5000  1027.5153     1.3548  105.8378  0.1203   
 940   3015.77  2440.940  2194.3778  1265.1715     0.7588  100.1833  0.1204   
 941   2958.22  2467.500  2227.2444  1129.8739     1.2803  102.7956  0.1188   
 942   2945.90  2539.580  2227.2444  1129.8739     1.2803  102.7956  0.1188   
 943   3087.24  2470.280  2060.6600  1410.3599     3.8894   96.6256  0.1181   
 944   2964.71  2495.700  2202.3556  1020.9264     0.8676  102.7833  0.1200   
 945   2964.00  2475.500  2227.2444  1129.8739     1.2803  102.7956  0.1188   
 946   2980.29  2508.520  2060.6600  1410.3599     3.8894   96.6256  0.1181   
 947   3048.93  2451.820  2197.3222  1586.9476     2.0983   97.6267  0.1249   
 948   3055.29  2705.000  2201.0667  1285.2144     1.3168  101.5122  0.1224   
 949   3001.53  2540.070  2060.6600  1410.3599     3.8894   96.6256  0.1181   
 950   3077.04  2469.100  2060.6600  1410.3599     3.8894   96.6256  0.1181   
 951   3062.47  2442.490  2168.7333   999.5929     1.3739  104.8544  0.1217   
 952   3071.15  2501.000  2228.7222  2941.8341     1.7383   94.6056  0.1248   
 953   3075.67  2516.470  2213.9111  1973.9690     1.2541   96.9456  0.1238   
 954   2971.86  2528.730  2204.5000  1768.1986     1.3398   91.8511  0.1257   
 955   2892.86  2538.740  2197.3222  1586.9476     2.0983   97.6267  0.1249   
 956   2996.69  2461.170  2211.4334  2130.9862     1.1351   86.7956  0.1237   
 957   2999.04  2438.560  2184.7222  1009.0221     1.3187  101.9178  0.1219   
 958   3037.98  2370.440  2168.7333   999.5929     1.3739  104.8544  0.1217   
 959   3024.67  2620.960  2206.1444  1876.9899     2.0607   95.9511  0.1246   
 960   3034.43  2586.860  2197.3222  1586.9476     2.0983   97.6267  0.1249   
 961   3044.39  2526.850  2211.4334  2130.9862     1.1351   86.7956  0.1237   
 962   3040.48  2512.030  2206.1444  1876.9899     2.0607   95.9511  0.1246   
 963   3082.05  2447.430  2210.2778  2120.5760     1.0700   95.1089  0.1230   
 964   3101.48  2577.440  2168.7333   999.5929     1.3739  104.8544  0.1217   
 965   2947.91  2456.680  2195.5000  1027.5153     1.3548  105.8378  0.1203   
 966   2928.24  2524.910  2227.2444  1129.8739     1.2803  102.7956  0.1188   
 967   3074.24  2426.860  2213.9111  1973.9690     1.2541   96.9456  0.1238   
 968   2941.85  2462.750  2172.3111  1202.5347     1.2955  101.3556  0.1212   
 969   2945.05  2469.590  2204.5000  1768.1986     1.3398   91.8511  0.1257   
 970   3066.18  2539.010  2180.5556  1165.1351     0.7892  101.4578  0.1226   
 971   3087.64  2403.240  2197.3222  1586.9476     2.0983   97.6267  0.1249   
 972   3043.12  2473.670  2180.5556  1165.1351     0.7892  101.4578  0.1226   
 973   2971.54  2457.640  2180.5556  1165.1351     0.7892  101.4578  0.1226   
 974   2931.17  2467.400  2206.1444  1876.9899     2.0607   95.9511  0.1246   
 975   3055.54  2508.880  2168.7333   999.5929     1.3739  104.8544  0.1217   
 976   2949.99  2498.670  2204.1555  1276.2565     0.7880  100.6922  0.1198   
 977   2927.94  2481.520  2206.5777   978.7832     1.3316  100.6189  0.1203   
 978   3041.48  2494.580  2213.9111  1973.9690     1.2541   96.9456  0.1238   
 979   3188.90  2445.410  2213.9111  1973.9690     1.2541   96.9456  0.1238   
 980   3083.31  2377.310  2172.3111  1202.5347     1.2955  101.3556  0.1212   
 981   3075.10  2614.510  2168.7333   999.5929     1.3739  104.8544  0.1217   
 982   3012.68  2420.350  2172.3111  1202.5347     1.2955  101.3556  0.1212   
 983   2981.70  2415.790  2201.8666  1185.4396     1.3996   99.9211  0.1199   
 984   3000.77  2428.200  2201.8666  1185.4396     1.3996   99.9211  0.1199   
 985   3058.91  2498.770  2060.6600  1410.3599     3.8894   96.6256  0.1181   
 986   2999.96  2510.240  2206.1444  1876.9899     2.0607   95.9511  0.1246   
 987   3079.44  2395.390  2209.0889  1459.4739     2.1612   98.9011  0.1242   
 988   3234.24  2347.020  2202.3556  1854.7633     1.3593  103.2378  0.1239   
 989   2994.15  2522.160  2211.6000  2122.2580     1.4470   94.2089  0.1242   
 990   3043.23  2411.350  2172.3111  1202.5347     1.2955  101.3556  0.1212   
 991   2940.53  2517.420  2206.1444  1876.9899     2.0607   95.9511  0.1246   
 992   3040.06  2520.630  2201.8666  1185.4396     1.3996   99.9211  0.1199   
 993   2966.26  2524.570  2213.9111  1973.9690     1.2541   96.9456  0.1238   
 994   3086.92  2547.790  2215.5111  2039.3196     1.2150   99.4189  0.1238   
 995   2982.40  2528.210  2201.0667  1285.2144     1.3168  101.5122  0.1224   
 996   2901.62  2569.450  2223.9000  1745.3724     1.9974   96.7567  0.1241   
 997   3053.64  2351.940  2206.1444  1876.9899     2.0607   95.9511  0.1246   
 998   2938.84  2453.250  2188.5444  1251.2688     0.7980  103.5944  0.1199   
 999   3045.11  2444.870  2215.1778  2192.1867     1.8829   85.6589  0.1237   
 1000  3033.11  2435.140  2172.3111  1202.5347     1.2955  101.3556  0.1212   
 1001  3022.60  2533.920  2184.7222  1009.0221     1.3187  101.9178  0.1219   
 1002  2976.58  2495.160  2215.5111  2039.3196     1.2150   99.4189  0.1238   
 1003  2917.47  2481.560  2247.3666  1039.7410     1.3594  104.3600  0.1171   
 1004  3071.03  2483.660  2215.5111  2039.3196     1.2150   99.4189  0.1238   
 1005  2945.22  2570.440  2194.9556  2341.7833     2.3917   86.8100  0.1231   
 1006  3072.86  2434.600  2206.1444  1876.9899     2.0607   95.9511  0.1246   
 1007  3075.12  2476.800  2189.3556  2363.6412     2.1415   83.4233  0.1246   
 1008  3004.57  2460.590  2166.2111   894.8612     1.4073  105.6256  0.1219   
 1009  3055.47  2423.380  2226.3111  2252.1538     1.2295   92.6178  0.1239   
 1010  2965.50  2500.250  2206.1444  1876.9899     2.0607   95.9511  0.1246   
 1011  2957.70  2509.620  2226.3111  2252.1538     1.2295   92.6178  0.1239   
 1012  2992.40  2590.800  2223.9000  1745.3724     1.9974   96.7567  0.1241   
 1013  2933.15  2497.760  2218.6223  1416.6631     1.9010   99.6067  0.1260   
 1014  3099.58  2397.990  2153.9778  1192.6994     1.3522  100.9367  0.1225   
 1015  2990.68  2449.840  2172.3111  1202.5347     1.2955  101.3556  0.1212   
 1016  3061.73  2463.720  2153.9778  1192.6994     1.3522  100.9367  0.1225   
 1017  3053.34  2353.230  2211.6000  2122.2580     1.4470   94.2089  0.1242   
 1018  2972.60  2622.600  2169.8334  1301.9348     0.8012  101.9733  0.1216   
 1019  3083.80  2494.570  2201.8666  1185.4396     1.3996   99.9211  0.1199   
 1020  2994.92  2516.350  2146.7778  3715.0417     0.9069   84.9267  0.1286   
 1021  3026.09  2553.900  2215.5111  2039.3196     1.2150   99.4189  0.1238   
 1022  2994.06  2548.910  2247.3666  1039.7410     1.3594  104.3600  0.1171   
 1023  2978.33  2405.530  2203.4445  1915.7558     1.2616  101.1833  0.1261   
 1024  3044.47  2361.710  2192.5556  1908.5530     1.3766   96.1400  0.1258   
 1025  2975.13  2557.370  2194.9555  1108.2246     1.2476  102.2822  0.1202   
 1026  3014.07  2554.210  2226.0667  2073.6381     1.2717   95.7489  0.1238   
 1027  3064.09  2488.150  2231.6111  2005.8966     1.2969   93.7522  0.1234   
 1028  3070.16  2561.210  2247.3666  1039.7410     1.3594  104.3600  0.1171   
 1030  3073.48  2467.180  2200.2000  1121.1875     1.3171  103.8978  0.1191   
 1031  3027.61  2430.030  2219.7667  2086.4710     1.3381   98.8900  0.1234   
 1032  2950.97  2533.950  2249.2556  2065.0624     2.1216   95.7967  0.1222   
 1033  3093.12  2500.900  2219.7667  2086.4710     1.3381   98.8900  0.1234   
 1034  2988.76  2497.580  2153.9778  1192.6994     1.3522  100.9367  0.1225   
 1035  2990.69  2428.850  2249.2556  2065.0624     2.1216   95.7967  0.1222   
 1036  2978.77  2441.170  2254.7111  1981.2999     2.1046   90.9167  0.1224   
 1037  3036.99  2448.380  2194.9555  1108.2246     1.2476  102.2822  0.1202   
 1038  3006.75  2517.090  2254.7111  1981.2999     2.1046   90.9167  0.1224   
 1039  2960.63  2570.460  2206.1444  1876.9899     2.0607   95.9511  0.1246   
 1040  3021.61  2440.290  2227.2222  2043.5876     1.1892   95.5267  0.1253   
 1041  2960.56  2465.420  2213.9111  1973.9690     1.2541   96.9456  0.1238   
 1042  2978.15  2506.000  2153.9778  1192.6994     1.3522  100.9367  0.1225   
 1043  3053.48  2674.540  2231.6111  2005.8966     1.2969   93.7522  0.1234   
 1044  3031.22  2538.560  2211.5222  1079.7336     0.8126  105.1278  0.1190   
 1045  2935.34  2551.270  2276.4556  2148.5397     1.2317   93.6778  0.1199   
 1046  2972.34  2465.620  2231.6111  2005.8966     1.2969   93.7522  0.1234   
 1047  3051.68  2505.270  2219.7667  2086.4710     1.3381   98.8900  0.1234   
 1048  3056.49  2488.900  2192.5556  1908.5530     1.3766   96.1400  0.1258   
 1049  3034.13  2481.640  2219.7667  2086.4710     1.3381   98.8900  0.1234   
 1050  3090.72  2408.280  2226.0667  2073.6381     1.2717   95.7489  0.1238   
 1051  3012.45  2422.480  2208.5222  1838.7054     1.1571   95.2056  0.1249   
 1052  2990.29  2522.350  2200.2000  1121.1875     1.3171  103.8978  0.1191   
 1053  2997.67  2446.470  2147.1111  1081.8758     1.3550  103.2778  0.1226   
 1054  3040.28  2577.440  2233.6334  1962.0026     1.2120   99.0133  0.1222   
 1055  2995.73  2515.830  2231.6111  2005.8966     1.2969   93.7522  0.1234   
 1056  2908.22  2481.320  2168.7333   999.5929     1.3739  104.8544  0.1217   
 1057  3005.31  2409.490  2200.2000  1121.1875     1.3171  103.8978  0.1191   
 1058  3005.52  2444.920  2227.2222  2043.5876     1.1892   95.5267  0.1253   
 1059  3208.56  2398.660  2147.1111  1081.8758     1.3550  103.2778  0.1226   
 1060  3063.64  2519.770  2231.6111  2005.8966     1.2969   93.7522  0.1234   
 1061  3150.47  2621.330  2231.6111  2005.8966     1.2969   93.7522  0.1234   
 1063  2940.74  2596.710  2200.2000  1121.1875     1.3171  103.8978  0.1191   
 1064  2981.49  2536.820  2147.1111  1081.8758     1.3550  103.2778  0.1226   
 1065  3026.53  2558.280  2147.1111  1081.8758     1.3550  103.2778  0.1226   
 1066  3054.95  2531.780  2178.5889  1135.3544     0.8614  103.7022  0.1205   
 1067  3060.97  2579.880  2203.4445  1915.7558     1.2616  101.1833  0.1261   
 1068  2996.61  2496.090  2178.4667   993.1907     0.7649  104.6267  0.1215   
 1069  2994.22  2541.270  2222.0111  1229.4964     0.7647  105.2711  0.1183   
 1070  2945.66  2572.150  2185.1000  1201.0491     0.7821  105.8489  0.1208   
 1071  3160.57  2569.690  2244.9778  2208.4483     1.9074   87.2789  0.1217   
 1072  3180.09  2327.290  2200.2000  1121.1875     1.3171  103.8978  0.1191   
 1073  3069.30  2501.130  2200.2000  1121.1875     1.3171  103.8978  0.1191   
 1074  3136.15  2504.960  2178.4667   993.1907     0.7649  104.6267  0.1215   
 1075  2988.21  2490.260  2219.7667  2086.4710     1.3381   98.8900  0.1234   
 1076  3068.67  2517.040  2258.2778  2073.0764     1.2329   96.9478  0.1208   
 1077  3225.61  2493.130  2266.8333  2040.1937     1.6558   94.8489  0.1216   
 1078  2936.51  2483.290  2266.8333  2040.1937     1.6558   94.8489  0.1216   
 1079  3170.24  2485.640  2186.9000   936.4327     1.3493  104.3367  0.1206   
 1080  3039.40  2473.480  2178.4667   993.1907     0.7649  104.6267  0.1215   
 1081  3078.20  2573.390  2187.6667   991.7358     1.2727  105.0156  0.1212   
 1082  3010.30  2421.700  2249.2556  2065.0624     2.1216   95.7967  0.1222   
 1083  3016.34  2538.440  2266.8333  2040.1937     1.6558   94.8489  0.1216   
 1084  2978.96  2499.405  2250.3667  1847.0925     2.0218   91.0589  0.1214   
 1085  2911.84  2507.040  2176.4667  1024.0477     1.2782  105.7178  0.1203   
 1086  2888.58  2481.370  2200.2000  1121.1875     1.3171  103.8978  0.1191   
 1087  3043.77  2588.520  2219.7667  2086.4710     1.3381   98.8900  0.1234   
 1088  2888.90  2553.540  2187.6667   991.7358     1.2727  105.0156  0.1212   
 1089  3059.68  2463.230  2227.9778  3619.7397     1.6656   87.2200  0.1239   
 1090  3009.94  2524.620  2200.2000  1121.1875     1.3171  103.8978  0.1191   
 1091  2966.24  2540.030  2169.9444  1095.3519     1.3425  104.0144  0.1208   
 1092  3186.16  2316.010  2169.9444  1095.3519     1.3425  104.0144  0.1208   
 1093  3162.14  2266.530  2169.9444  1095.3519     1.3425  104.0144  0.1208   
 1094  3018.33  2438.580  2169.9444  1095.3519     1.3425  104.0144  0.1208   
 1095  3104.27  2448.680  2169.9444  1095.3519     1.3425  104.0144  0.1208   
 1096  3062.78  2491.310  2266.8333  2040.1937     1.6558   94.8489  0.1216   
 1097  3070.78  2592.560  2222.0111  1229.4964     0.7647  105.2711  0.1183   
 1098  3072.44  2458.090  2179.1111  1305.8262     0.8117  103.5033  0.1203   
 1099  3160.54  2304.830  2233.6334  1962.0026     1.2120   99.0133  0.1222   
 1100  2880.58  2489.760  2256.6000  2365.4787     1.1384   85.4278  0.1205   
 1101  3099.37  2450.290  2178.5889  1135.3544     0.8614  103.7022  0.1205   
 1102  2998.44  2509.540  2249.2556  2065.0624     2.1216   95.7967  0.1222   
 1103  3065.12  2627.290  2249.2556  2065.0624     2.1216   95.7967  0.1222   
 1104  3088.41  2443.660  2249.2556  2065.0624     2.1216   95.7967  0.1222   
 1105  3018.57  2500.010  2137.6111   950.5720     0.9199  105.1889  0.1251   
 1106  3034.01  2545.830  2256.6000  2365.4787     1.1384   85.4278  0.1205   
 1107  2991.47  2493.200  2276.4556  2148.5397     1.2317   93.6778  0.1199   
 1108  3052.93  2563.530  2260.7333  1969.7867     1.2109   91.3367  0.1207   
 1109  2885.80  2607.010  2231.9555  1943.0435     1.2758   96.8789  0.1224   
 1110  3151.80  2571.060  2244.9778  2208.4483     1.9074   87.2789  0.1217   
 1111  3010.83  2501.260  2158.7000   902.9779     1.3177  106.3611  0.1228   
 1112  3135.88  2578.760  2227.9778  3619.7397     1.6656   87.2200  0.1239   
 1113  2916.93  2424.520  2231.9555  1943.0435     1.2758   96.8789  0.1224   
 1114  2910.99  2533.390  2186.9000   936.4327     1.3493  104.3367  0.1206   
 1115  3182.35  2520.400  2187.7888  1350.3395     0.7945  101.9267  0.1200   
 1116  3046.14  2475.590  2231.9555  1943.0435     1.2758   96.8789  0.1224   
 1117  2983.56  2595.740  2206.1444  1876.9899     2.0607   95.9511  0.1246   
 1118  3052.22  2496.490  2219.7667  2086.4710     1.3381   98.8900  0.1234   
 1119  3059.62  2376.270  2266.8333  2040.1937     1.6558   94.8489  0.1216   
 1120  3002.82  2450.490  2269.2556  2023.4334     1.1215   91.6400  0.1194   
 1121  3038.68  2512.100  2182.6222  1209.5092     0.8975  104.6689  0.1226   
 1122  2935.99  2577.580  2276.4556  2148.5397     1.2317   93.6778  0.1199   
 1123  3030.88  2398.400  2211.4334  2130.9862     1.1351   86.7956  0.1237   
 1124  3033.32  2562.200  2221.9778  2434.5273     1.2596   86.1700  0.1219   
 1125  3180.23  2517.210  2147.1111  1081.8758     1.3550  103.2778  0.1226   
 1126  3038.74  2432.830  2222.0111  1229.4964     0.7647  105.2711  0.1183   
 1127  3230.20  2476.100  2276.4556  2148.5397     1.2317   93.6778  0.1199   
 1128  3041.09  2630.420  2193.3555  1087.7379     1.2169  104.4778  0.1195   
 1129  3078.17  2450.310  2260.7333  1969.7867     1.2109   91.3367  0.1207   
 1130  2995.26  2447.220  2185.1000  1201.0491     0.7821  105.8489  0.1208   
 1131  2939.59  2567.480  2165.0333   926.8103     1.3455  102.1067  0.1239   
 1132  2938.90  2532.400  2221.9778  2434.5273     1.2596   86.1700  0.1219   
 1133  3217.96  2485.800  2137.6111   950.5720     0.9199  105.1889  0.1251   
 1134  3056.65  2427.200  2266.8333  2040.1937     1.6558   94.8489  0.1216   
 1135  3013.15  2573.370  2222.0111  1229.4964     0.7647  105.2711  0.1183   
 1136  3074.58  2452.150  2221.9778  2434.5273     1.2596   86.1700  0.1219   
 1137  2895.09  2552.960  2260.7333  1969.7867     1.2109   91.3367  0.1207   
 1138  2939.80  2605.970  2231.6111  2005.8966     1.2969   93.7522  0.1234   
 1139  3052.44  2628.680  2231.6111  2005.8966     1.2969   93.7522  0.1234   
 1140  2972.00  2555.270  2201.0667  1285.2144     1.3168  101.5122  0.1224   
 1141  2952.57  2552.650  2243.8556  2209.0871     2.1422   89.4944  0.1216   
 1142  3031.01  2431.120  2168.7000   895.5849     0.8484  108.0122  0.1241   
 1143  3037.20  2456.060  2178.5889  1135.3544     0.8614  103.7022  0.1205   
 1145  3063.69  2382.180  2244.5000  1830.2771     1.2287   90.0089  0.1207   
 1146  2996.48  2495.580  2158.7000   902.9779     1.3177  106.3611  0.1228   
 1147  2983.51  2522.770  2244.5000  1830.2771     1.2287   90.0089  0.1207   
 1148  3232.64  2512.140  2165.0333   926.8103     1.3455  102.1067  0.1239   
 1149  3054.52  2499.040  2165.0333   926.8103     1.3455  102.1067  0.1239   
 1150  3035.18  2438.110  2185.1000  1201.0491     0.7821  105.8489  0.1208   
 1152  3111.43  2528.730  2260.7333  1969.7867     1.2109   91.3367  0.1207   
 1153  3005.05  2510.260  2168.7000   895.5849     0.8484  108.0122  0.1241   
 1154  3171.54  2435.850  2168.7000   895.5849     0.8484  108.0122  0.1241   
 1155  2990.36  2563.280  2209.9555  2325.1727     2.1685   85.7689  0.1222   
 1156  3023.37  2574.380  2179.1111  1305.8262     0.8117  103.5033  0.1203   
 1157  3254.32  2397.740  2175.1222  1004.5029     1.2964  105.9689  0.1249   
 1158  3026.07  2449.840  2170.7444  1128.0791     1.3753  104.2989  0.1246   
 1159  2932.10  2365.170  2228.9445  1502.7821     1.2895  100.6867  0.1235   
 1160  3035.89  2412.640  2243.8556  2209.0871     2.1422   89.4944  0.1216   
 1161  3098.02  2401.500  2244.5000  1830.2771     1.2287   90.0089  0.1207   
 1162  2982.89  2558.880  2182.0222   939.3851     1.3267  103.1978  0.1207   
 1163  3094.57  2492.720  2188.5111   940.2228     0.8673  105.8678  0.1232   
 1164  3023.95  2462.090  2175.1222  1004.5029     1.2964  105.9689  0.1249   
 1165  3065.48  2539.610  2175.1222  1004.5029     1.2964  105.9689  0.1249   
 1166  3020.29  2452.100  2241.1778  1521.1987     1.3349   98.5900  0.1217   
 1167  3010.75  2523.210  2175.1222  1004.5029     1.2964  105.9689  0.1249   
 1168  3070.94  2502.420  2170.7444  1128.0791     1.3753  104.2989  0.1246   
 1169  3003.05  2508.940  2228.9445  1502.7821     1.2895  100.6867  0.1235   
 1170  2829.84  2595.700  2168.7000   895.5849     0.8484  108.0122  0.1241   
 1171  3169.93  2618.120  2183.0556  1257.1118     0.9762  102.9956  0.1231   
 1172  3044.34  2585.920  2241.1778  1521.1987     1.3349   98.5900  0.1217   
 1173  3025.07  2391.870  2182.6222  1209.5092     0.8975  104.6689  0.1226   
 1174  3195.57  2464.450  2215.0222  1548.9419     1.2557   96.7900  0.1225   
 1175  2939.73  2546.740  2175.2556  1022.1660     1.2833  100.6222  0.1250   
 1176  3097.21  2418.410  2213.3444  1731.1527     2.1197   96.1589  0.1220   
 1177  2960.58  2453.340  2175.2556  1022.1660     1.2833  100.6222  0.1250   
 1178  2923.36  2503.520  2186.9000   936.4327     1.3493  104.3367  0.1206   
 1179  2953.29  2496.320  2215.0222  1548.9419     1.2557   96.7900  0.1225   
 1180  3093.46  2479.170  2221.3667  2490.6947     1.2667   88.6578  0.1220   
 1181  2997.33  2472.600  2221.5667  1593.3485     1.9170  100.7456  0.1227   
 1182  3093.16  2631.920  2260.2222  1602.2369     1.2100   97.1467  0.1196   
 1183  2958.48  2611.650  2188.5111   940.2228     0.8673  105.8678  0.1232   
 1184  3299.40  2490.630  2226.5444  1299.0694     1.9946   98.5244  0.1217   
 1186  3049.97  2441.120  2263.3222  1383.8334     2.3986   85.1778  0.1196   
 1187  2935.84  2549.210  2241.1778  1521.1987     1.3349   98.5900  0.1217   
 1188  3045.88  2603.920  2243.8556  2209.0871     2.1422   89.4944  0.1216   
 1190  3216.05  2257.950  2228.9445  1502.7821     1.2895  100.6867  0.1235   
 1191  3105.06  2266.000  2241.1778  1521.1987     1.3349   98.5900  0.1217   
 1192  2890.83  2474.900  2172.1111   999.5716     1.2689  103.3633  0.1250   
 1193  2973.08  2514.520  2222.0111  1229.4964     0.7647  105.2711  0.1183   
 1194  2952.23  2485.730  2260.2222  1602.2369     1.2100   97.1467  0.1196   
 1195  3036.32  2449.590  2232.0778  1606.3548     2.1498   98.0044  0.1214   
 1196  2942.64  2567.330  2231.9555  1943.0435     1.2758   96.8789  0.1224   
 1197  3050.71  2512.440  2170.7444  1128.0791     1.3753  104.2989  0.1246   
 1198  3075.32  2491.070  2185.1000  1201.0491     0.7821  105.8489  0.1208   
 1199  3030.56  2530.990  2183.0556  1257.1118     0.9762  102.9956  0.1231   
 1200  2972.06  2491.010  2218.7778  2080.6085     2.1254   91.0211  0.1229   
 1201  3095.02  2501.260  2188.5111   940.2228     0.8673  105.8678  0.1232   
 1202  2943.49  2462.850  2260.2222  1602.2369     1.2100   97.1467  0.1196   
 1203  3002.15  2486.940  2218.7778  2080.6085     2.1254   91.0211  0.1229   
 1204  3051.88  2501.910  2239.7222  1604.7488     1.1871   96.9100  0.1222   
 1205  2979.97  2454.380  2227.9778  3619.7397     1.6656   87.2200  0.1239   
 1206  3024.75  2587.370  2269.2556  2023.4334     1.1215   91.6400  0.1194   
 1207  3025.09  2548.760  2209.9555  2325.1727     2.1685   85.7689  0.1222   
 1208  3217.18  2572.850  2178.5889  1135.3544     0.8614  103.7022  0.1205   
 1209  3093.05  2522.030  2170.7444  1128.0791     1.3753  104.2989  0.1246   
 1210  3021.25  2473.650  2221.3667  2490.6947     1.2667   88.6578  0.1220   
 1212  3070.43  2446.380  2255.5222  1763.0739     1.2226  101.5878  0.1200   
 1213  3201.05  2233.290  2172.8111   969.3436     1.2736  102.7367  0.1243   
 1214  2924.32  2629.960  2172.8111   969.3436     1.2736  102.7367  0.1243   
 1215  3051.02  2565.730  2165.5333   994.3737     0.7992  102.9378  0.1247   
 1216  3031.85  2439.420  2172.8111   969.3436     1.2736  102.7367  0.1243   
 1217  3001.83  2545.770  2246.0666  1477.0365     1.8881  103.6100  0.1207   
 1218  3000.22  2499.405  2172.8111   969.3436     1.2736  102.7367  0.1243   
 1219  2952.88  2521.930  2243.8556  2209.0871     2.1422   89.4944  0.1216   
 1220  2973.99  2574.340  2226.4667  1591.2235     2.1717   90.3111  0.1219   
 1221  2965.67  2553.640  2172.8111   969.3436     1.2736  102.7367  0.1243   
 1222  2947.50  2499.405  2173.4889  1145.7970     0.9402  104.0556  0.1247   
 1223  3075.05  2572.480  2173.4889  1145.7970     0.9402  104.0556  0.1247   
 1224  2975.80  2427.360  2191.8445  1093.0818     0.8725  101.4244  0.1229   
 1225  3071.13  2487.120  2199.6556  1140.3983     1.3369  103.0967  0.1227   
 1226  2957.77  2555.410  2199.6556  1140.3983     1.3369  103.0967  0.1227   
 1228  2908.28  2502.090  2198.8556  1031.1917     1.3204  102.7622  0.1235   
 1229  2979.32  2493.340  2226.4667  1591.2235     2.1717   90.3111  0.1219   
 1230  3045.98  2544.850  2253.3444  1873.8678     1.1691   94.9722  0.1210   
 1231  3151.98  2563.750  2175.2556  1022.1660     1.2833  100.6222  0.1250   
 1232  3071.18  2489.860  2195.3000  1151.8233     0.9220  103.5467  0.1232   
 1233  2958.46  2523.780  2171.8556  1156.6018     1.4025  100.1367  0.1243   
 1234  3196.21  2413.390  2255.5222  1763.0739     1.2226  101.5878  0.1200   
 1235  3005.64  2522.400  2173.4889  1145.7970     0.9402  104.0556  0.1247   
 1236  3016.93  2547.200  2173.4889  1145.7970     0.9402  104.0556  0.1247   
 1237  3013.73  2567.760  2172.8111   969.3436     1.2736  102.7367  0.1243   
 1239  2937.77  2532.300  2198.8556  1031.1917     1.3204  102.7622  0.1235   
 1240  2976.56  2506.210  2255.5222  1763.0739     1.2226  101.5878  0.1200   
 1243  3028.50  2486.530  2236.1111  1546.5931     2.0300   90.4233  0.1224   
 1244  3011.82  2446.490  2164.3666   958.7313     1.3409  104.1344  0.1248   
 1245  3022.33  2357.090  2221.3667  2490.6947     1.2667   88.6578  0.1220   
 1246  2929.54  2501.540  2173.2778  1116.2950     0.8525  103.8200  0.1237   
 1247  3118.54  2519.920  2201.6333  1554.3362     2.1917   97.3978  0.1233   
 1248  2916.18  2514.380  2167.0889  1253.2140     1.3679  101.6667  0.1243   
 1249  2995.33  2405.190  2199.6556  1140.3983     1.3369  103.0967  0.1227   
 1250  3185.69  2537.680  2173.2778  1116.2950     0.8525  103.8200  0.1237   
 1251  2971.64  2525.840  2236.1111  1546.5931     2.0300   90.4233  0.1224   
 1252  2958.99  2459.560  2191.8445  1093.0818     0.8725  101.4244  0.1229   
 1253  3028.08  2525.560  2173.2778  1116.2950     0.8525  103.8200  0.1237   
 1255  2970.86  2510.190  2236.1111  1546.5931     2.0300   90.4233  0.1224   
 1256  2914.04  2487.100  2238.1444  1580.6951     1.0062   91.0489  0.1230   
 1257  2889.78  2434.420  2211.4000  1511.7842     1.3004   97.4700  0.1237   
 1258  3133.75  2673.380  2173.2778  1116.2950     0.8525  103.8200  0.1237   
 1259  3190.78  2424.110  2191.2111  1437.5003     2.2073   97.6444  0.1235   
 1260  2953.63  2384.040  2191.2111  1437.5003     2.2073   97.6444  0.1235   
 1261  3182.87  2467.440  2162.1333   998.9095     0.8826  104.9722  0.1246   
 1262  2982.67  2541.550  2173.4889  1145.7970     0.9402  104.0556  0.1247   
 1263  2983.73  2459.870  2162.1333   998.9095     0.8826  104.9722  0.1246   
 1264  3110.08  2507.570  2208.9778  1285.2144     2.4210   97.8400  0.1238   
 1265  3028.40  2485.820  2173.4889  1145.7970     0.9402  104.0556  0.1247   
 1266  2987.39  2548.790  2173.2778  1116.2950     0.8525  103.8200  0.1237   
 1267  3225.54  2500.380  2211.4000  1511.7842     1.3004   97.4700  0.1237   
 1268  3033.78  2412.910  2226.4667  1591.2235     2.1717   90.3111  0.1219   
 1269  3036.15  2477.250  2208.9778  1285.2144     2.4210   97.8400  0.1238   
 1270  3062.36  2396.640  2173.2778  1116.2950     0.8525  103.8200  0.1237   
 1271  3024.55  2423.980  2255.5222  1763.0739     1.2226  101.5878  0.1200   
 1272  2831.18  2579.490  2200.1222  1478.2288     2.2079   97.7778  0.1233   
 1273  3043.08  2484.430  2209.8000  1175.5508     1.1464  112.1367  0.1227   
 1274  3021.37  2523.940  2209.1000  1244.9641     1.3724  107.3156  0.1238   
 1275  3040.17  2582.120  2209.8000  1175.5508     1.1464  112.1367  0.1227   
 1276  2999.28  2517.110  2209.1000  1244.9641     1.3724  107.3156  0.1238   
 1277  3092.74  2524.880  2209.8000  1175.5508     1.1464  112.1367  0.1227   
 1278  3083.67  2465.320  2191.8445  1093.0818     0.8725  101.4244  0.1229   
 1279  2983.17  2488.940  2208.9778  1285.2144     2.4210   97.8400  0.1238   
 1280  3057.69  2464.860  2164.2667   711.0258     1.2884  129.2522  0.1252   
 1281  2961.37  2497.330  2203.1667  1312.9527     0.9448   96.3311  0.1227   
 1282  2927.71  2525.390  2199.6556  1140.3983     1.3369  103.0967  0.1227   
 1283  2865.00  2538.760  2197.2001  1710.2780     2.2328   96.6233  0.1229   
 1284  3050.96  2576.980  2206.7999  1415.2378     0.9778   97.3911  0.1220   
 1285  2999.19  2429.470  2200.1222  1478.2288     2.2079   97.7778  0.1233   
 1286  3040.48  2486.230  2191.6445  1215.8490     1.6211   96.4644  0.1239   
 1287  3007.39  2504.010  2209.8000  1175.5508     1.1464  112.1367  0.1227   
 1288  3018.78  2520.880  2202.4556  1199.9759     2.5874  110.9933  0.1229   
 1289  2939.94  2485.470  2203.1667  1312.9527     0.9448   96.3311  0.1227   
 1290  2971.13  2493.560  2202.4556  1199.9759     2.5874  110.9933  0.1229   
 1291  2889.70  2583.050  2202.4556  1199.9759     2.5874  110.9933  0.1229   
 1292  3202.90  2551.060  2203.1667  1312.9527     0.9448   96.3311  0.1227   
 1293  3154.54  2450.890  2203.1667  1312.9527     0.9448   96.3311  0.1227   
 1294  3020.70  2563.190  2164.2667   711.0258     1.2884  129.2522  0.1252   
 1295  3089.26  2414.630  2209.8000  1175.5508     1.1464  112.1367  0.1227   
 1296  2951.06  2503.180  2228.4778  1721.1108     1.4301   93.6222  0.1221   
 1297  3006.75  2415.790  2206.7999  1415.2378     0.9778   97.3911  0.1220   
 1298  3054.01  2537.390  2209.1000  1244.9641     1.3724  107.3156  0.1238   
 1299  2982.27  2449.210  2164.2667   711.0258     1.2884  129.2522  0.1252   
 1300  2851.68  2503.480  2162.1333   998.9095     0.8826  104.9722  0.1246   
 1301  2870.13  2563.640  2209.1000  1244.9641     1.3724  107.3156  0.1238   
 1304  3014.42  2440.560  2224.1889   997.7953  1112.1600   86.1644  0.1224   
 1305  3011.77  2462.340  2211.4000  1511.7842     1.3004   97.4700  0.1237   
 1306  2941.26  2532.780  2209.1000  1244.9641     1.3724  107.3156  0.1238   
 1307  3045.07  2414.430  2203.1667  1312.9527     0.9448   96.3311  0.1227   
 1308  3282.87  2419.550  2189.6778  1295.2883     2.1394  107.4811  0.1238   
 1309  3014.85  2453.450  2189.6778  1295.2883     2.1394  107.4811  0.1238   
 1310  2918.56  2568.550  2189.6778  1295.2883     2.1394  107.4811  0.1238   
 1311  2908.11  2535.930  2208.9000   934.7558     1.9469  119.3544  0.1222   
 1312  2986.79  2527.320  2189.6778  1295.2883     2.1394  107.4811  0.1238   
 1313  3339.93  2839.460  2189.6778  1295.2883     2.1394  107.4811  0.1238   
 1314  3038.53  2474.550  2205.2555  1412.7131     0.9785   95.2556  0.1216   
 1315  2942.21  2459.020  2210.8556  1665.0062     2.4475   97.1056  0.1227   
 1316  2974.65  2545.400  2210.8556  1665.0062     2.4475   97.1056  0.1227   
 1317  2992.34  2512.630  2236.1111  1546.5931     2.0300   90.4233  0.1224   
 1318  3212.70  2472.920  2210.8556  1665.0062     2.4475   97.1056  0.1227   
 1319  2912.24  2438.170  2203.1667  1312.9527     0.9448   96.3311  0.1227   
 1320  2990.66  2535.430  2213.2889  1346.8352  1112.0769   85.4500  0.1224   
 1321  2907.65  2516.340  2208.9000   934.7558     1.9469  119.3544  0.1222   
 1322  2965.41  2577.390  2210.8556  1665.0062     2.4475   97.1056  0.1227   
 1323  3023.56  2555.880  2205.2555  1412.7131     0.9785   95.2556  0.1216   
 1326  3033.52  2461.080  2203.1667  1312.9527     0.9448   96.3311  0.1227   
 1330  2996.01  2541.130  2205.2555  1412.7131     0.9785   95.2556  0.1216   
 1331  3001.56  2506.620  2223.0444  1194.5986     1.2016  112.5811  0.1229   
 1332  3045.48  2408.850  2223.0444  1194.5986     1.2016  112.5811  0.1229   
 1333  2971.75  2566.300  2206.4000   982.5452     1.1853  116.8167  0.1228   
 1334  2983.98  2744.540  2212.3000  1807.5841     0.9816   92.3700  0.1212   
 1335  3039.64  2529.790  2223.0444  1194.5986     1.2016  112.5811  0.1229   
 1336  2908.47  2551.780  2223.0444  1194.5986     1.2016  112.5811  0.1229   
 1337  2915.54  2496.520  2210.8556  1665.0062     2.4475   97.1056  0.1227   
 1338  2997.45  2555.010  2206.4000   982.5452     1.1853  116.8167  0.1228   
 1339  2990.85  2472.740  2223.0444  1194.5986     1.2016  112.5811  0.1229   
 1340  3029.98  2532.340  2206.4000   982.5452     1.1853  116.8167  0.1228   
 1341  3033.51  2556.350  2207.9555  1283.4368     1.8467   95.4022  0.1216   
 1344  2964.69  2474.970  2212.3000  1807.5841     0.9816   92.3700  0.1212   
 1345  2909.43  2534.490  2223.0444  1194.5986     1.2016  112.5811  0.1229   
 1346  3014.42  2442.510  2212.3000  1807.5841     0.9816   92.3700  0.1212   
 1347  2997.84  2449.850  2223.0444  1194.5986     1.2016  112.5811  0.1229   
 1348  2873.83  2540.170  2223.0444  1194.5986     1.2016  112.5811  0.1229   
 1349  3001.74  2446.550  2207.9555  1283.4368     1.8467   95.4022  0.1216   
 1350  3057.49  2584.520  2207.9555  1283.4368     1.8467   95.4022  0.1216   
 1351  2910.77  2529.900  2212.3000  1807.5841     0.9816   92.3700  0.1212   
 1352  2985.31  2584.240  2209.6667  1586.6088     1.6782   89.7222  0.1213   
 1353  3020.62  2483.200  2206.4000   982.5452     1.1853  116.8167  0.1228   
 1354  3019.44  2494.330  2231.0555  1303.5386     0.9751   95.7878  0.1198   
 1355  3185.63  2386.260  2209.6667  1586.6088     1.6782   89.7222  0.1213   
 1356  3029.94  2517.440  2206.4000   982.5452     1.1853  116.8167  0.1228   
 1357  3117.30  2527.680  2212.3000  1807.5841     0.9816   92.3700  0.1212   
 1358  3084.17  2451.850  2231.0555  1303.5386     0.9751   95.7878  0.1198   
 1359  3005.90  2534.680  2231.0555  1303.5386     0.9751   95.7878  0.1198   
 1360  3024.48  2538.130  2207.9555  1283.4368     1.8467   95.4022  0.1216   
 1361  2981.72  2580.440  2207.9555  1283.4368     1.8467   95.4022  0.1216   
 1362  3001.90  2465.510  2223.0444  1194.5986     1.2016  112.5811  0.1229   
 1366  3013.32  2538.520  2231.0555  1303.5386     0.9751   95.7878  0.1198   
 1367  2886.03  2499.460  2231.0555  1303.5386     0.9751   95.7878  0.1198   
 1368  2916.12  2806.910  2231.0555  1303.5386     0.9751   95.7878  0.1198   
 1369  3103.67  2424.950  2209.4667  1556.3930     1.4884   95.1156  0.1206   
 1370  3222.84  2508.030  2207.9555  1283.4368     1.8467   95.4022  0.1216   
 1371  2979.93  2525.740  2206.9667  1314.6277     1.5512   97.1700  0.1216   
 1372  2881.10  2493.030  2209.6667  1586.6088     1.6782   89.7222  0.1213   
 1373  2945.78  2457.480  2209.4667  1556.3930     1.4884   95.1156  0.1206   
 1374  2996.28  2581.640  2212.3000  1807.5841     0.9816   92.3700  0.1212   
 1375  3062.34  2594.520  2209.4667  1556.3930     1.4884   95.1156  0.1206   
 1376  3028.38  2494.430  2209.6667  1586.6088     1.6782   89.7222  0.1213   
 1377  3017.35  2580.250  2223.0444  1194.5986     1.2016  112.5811  0.1229   
 1378  3013.79  2480.710  2206.4000   982.5452     1.1853  116.8167  0.1228   
 1379  3043.32  2494.820  2206.4000   982.5452     1.1853  116.8167  0.1228   
 1380  3093.31  2456.820  2207.9555  1283.4368     1.8467   95.4022  0.1216   
 1381  2862.69  2593.670  2206.9667  1314.6277     1.5512   97.1700  0.1216   
 1382  3097.89  2583.010  2218.7333  1190.9639     1.7047   97.4300  0.1202   
 1383  2875.39  2499.150  2216.8333  1468.5974     1.7074   95.9856  0.1203   
 1384  3060.16  2465.710  2207.1444  1109.3369     1.5555   97.5678  0.1216   
 1385  2975.87  2514.440  2206.4000   982.5452     1.1853  116.8167  0.1228   
 1386  2955.82  2541.600  2231.0555  1303.5386     0.9751   95.7878  0.1198   
 1387  2994.64  2484.710  2200.2111  1048.3108     1.6485   98.3222  0.1214   
 1388  3053.36  2538.380  2192.1889  1435.9611     2.3870  107.3989  0.1229   
 1389  2871.59  2556.950  2217.0000  1436.8313     1.6604   93.6611  0.1199   
 1390  3001.71  2504.200  2210.7333  1368.7060     2.2449   99.4200  0.1228   
 1391  3042.43  2442.600  2217.0000  1436.8313     1.6604   93.6611  0.1199   
 1392  3001.81  2424.390  2217.0000  1436.8313     1.6604   93.6611  0.1199   
 1393  2915.71  2560.380  2196.1111  1472.6400     1.5599   94.6522  0.1212   
 1394  3056.37  2477.750  2200.2111  1048.3108     1.6485   98.3222  0.1214   
 1395  3015.36  2427.140  2217.5778  1283.3701     2.5361  104.9022  0.1222   
 1396  2981.04  2475.900  2215.8111  1389.3065     2.3183   98.4500  0.1214   
 1397  2984.41  2550.880  2192.1889  1435.9611     2.3870  107.3989  0.1229   
 1398  2852.18  2573.940  2216.8333  1468.5974     1.7074   95.9856  0.1203   
 1399  2864.77  2521.490  2192.1889  1435.9611     2.3870  107.3989  0.1229   
 1401  2870.76  2583.560  2192.1889  1435.9611     2.3870  107.3989  0.1229   
 1402  3014.77  2493.110  2210.7333  1368.7060     2.2449   99.4200  0.1228   
 1403  3027.72  2430.700  2192.1889  1435.9611     2.3870  107.3989  0.1229   
 1404  2893.19  2498.090  2200.2111  1048.3108     1.6485   98.3222  0.1214   
 1405  3006.50  2516.640  2217.5778  1283.3701     2.5361  104.9022  0.1222   
 1406  3094.77  2454.330  2215.8111  1389.3065     2.3183   98.4500  0.1214   
 1407  2985.78  2460.970  2200.2111  1048.3108     1.6485   98.3222  0.1214   
 1408  3171.42  2634.430  2217.0000  1436.8313     1.6604   93.6611  0.1199   
 1409  3068.47  2489.070  2200.2111  1048.3108     1.6485   98.3222  0.1214   
 1410  3012.98  2403.890  2215.8111  1389.3065     2.3183   98.4500  0.1214   
 1411  3060.55  2420.030  2217.0000  1436.8313     1.6604   93.6611  0.1199   
 1412  3025.46  2516.060  2195.9778  1388.2869     1.5605  103.2400  0.1234   
 1413  2973.39  2528.670  2195.9778  1388.2869     1.5605  103.2400  0.1234   
 1414  3014.16  2384.100  2211.0667  1107.1227     1.5788   97.0089  0.1213   
 1415  3037.71  2441.770  2150.0556  1215.2183     1.4756  100.9744  0.1244   
 1416  2999.11  2419.250  2217.0000  1436.8313     1.6604   93.6611  0.1199   
 1417  2991.63  2599.930  2150.0556  1215.2183     1.4756  100.9744  0.1244   
 1418  2958.30  2574.700  2200.2111  1048.3108     1.6485   98.3222  0.1214   
 1419  2998.88  2493.210  2195.9778  1388.2869     1.5605  103.2400  0.1234   
 1420  2975.74  2517.350  2162.5556  1041.0369     1.4305  100.4111  0.1238   
 1421  2939.57  2564.930  2205.5000  1287.3538     2.3842  111.7644  0.1241   
 1422  3095.53  2434.790  2211.0667  1107.1227     1.5788   97.0089  0.1213   
 1423  3073.26  2470.500  2150.0556  1215.2183     1.4756  100.9744  0.1244   
 1424  3077.47  2387.730  2180.8889  1084.7221     0.9085   94.2467  0.1226   
 1425  3169.00  2265.600  2187.9889  1096.3790     0.9065   97.6567  0.1221   
 1426  2923.19  2516.400  2180.8889  1084.7221     0.9085   94.2467  0.1226   
 1427  3003.43  2448.340  2205.5000  1287.3538     2.3842  111.7644  0.1241   
 1428  2953.63  2500.050  2195.9778  1388.2869     1.5605  103.2400  0.1234   
 1429  3034.74  2458.900  2192.1889  1435.9611     2.3870  107.3989  0.1229   
 1430  3038.57  2380.390  2205.5000  1287.3538     2.3842  111.7644  0.1241   
 1431  2902.88  2259.490  2205.5000  1287.3538     2.3842  111.7644  0.1241   
 1432  3001.45  2481.720  2162.5556  1041.0369     1.4305  100.4111  0.1238   
 1433  2975.72  2585.720  2150.0556  1215.2183     1.4756  100.9744  0.1244   
 1434  3356.35  2815.310  2217.0000  1436.8313     1.6604   93.6611  0.1199   
 1435  3053.48  2524.950  2215.8111  1389.3065     2.3183   98.4500  0.1214   
 1436  2816.81  2557.200  2195.9778  1388.2869     1.5605  103.2400  0.1234   
 1437  3100.96  2507.020  2205.5000  1287.3538     2.3842  111.7644  0.1241   
 1439  3004.07  2458.880  2180.8889  1084.7221     0.9085   94.2467  0.1226   
 1440  2974.97  2479.730  2195.9778  1388.2869     1.5605  103.2400  0.1234   
 1441  2987.80  2489.530  2216.8333  1468.5974     1.7074   95.9856  0.1203   
 1442  2955.33  2521.870  2187.9889  1096.3790     0.9065   97.6567  0.1221   
 1444  2990.57  2534.660  2162.5556  1041.0369     1.4305  100.4111  0.1238   
 1445  3219.75  2373.310  2239.4111  1080.4985     1.2079  115.6522  0.1203   
 1446  2962.45  2496.670  2281.5667  1161.3004  1114.5366  108.3144  0.0000   
 1447  2949.12  2553.240  2176.8000  1461.4374     0.8864   96.2367  0.1232   
 1448  3047.38  2597.000  2201.0667  1285.2144     1.3168  101.5122  0.0000   
 1449  3214.14  2445.560  2239.4111  1080.4985     1.2079  115.6522  0.1203   
 1450  3043.18  2574.430  2201.0667  1285.2144     1.3168  101.5122  0.1207   
 1451  2976.92  2510.090  2187.9889  1096.3790     0.9065   97.6567  0.1221   
 1452  3059.44  2423.750  2239.4111  1080.4985     1.2079  115.6522  0.1203   
 1453  2995.84  2504.480  2167.6888  1096.0264     1.5066   99.5989  0.1234   
 1454  2913.84  2393.970  2218.8111  1021.2003     1.8970  115.3800  0.1230   
 1455  3022.41  2507.660  2239.4111  1080.4985     1.2079  115.6522  0.1203   
 1456  3026.86  2497.620  2306.1000     0.0000  1112.4728   88.4722  0.0000   
 1457  3048.58  2531.020  2218.8111  1021.2003     1.8970  115.3800  0.1230   
 1458  2928.89  2504.810  2236.1111  1546.5931     2.0300   90.4233  0.1224   
 1459  3006.80  2536.870  2201.0667  1285.2144     1.3168  101.5122  0.0000   
 1460  3051.80  2527.700  2201.0667  1285.2144     1.3168  101.5122  0.0000   
 1461  2900.29  2451.500  2201.0667  1285.2144     1.3168  101.5122  0.0000   
 1462  3051.10  2509.140  2195.1000  1526.4440     0.8279   96.3100  0.1203   
 1463  2982.03  2507.740  2195.1000  1526.4440     0.8279   96.3100  0.1203   
 1464  3088.81  2590.620  2207.1000  1298.4309     1.0251   96.9478  0.1203   
 1465  2969.65  2449.490  2184.4222  1017.9147     1.3713   99.4367  0.1213   
 1466  2963.26  2530.970  2199.3000  1106.9703     0.8938  113.3544  0.1249   
 1467  2936.59  2444.690  2201.0667  1285.2144     1.3168  101.5122  0.0000   
 1468  2968.78  2451.530  2207.1000  1298.4309     1.0251   96.9478  0.1203   
 1469  2898.17  2547.650  2201.0667  1285.2144     1.3168  101.5122  0.0000   
 1470  3045.87  2493.720  2168.5111  1171.6442     0.8752  101.6567  0.1235   
 1471  2926.38  2383.760  2207.1000  1298.4309     1.0251   96.9478  0.1203   
 1472  3033.46  2424.390  2204.6667  1078.7860     0.9152  103.9611  0.1196   
 1473  3064.53  2349.850  2182.5555  1261.0898     1.2110  112.2922  0.1252   
 1474  2959.51  2538.410  2195.1000  1526.4440     0.8279   96.3100  0.1203   
 1475  3068.72  2472.410  2195.1000  1526.4440     0.8279   96.3100  0.1203   
 1476  3000.24  2477.370  2207.1000  1298.4309     1.0251   96.9478  0.1203   
 1477  3284.82  2417.920  2201.2889  1015.4370     2.1366  119.9011  0.1251   
 1478  2938.51  2515.150  2207.1000  1298.4309     1.0251   96.9478  0.1203   
 1479  3126.61  2426.760  2204.2889  2637.9989     1.5549   86.1089  0.1234   
 1480  3016.68  2482.880  2204.6667  1078.7860     0.9152  103.9611  0.1196   
 1481  3025.04  2590.880  2182.4111  3355.2007     1.4529   82.1311  0.1248   
 1482  2983.64  2542.560  2182.4111  3355.2007     1.4529   82.1311  0.1248   
 1483  3036.24  2548.460  2204.6667  1078.7860     0.9152  103.9611  0.1196   
 1484  2990.46  2507.010  2204.6667  1078.7860     0.9152  103.9611  0.1196   
 1485  2911.22  2556.640  2204.2889  2637.9989     1.5549   86.1089  0.1234   
 1486  2984.93  2432.380  2177.4333  2945.8855     1.3321   83.1700  0.1253   
 1487  2987.19  2512.750  2191.5000  1147.9162     0.9431   97.7200  0.1212   
 1488  2970.42  2494.390  2191.5000  1147.9162     0.9431   97.7200  0.1212   
 1489  3190.97  2480.560  2181.1889  1338.8895     2.1195  108.1400  0.1263   
 1490  2977.92  2538.370  2192.7556   867.3027     1.7393  123.4244  0.1251   
 1491  3042.12  2444.670  2195.4444  2914.1792     1.5978   85.1011  0.1235   
 1492  3070.74  2499.350  2181.1889  1338.8895     2.1195  108.1400  0.1263   
 1493  3002.54  2549.850  2182.5555  1261.0898     1.2110  112.2922  0.1252   
 1494  3068.64  2498.020  2192.7556   867.3027     1.7393  123.4244  0.1251   
 1495  2983.24  2459.970  2187.9889  1096.3790     0.9065   97.6567  0.1221   
 1496  3073.57  2528.590  2217.4111  1032.2836     1.4802  101.3511  0.1195   
 1497  2962.43  2543.100  2201.2889  1015.4370     2.1366  119.9011  0.1251   
 1498  2918.48  2505.290  2204.6667  1078.7860     0.9152  103.9611  0.1196   
 1499  2924.54  2451.310  2190.7666  3530.2362     0.8017   83.8767  0.1249   
 1500  3107.42  2404.670  2177.4333  2945.8855     1.3321   83.1700  0.1253   
 1501  3033.22  2546.240  2187.3444  2882.8558     1.5876   85.4189  0.1235   
 1502  2989.59  2506.000  2204.2889  2637.9989     1.5549   86.1089  0.1234   
 1503  2921.78  2483.770  2177.4333  2945.8855     1.3321   83.1700  0.1253   
 1504  2905.07  2493.470  2192.7556   867.3027     1.7393  123.4244  0.1251   
 1505  3026.67  2529.820  2192.7889  1268.5852     1.9935  104.5867  0.1268   
 1506  2883.88  2515.460  2177.4333  2945.8855     1.3321   83.1700  0.1253   
 1507  2996.65  2512.020  2191.3556  1111.4764     1.4549   97.1556  0.1210   
 1508  2992.07  2354.500  2191.3556  1111.4764     1.4549   97.1556  0.1210   
 1509  2931.11  2485.990  2208.5889  1116.3316     0.8396  107.6300  0.1207   
 1510  2966.20  2588.360  2192.7889  1268.5852     1.9935  104.5867  0.1268   
 1511  2984.99  2501.050  2190.7666  3530.2362     0.8017   83.8767  0.1249   
 1512  3031.93  2580.670  2205.5000  1287.3538     2.3842  111.7644  0.1241   
 1513  3067.02  2608.210  2190.7666  3530.2362     0.8017   83.8767  0.1249   
 1514  2949.90  2447.260  2264.7000  1211.1291     0.8785   99.6356  0.1173   
 1515  3012.34  2416.090  2192.7889  1268.5852     1.9935  104.5867  0.1268   
 1516  3018.70  2430.370  2192.7889  1268.5852     1.9935  104.5867  0.1268   
 1517  2935.39  2575.410  2179.7333  3085.3781     1.4843   82.2467  0.1248   
 1518  3273.46  2276.930  2223.5333  1352.1869     0.8714   97.2189  0.1196   
 1520  2998.06  2465.430  2223.5333  1352.1869     0.8714   97.2189  0.1196   
 1521  2943.76  2481.990  2190.7666  3530.2362     0.8017   83.8767  0.1249   
 1522  3015.71  2531.500  2204.6667  1078.7860     0.9152  103.9611  0.1196   
 1523  2930.66  2538.330  2187.3444  2882.8558     1.5876   85.4189  0.1235   
 1524  3142.21  2389.910  2223.5333  1352.1869     0.8714   97.2189  0.1196   
 1525  2977.74  2611.500  2192.7889  1268.5852     1.9935  104.5867  0.1268   
 1526  3042.45  2530.960  2191.3556  1111.4764     1.4549   97.1556  0.1210   
 1527  3051.20  2401.590  2190.7666  3530.2362     0.8017   83.8767  0.1249   
 1528  3028.34  2499.720  2179.7333  3085.3781     1.4843   82.2467  0.1248   
 1529  3068.78  2452.520  2217.4111  1032.2836     1.4802  101.3511  0.1195   
 1530  3072.29  2354.240  2177.0333  1183.7287     1.5726   98.7978  0.1213   
 1531  2959.86  2437.760  2192.7889  1268.5852     1.9935  104.5867  0.1268   
 1532  3169.64  2563.950  2167.4222  2837.8788     1.4892   83.8222  0.1255   
 1533  3011.32  2417.430  2184.9889  1124.2096     2.1987  114.5856  0.1265   
 1534  2969.29  2558.560  2155.8556   812.1294     1.0047  123.4100  0.1263   
 1535  3028.64  2532.710  2192.7889  1268.5852     1.9935  104.5867  0.1268   
 1536  3059.59  2465.540  2191.3556  1111.4764     1.4549   97.1556  0.1210   
 1537  3006.22  2525.200  2192.7889  1268.5852     1.9935  104.5867  0.1268   
 1538  3128.11  2367.160  2223.5333  1352.1869     0.8714   97.2189  0.1196   
 1539  2908.94  2560.990  2187.3444  2882.8558     1.5876   85.4189  0.1235   
 1540  2996.04  2555.920  2190.7666  3530.2362     0.8017   83.8767  0.1249   
 1541  3246.31  2499.790  2216.8111  1190.4067     2.5148  114.5533  0.1230   
 1542  2965.57  2487.910  2210.3556   910.7177     1.6941  119.8822  0.1268   
 1543  3109.18  2447.970  2210.3556   910.7177     1.6941  119.8822  0.1268   
 1544  3108.98  2537.730  2210.3556   910.7177     1.6941  119.8822  0.1268   
 1545  3100.19  2490.600  2212.8445  1068.5644     1.7835  113.8833  0.1249   
 1546  3093.24  2488.180  2212.8445  1068.5644     1.7835  113.8833  0.1249   
 1547  3008.77  2542.360  2167.4222  2837.8788     1.4892   83.8222  0.1255   
 1548  3027.01  2464.980  2212.6334  1081.5662     1.0096  113.4278  0.1253   
 1549  3183.63  2498.000  2195.4444  2914.1792     1.5978   85.1011  0.1235   
 1550  3072.20  2406.470  2195.4444  2914.1792     1.5978   85.1011  0.1235   
 1551  2958.43  2489.060  2192.7556   867.3027     1.7393  123.4244  0.1251   
 1552  2939.35  2521.980  2195.1000  1526.4440     0.8279   96.3100  0.1203   
 1553  3020.79  2500.190  2210.3556   910.7177     1.6941  119.8822  0.1268   
 1554  3031.78  2528.550  2182.5555  1261.0898     1.2110  112.2922  0.1252   
 1555  2902.96  2515.030  2181.1889  1338.8895     2.1195  108.1400  0.1263   
 1556  3025.21  2503.300  2179.7333  3085.3781     1.4843   82.2467  0.1248   
 1557  3072.10  2534.870  2177.4333  2945.8855     1.3321   83.1700  0.1253   
 1558  3012.30  2466.840  2217.4111  1032.2836     1.4802  101.3511  0.1195   
 1559  3076.33  2456.130  2217.4111  1032.2836     1.4802  101.3511  0.1195   
 1560  2770.40  2549.420  2204.2889  2637.9989     1.5549   86.1089  0.1234   
 1561  2951.14  2326.590  2212.6334  1081.5662     1.0096  113.4278  0.1253   
 1562  2899.41  2464.360  2179.7333  3085.3781     1.4843   82.2467  0.1248   
 1563  3052.31  2522.550  2198.5667  1124.6595     0.8763   98.4689  0.1205   
 1564  2978.81  2379.780  2206.3000  1110.4967     0.8236   99.4122  0.1208   
 1565  2894.92  2532.010  2177.0333  1183.7287     1.5726   98.7978  0.1213   
 1566  2944.92  2450.760  2195.4444  2914.1792     1.5978   85.1011  0.1235   
 
            8       9      10  ...      565       570     571       572  \
 0     1.5005  0.0162 -0.0034  ...  0.11955  533.8500  2.1113    8.9500   
 1     1.4966 -0.0005 -0.0148  ...  0.11955  535.0164  2.4335    5.9200   
 3     1.4882 -0.0124 -0.0033  ...  0.16300  530.5682  2.0253    9.3300   
 4     1.5031 -0.0031 -0.0072  ...  0.11955  532.0155  2.0275    8.8300   
 5     1.5287  0.0167  0.0055  ...  0.19050  534.2091  2.3236    8.9100   
 6     1.5816 -0.0270  0.0105  ...  0.11955  541.9036  2.4229    6.4800   
 7     1.5153  0.0157  0.0007  ...  0.13340  493.0054  2.2008  278.1900   
 8     1.5358  0.0111 -0.0066  ...  0.11450  535.1818  2.2170    7.0900   
 9     1.5381  0.0159  0.0049  ...  0.03610  533.4200  2.2598    3.5400   
 12    1.5310 -0.0259  0.0216  ...  0.11955  530.1800  1.9690    9.9500   
 13    1.5236 -0.0209 -0.0031  ...  0.19380  533.2464  2.2354    9.9300   
 15    1.5465  0.0250 -0.0024  ...  0.11955  536.1118  2.3120    8.5300   
 16    1.4368  0.0150 -0.0037  ...  0.11650  537.8145  2.1113    8.4300   
 17    1.5537  0.0220 -0.0027  ...  0.10890  531.8418  2.1849   10.2600   
 18    1.5481 -0.0367  0.0014  ...  0.09100  532.2673  2.2820    6.0000   
 19    1.5362 -0.0259 -0.0179  ...  0.11955  534.6455  2.0642    7.8100   
 20    1.6343 -0.0263  0.0116  ...  0.06060  531.2491  2.2134    6.8600   
 21    1.5559  0.0002 -0.0044  ...  0.13760  530.1291  2.0645    8.1000   
 22    1.5465  0.0195 -0.0114  ...  0.10800  528.1864  2.3338    7.4900   
 24    1.4227  0.0194  0.0073  ...  0.11955  535.7491  2.2233    7.1700   
 25    1.5136  0.0018  0.0058  ...  0.18320  532.6318  2.2530    9.2500   
 26    1.4860 -0.0019 -0.0056  ...  0.22330  530.4545  2.3075    5.4000   
 27    1.5582 -0.0101  0.0204  ...  0.20080  528.4100  2.3265    9.3700   
 28    1.5438  0.0065  0.0032  ...  0.09720  521.2055  2.2750    7.4100   
 29    1.4200 -0.0016  0.0138  ...  0.09370  534.3927  2.2461    8.7800   
 30    1.4481 -0.0134  0.0177  ...  0.11955  530.4073  2.1823    7.7300   
 31    1.5602  0.0041 -0.0056  ...  0.13930  533.2154  2.2671    7.9500   
 32    1.5837 -0.0266 -0.0029  ...  0.17010  536.5709  2.1983   10.8600   
 33    1.5047 -0.0216  0.0064  ...  0.23650  529.3927  2.1345   11.7900   
 34    1.5374 -0.0177  0.0170  ...  0.10710  529.5609  2.2805    8.0900   
 35    1.3475 -0.0152  0.0115  ...  0.25150  531.5427  2.2580    5.2000   
 36    1.5316 -0.0194  0.0088  ...  0.11955  532.7664  2.4052   10.9000   
 37    1.4400 -0.0163  0.0211  ...  0.11670  533.7700  2.2632   10.3500   
 39    1.5335  0.0098 -0.0013  ...  0.11955  532.1118  2.2194    6.2300   
 41    1.4652 -0.0008  0.0137  ...  0.19000  531.3755  2.4747    6.5900   
 42    1.5042  0.0082  0.0092  ...  0.12840  532.8618  2.3145   10.4300   
 43    1.5337 -0.0207  0.0067  ...  0.11955  527.8355  2.1599   12.1900   
 44    1.5764 -0.0219 -0.0080  ...  0.11955  535.6282  2.2383   11.6200   
 46    1.4919 -0.0030  0.0034  ...  0.11955  529.9682  2.1756   12.3000   
 47    1.5179  0.0114  0.0110  ...  0.11955  533.6809  2.1400   10.3100   
 51    1.5277 -0.0187 -0.0076  ...  0.07680  534.2509  2.5108    9.2900   
 52    1.4710  0.0442  0.0146  ...  0.11955  530.8764  2.1917    7.0000   
 53    1.5321 -0.0008  0.0047  ...  0.06050  532.5454  2.1461    8.7400   
 54    1.6486 -0.0239  0.0031  ...  0.14450  535.2400  2.3483    8.2800   
 55    1.5512 -0.0019  0.0033  ...  0.11955  535.5864  2.3608    8.0699   
 56    1.4873  0.0362  0.0029  ...  0.09620  536.4636  2.0117    9.2700   
 59    1.5632 -0.0156 -0.0010  ...  0.19930  529.9773  2.3048    7.2400   
 60    1.5928  0.0163 -0.0042  ...  0.20780  531.7764  2.2132    8.5800   
 61    1.5644  0.0201  0.0044  ...  0.21870  535.7673  2.0382   10.6600   
 63    1.5068  0.0126 -0.0023  ...  0.09650  536.7873  2.2478    6.2400   
 65    1.4970 -0.0077  0.0068  ...  0.09540  533.8600  2.3365    6.1400   
 66    1.4727 -0.0044  0.0136  ...  0.08510  537.2882  2.0190    6.9800   
 67    1.5079 -0.0086  0.0070  ...  0.12860  535.0427  2.0348   10.3000   
 68    1.4440  0.0007  0.0090  ...  0.08340  534.6255  2.4592    7.9100   
 69    1.5569  0.0189  0.0077  ...  0.06620  536.5182  2.0018   12.5400   
 70    1.5693  0.0064 -0.0058  ...  0.16090  532.4236  2.5469    6.4900   
 71    1.5769 -0.0359  0.0076  ...  0.16600  535.4264  2.0184    7.5500   
 72    1.4395  0.0012  0.0011  ...  0.11955  534.5155  2.4013   10.5700   
 73    1.4829 -0.0176  0.0064  ...  0.16070  523.0227  2.3358    5.3900   
 74    1.4981  0.0234 -0.0042  ...  0.29290  537.5900  1.7633    6.3900   
 75    1.4656 -0.0103 -0.0029  ...  0.02450  536.3945  2.0371    9.6000   
 76    1.4408  0.0247  0.0022  ...  0.08340  534.8927  2.4332   10.7500   
 77    1.5371 -0.0160  0.0022  ...  0.14170  537.1373  2.1412    7.9300   
 78    1.4615 -0.0034 -0.0042  ...  0.10890  537.6418  2.2678   10.2200   
 79    1.5264  0.0170 -0.0099  ...  0.11955  529.3318  2.3418   10.5000   
 80    1.4912 -0.0004 -0.0016  ...  0.07620  533.6146  2.0713   11.0900   
 81    1.5218 -0.0283 -0.0010  ...  0.09810  535.7664  1.9407    6.1500   
 83    1.5884 -0.0108  0.0165  ...  0.18200  531.1209  2.2509    9.5300   
 84    1.6026 -0.0252  0.0033  ...  0.11955  539.2336  2.1559    7.5400   
 85    1.4880  0.0252 -0.0044  ...  0.11955  510.9145  1.7483  439.0500   
 86    1.5553  0.0245 -0.0111  ...  0.07030  510.9145  1.7483  439.0500   
 87    1.3296 -0.0011  0.0007  ...  0.14170  533.1773  2.3063    9.9300   
 88    1.3167  0.0428  0.0230  ...  0.11955  533.8209  2.2711   10.2700   
 89    1.4989 -0.0042 -0.0009  ...  0.16670  510.9145  1.7483  439.0500   
 90    1.4527 -0.0077 -0.0098  ...  0.16890  533.3118  2.3907    7.0100   
 91    1.5867  0.0086  0.0349  ...  0.18170  529.3927  2.1345   11.7900   
 92    1.5414 -0.0095 -0.0167  ...  0.12970  529.3927  2.1345   11.7900   
 93    1.5024 -0.0039  0.0008  ...  0.20480  535.1736  2.4707    9.8400   
 94    1.5409 -0.0097  0.0043  ...  0.06710  510.9145  1.7483  439.0500   
 95    1.5048 -0.0108  0.0070  ...  0.11380  534.9591  1.8002    8.2900   
 97    1.5310  0.0006  0.0028  ...  0.11955  531.6718  2.3956   10.2600   
 98    1.6227  0.0062  0.0194  ...  0.11955  529.0600  2.3468    8.1900   
 99    1.5115 -0.0100  0.0053  ...  0.11955  530.9282  2.1959   11.7800   
 100   1.5035  0.0046  0.0041  ...  0.11955  529.8373  2.3027   12.5000   
 101   1.3609 -0.0003 -0.0046  ...  0.10090  530.9864  2.3854    7.9300   
 102   1.4581  0.0002  0.0004  ...  0.32570  550.5855  2.1543  420.2400   
 103   1.5748 -0.0062 -0.0024  ...  0.11955  529.8309  2.3190    9.6000   
 104   1.4051 -0.0276 -0.0093  ...  0.13610  535.1718  2.4874    8.1400   
 105   1.4039  0.0088  0.0169  ...  0.11160  534.1436  1.6744    8.3000   
 106   1.3985 -0.0092  0.0044  ...  0.11820  533.7727  2.3508    9.7000   
 107   1.4876  0.0018  0.0094  ...  0.14380  536.7136  2.4272    7.0900   
 108   1.4321 -0.0114  0.0026  ...  0.07600  537.0518  1.8363    8.1000   
 109   1.4639 -0.0091 -0.0063  ...  0.11955  536.3864  2.4386    7.5700   
 110   1.5242  0.0221  0.0153  ...  0.23820  530.7191  2.5798    6.1000   
 111   1.5847  0.0283  0.0118  ...  0.10730  533.3509  2.1114    8.0900   
 112   1.5497 -0.0019  0.0029  ...  0.06370  534.8827  2.5700    8.4200   
 113   1.4453 -0.0126  0.0152  ...  0.11955  532.7255  1.8964    9.6000   
 114   1.4634 -0.0094  0.0006  ...  0.11955  529.1618  2.3860    8.2600   
 116   1.4443  0.0008 -0.0059  ...  0.27170  536.0627  2.3428    9.7200   
 117   1.5166 -0.0141  0.0090  ...  0.11740  535.7082  2.1497    7.7700   
 118   1.4740  0.0024  0.0069  ...  0.09290  532.4836  2.2079   11.5000   
 119   1.4046 -0.0080  0.0040  ...  0.56310  533.3209  2.4315    5.9500   
 120   1.4649  0.0309 -0.0068  ...  0.06820  528.6918  2.3251   10.2401   
 121   1.5385  0.0329 -0.0010  ...  0.16640  533.2491  2.0020    8.9800   
 122   1.5370 -0.0013  0.0006  ...  0.16320  534.6418  2.1732   10.8900   
 123   1.5374 -0.0131 -0.0007  ...  0.10240  536.4745  2.1084    9.6100   
 124   1.4974 -0.0046 -0.0002  ...  0.14940  531.9991  2.3672    8.4600   
 125   1.3642 -0.0167  0.0033  ...  0.54510  533.7818  2.4911   10.2100   
 126   1.4125 -0.0003  0.0140  ...  0.06470  534.8573  2.4009    9.0500   
 127   1.5430  0.0059 -0.0057  ...  0.13430  532.3064  2.5503    8.6801   
 128   1.4306  0.0383 -0.0045  ...  0.11955  526.4382  2.4025    8.5200   
 129   1.4402 -0.0180 -0.0013  ...  0.19010  534.0064  2.4764    9.5700   
 130   1.4700 -0.0092  0.0083  ...  0.19100  531.6346  2.3149    8.3000   
 132   1.3872 -0.0089  0.0028  ...  0.11955  532.6609  2.1807   11.3900   
 133   1.4712 -0.0150  0.0056  ...  0.13600  523.4991  1.8204    6.4300   
 134   1.4845 -0.0079 -0.0004  ...  0.23170  466.5409  2.4894  283.0300   
 135   1.3325  0.0067  0.0098  ...  0.09400  535.6018  2.0446   11.3200   
 136   1.4704  0.0024  0.0124  ...  0.08580  531.3545  2.4380    6.2900   
 137   1.3714  0.0037  0.0070  ...  0.13730  531.6273  2.2843   11.7300   
 138   1.4523 -0.0032  0.0093  ...  0.12640  531.8255  2.0893    7.9500   
 139   1.3809 -0.0025 -0.0149  ...  0.18980  535.1736  2.4707    9.8400   
 140   1.5668 -0.0074 -0.0074  ...  0.14520  534.5045  2.4213   10.1300   
 141   1.4491  0.0015 -0.0036  ...  0.18640  534.5045  2.4213   10.1300   
 142   1.4921 -0.0113  0.0136  ...  0.25080  539.0418  2.5479    8.8400   
 143   1.4149 -0.0041  0.0036  ...  0.20970  531.3073  2.3028   11.9400   
 144   1.4786  0.0017 -0.0080  ...  0.08600  531.4036  2.2910   12.2100   
 145   1.4785 -0.0205 -0.0057  ...  0.13530  536.2418  2.4342    8.7400   
 146   1.4599  0.0032  0.0067  ...  0.14680  533.2282  2.3215    7.1100   
 147   1.5572  0.0003 -0.0031  ...  0.12390  537.3973  2.0402    8.2700   
 148   1.5637 -0.0137 -0.0048  ...  0.23690  531.2291  2.4177   11.4700   
 149   1.4416 -0.0037 -0.0035  ...  0.09520  534.9591  1.8002    8.2900   
 150   1.5708 -0.0139 -0.0027  ...  0.11240  534.5045  2.4213   10.1300   
 151   1.5796  0.0103 -0.0029  ...  0.11955  530.1700  2.1279   11.1800   
 152   1.4635  0.0001  0.0047  ...  0.12700  532.5973  2.0612    8.7000   
 153   1.4313  0.0022  0.0085  ...  0.10010  534.2527  1.9665    9.0800   
 155   1.4891 -0.0007 -0.0088  ...  0.09180  531.0300  2.2599    9.0800   
 156   1.5568 -0.0214 -0.0043  ...  0.12870  534.5873  2.1965    6.9900   
 159   1.5859 -0.0139 -0.0085  ...  0.29800  530.5818  2.2833   11.5800   
 160   1.4549 -0.0125 -0.0196  ...  0.16320  535.0645  2.2855    5.7000   
 161   1.4519 -0.0206  0.0026  ...  0.09840  537.2082  2.4297    8.0100   
 162   1.4580  0.0063 -0.0133  ...  0.16070  535.1564  2.2877    5.7200   
 163   1.4059 -0.0083  0.0059  ...  0.11955  510.9145  1.7483  439.0500   
 164   1.5554 -0.0127  0.0041  ...  0.07750  534.5045  2.4213   10.1300   
 165   1.4704 -0.0159  0.0082  ...  0.23940  533.2418  2.4157    6.5800   
 166   1.5129 -0.0152 -0.0038  ...  0.13580  534.9627  2.3420    6.0100   
 168   1.4377 -0.0126 -0.0008  ...  0.18790  534.6009  2.2614    5.6500   
 170   1.4435 -0.0119 -0.0013  ...  0.15660  534.3564  2.5551    9.5300   
 171   1.5186 -0.0048 -0.0042  ...  0.11955  536.1964  2.0946   12.5500   
 172   1.4767 -0.0073  0.0146  ...  0.18100  534.5045  2.4213   10.1300   
 173   1.5792  0.0016  0.0177  ...  0.29840  532.2736  2.1116    9.9800   
 174   1.5884  0.0097  0.0031  ...  0.14670  537.6355  2.1349    7.9800   
 175   1.6138  0.0146  0.0080  ...  0.24910  535.1318  2.3810    9.6900   
 176   1.3868 -0.0023  0.0035  ...  0.08770  535.3736  2.3921    8.9700   
 177   1.5442 -0.0108  0.0060  ...  0.11955  535.3736  2.3921    8.9700   
 178   1.5622  0.0167 -0.0117  ...  0.08770  536.0109  2.2047    6.1100   
 179   1.5496  0.0106  0.0001  ...  0.08770  532.6700  2.2124    8.8000   
 181   1.4779  0.0052 -0.0013  ...  0.19720  367.4400  2.4357  142.5000   
 183   1.4554 -0.0267  0.0027  ...  0.11955  528.6918  2.3251   10.2401   
 184   1.6081  0.0143 -0.0146  ...  0.19080  535.3736  2.3921    8.9700   
 185   1.4976  0.0056 -0.0011  ...  0.11955  530.5091  2.3854   11.1800   
 187   1.4616 -0.0018  0.0097  ...  0.11955  534.5918  2.3970    6.9800   
 190   1.4957 -0.0119  0.0129  ...  0.09230  533.8218  2.0970    8.1400   
 191   1.6030 -0.0158  0.0097  ...  0.08950  535.7864  2.1227    7.2300   
 192   1.3840 -0.0150  0.0095  ...  0.11955  531.0818  2.3445    9.0000   
 193   1.6035  0.0023  0.0114  ...  0.08600  532.5227  2.0369    9.6200   
 194   1.4845 -0.0326  0.0097  ...  0.09670  534.9427  1.9155    6.6000   
 195   1.5150  0.0031  0.0097  ...  0.11955  534.9591  1.8002    8.2900   
 196   1.5334 -0.0185  0.0126  ...  0.19540  531.9955  2.2473    9.0300   
 197   1.4833 -0.0101  0.0130  ...  0.07880  534.5045  2.4213   10.1300   
 198   1.5679 -0.0197  0.0100  ...  0.11955  535.3736  2.3921    8.9700   
 199   1.3479  0.0306 -0.0106  ...  0.11955  537.9418  1.9847    9.9100   
 200   1.4281  0.0055 -0.0119  ...  0.09750  533.9245  2.1340    7.8700   
 201   1.4005  0.0056 -0.0192  ...  0.11955  538.5064  2.5244    8.1100   
 202   1.5362 -0.0088  0.0039  ...  0.11955  529.2545  1.7725    9.0900   
 203   1.4334  0.0056  0.0012  ...  0.11955  534.8400  1.4942   10.1800   
 204   1.4310  0.0090  0.0033  ...  0.14670  535.2100  2.1136    7.9700   
 205   1.5101  0.0061  0.0093  ...  0.13630  532.7000  2.3601   10.4600   
 206   1.3933 -0.0229 -0.0137  ...  0.12850  532.7482  2.2585    5.8300   
 207   1.4969 -0.0171  0.0030  ...  0.11955  534.9527  2.2757    6.8700   
 208   1.4659  0.0055 -0.0135  ...  0.08450  537.2691  2.0028    7.0700   
 209   1.4845 -0.0178  0.0084  ...  0.11110  531.9700  2.2792    7.8700   
 210   1.3780 -0.0219 -0.0019  ...  0.11955  531.8709  2.2561    9.7500   
 211   1.5094 -0.0027 -0.0015  ...  0.13160  508.2764  2.0687  272.4100   
 212   1.5573  0.0019  0.0117  ...  0.11955  534.8200  1.8025    7.7200   
 213   1.4281  0.0049  0.0092  ...  0.11955  533.3809  1.5992    9.3300   
 214   1.4510 -0.0129 -0.0106  ...  0.11955  533.8691  2.0721    7.4800   
 215   1.4538  0.0161  0.0071  ...  0.11955  535.9764  2.2302    7.2200   
 216   1.3580 -0.0024 -0.0143  ...  0.28120  531.5982  1.5656    8.5500   
 217   1.5337 -0.0010 -0.0018  ...  0.14470  529.6709  2.4583   11.3600   
 219   1.4029  0.0174 -0.0023  ...  0.11930  535.9227  1.9110    8.6500   
 220   1.5210  0.0075 -0.0127  ...  0.11600  532.7000  2.3601   10.4600   
 221   1.5356  0.0043 -0.0008  ...  0.17970  537.2809  2.0931    6.8000   
 223   1.4355  0.0135 -0.0063  ...  0.11955  529.2827  2.2933    9.7400   
 224   1.5059  0.0218  0.0018  ...  0.17960  530.9864  2.4457   11.1300   
 225   1.4785 -0.0004 -0.0113  ...  0.20430  529.5091  2.5085   11.3700   
 226   1.4703  0.0275 -0.0061  ...  0.14420  530.0736  2.5630    9.1000   
 227   1.4406  0.0156 -0.0068  ...  0.11955  529.4427  2.5058    6.1400   
 228   1.4685  0.0016  0.0004  ...  0.16750  528.8918  2.2461    9.1100   
 229   1.4557 -0.0085 -0.0119  ...  0.11955  531.6709  2.2584    7.7900   
 230   1.3731  0.0056 -0.0015  ...  0.11955  535.7309  1.7132    9.7700   
 232   1.3906 -0.0018 -0.0008  ...  0.09570  535.4118  1.5550    7.7700   
 233   1.4881 -0.0054 -0.0037  ...  0.08770  534.7509  1.5983    6.6800   
 234   1.4476 -0.0234 -0.0001  ...  0.12400  529.1691  2.1248    8.0700   
 237   1.5221 -0.0175  0.0086  ...  0.04090  534.7382  1.4832    9.3200   
 239   1.3595  0.0063 -0.0034  ...  0.22810  528.7264  2.4341    6.3400   
 242   1.4737  0.0046 -0.0030  ...  0.13770  534.0745  1.4351    8.0700   
 245   1.4190 -0.0296  0.0073  ...  0.11955  532.5527  2.1698   10.2400   
 246   1.4245 -0.0056 -0.0089  ...  0.22030  533.4982  2.3658    8.4200   
 247   1.4925 -0.0074 -0.0043  ...  0.11955  532.8891  2.2839    9.1500   
 248   1.4493  0.0176 -0.0025  ...  0.13540  391.5764  2.2395  177.9300   
 249   1.4808 -0.0107 -0.0050  ...  0.11955  401.4482  2.4312  282.4700   
 250   1.5145  0.0345 -0.0064  ...  0.11955  531.5309  1.8496    7.5400   
 251   1.5198  0.0064 -0.0012  ...  0.22110  532.6864  2.3328    7.6200   
 252   1.4777  0.0324 -0.0037  ...  0.11955  533.2682  1.8703    8.7700   
 253   1.4360 -0.0057  0.0101  ...  0.12870  364.1436  2.2891  280.7900   
 254   1.4025 -0.0365  0.0008  ...  0.16310  532.8455  1.6887    8.7100   
 255   1.4840  0.0045 -0.0019  ...  0.11350  535.7282  2.1451    7.4900   
 256   1.4596  0.0034  0.0057  ...  0.11955  534.7900  1.6127    9.2300   
 257   1.4400 -0.0281 -0.0044  ...  0.13200  531.0027  2.4523    9.9800   
 258   1.4378 -0.0282  0.0012  ...  0.20130  532.3182  2.2750   10.1900   
 259   1.5193 -0.0212 -0.0044  ...  0.17140  534.9600  1.8846   10.0300   
 260   1.5183 -0.0161  0.0004  ...  0.11955  535.2782  1.8070    8.9200   
 261   1.4420  0.0183  0.0023  ...  0.11955  538.1409  1.7150    5.9300   
 262   1.4067 -0.0234  0.0029  ...  0.11955  529.1055  2.3400   11.1000   
 263   1.4896  0.0064 -0.0013  ...  0.11660  531.0218  2.4000    8.9400   
 264   1.4929 -0.0252  0.0112  ...  0.11955  526.3709  2.3553    7.8200   
 265   1.5032 -0.0042  0.0021  ...  0.11955  533.1318  2.5132    8.6200   
 266   1.4701 -0.0003  0.0026  ...  0.11955  534.4582  2.3295    6.9900   
 267   1.5155 -0.0045  0.0027  ...  0.12490  531.1618  1.7323    6.9700   
 268   1.3993  0.0042 -0.0103  ...  0.11955  531.4909  2.3698   10.5000   
 269   1.4193  0.0015  0.0015  ...  0.11955  529.9355  2.1598    7.4300   
 270   1.4305  0.0001 -0.0054  ...  0.11955  534.1545  1.6889    9.8800   
 271   1.3669 -0.0005  0.0017  ...  0.11955  530.0118  1.7527   10.3200   
 272   1.4150 -0.0062 -0.0032  ...  0.11955  532.8855  1.8536    8.8500   
 274   1.3989  0.0127  0.0069  ...  0.11955  532.7391  2.4119    8.8800   
 275   1.4984 -0.0068  0.0010  ...  0.11955  513.3955  2.3483  274.8700   
 276   1.3971  0.0300  0.0118  ...  0.08770  532.9491  2.2987   10.4100   
 278   1.4049  0.0389 -0.0147  ...  0.04810  532.9782  2.1360    6.5000   
 279   1.4547  0.0285 -0.0017  ...  0.18900  532.0100  2.3061    7.5100   
 280   1.4732 -0.0009  0.0068  ...  0.11955  533.9900  1.7577    6.9600   
 281   1.4763 -0.0043  0.0067  ...  0.11955  532.0454  2.1206    9.8100   
 283   1.4389  0.0172  0.0306  ...  0.27910  535.5327  1.7027    7.8300   
 284   1.4250 -0.0063  0.0076  ...  0.11955  531.0309  2.4452    9.6200   
 285   1.4246 -0.0184  0.0070  ...  0.11955  534.1218  1.9309    5.8000   
 286   1.4459 -0.0408  0.0113  ...  0.11955  527.8727  2.2436    7.5600   
 287   1.4282  0.0005 -0.0113  ...  0.29540  533.3264  1.6541    8.8800   
 288   1.4738 -0.0213  0.0022  ...  0.11750  532.9573  2.2141    7.9100   
 289   1.5040 -0.0376 -0.0025  ...  0.29550  529.8873  2.3048   11.6800   
 290   1.3576  0.0000 -0.0046  ...  0.11955  535.3509  2.0813    7.1800   
 292   1.4470 -0.0163  0.0530  ...  0.12550  537.2809  2.0931    6.8000   
 293   1.4173 -0.0066 -0.0093  ...  0.15810  531.6236  2.5605    9.2000   
 295   1.3812  0.0031 -0.0046  ...  0.11955  526.8464  2.3356    7.0600   
 296   1.3743 -0.0005  0.0024  ...  0.11955  530.5527  2.5117   10.4400   
 297   1.4610  0.0030 -0.0042  ...  0.08770  525.9545  2.4083    9.5000   
 298   1.3526 -0.0040 -0.0105  ...  0.11955  533.2600  2.5306    8.2600   
 299   1.4814 -0.0238  0.0396  ...  0.11230  535.9227  1.9110    8.6500   
 300   1.4700 -0.0102 -0.0073  ...  0.35880  525.9545  2.4083    9.5000   
 301   1.2921 -0.0108 -0.0020  ...  0.21490  532.4264  2.3599    8.2100   
 302   1.4715  0.0040 -0.0103  ...  0.06980  331.9618  2.3704  129.1200   
 303   1.5386 -0.0090  0.0057  ...  0.11955  530.2800  2.3134   12.4000   
 304   1.4339  0.0032 -0.0067  ...  0.16840  533.9673  2.3091    8.4300   
 305   1.3800 -0.0218 -0.0145  ...  0.11955  534.5736  2.3528    7.3500   
 306   1.4965 -0.0045 -0.0006  ...  0.12190  532.7046  1.6932    9.4800   
 307   1.4043 -0.0210 -0.0052  ...  0.11955  534.6436  2.3390    8.3500   
 308   1.4280 -0.0169  0.0016  ...  0.11955  533.7055  1.8993   11.7800   
 309   1.4542 -0.0362 -0.0025  ...  0.14010  533.6318  2.2147    8.3700   
 310   1.4350 -0.0004 -0.0057  ...  0.17420  534.1773  2.2813    9.7800   
 311   1.4147  0.0130  0.0050  ...  0.15600  531.1391  2.3129   10.0800   
 312   1.4984  0.0020 -0.0076  ...  0.23500  534.1755  2.5320    6.7900   
 313   1.4293 -0.0171 -0.0075  ...  0.17630  535.8209  2.0976    7.3000   
 314   1.4291 -0.0048 -0.0185  ...  0.08880  531.3882  2.2970    9.6200   
 315   1.4204  0.0011 -0.0135  ...  0.14650  531.5864  2.4302    8.3800   
 316   1.4446 -0.0318  0.0366  ...  0.07070  534.3564  2.3688    7.5500   
 317   1.5305  0.0007 -0.0068  ...  0.08770  535.5691  1.7613    6.3000   
 318   1.5357 -0.0337 -0.0220  ...  0.30870  534.8873  2.4691    6.6200   
 319   1.3899  0.0054  0.0103  ...  0.10610  535.5691  1.7613    6.3000   
 320   1.4609  0.0048 -0.0034  ...  0.07690  480.5291  2.4944  277.1100   
 322   1.5695 -0.0013  0.0024  ...  0.11955  531.9045  2.0793   11.2400   
 324   1.4010 -0.0001 -0.0054  ...  0.11955  531.9673  2.3808   10.2600   
 325   1.5176 -0.0049  0.0084  ...  0.11955  533.5627  2.3471    8.6100   
 328   1.5546 -0.0107  0.0072  ...  0.11955  535.2782  1.8070    8.9200   
 329   1.5279 -0.0061  0.0031  ...  0.11955  534.3055  2.4677    7.1800   
 330   1.5091 -0.0064  0.0044  ...  0.18570  530.9227  2.3869   12.6600   
 331   1.5400 -0.0167 -0.0036  ...  0.08980  537.2809  2.0931    6.8000   
 332   1.4535  0.0141  0.0026  ...  0.04500  533.4627  2.3152    9.2700   
 333   1.4941  0.0037  0.0086  ...  0.13380  532.5527  2.1698   10.2400   
 334   1.4783 -0.0129 -0.0051  ...  0.16690  534.7936  2.1494    8.9800   
 335   1.4512 -0.0018 -0.0032  ...  0.11955  487.2691  2.4733  280.6600   
 337   1.5979 -0.0142  0.0010  ...  0.11955  535.4818  2.4326    4.9200   
 338   1.4909 -0.0039  0.0082  ...  0.11955  534.9700  2.3699    8.2100   
 339   1.5971 -0.0160 -0.0023  ...  0.11955  534.9600  2.4029    8.4900   
 340   1.4400  0.0092 -0.0113  ...  0.16950  535.9227  1.9110    8.6500   
 341   1.4975 -0.0130  0.0016  ...  0.20740  379.0045  2.3354   12.8800   
 342   1.6103  0.0100  0.0049  ...  0.14260  532.5764  2.5229    7.8400   
 343   1.4551  0.0117  0.0057  ...  0.08770  535.9227  1.9110    8.6500   
 345   1.4752 -0.0118  0.0105  ...  0.19260  533.5964  2.2693   10.7000   
 346   1.5126 -0.0192  0.0107  ...  0.11955  530.6173  2.4608   10.5600   
 347   1.3806 -0.0090  0.0117  ...  0.11000  360.7645  2.3897  152.8700   
 348   1.4036  0.0035  0.0141  ...  0.08770  534.9600  1.8846   10.0300   
 349   1.4042  0.0093  0.0087  ...  0.14840  508.2764  2.0687  272.4100   
 350   1.3428  0.0093  0.0052  ...  0.08770  536.0973  2.0779    9.0200   
 352   1.5037 -0.0109  0.0069  ...  0.07740  534.4473  2.4356    9.3300   
 353   1.5272 -0.0219  0.0083  ...  0.08770  535.9227  1.9110    8.6500   
 354   1.4636  0.0009  0.0031  ...  0.19530  531.5445  2.4212    7.4900   
 355   1.5852 -0.0077  0.0048  ...  0.11690  535.7282  2.1451    7.4900   
 356   1.4629  0.0175  0.0097  ...  0.09160  530.6027  2.3970    7.1100   
 357   1.5923  0.0098  0.0039  ...  0.08770  536.0973  2.0779    9.0200   
 358   1.3924  0.0036 -0.0017  ...  0.24970  486.4218  2.4624  276.5200   
 359   1.4103 -0.0137  0.0004  ...  0.08770  534.0518  2.4314    8.0700   
 360   1.4281 -0.0109  0.0089  ...  0.08770  535.7282  2.1451    7.4900   
 361   1.5108  0.0255  0.0178  ...  0.08770  532.4355  2.3806    8.2100   
 362   1.4428 -0.0110  0.0101  ...  0.11955  529.5727  2.3873   10.3700   
 363   1.4414  0.0124  0.0015  ...  0.02240  534.4473  2.4356    9.3300   
 364   1.4956  0.0151  0.0107  ...  0.15770  530.8909  2.5326    8.5000   
 365   1.5829 -0.0278 -0.0324  ...  0.08770  536.8282  2.1262    6.8800   
 366   1.4611 -0.0090  0.0111  ...  0.13170  534.9600  1.8846   10.0300   
 367   1.5758 -0.0093  0.0013  ...  0.23140  532.4355  2.3806    8.2100   
 369   1.2260  0.0307  0.0087  ...  0.08770  537.2809  2.0931    6.8000   
 370   1.5389 -0.0027  0.0095  ...  0.07870  534.3564  2.3688    7.5500   
 371   1.5996 -0.0012 -0.0017  ...  0.07540  529.8082  1.5705    9.2700   
 372   1.4859  0.0015  0.0041  ...  0.11955  534.8045  1.5655    9.6200   
 374   1.4600  0.0109 -0.0021  ...  0.08770  534.9600  1.8846   10.0300   
 375   1.3814  0.0013 -0.0046  ...  0.11955  535.2709  2.3676    7.9300   
 376   1.4598 -0.0194 -0.0329  ...  0.11955  537.4455  1.5694    7.5000   
 377   1.5168  0.0143  0.0072  ...  0.08770  534.3564  2.3688    7.5500   
 378   1.4603  0.0039  0.0152  ...  0.11955  532.7618  2.3637    8.7700   
 379   1.5362  0.0138  0.0029  ...  0.11955  534.5891  2.4801    6.5100   
 380   1.4388  0.0009 -0.0064  ...  0.27660  532.7818  2.4880    7.5800   
 381   1.4604 -0.0151 -0.0036  ...  0.13820  534.4727  2.1534    7.6800   
 382   1.5608 -0.0004 -0.0052  ...  0.16560  533.4909  1.6300    8.6801   
 383   1.5169  0.0136  0.0019  ...  0.11955  511.1427  1.6450  434.0400   
 384   1.5400  0.0221  0.0091  ...  0.11955  529.6518  2.5380   10.7300   
 385   1.5001 -0.0199 -0.0289  ...  0.20070  533.1600  2.2493    9.7000   
 386   1.5016  0.0110 -0.0001  ...  0.14350  534.0436  2.1886   10.7400   
 387   1.4313 -0.0077 -0.0013  ...  0.11955  536.1245  2.3189    7.7300   
 388   1.4712  0.0267 -0.0046  ...  0.11955  535.9118  1.7915    8.5200   
 389   1.5432 -0.0163  0.0048  ...  0.12310  529.1527  2.5264    9.1400   
 390   1.5865  0.0080 -0.0216  ...  0.07780  534.0609  2.4197    8.3600   
 391   1.4822  0.0105 -0.0170  ...  0.11955  533.9055  2.3606    7.4600   
 393   1.3531  0.0069 -0.0019  ...  0.19380  336.6718  2.2705  150.7700   
 394   1.1910  0.0101 -0.0181  ...  0.10300  532.8691  2.3668   10.3800   
 395   1.5387  0.0126  0.0063  ...  0.13650  533.1491  2.4279    9.6600   
 396   1.2476  0.0170  0.0125  ...  0.09510  534.4473  2.4356    9.3300   
 397   1.4426  0.0132 -0.0185  ...  0.11955  534.9473  1.6309    7.8500   
 398   1.5381 -0.0077  0.0018  ...  0.12530  532.3445  2.4733    8.3800   
 399   1.5320  0.0068 -0.0173  ...  0.11955  532.6755  2.3130    8.4700   
 400   1.5164  0.0030 -0.0203  ...  0.06490  532.1827  2.2771    7.4700   
 401   1.3967  0.0176 -0.0239  ...  0.11955  533.9036  2.3605    9.3500   
 402   1.4991 -0.0064 -0.0148  ...  0.18180  534.3564  2.3688    7.5500   
 403   1.4319 -0.0067 -0.0146  ...  0.11955  535.4736  2.3436    7.6700   
 404   1.5327 -0.0192  0.0023  ...  0.23930  535.2009  1.6920    8.9000   
 405   1.4732 -0.0063 -0.0148  ...  0.07270  534.3564  2.3688    7.5500   
 407   1.6128  0.0004 -0.0081  ...  0.18500  536.8282  2.1262    6.8800   
 408   1.5518  0.0130 -0.0008  ...  0.15630  534.7936  2.2749    6.2100   
 409   1.4603  0.0214 -0.0018  ...  0.08770  532.7645  2.4112    9.8300   
 410   1.4768  0.0193  0.0009  ...  0.11955  530.3155  2.2485   10.3600   
 411   1.5410 -0.0064 -0.0083  ...  0.09740  533.5927  2.2888    9.4800   
 412   1.5330 -0.0059  0.0228  ...  0.12080  530.6855  2.2688   11.1800   
 413   1.5082 -0.0057 -0.0069  ...  0.09470  539.1145  1.7970    7.9600   
 414   1.5611  0.0128 -0.0103  ...  0.11955  536.0091  2.0315    8.7700   
 415   1.5440 -0.0163 -0.0238  ...  0.16980  534.5991  2.3354   10.8200   
 416   1.5316 -0.0105 -0.0218  ...  0.08770  531.4409  2.3579    7.6900   
 417   1.5238  0.0007 -0.0247  ...  0.11955  531.7354  2.3128    7.7900   
 418   1.5262  0.0065 -0.0052  ...  0.20620  532.3982  2.3705    8.5699   
 419   1.5973  0.0013 -0.0057  ...  0.08770  534.4964  2.4582    9.1400   
 420   1.6405 -0.0410 -0.0067  ...  0.16750  532.7645  1.6780    8.5800   
 421   1.5164 -0.0013 -0.0059  ...  0.08770  531.6036  2.0159    7.3500   
 422   1.5116  0.0130  0.0007  ...  0.10740  532.9282  2.4995    7.8400   
 423   1.4458  0.0006 -0.0029  ...  0.13440  531.2591  2.3950    5.9600   
 425   1.4794 -0.0198 -0.0004  ...  0.11955  532.4618  2.2846    9.3600   
 426   1.5322  0.0205 -0.0213  ...  0.13740  410.2409  2.4691   59.5800   
 427   1.4945  0.0247 -0.0049  ...  0.11955  538.3636  1.9631    6.0400   
 428   1.5680  0.0009  0.0094  ...  0.11955  534.5545  2.3853    8.2000   
 429   1.4920 -0.0095  0.0165  ...  0.26500  532.9282  2.4995    7.8400   
 430   1.5383 -0.0268 -0.0010  ...  0.11955  534.9382  2.3874    8.4500   
 431   1.4688  0.0147 -0.0100  ...  0.11955  510.2164  2.4690  280.0000   
 432   1.4925  0.0203 -0.0109  ...  0.11955  531.4355  2.4873    9.5000   
 433   1.5356  0.0278 -0.0019  ...  0.11955  534.8673  2.2965    6.5699   
 434   1.4398 -0.0075 -0.0055  ...  0.08710  536.0973  2.0779    9.0200   
 435   1.4501  0.0011 -0.0140  ...  0.11955  532.0082  2.2994    6.8900   
 436   1.5270  0.0066 -0.0124  ...  0.08770  530.6027  2.3970    7.1100   
 437   1.4266  0.0181 -0.0118  ...  0.19850  536.8282  2.1262    6.8800   
 438   1.3787  0.0025 -0.0085  ...  0.11955  365.3818  2.5672  137.8500   
 439   1.5533  0.0196 -0.0030  ...  0.14320  384.6536  2.5236  287.1900   
 440   1.5760  0.0272  0.0009  ...  0.05800  564.5500  2.0868  454.5600   
 442   1.4117  0.0089 -0.0093  ...  0.11955  537.0273  1.4573    7.5900   
 443   1.4607  0.0155  0.0093  ...  0.17180  335.5982  2.6271  125.6600   
 444   1.4164  0.0006  0.0056  ...  0.11955  531.3936  2.3207   10.1900   
 445   1.5604  0.0054  0.0034  ...  0.08840  536.1555  1.8940    9.4100   
 446   1.4794  0.0083  0.0005  ...  0.08540  529.4873  2.4094    7.7600   
 447   1.5130  0.0011  0.0026  ...  0.10740  530.2100  2.2897   10.4000   
 449   1.5766 -0.0104  0.0042  ...  0.08770  536.4400  1.4805    8.3300   
 450   1.6004 -0.0024  0.0056  ...  0.20880  530.5864  2.4746    9.9600   
 451   1.5261  0.0019  0.0034  ...  0.08770  533.0191  2.0856   10.4301   
 452   1.4337 -0.0006  0.0082  ...  0.08770  533.0191  2.0856   10.4301   
 453   1.4873  0.0000  0.0031  ...  0.08770  533.0191  2.0856   10.4301   
 454   1.4396  0.0043  0.0064  ...  0.11955  532.3800  1.9613    7.1400   
 455   1.4110 -0.0016  0.0129  ...  0.14040  535.1664  2.0973    7.1900   
 456   1.4482 -0.0321  0.0048  ...  0.33990  532.5227  1.9491    7.1800   
 457   1.4035  0.0020  0.0027  ...  0.13460  534.3400  2.0440    9.1700   
 458   1.5448 -0.0160  0.0016  ...  0.11955  533.3100  2.3610    9.1600   
 459   1.5458 -0.0107 -0.0048  ...  0.13010  531.4773  2.3788    8.9500   
 460   1.4499  0.0212 -0.0027  ...  0.11955  535.0200  2.4763    7.9900   
 461   1.3732  0.0314  0.0041  ...  0.21640  534.6536  1.6344    6.4500   
 462   1.5477  0.0145 -0.0032  ...  0.09150  528.1664  2.0671   10.1500   
 463   1.4680 -0.0330 -0.0022  ...  0.09990  533.3109  2.2646   10.3700   
 464   1.4116 -0.0041  0.0025  ...  0.12380  534.3600  2.3676    8.1000   
 465   1.4526  0.0383 -0.0115  ...  0.11955  534.5873  2.3512    9.0200   
 466   1.5446 -0.0034  0.0034  ...  0.23580  538.3864  2.2819  234.7800   
 467   1.4868 -0.0325  0.0003  ...  0.08770  532.7609  1.9518    7.9901   
 468   1.3750  0.0022  0.0193  ...  0.11955  529.3827  1.9833    8.3900   
 469   1.3647  0.0106  0.0039  ...  0.21150  538.7818  1.7819    5.3200   
 470   1.5545 -0.0370 -0.0001  ...  0.11955  531.4127  2.1005   11.1100   
 471   1.5067 -0.0081  0.0037  ...  0.11955  536.5600  1.5473    7.6800   
 472   1.4412  0.0051 -0.0004  ...  0.11955  534.1073  2.0960    7.3000   
 473   1.4144 -0.0061 -0.0072  ...  0.30600  533.0927  2.4479    8.4600   
 474   1.4738 -0.0045  0.0017  ...  0.11955  531.2018  2.0455    5.5500   
 475   1.5216 -0.0257  0.0020  ...  0.06880  539.0582  2.0084    5.4100   
 476   1.4633  0.0204 -0.0066  ...  0.23320  533.9609  2.2365    7.5700   
 477   1.4441  0.0086 -0.0027  ...  0.11955  533.4455  2.2724    9.2300   
 478   1.4487 -0.0145  0.0083  ...  0.11955  532.5227  1.9491    7.1800   
 479   1.4964  0.0204  0.0133  ...  0.08770  535.3327  2.0886    9.3700   
 480   1.5320 -0.0005 -0.0051  ...  0.13020  533.3591  2.3759    9.7500   
 481   1.4411  0.0238 -0.0183  ...  0.11955  531.4409  2.3579    7.6900   
 482   1.4909 -0.0034 -0.0088  ...  0.11955  530.7018  2.5277   10.1700   
 483   1.4102  0.0308 -0.0119  ...  0.11955  534.1400  2.4975    7.8000   
 484   1.3539  0.0088 -0.0022  ...  0.11955  537.5418  1.7735    7.9300   
 485   1.5448 -0.0088  0.0057  ...  0.11955  539.0582  2.0084    5.4100   
 486   1.5713 -0.0125 -0.0065  ...  0.11955  535.5609  2.0970    8.4000   
 487   1.5975  0.0104 -0.0095  ...  0.19650  535.1664  2.0973    7.1900   
 488   1.4537 -0.0275  0.0002  ...  0.23670  535.2218  2.2168    6.7100   
 489   1.4413  0.0136  0.0083  ...  0.12740  533.7936  2.3170    9.1400   
 490   1.5171 -0.0154 -0.0165  ...  0.11955  535.6864  2.2420    6.5500   
 491   1.3322  0.0256  0.0073  ...  0.08770  535.9227  1.9110    8.6500   
 492   1.5150 -0.0114 -0.0075  ...  0.31890  533.1246  2.1672    9.4300   
 493   1.3552  0.0024 -0.0150  ...  0.11955  536.1264  1.8394    7.2700   
 494   1.3947  0.0018 -0.0125  ...  0.11955  532.7718  2.3951    7.6400   
 496   1.4547 -0.0106  0.0088  ...  0.13220  535.3318  2.0870    7.7400   
 497   1.3784  0.0166  0.0083  ...  0.11955  328.4664  2.6883  127.5100   
 498   1.4699  0.0080  0.0058  ...  0.11955  529.7355  2.0133    8.8200   
 499   1.4560  0.0142 -0.0041  ...  0.11955  532.4136  2.3575   10.3700   
 500   1.4192  0.0247 -0.0041  ...  0.11955  529.3827  1.9833    8.3900   
 501   1.4207  0.0016 -0.0056  ...  0.15390  533.6555  2.2472    8.0800   
 502   1.4902 -0.0013  0.0017  ...  0.11955  531.3900  2.1855    9.8200   
 503   1.4711 -0.0106  0.0065  ...  0.11955  536.8291  2.2289    7.0500   
 504   1.5188 -0.0109 -0.0005  ...  0.23130  535.1664  2.0973    7.1900   
 505   1.5634 -0.0059 -0.0029  ...  0.11955  532.3055  2.0377    8.2400   
 506   1.4699  0.0112 -0.0112  ...  0.11955  536.1964  2.1583    7.2100   
 507   1.4161  0.0209 -0.0095  ...  0.09930  536.8282  2.3113    6.8100   
 509   1.5299 -0.0037 -0.0037  ...  0.08770  533.1246  2.1672    9.4300   
 510   1.4469  0.0208 -0.0032  ...  0.16440  513.0527  1.8366  426.6900   
 511   1.5094  0.0039  0.0059  ...  0.16890  533.5409  2.0568    9.1100   
 512   1.5032  0.0266 -0.0081  ...  0.11955  534.6782  2.3952   10.1800   
 513   1.4119 -0.0031  0.0117  ...  0.11955  535.5236  2.0048    8.5800   
 514   1.3761  0.0108 -0.0013  ...  0.11890  535.3318  2.0870    7.7400   
 515   1.4710  0.0154 -0.0039  ...  0.11955  533.0191  2.0856   10.4301   
 516   1.5015 -0.0097  0.0133  ...  0.11610  533.8791  1.9980    8.7400   
 517   1.4805 -0.0073  0.0025  ...  0.04860  538.8482  2.2732    7.0900   
 519   1.3973  0.0348 -0.0170  ...  0.24290  534.7291  2.4644    8.8800   
 520   1.5460 -0.0149 -0.0024  ...  0.10790  534.5173  1.5036   10.3000   
 521   1.5514 -0.0275 -0.0001  ...  0.08770  539.0582  2.0084    5.4100   
 522   1.5700  0.0178 -0.0002  ...  0.11955  535.1664  2.0973    7.1900   
 523   1.5877 -0.0046 -0.0013  ...  0.14930  531.3473  2.0767    9.2100   
 524   1.4510  0.0041  0.0021  ...  0.20870  570.9627  2.1059  445.3200   
 525   1.5755 -0.0009 -0.0027  ...  0.04390  534.3400  2.0440    9.1700   
 526   1.5602 -0.0006 -0.0044  ...  0.11955  531.4073  2.0161    8.0500   
 527   1.5274  0.0075  0.0044  ...  0.16030  533.0809  2.0118    7.8400   
 528   1.5288 -0.0144 -0.0046  ...  0.11955  533.4936  1.9555    8.2000   
 529   1.4619  0.0189 -0.0060  ...  0.24550  534.6555  2.0338    6.9300   
 530   1.4357  0.0089  0.0052  ...  0.26980  533.2291  2.2587    8.2800   
 531   1.5527  0.0119 -0.0082  ...  0.11955  531.9209  2.2497    7.9900   
 532   1.4588 -0.0143  0.0017  ...  0.11955  537.4100  2.1437    5.1700   
 533   1.5973 -0.0534 -0.0284  ...  0.11955  530.8673  2.3145    9.8500   
 534   1.5525 -0.0078 -0.0005  ...  0.11955  535.8409  2.2266    6.3700   
 535   1.4509  0.0028 -0.0059  ...  0.15930  535.0791  2.1879    8.1500   
 536   1.4810  0.0191 -0.0029  ...  0.16580  535.1218  1.9541    7.2400   
 537   1.3906  0.0047 -0.0069  ...  0.11955  586.9145  2.1612  451.6900   
 538   1.5301 -0.0063  0.0039  ...  0.11955  533.0000  2.3950    8.4200   
 539   1.5495 -0.0180 -0.0218  ...  0.11955  533.3336  2.0324    8.0900   
 540   1.5657 -0.0079  0.0028  ...  0.11955  533.2618  2.2691   10.5600   
 541   1.4511 -0.0018  0.0013  ...  0.11955  532.7309  1.8891    8.4000   
 542   1.5391  0.0086 -0.0063  ...  0.11955  532.7427  2.3382    9.1300   
 543   1.5166 -0.0088  0.0049  ...  0.18700  512.4609  1.5211  426.8500   
 544   1.5308  0.0055 -0.0062  ...  0.11955  532.5182  2.0621    8.8700   
 545   1.5219 -0.0022 -0.0003  ...  0.08920  537.4627  2.1325    7.4800   
 546   1.5168 -0.0097  0.0009  ...  0.16530  535.0918  2.0927    6.3600   
 547   1.6053 -0.0017 -0.0049  ...  0.16260  533.7682  1.9722    5.6201   
 548   1.5190 -0.0053 -0.0124  ...  0.10040  531.0854  2.0685    9.9300   
 549   1.5886 -0.0044  0.0092  ...  0.08060  529.4909  2.0437   10.6600   
 550   1.4863 -0.0064 -0.0059  ...  0.11410  534.1364  2.3033    9.1400   
 551   1.4790 -0.0041  0.0095  ...  0.11955  533.1246  2.1672    9.4300   
 552   1.5520  0.0175  0.0027  ...  0.11380  533.7700  2.0384    6.6600   
 553   1.4103 -0.0089  0.0061  ...  0.11955  533.5727  2.1470   10.2100   
 554   1.4862  0.0014 -0.0056  ...  0.11955  533.7791  1.6068    9.3100   
 555   1.5828  0.0091 -0.0045  ...  0.11955  535.1664  2.0973    7.1900   
 556   1.5870 -0.0191  0.0064  ...  0.11955  532.1109  2.0349    9.4900   
 557   1.5267  0.0228  0.0068  ...  0.11955  589.5082  2.0367  452.5400   
 558   1.4616 -0.0100  0.0037  ...  0.11955  536.2118  1.9617    9.6400   
 559   1.5006 -0.0321 -0.0087  ...  0.16410  536.1500  1.9941    8.7800   
 560   1.5211  0.0206 -0.0159  ...  0.11955  533.0327  2.0001    9.3000   
 561   1.5740 -0.0116 -0.0051  ...  0.08770  532.5646  2.7395   13.3500   
 562   1.4899  0.0005  0.0055  ...  0.10410  532.5646  2.7395   13.3500   
 563   1.5643  0.0263 -0.0021  ...  0.11955  532.6046  2.1100    9.9200   
 564   1.3701 -0.0081  0.0013  ...  0.11955  529.8882  1.9955    8.8800   
 565   1.4866  0.0018 -0.0018  ...  0.16160  531.4073  2.0161    8.0500   
 566   1.5613 -0.0028  0.0008  ...  0.11955  533.5555  2.0417    6.0800   
 567   1.4234 -0.0020  0.0027  ...  0.11955  534.7282  1.6257    7.0000   
 568   1.5607  0.0138  0.0039  ...  0.08770  532.8245  1.9063   10.0099   
 569   1.4330  0.0388  0.0167  ...  0.11955  531.6773  1.9737    7.5800   
 570   1.5624 -0.0159  0.0013  ...  0.11955  531.8827  2.0572    8.3100   
 571   1.4413 -0.0206  0.0151  ...  0.08770  534.0555  2.0153    8.5800   
 572   1.5996 -0.0095 -0.0072  ...  0.09900  534.3355  2.2662    9.0300   
 573   1.4922  0.0195  0.0003  ...  0.12940  536.4800  1.9240    8.9800   
 574   1.5835 -0.0236  0.0044  ...  0.08770  532.5646  2.7395   13.3500   
 575   1.4838  0.0000 -0.0074  ...  0.11955  536.6973  1.5839    8.1400   
 577   1.5731  0.0076  0.0040  ...  0.08770  531.2927  1.9943   11.3300   
 578   1.4981  0.0022 -0.0020  ...  0.11955  531.1664  2.0013   11.1700   
 579   1.3725  0.0335  0.0001  ...  0.13670  531.5855  2.3460    9.0100   
 580   1.3976 -0.0022 -0.0064  ...  0.07620  533.3591  2.3759    9.7500   
 581   1.4268  0.0127 -0.0005  ...  0.20480  532.4464  2.3405    9.2700   
 582   1.4794  0.0042  0.0089  ...  0.15980  533.3945  2.3192    8.1900   
 584   1.5415  0.0055 -0.0208  ...  0.09510  538.5400  2.0768    5.3000   
 585   1.5859  0.0286 -0.0035  ...  0.08770  532.5646  2.7395   13.3500   
 586   1.5071 -0.0085 -0.0068  ...  0.11955  534.2982  1.9700    5.7900   
 587   1.4773  0.0028  0.0101  ...  0.09170  534.0555  2.0153    8.5800   
 588   1.5470 -0.0021  0.0030  ...  0.12750  534.6764  2.0007    8.0900   
 589   1.4661 -0.0057  0.0036  ...  0.11955  534.6764  2.0007    8.0900   
 590   1.5157 -0.0158  0.0008  ...  0.15200  532.9764  2.3909   10.9800   
 591   1.5055  0.0354 -0.0032  ...  0.09330  531.4073  2.0161    8.0500   
 592   1.4527  0.0000 -0.0055  ...  0.13090  536.0000  2.0571    7.9100   
 593   1.4713  0.0384  0.0011  ...  0.08770  532.5336  2.2723    8.0000   
 594   1.4817  0.0030  0.0049  ...  0.11955  531.4073  2.0161    8.0500   
 595   1.5186  0.0171  0.0043  ...  0.07090  532.7527  2.4554    8.6900   
 596   1.4309  0.0455 -0.0186  ...  0.16180  536.0573  1.8813    8.4200   
 597   1.4043  0.0069 -0.0041  ...  0.11955  531.4073  2.0161    8.0500   
 598   1.4708  0.0121 -0.0155  ...  0.16540  533.9782  2.3725    7.7900   
 599   1.5477  0.0197  0.0057  ...  0.08770  535.0918  2.0927    6.3600   
 600   1.3698  0.0141  0.0040  ...  0.11955  533.5309  1.9940    8.0100   
 602   1.4868 -0.0089  0.0142  ...  0.11955  538.0664  2.1359    8.2300   
 603   1.4207  0.0101 -0.0269  ...  0.11955  528.7891  2.0351   11.1500   
 604   1.3946 -0.0019  0.0094  ...  0.11955  533.1082  2.1742    9.6500   
 606   1.4791 -0.0117 -0.0053  ...  0.12380  535.1145  2.1058    8.9000   
 607   1.4444 -0.0014  0.0001  ...  0.11955  532.0600  2.1892    7.8800   
 608   1.4522 -0.0099 -0.0020  ...  0.15630  532.9918  2.0121    9.2599   
 609   1.4209 -0.0262  0.0018  ...  0.08390  532.7609  1.9518    7.9901   
 610   1.3843 -0.0010 -0.0004  ...  0.11955  533.0873  2.0606    8.6700   
 611   1.4750  0.0248 -0.0097  ...  0.17940  531.0854  2.0685    9.9300   
 612   1.5334  0.0098  0.0010  ...  0.14460  531.4291  2.1392    9.3200   
 613   1.4373 -0.0027  0.0034  ...  0.13540  530.0355  2.0373    7.8500   
 614   1.3949  0.0084 -0.0031  ...  0.11955  533.4727  2.0774   10.6300   
 615   1.4635  0.0101 -0.0075  ...  0.11955  529.1382  2.1146    8.8300   
 616   1.5227  0.0088 -0.0164  ...  0.15220  534.9636  2.1236    7.5400   
 617   1.4402  0.0035 -0.0001  ...  0.11955  530.8864  2.0458    8.4600   
 618   1.4701 -0.0181 -0.0010  ...  0.14580  533.5636  2.0273    7.3300   
 619   1.4470 -0.0093 -0.0009  ...  0.11955  534.9282  2.0004    8.1700   
 620   1.4256 -0.0125  0.0031  ...  0.11955  510.2891  2.0217  435.5700   
 621   1.4002  0.0153  0.0065  ...  0.11955  532.8618  1.9972    9.0600   
 622   1.4385 -0.0171  0.0070  ...  0.08770  532.2100  2.2089    8.2200   
 623   1.3949  0.0065 -0.0018  ...  0.08770  533.4755  2.1902    9.0800   
 624   1.5301 -0.0023  0.0020  ...  0.20200  538.5400  2.0768    5.3000   
 625   1.5455 -0.0140 -0.0039  ...  0.08770  531.3891  2.0882    8.0800   
 626   1.4647 -0.0212  0.0009  ...  0.09160  535.4364  2.0862    6.2300   
 627   1.4262 -0.0209  0.0020  ...  0.22620  531.4073  2.0161    8.0500   
 628   1.4849 -0.0072 -0.0033  ...  0.17340  533.7309  2.1724    8.3500   
 629   1.5549 -0.0130  0.0074  ...  0.11955  531.2355  2.0715    9.7200   
 630   1.5097 -0.0023  0.0045  ...  0.09990  539.2554  1.6494    5.2200   
 631   1.4867  0.0014  0.0092  ...  0.10640  535.2109  2.0871    7.4500   
 632   1.4819 -0.0153  0.0129  ...  0.08770  533.6900  2.0464    8.8900   
 633   1.4464 -0.0081 -0.0037  ...  0.11955  532.9591  2.0687    7.1700   
 635   1.3720 -0.0005  0.0052  ...  0.11955  534.1891  2.0627    7.7200   
 636   1.5782 -0.0092  0.0054  ...  0.11955  535.8736  1.5932    8.8500   
 637   1.4872 -0.0065  0.0066  ...  0.20200  536.4400  2.0978    6.7800   
 638   1.4449 -0.0099  0.0037  ...  0.17330  536.6582  2.1390    5.9300   
 639   1.5516 -0.0179  0.0036  ...  0.11550  533.9782  2.3449    6.0000   
 640   1.4266 -0.0146 -0.0023  ...  0.11955  532.5291  2.0436    6.8100   
 641   1.5456 -0.0208  0.0168  ...  0.21170  533.1246  2.1672    9.4300   
 642   1.3575 -0.0252 -0.0028  ...  0.11955  535.9100  2.0366    7.3800   
 643   1.5283 -0.0027  0.0095  ...  0.11320  529.3191  2.0905   11.2500   
 644   1.4403 -0.0169  0.0213  ...  0.12280  537.1036  1.9971    7.0000   
 645   1.4670 -0.0088 -0.0010  ...  0.21740  533.0691  1.9841    8.4900   
 646   1.3984 -0.0108  0.0100  ...  0.22360  534.8727  2.3094    6.1600   
 647   1.5166 -0.0118 -0.0060  ...  0.12720  530.5964  2.0289   12.0600   
 648   1.5316 -0.0214 -0.0073  ...  0.16040  531.0918  2.0267   10.6300   
 649   1.4955 -0.0222  0.0132  ...  0.11955  534.8518  2.1990    8.5200   
 650   1.5089 -0.0131  0.0210  ...  0.08770  531.0854  2.0685    9.9300   
 651   1.4149 -0.0045  0.0085  ...  0.11955  532.5764  2.0424   11.1000   
 652   1.4829 -0.0072  0.0114  ...  0.11070  531.0854  2.0685    9.9300   
 653   1.3785 -0.0026  0.0055  ...  0.03060  535.4364  2.0862    6.2300   
 654   1.4479  0.0073  0.0083  ...  0.10690  534.1318  2.0039    8.2900   
 655   1.4821 -0.0171  0.0073  ...  0.08770  533.0691  1.9841    8.4900   
 656   1.3969  0.0116  0.0016  ...  0.11955  534.9473  2.3269    7.3500   
 657   1.5869 -0.0344 -0.0019  ...  0.14600  532.0036  2.0531    8.1400   
 658   1.4012 -0.0112  0.0029  ...  0.11955  534.6764  2.0007    8.0900   
 659   1.4473 -0.0207  0.0196  ...  0.11955  532.2100  2.2089    8.2200   
 660   1.4679 -0.0022  0.0098  ...  0.05350  536.5882  2.0221    6.9500   
 661   1.4895 -0.0393 -0.0032  ...  0.11955  533.7327  2.2848    6.8700   
 662   1.4950 -0.0097 -0.0054  ...  0.11955  534.8336  2.1127    6.3800   
 663   1.3857 -0.0027  0.0031  ...  0.11955  536.0336  2.2003    7.3900   
 664   1.4786 -0.0179  0.0085  ...  0.09300  538.9091  2.0618    5.4100   
 665   1.3877  0.0149  0.0131  ...  0.13890  533.9636  2.0317    8.6500   
 666   1.4975 -0.0248  0.0135  ...  0.20340  532.5336  2.2723    8.0000   
 667   1.4890 -0.0058 -0.0026  ...  0.09050  538.9091  2.0618    5.4100   
 668   1.5095 -0.0272 -0.0044  ...  0.15020  532.4936  2.1201    8.0200   
 669   1.4830 -0.0328  0.0048  ...  0.13650  530.7118  2.0223    9.2900   
 670   1.5227 -0.0207  0.0192  ...  0.11955  535.4364  2.0862    6.2300   
 671   1.4548  0.0086  0.0061  ...  0.11955  532.6964  2.0363    9.3200   
 672   1.5277 -0.0239  0.0149  ...  0.08770  532.5646  2.7395   13.3500   
 673   1.4611 -0.0088  0.0121  ...  0.21040  536.7600  2.1073    6.8300   
 674   1.3898 -0.0229  0.0191  ...  0.08130  537.1036  1.9971    7.0000   
 675   1.4429 -0.0293  0.0013  ...  0.02260  535.4364  2.0862    6.2300   
 676   1.4252 -0.0221  0.0046  ...  0.11955  533.6900  2.0464    8.8900   
 677   1.4805  0.0203 -0.0178  ...  0.11955  534.0618  2.1214    7.9400   
 678   1.5092 -0.0133 -0.0014  ...  0.06660  530.9555  2.1422    8.2100   
 679   1.3725 -0.0152  0.0169  ...  0.11955  487.6227  2.3371  435.2600   
 680   1.4349  0.0062  0.0084  ...  0.11955  532.4227  2.2358    8.1000   
 681   1.4671 -0.0140  0.0142  ...  0.07650  539.2554  1.6494    5.2200   
 682   1.5182 -0.0163  0.0034  ...  0.11955  535.2718  1.6985    7.3600   
 683   1.3700 -0.0234 -0.0043  ...  0.11955  536.1436  2.2328    7.1500   
 684   1.4623  0.0030 -0.0017  ...  0.11955  535.4846  2.1608    7.4300   
 685   1.4341 -0.0101 -0.0072  ...  0.11955  529.5782  2.0500   11.3000   
 686   1.5034  0.0083 -0.0002  ...  0.11955  529.3191  2.0905   11.2500   
 687   1.5291 -0.0110 -0.0094  ...  0.11955  534.3236  1.9533    9.1000   
 688   1.3636 -0.0236 -0.0121  ...  0.12500  533.7836  2.0600    8.7800   
 689   1.4243  0.0076 -0.0058  ...  0.11955  532.1318  2.3355    9.3701   
 690   1.4643  0.0047  0.0039  ...  0.08770  532.2100  2.2089    8.2200   
 691   1.4853 -0.0186 -0.0069  ...  0.15470  532.7591  2.1800    8.9800   
 692   1.4644  0.0049 -0.0020  ...  0.21390  530.7909  2.0698   10.5500   
 693   1.4601  0.0047 -0.0072  ...  0.17730  537.1036  1.9971    7.0000   
 694   1.5099 -0.0135  0.0035  ...  0.08770  532.5646  2.7395   13.3500   
 695   1.4130 -0.0167  0.0224  ...  0.18630  531.4073  2.0161    8.0500   
 696   1.4063 -0.0106  0.0145  ...  0.11955  534.6764  2.0007    8.0900   
 697   1.4165  0.0148  0.0309  ...  0.11955  532.7591  2.1800    8.9800   
 698   1.4194 -0.0027  0.0217  ...  0.14450  533.0691  1.9841    8.4900   
 699   1.4020 -0.0170 -0.0026  ...  0.11990  531.3891  2.0882    8.0800   
 700   1.4443  0.0005  0.0022  ...  0.18000  530.8818  2.1238    9.1900   
 701   1.4721  0.0008  0.0077  ...  0.22300  538.5400  2.0768    5.3000   
 702   1.4602 -0.0061 -0.0043  ...  0.21610  533.5555  2.0417    6.0800   
 703   1.3807 -0.0119  0.0126  ...  0.67290  538.0000  2.0030    4.9400   
 704   1.4416  0.0135  0.0051  ...  0.08770  531.4073  2.0161    8.0500   
 705   1.5214 -0.0037 -0.0055  ...  0.08770  538.5400  2.0768    5.3000   
 706   1.5126 -0.0133  0.0132  ...  0.09450  537.1036  1.9971    7.0000   
 707   1.4667 -0.0472 -0.0077  ...  0.13720  533.0691  1.9841    8.4900   
 708   1.4924 -0.0185  0.0057  ...  0.08770  531.4073  2.0161    8.0500   
 710   1.4922  0.0045 -0.0087  ...  0.18040  533.0691  1.9841    8.4900   
 711   1.3923  0.0061  0.0091  ...  0.14120  533.6900  2.0464    8.8900   
 712   1.4112 -0.0156  0.0174  ...  0.14720  531.0854  2.0685    9.9300   
 713   1.4333  0.0022 -0.0045  ...  0.35440  530.9718  2.1158   10.7100   
 714   1.4063 -0.0101  0.0022  ...  0.34820  535.5664  1.9829    7.0200   
 715   1.4125 -0.0199  0.0024  ...  0.28770  530.8818  2.1238    9.1900   
 716   1.4918 -0.0068  0.0005  ...  0.31730  534.9391  2.1006   10.4600   
 717   1.4674  0.0161  0.0005  ...  0.11955  533.0700  2.0591    7.5800   
 718   1.4271 -0.0136  0.0197  ...  0.11955  531.0854  2.0685    9.9300   
 719   1.4586 -0.0011  0.0056  ...  0.08770  532.9600  1.9280   10.2300   
 720   1.4374 -0.0128 -0.0064  ...  0.11640  538.0000  2.0030    4.9400   
 721   1.4115 -0.0130 -0.0022  ...  0.03090  535.5664  1.9829    7.0200   
 722   1.4906 -0.0061 -0.0084  ...  0.08770  537.1036  1.9971    7.0000   
 723   1.4874 -0.0016 -0.0119  ...  0.08360  530.8818  2.1238    9.1900   
 724   1.4401 -0.0214 -0.0023  ...  0.08770  536.0100  2.1350    9.5601   
 725   1.4968  0.0063 -0.0024  ...  0.24570  536.6173  1.9323    6.6200   
 726   1.4476  0.0169  0.0025  ...  0.08770  533.9782  2.3449    6.0000   
 727   1.4436 -0.0053 -0.0014  ...  0.21470  533.9782  2.3449    6.0000   
 728   1.4583  0.0144  0.0015  ...  0.20490  532.5646  2.7395   13.3500   
 729   1.5453 -0.0101  0.0023  ...  0.08770  530.8818  2.1238    9.1900   
 730   1.4285 -0.0222 -0.0006  ...  0.06920  534.2818  2.2407    8.5400   
 731   1.5251  0.0061 -0.0002  ...  0.29970  534.9900  2.0051    8.5400   
 732   1.5031  0.0145 -0.0005  ...  0.11955  530.8818  2.1238    9.1900   
 733   1.5251  0.0468 -0.0010  ...  0.12320  534.5745  1.9791    9.7000   
 734   1.4645  0.0148  0.0016  ...  0.21350  535.5664  1.9829    7.0200   
 735   1.4621  0.0084 -0.0021  ...  0.11955  533.8954  2.0083    8.7100   
 736   1.4231 -0.0196 -0.0041  ...  0.22560  534.1500  2.3534    8.4200   
 737   1.5131 -0.0105  0.0057  ...  0.08770  534.1500  2.3534    8.4200   
 738   1.4903 -0.0064 -0.0028  ...  0.08770  534.1500  2.3534    8.4200   
 739   1.5271 -0.0070  0.0069  ...  0.08770  534.2327  2.2409    5.8400   
 740   1.4883  0.0109  0.0157  ...  0.08770  535.8591  2.3048    8.5500   
 741   1.4355 -0.0153 -0.0035  ...  0.13290  530.8573  2.2566    9.1100   
 742   1.4652 -0.0142  0.0150  ...  0.08770  535.8591  2.3048    8.5500   
 743   1.4446 -0.0016 -0.0055  ...  0.09960  532.1391  2.3488    9.4400   
 744   1.5059 -0.0084 -0.0024  ...  0.18310  533.2609  2.3318    6.5500   
 745   1.5753 -0.0111 -0.0029  ...  0.12530  535.0991  2.1541    9.0500   
 746   1.4855 -0.0025  0.0147  ...  0.23080  535.9764  2.6756    8.5300   
 747   1.5527 -0.0216  0.0013  ...  0.19240  534.9691  2.3278    9.1000   
 748   1.5160 -0.0041  0.0084  ...  0.12170  530.1873  2.2967   11.2800   
 749   1.5343 -0.0013  0.0035  ...  0.07360  535.3618  2.3768    9.7000   
 750   1.5361  0.0188 -0.0002  ...  0.08770  536.6882  2.1806    6.5800   
 751   1.4187  0.0012  0.0004  ...  0.05050  534.5309  2.2417    6.3700   
 752   1.5629 -0.0261 -0.0034  ...  0.09210  531.0464  2.3151    5.5500   
 753   1.4814  0.0135 -0.0121  ...  0.08770  532.5646  2.7395   13.3500   
 754   1.5284 -0.0136 -0.0090  ...  0.10490  531.9818  2.4070    6.3400   
 755   1.4763  0.0012 -0.0127  ...  0.08700  532.3936  2.2964   10.0600   
 756   1.5165 -0.0228  0.0101  ...  0.08770  536.0100  2.1350    9.5601   
 757   1.5079  0.0031 -0.0105  ...  0.06980  530.8818  2.1238    9.1900   
 758   1.4955  0.0236 -0.0040  ...  0.08770  531.2564  2.3999   11.9300   
 759   1.3835  0.0042 -0.0087  ...  0.15160  537.8600  2.4783    5.7700   
 760   1.4206 -0.0052  0.0010  ...  0.10230  532.5973  2.3271    9.0200   
 761   1.4898  0.0200 -0.0153  ...  0.05060  535.9764  2.6756    8.5300   
 762   1.4405  0.0019  0.0027  ...  0.08770  536.6509  2.4087    5.8600   
 763   1.3602 -0.0153  0.0032  ...  0.08770  535.3036  2.3086    8.3000   
 764   1.4372 -0.0113 -0.0075  ...  0.08020  533.9909  2.2169    8.9100   
 765   1.4754  0.0089 -0.0046  ...  0.08130  536.0100  2.1350    9.5601   
 766   1.4714  0.0065  0.0147  ...  0.08770  531.2564  2.3999   11.9300   
 767   1.5245 -0.0076 -0.0050  ...  0.08770  536.6882  2.1806    6.5800   
 768   1.4434 -0.0183 -0.0101  ...  0.09940  534.1500  2.3534    8.4200   
 769   1.4801 -0.0301 -0.0123  ...  0.16960  533.2545  2.3218    7.0699   
 770   1.5106  0.0072 -0.0022  ...  0.07000  531.2564  2.3999   11.9300   
 771   1.5849 -0.0076 -0.0069  ...  0.17490  535.5664  1.9829    7.0200   
 772   1.4346  0.0051  0.0084  ...  0.14950  536.6509  2.4087    5.8600   
 773   1.4432  0.0085 -0.0151  ...  0.22330  533.6400  2.3592   10.5200   
 774   1.4374 -0.0117 -0.0066  ...  0.15160  535.0509  1.6984    7.7100   
 775   1.3619 -0.0228  0.0057  ...  0.08770  533.6536  2.1648   10.3700   
 776   1.4723 -0.0093 -0.0039  ...  0.06220  535.7755  2.2816   10.5800   
 777   1.4080 -0.0032  0.0049  ...  0.25300  538.4991  2.2135    8.7200   
 778   1.4323 -0.0180  0.0090  ...  0.08770  538.4991  2.2135    8.7200   
 779   1.4250 -0.0210 -0.0018  ...  0.08770  536.6509  2.4087    5.8600   
 780   1.3420  0.0008  0.0022  ...  0.11955  534.0891  2.1926    9.8100   
 781   1.4692 -0.0216 -0.0056  ...  0.18780  532.5573  2.1761   13.3500   
 782   1.4418 -0.0096  0.0006  ...  0.08770  535.4918  2.2717    6.5600   
 783   1.4279 -0.0212 -0.0178  ...  0.08770  536.6509  2.4087    5.8600   
 784   1.4169 -0.0103 -0.0171  ...  0.08770  533.3882  2.3461    8.6300   
 785   1.3596 -0.0138 -0.0025  ...  0.12540  535.6282  2.2411    9.1100   
 786   1.3386 -0.0006  0.0060  ...  0.08770  534.1500  2.3534    8.4200   
 787   1.4050 -0.0120  0.0055  ...  0.17150  535.6282  2.2411    9.1100   
 788   1.3675  0.0038  0.0007  ...  0.19190  532.8364  1.6423   11.8100   
 789   1.4209  0.0035  0.0107  ...  0.12870  534.1482  1.6633   10.9200   
 790   1.4122 -0.0058  0.0057  ...  0.08770  537.5200  1.7312   10.4400   
 791   1.3886  0.0107  0.0077  ...  0.12640  540.1218  1.7997    6.8700   
 792   1.3735 -0.0023  0.0086  ...  0.08770  540.1218  1.7997    6.8700   
 793   1.3920 -0.0044 -0.0165  ...  0.08770  534.1500  2.3534    8.4200   
 794   1.4221 -0.0098 -0.0166  ...  0.04200  536.6882  2.1806    6.5800   
 796   1.3819 -0.0041  0.0036  ...  0.12700  529.9273  2.3342    9.5000   
 798   1.5366  0.0091  0.0057  ...  0.18080  533.3382  2.4033    9.4300   
 799   1.4588  0.0067  0.0055  ...  0.13750  532.2855  2.2181    6.8600   
 800   1.4149 -0.0092 -0.0008  ...  0.18030  535.0509  1.6984    7.7100   
 801   1.3794 -0.0052 -0.0035  ...  0.11955  536.4418  1.9814    7.6801   
 802   1.5093 -0.0022  0.0126  ...  0.08770  534.1482  1.6633   10.9200   
 803   1.4547  0.0007  0.0159  ...  0.17310  534.1482  1.6633   10.9200   
 804   1.3500  0.0002  0.0023  ...  0.08770  536.6509  2.4087    5.8600   
 805   1.4480 -0.0233  0.0087  ...  0.10690  529.6845  2.3500    9.0300   
 806   1.4056  0.0177  0.0153  ...  0.13730  534.1482  1.6633   10.9200   
 807   1.4745 -0.0053  0.0080  ...  0.08580  538.4991  2.2135    8.7200   
 808   1.4070  0.0053  0.0047  ...  0.14570  538.4991  2.2135    8.7200   
 809   1.4700 -0.0283 -0.0086  ...  0.11900  534.1482  1.6633   10.9200   
 810   1.3640  0.0035  0.0057  ...  0.09990  533.3382  2.4033    9.4300   
 811   1.4005  0.0032  0.0055  ...  0.18630  536.6509  2.4087    5.8600   
 812   1.3958  0.0024  0.0053  ...  0.08770  535.6282  2.2411    9.1100   
 813   1.3761  0.0023  0.0032  ...  0.22350  534.8373  2.4114    9.9100   
 814   1.5136 -0.0090  0.0129  ...  0.11120  533.6364  2.2904   13.0900   
 815   1.4339 -0.0124 -0.0078  ...  0.08770  534.6691  2.1795    9.5099   
 816   1.3753  0.0079  0.0145  ...  0.08770  536.6509  2.4087    5.8600   
 817   1.4353 -0.0001  0.0018  ...  0.13060  532.6945  2.2425   11.1500   
 818   1.3355 -0.0115 -0.0209  ...  0.17960  534.1482  1.6633   10.9200   
 819   1.4266  0.0126  0.0075  ...  0.22460  534.1482  1.6633   10.9200   
 820   1.4379  0.0053  0.0079  ...  0.19050  535.6282  2.2411    9.1100   
 821   1.4440 -0.0079 -0.0076  ...  0.16260  532.5954  2.1606   10.5900   
 822   1.4875  0.0052  0.0003  ...  0.12040  533.3882  2.3461    8.6300   
 823   1.3754  0.0003 -0.0028  ...  0.08770  532.8364  1.6423   11.8100   
 824   1.4854 -0.0006  0.0089  ...  0.10210  534.1245  2.1586    9.9700   
 825   1.4579 -0.0021  0.0059  ...  0.14090  535.0509  1.6984    7.7100   
 827   1.4008 -0.0014  0.0132  ...  0.09680  534.1482  1.6633   10.9200   
 828   1.4889 -0.0047 -0.0091  ...  0.19060  532.9891  1.7784    7.5000   
 829   1.5173  0.0069  0.0075  ...  0.11890  533.3882  2.3461    8.6300   
 830   1.5300 -0.0279 -0.0040  ...  0.22590  534.1482  1.6633   10.9200   
 832   1.3896  0.0138  0.0000  ...  0.18320  534.1482  1.6633   10.9200   
 833   1.4108 -0.0046 -0.0024  ...  0.08770  534.1482  1.6633   10.9200   
 834   1.5094 -0.0046  0.0121  ...  0.08770  530.6746  2.0193   10.8000   
 835   1.3940 -0.0073  0.0006  ...  0.27290  529.7436  2.2313    7.1900   
 836   1.4517  0.0069  0.0094  ...  0.15730  530.6746  2.0193   10.8000   
 837   1.4798  0.0046  0.0181  ...  0.21680  534.6127  2.2455    9.5700   
 838   1.4563  0.0075  0.0031  ...  0.08770  529.7436  2.2313    7.1900   
 839   1.4696  0.0081  0.0063  ...  0.08770  534.1482  1.6633   10.9200   
 840   1.5209  0.0097  0.0106  ...  0.19390  532.2636  2.2625    8.3200   
 841   1.4055 -0.0134 -0.0135  ...  0.22160  534.1482  1.6633   10.9200   
 842   1.4997  0.0027  0.0056  ...  0.15400  532.0164  2.3658   13.4900   
 843   1.4175 -0.0144 -0.0119  ...  0.18940  536.4418  2.3416    6.6600   
 844   1.5172 -0.0135  0.0070  ...  0.08040  536.6727  1.7324    7.4600   
 845   1.4959  0.0137  0.0050  ...  0.27560  533.3382  2.4033    9.4300   
 846   1.6063 -0.0020  0.0066  ...  0.22660  532.9764  2.4327   13.4400   
 847   1.5568  0.0107  0.0070  ...  0.19280  530.6746  2.0193   10.8000   
 848   1.4914 -0.0023  0.0157  ...  0.28780  530.6746  2.0193   10.8000   
 849   1.4612 -0.0052 -0.0130  ...  0.08770  530.6746  2.0193   10.8000   
 850   1.3964  0.0065 -0.0080  ...  0.25940  532.5954  2.1606   10.5900   
 851   1.4106 -0.0181 -0.0130  ...  0.35120  534.7818  2.0324    8.7000   
 852   1.5189 -0.0090  0.0096  ...  0.09530  534.1482  1.6633   10.9200   
 853   1.4698 -0.0104  0.0075  ...  0.29130  532.1636  1.7199    8.9800   
 854   1.4788  0.0124  0.0147  ...  0.10030  535.0509  1.6984    7.7100   
 855   1.4282  0.0154  0.0120  ...  0.10200  534.1482  1.6633   10.9200   
 856   1.5590 -0.0032  0.0135  ...  0.10040  531.6791  2.0502    6.5900   
 857   1.4302  0.0058 -0.0060  ...  0.08770  532.5954  2.1606   10.5900   
 858   1.4446 -0.0109  0.0057  ...  0.16230  533.3709  2.1470    9.3400   
 859   1.3331 -0.0126  0.0076  ...  0.07070  532.0164  2.3658   13.4900   
 860   1.3997  0.0062 -0.0063  ...  0.08770  532.1736  2.4452   10.8300   
 861   1.3740 -0.0021 -0.0017  ...  0.13290  538.5400  2.0768    5.3000   
 862   1.3333 -0.0054  0.0045  ...  0.15660  529.9855  2.2416   10.0600   
 863   1.4964 -0.0099  0.0061  ...  0.11955  532.8382  2.0005   11.2401   
 864   1.3179 -0.0089 -0.0084  ...  0.13470  532.9700  2.2248   10.0900   
 865   1.5070  0.0090 -0.0038  ...  0.15040  534.1482  1.6633   10.9200   
 866   1.4691 -0.0088  0.0002  ...  0.17460  534.6127  2.2455    9.5700   
 867   1.4951 -0.0049 -0.0086  ...  0.09900  526.5336  2.6194    9.6000   
 868   1.4463 -0.0031 -0.0050  ...  0.41270  532.5954  2.1606   10.5900   
 869   1.4445 -0.0154  0.0125  ...  0.09930  530.5682  2.1417   10.8700   
 870   1.4873  0.0121  0.0121  ...  0.13350  538.4991  2.2135    8.7200   
 872   1.4542  0.0142 -0.0064  ...  0.26450  534.3127  2.1819    7.9300   
 873   1.5084 -0.0118 -0.0023  ...  0.24840  531.8455  2.2544    8.6400   
 874   1.5540 -0.0049  0.0129  ...  0.51860  531.5127  2.2373   13.8100   
 875   1.5250 -0.0100  0.0010  ...  0.08770  535.9636  2.2026    6.7700   
 876   1.5793  0.0021  0.0037  ...  0.30690  532.4645  2.2512    8.2500   
 877   1.4080  0.0130  0.0044  ...  0.12200  531.8455  2.2544    8.6400   
 878   1.4411  0.0096  0.0090  ...  0.18740  532.0973  2.1448    9.6000   
 879   1.4108  0.0095 -0.0026  ...  0.06060  531.8455  2.2544    8.6400   
 880   1.5960  0.0023  0.0084  ...  0.21710  535.1345  2.1394    5.7300   
 881   1.4500 -0.0053  0.0005  ...  0.08770  531.3346  2.2421    8.4900   
 882   1.3746  0.0012  0.0107  ...  0.08770  532.0973  2.1448    9.6000   
 883   1.4919 -0.0001 -0.0018  ...  0.14700  528.7918  2.5088   10.3300   
 884   1.6411 -0.0068 -0.0033  ...  0.11280  531.8455  2.2544    8.6400   
 885   1.5762  0.0028 -0.0066  ...  0.04770  533.3073  2.3490   10.1900   
 886   1.4335  0.0046  0.0042  ...  0.08770  534.6682  2.0435    9.4300   
 887   1.4503  0.0149  0.0010  ...  0.08770  533.2873  2.3473   10.9300   
 888   1.3434  0.0030  0.0000  ...  0.14290  530.5682  2.1417   10.8700   
 889   1.4334  0.0082 -0.0027  ...  0.20920  504.9173  2.2656  438.8700   
 890   1.4532  0.0049 -0.0048  ...  0.08770  533.2873  2.3473   10.9300   
 891   1.4017  0.0003  0.0041  ...  0.12260  532.5027  2.4053    7.5400   
 892   1.5475 -0.0087 -0.0093  ...  0.08770  529.2209  1.9102    7.9800   
 893   1.3481 -0.0025  0.0150  ...  0.38660  535.9636  2.2026    6.7700   
 894   1.4521 -0.0124 -0.0099  ...  0.08770  532.0973  2.1448    9.6000   
 895   1.4113  0.0108  0.0062  ...  0.08770  530.5682  2.1417   10.8700   
 896   1.5241 -0.0025 -0.0096  ...  0.13210  533.8955  2.3306    9.6400   
 897   1.3882  0.0008  0.0050  ...  0.08770  530.6746  2.0193   10.8000   
 898   1.4463  0.0073  0.0046  ...  0.08770  531.3346  2.2421    8.4900   
 899   1.4958  0.0004  0.0037  ...  0.11370  531.0009  2.0786   11.1500   
 900   1.3623 -0.0076  0.0112  ...  0.11550  526.5336  2.6194    9.6000   
 901   1.3824 -0.0001 -0.0050  ...  0.38220  526.5336  2.6194    9.6000   
 902   1.3907  0.0017 -0.0045  ...  0.17630  528.7918  2.5088   10.3300   
 903   1.3947  0.0075 -0.0146  ...  0.20530  533.2873  2.3473   10.9300   
 904   1.5196  0.0183 -0.0008  ...  0.08770  530.5682  2.1417   10.8700   
 905   1.4684  0.0013  0.0044  ...  0.10440  526.5336  2.6194    9.6000   
 906   1.3487  0.0050  0.0006  ...  0.14300  504.9173  2.2656  438.8700   
 907   1.4678  0.0024 -0.0017  ...  0.25980  532.0973  2.1448    9.6000   
 908   1.5424  0.0197  0.0034  ...  0.14830  504.9173  2.2656  438.8700   
 909   1.4418 -0.0002  0.0028  ...  0.10230  532.0973  2.1448    9.6000   
 910   1.5794 -0.0034 -0.0127  ...  0.11955  532.4618  2.2425   10.2700   
 911   1.4706  0.0152 -0.0055  ...  0.11955  530.9809  2.3259    9.9400   
 912   1.3527  0.0198  0.0076  ...  0.08770  530.5682  2.1417   10.8700   
 913   1.3713  0.0102  0.0058  ...  0.20820  532.8227  2.3242   11.7600   
 915   1.4526 -0.0175 -0.0028  ...  0.08770  526.6064  1.9339   10.0400   
 916   1.3952 -0.0198  0.0026  ...  0.24870  528.0491  1.9226    8.6800   
 917   1.3885 -0.0096 -0.0038  ...  0.08770  533.2855  2.0362    6.3200   
 918   1.4756 -0.0025  0.0025  ...  0.08770  533.2855  2.0362    6.3200   
 919   1.5947  0.0143 -0.0064  ...  0.56640  535.1345  2.1394    5.7300   
 920   1.4646  0.0021  0.0059  ...  0.28400  536.4418  2.3416    6.6600   
 921   1.5865 -0.0199  0.0021  ...  0.08940  529.5955  2.1619    8.0099   
 922   1.4446 -0.0050 -0.0007  ...  0.21570  530.5682  2.1417   10.8700   
 923   1.4406 -0.0265 -0.0033  ...  0.04930  532.9891  1.7784    7.5000   
 925   1.5337  0.0090  0.0058  ...  0.08770  533.9718  2.5640    9.2800   
 927   1.3603 -0.0031  0.0086  ...  0.08770  532.0973  2.1448    9.6000   
 928   1.4493 -0.0194 -0.0018  ...  0.46980  534.0800  2.1203   10.3700   
 930   1.4483 -0.0103  0.0151  ...  0.08770  530.9809  2.3259    9.9400   
 931   1.5347 -0.0254  0.0247  ...  0.35230  532.8227  2.3242   11.7600   
 932   1.3059  0.0013 -0.0045  ...  0.13290  533.2873  2.3473   10.9300   
 933   1.4455 -0.0218 -0.0068  ...  0.08770  533.2873  2.3473   10.9300   
 934   1.4478 -0.0017  0.0072  ...  0.22660  533.2855  2.0362    6.3200   
 935   1.3813  0.0025  0.0028  ...  0.15000  532.0973  2.1448    9.6000   
 936   1.6316 -0.0195 -0.0008  ...  0.20070  504.9173  2.2656  438.8700   
 937   1.6383 -0.0154  0.0052  ...  0.07360  533.2873  2.3473   10.9300   
 938   1.5569  0.0095  0.0007  ...  0.44580  530.5682  2.1417   10.8700   
 939   1.5979 -0.0108 -0.0104  ...  0.18440  533.4773  2.2113    7.2800   
 940   1.4802 -0.0038  0.0130  ...  0.08770  531.1709  2.2895   10.4400   
 941   1.6157 -0.0112  0.0135  ...  0.08770  533.4773  2.2113    7.2800   
 942   1.5401  0.0036 -0.0036  ...  0.05670  530.9809  2.3259    9.9400   
 943   1.3824 -0.0070  0.0010  ...  0.33430  531.3346  2.2421    8.4900   
 944   1.4226 -0.0130  0.0063  ...  0.08770  530.9809  2.3259    9.9400   
 945   1.4910 -0.0159  0.0087  ...  0.14930  530.9809  2.3259    9.9400   
 946   1.6057  0.0023  0.0070  ...  0.08770  532.5954  2.1606   10.5900   
 947   1.5119 -0.0087  0.0051  ...  0.11340  530.3073  2.4103    8.0500   
 948   1.4326  0.0138 -0.0096  ...  0.13670  534.9155  1.7591    9.0800   
 949   1.5317 -0.0036 -0.0045  ...  0.08770  531.8455  2.2544    8.6400   
 950   1.5410  0.0211 -0.0038  ...  0.24280  530.6746  2.0193   10.8000   
 951   1.5498  0.0168  0.0055  ...  0.08770  533.4773  2.2113    7.2800   
 952   1.5431 -0.0093  0.0105  ...  0.29460  532.8227  2.3242   11.7600   
 953   1.5651 -0.0131 -0.0085  ...  0.14280  531.1709  2.2895   10.4400   
 954   1.4570 -0.0068 -0.0070  ...  0.11830  531.7509  2.3526    8.0900   
 955   1.5620  0.0069 -0.0021  ...  0.08770  530.3073  2.4103    8.0500   
 956   1.4056  0.0013 -0.0020  ...  0.20560  504.9173  2.2656  438.8700   
 957   1.4684 -0.0116 -0.0053  ...  0.30590  536.3264  2.2421    7.7000   
 958   1.4938  0.0078  0.0010  ...  0.08770  531.8455  2.2544    8.6400   
 959   1.5648  0.0153 -0.0165  ...  0.27080  533.3073  2.3490   10.1900   
 960   1.5842 -0.0143  0.0049  ...  0.08770  533.4773  2.2113    7.2800   
 961   1.3656 -0.0251  0.0018  ...  0.45810  531.3346  2.2421    8.4900   
 962   1.6242  0.0327 -0.0061  ...  0.08770  530.9809  2.3259    9.9400   
 963   1.5367  0.0285  0.0111  ...  0.27920  526.5336  2.6194    9.6000   
 964   1.6045 -0.0086 -0.0037  ...  0.16020  530.9809  2.3259    9.9400   
 965   1.4765  0.0103 -0.0049  ...  0.15790  533.2855  2.0362    6.3200   
 966   1.6373 -0.0139  0.0062  ...  0.09840  533.7718  2.2619    5.9700   
 967   1.6539 -0.0149 -0.0039  ...  0.15580  530.9809  2.3259    9.9400   
 968   1.4078  0.0146 -0.0112  ...  0.08770  530.3073  2.4103    8.0500   
 969   1.3786  0.0069  0.0023  ...  0.43630  530.3073  2.4103    8.0500   
 970   1.4454  0.0177  0.0017  ...  0.08770  531.3346  2.2421    8.4900   
 971   1.4314  0.0200  0.0062  ...  0.11955  530.9809  2.3259    9.9400   
 972   1.4276  0.0121 -0.0051  ...  0.08800  531.1709  2.2895   10.4400   
 973   1.4785  0.0279  0.0092  ...  0.08770  532.7764  2.3109    6.8700   
 974   1.5922  0.0105 -0.0011  ...  0.09920  531.0918  2.4015   10.1800   
 975   1.4461  0.0196 -0.0125  ...  0.08770  530.9809  2.3259    9.9400   
 976   1.5400 -0.0071 -0.0095  ...  0.42880  533.9936  2.2318    8.2700   
 977   1.4675 -0.0146  0.0005  ...  0.16650  535.9636  2.2026    6.7700   
 978   1.4380 -0.0062 -0.0033  ...  0.12960  533.2855  2.0362    6.3200   
 979   1.6564 -0.0135 -0.0002  ...  0.16080  531.2973  1.9279    9.1200   
 980   1.6136 -0.0201  0.0005  ...  0.20080  532.7455  1.7255    7.0400   
 981   1.5578  0.0110  0.0008  ...  0.08770  504.9173  2.2656  438.8700   
 982   1.3885  0.0017 -0.0110  ...  0.08770  504.9173  2.2656  438.8700   
 983   1.4947 -0.0003  0.0077  ...  0.08770  531.3346  2.2421    8.4900   
 984   1.6202 -0.0158 -0.0065  ...  0.10010  529.8718  1.9408    6.3300   
 985   1.5438  0.0014 -0.0057  ...  0.08770  532.1636  1.7199    8.9800   
 986   1.4172  0.0285 -0.0078  ...  0.08770  532.8227  2.3242   11.7600   
 987   1.5335  0.0158 -0.0095  ...  0.08770  530.9173  1.7153    7.3600   
 988   1.3924 -0.0116 -0.0132  ...  0.08770  530.3073  2.4103    8.0500   
 989   1.5215 -0.0164  0.0102  ...  0.08770  531.1982  2.2418   10.6300   
 990   1.5698  0.0141  0.0111  ...  0.27940  533.3073  2.3490   10.1900   
 991   1.4438  0.0120 -0.0083  ...  0.08770  533.3073  2.3490   10.1900   
 992   1.6092 -0.0339 -0.0043  ...  0.18630  533.2873  2.3473   10.9300   
 993   1.4323  0.0104 -0.0195  ...  0.08770  529.5955  2.1619    8.0099   
 994   1.5017  0.0173  0.0111  ...  0.10990  504.9173  2.2656  438.8700   
 995   1.4805  0.0123  0.0009  ...  0.08730  533.6364  2.2904   13.0900   
 996   1.5950 -0.0163  0.0061  ...  0.08770  531.8455  2.2544    8.6400   
 997   1.4587  0.0284 -0.0070  ...  0.17380  533.3073  2.3490   10.1900   
 998   1.5136  0.0225 -0.0269  ...  0.08770  532.4564  1.9432   11.6500   
 999   1.4193  0.0113 -0.0007  ...  0.08770  530.5682  2.1417   10.8700   
 1000  1.4620 -0.0172 -0.0007  ...  0.08770  530.9809  2.3259    9.9400   
 1001  1.4772  0.0283 -0.0072  ...  0.08510  532.0973  2.1448    9.6000   
 1002  1.6147 -0.0132  0.0009  ...  0.08770  530.5682  2.1417   10.8700   
 1003  1.5351  0.0039 -0.0046  ...  0.08770  530.9809  2.3259    9.9400   
 1004  1.4726  0.0095 -0.0009  ...  0.16830  529.5955  2.1619    8.0099   
 1005  1.4207  0.0086  0.0071  ...  0.13660  504.9173  2.2656  438.8700   
 1006  1.5587  0.0031  0.0024  ...  0.15880  533.3073  2.3490   10.1900   
 1007  1.5631  0.0084  0.0014  ...  0.17270  533.1418  2.0876   10.5800   
 1008  1.3830  0.0305  0.0018  ...  0.13060  533.4773  2.2113    7.2800   
 1009  1.5948  0.0077 -0.0078  ...  0.31010  530.3073  2.4103    8.0500   
 1010  1.5332  0.0097  0.0014  ...  0.19440  533.3073  2.3490   10.1900   
 1011  1.3561  0.0045 -0.0011  ...  0.11955  533.4773  2.2113    7.2800   
 1012  1.5519 -0.0273  0.0063  ...  0.31670  531.1982  2.2418   10.6300   
 1013  1.5314 -0.0241 -0.0076  ...  0.08770  529.8718  1.9408    6.3300   
 1014  1.4823 -0.0012  0.0233  ...  0.08770  531.0518  1.8932    6.8000   
 1015  1.4616  0.0106 -0.0046  ...  0.11955  530.9809  2.3259    9.9400   
 1016  1.5714 -0.0073  0.0170  ...  0.08770  524.2318  2.0104  436.5200   
 1017  1.5416 -0.0257  0.0041  ...  0.08770  533.2855  2.0362    6.3200   
 1018  1.5264 -0.0130 -0.0192  ...  0.08770  533.9982  1.8821    7.4500   
 1019  1.6255 -0.0172 -0.0049  ...  0.08770  531.1982  2.2418   10.6300   
 1020  1.6187  0.0027  0.0077  ...  0.08770  530.3073  2.4103    8.0500   
 1021  1.4328  0.0054  0.0105  ...  0.14510  529.5955  2.1619    8.0099   
 1022  1.4459  0.0172  0.0028  ...  0.37320  529.8718  1.9408    6.3300   
 1023  1.3376  0.0201 -0.0050  ...  0.08770  532.8227  2.3242   11.7600   
 1024  1.3601  0.0031 -0.0080  ...  0.30610  529.5955  2.1619    8.0099   
 1025  1.4465  0.0091 -0.0017  ...  0.19520  533.3391  1.9511    5.8500   
 1026  1.4422 -0.0013  0.0031  ...  0.08770  529.5955  2.1619    8.0099   
 1027  1.3650  0.0034 -0.0068  ...  0.08770  533.3073  2.3490   10.1900   
 1028  1.5275  0.0159  0.0168  ...  0.13580  535.9636  2.2026    6.7700   
 1030  1.4015  0.0095 -0.0070  ...  0.08770  528.7673  1.9809    6.9500   
 1031  1.4090  0.0160  0.0018  ...  0.08770  530.1782  1.9155   10.7800   
 1032  1.3418 -0.0112  0.0015  ...  0.08770  531.4064  2.0135    9.9600   
 1033  1.4117  0.0087 -0.0108  ...  0.24050  529.8718  1.9408    6.3300   
 1034  1.4590 -0.0074 -0.0060  ...  0.08770  530.1782  1.9155   10.7800   
 1035  1.3682 -0.0110 -0.0085  ...  0.08770  530.1782  1.9155   10.7800   
 1036  1.4726  0.0041  0.0060  ...  0.08770  529.5955  2.1619    8.0099   
 1037  1.5149  0.0025 -0.0063  ...  0.20510  524.2318  2.0104  436.5200   
 1038  1.5901 -0.0071  0.0127  ...  0.42070  531.0518  1.8932    6.8000   
 1039  1.5341  0.0350 -0.0068  ...  0.08770  530.3073  2.4103    8.0500   
 1040  1.5335 -0.0083 -0.0095  ...  0.08770  530.3073  2.4103    8.0500   
 1041  1.6001  0.0105 -0.0099  ...  0.08770  529.5955  2.1619    8.0099   
 1042  1.4035  0.0222 -0.0014  ...  0.36500  528.4045  1.9943    9.0100   
 1043  1.4217  0.0029 -0.0119  ...  0.11955  529.8718  1.9408    6.3300   
 1044  1.4615 -0.0109  0.0140  ...  0.08770  531.0518  1.8932    6.8000   
 1045  1.4132 -0.0310  0.0069  ...  0.23130  532.8000  2.0925    6.2800   
 1046  1.3824  0.0045 -0.0068  ...  0.22930  533.9718  1.9524    5.4400   
 1047  1.3242  0.0130 -0.0102  ...  0.18670  533.9718  1.9524    5.4400   
 1048  1.5745  0.0050  0.0006  ...  0.08770  533.2855  2.0362    6.3200   
 1049  1.3910  0.0086 -0.0015  ...  0.19180  531.2973  1.9279    9.1200   
 1050  1.4945  0.0195 -0.0120  ...  0.08770  532.4618  2.2425   10.2700   
 1051  1.4939  0.0096  0.0071  ...  0.17440  533.9936  2.2318    8.2700   
 1052  1.4320  0.0142 -0.0170  ...  0.08770  533.9982  1.8821    7.4500   
 1053  1.4440  0.0159 -0.0135  ...  0.11960  529.8718  1.9408    6.3300   
 1054  1.4200  0.0150 -0.0019  ...  0.21010  528.7673  1.9809    6.9500   
 1055  1.4136  0.0129 -0.0125  ...  0.08770  529.5955  2.1619    8.0099   
 1056  1.5293  0.0115 -0.0053  ...  0.11670  504.9173  2.2656  438.8700   
 1057  1.4649  0.0077  0.0046  ...  0.08770  528.4045  1.9943    9.0100   
 1058  1.5712 -0.0121  0.0051  ...  0.10750  532.7764  2.3109    6.8700   
 1059  1.4675  0.0095  0.0052  ...  0.32770  533.9936  2.2318    8.2700   
 1060  1.3704  0.0152 -0.0170  ...  0.14480  531.8727  2.1285    8.0800   
 1061  1.4202  0.0150  0.0094  ...  0.08770  528.4045  1.9943    9.0100   
 1063  1.3836  0.0135  0.0013  ...  0.08770  532.4564  1.9432   11.6500   
 1064  1.6075  0.0095  0.0059  ...  0.08770  528.4045  1.9943    9.0100   
 1065  1.4286  0.0113 -0.0051  ...  0.24860  530.3073  2.4103    8.0500   
 1066  1.3499 -0.0009  0.0044  ...  0.08770  530.7036  2.1207    8.4000   
 1067  1.4293  0.0199  0.0076  ...  0.29660  531.0918  2.4015   10.1800   
 1068  1.4536  0.0192 -0.0152  ...  0.11955  532.6327  2.1085    7.7700   
 1069  1.4243  0.0059 -0.0034  ...  0.19260  533.9718  1.9524    5.4400   
 1070  1.3900  0.0302 -0.0112  ...  0.08770  529.6464  2.0320    5.8100   
 1071  1.5688  0.0181  0.0180  ...  0.13590  530.7036  2.1207    8.4000   
 1072  1.3209 -0.0040 -0.0013  ...  0.25920  530.3836  1.8367   10.3400   
 1073  1.3496 -0.0018 -0.0094  ...  0.08770  528.7673  1.9809    6.9500   
 1074  1.4234 -0.0179 -0.0066  ...  0.08770  531.0418  2.0433    9.8200   
 1075  1.3825  0.0050  0.0018  ...  0.11955  524.2318  2.0104  436.5200   
 1076  1.3524  0.0020 -0.0088  ...  0.16680  532.7118  2.1958    8.7000   
 1077  1.4162  0.0020 -0.0136  ...  0.08770  533.9718  1.9524    5.4400   
 1078  1.4105 -0.0262  0.0064  ...  0.19150  531.4173  2.1444    6.7200   
 1079  1.3525  0.0313 -0.0088  ...  0.08770  531.0418  2.0433    9.8200   
 1080  1.4286 -0.0143 -0.0175  ...  0.15610  532.0064  1.9363   10.8900   
 1081  1.4182 -0.0029  0.0148  ...  0.08770  531.0364  2.1244    8.9700   
 1082  1.4818 -0.0088 -0.0064  ...  0.05470  533.9718  1.9524    5.4400   
 1083  1.3613  0.0006  0.0100  ...  0.08770  528.7673  1.9809    6.9500   
 1084  1.3809  0.0013 -0.0124  ...  0.08770  531.0518  1.8932    6.8000   
 1085  1.3573  0.0162 -0.0094  ...  0.28290  531.0364  2.1244    8.9700   
 1086  1.4342  0.0168 -0.0010  ...  0.26030  531.9282  2.1096   10.4500   
 1087  1.4575  0.0093 -0.0071  ...  0.08770  529.5955  2.1619    8.0099   
 1088  1.6040 -0.0226  0.0072  ...  0.38160  533.9718  1.9524    5.4400   
 1089  1.4368  0.0162 -0.0064  ...  0.17830  531.0418  2.0433    9.8200   
 1090  1.3661  0.0113 -0.0261  ...  0.08770  528.4045  1.9943    9.0100   
 1091  1.3825 -0.0062  0.0106  ...  0.23880  531.8136  2.0854    7.0100   
 1092  1.4757 -0.0106 -0.0048  ...  0.49370  529.9445  1.9740    7.9100   
 1093  1.4635  0.0004  0.0016  ...  0.08770  529.5182  2.1130   10.6500   
 1094  1.4587 -0.0028  0.0044  ...  0.08770  529.9445  1.9740    7.9100   
 1095  1.4007 -0.0133  0.0028  ...  0.16480  529.5182  2.1130   10.6500   
 1096  1.3909  0.0053 -0.0055  ...  0.14300  530.2227  1.9126    9.3900   
 1097  1.4145  0.0204 -0.0057  ...  0.16530  529.8718  1.9408    6.3300   
 1098  1.3837  0.0024 -0.0026  ...  0.08770  529.6464  2.0320    5.8100   
 1099  1.4381  0.0082 -0.0058  ...  0.08770  531.4591  1.9611    9.6600   
 1100  1.6114  0.0484  0.0043  ...  0.08770  531.0364  2.1244    8.9700   
 1101  1.5323  0.0171  0.0081  ...  0.12420  531.4591  1.9611    9.6600   
 1102  1.3096  0.0185 -0.0057  ...  0.26850  530.9364  1.9355   12.1000   
 1103  1.3728  0.0151 -0.0123  ...  0.24520  533.9936  2.2318    8.2700   
 1104  1.4021 -0.0085 -0.0078  ...  0.30990  529.8718  1.9408    6.3300   
 1105  1.3384  0.0336 -0.0136  ...  0.11160  524.4955  1.0912   11.1200   
 1106  1.4004  0.0228  0.0141  ...  0.08770  524.2318  2.0104  436.5200   
 1107  1.3917 -0.0103 -0.0018  ...  0.23140  531.0418  2.0433    9.8200   
 1108  1.3782  0.0118  0.0067  ...  0.08770  530.2409  1.9614   11.5400   
 1109  1.4912  0.0001 -0.0062  ...  0.17560  528.4045  1.9943    9.0100   
 1110  1.4500  0.0063  0.0099  ...  0.19180  531.4591  1.9611    9.6600   
 1111  1.3273 -0.0190  0.0009  ...  0.27440  530.3745  2.1745    7.9300   
 1112  1.3849 -0.0060 -0.0005  ...  0.12140  531.4591  1.9611    9.6600   
 1113  1.5826 -0.0089  0.0035  ...  0.08770  530.3836  1.8367   10.3400   
 1114  1.3954  0.0246 -0.0117  ...  0.03050  530.2409  1.9614   11.5400   
 1115  1.4629 -0.0164 -0.0114  ...  0.08770  528.7673  1.9809    6.9500   
 1116  1.4511 -0.0017  0.0030  ...  0.08770  524.2318  2.0104  436.5200   
 1117  1.4808 -0.0060 -0.0033  ...  0.12340  532.5954  2.1606   10.5900   
 1118  1.3088  0.0105 -0.0059  ...  0.06690  531.4064  2.0135    9.9600   
 1119  1.4303  0.0048 -0.0040  ...  0.08770  530.2409  1.9614   11.5400   
 1120  1.4048  0.0266  0.0006  ...  0.08770  531.4591  1.9611    9.6600   
 1121  1.4782  0.0069  0.0024  ...  0.08770  529.9445  1.9740    7.9100   
 1122  1.4040 -0.0025 -0.0007  ...  0.25890  529.0200  1.9969    9.8200   
 1123  1.4233 -0.0049  0.0021  ...  0.13150  504.9173  2.2656  438.8700   
 1124  1.4078  0.0052  0.0004  ...  0.21970  532.0064  1.9363   10.8900   
 1125  1.3813  0.0137 -0.0096  ...  0.16670  531.4064  2.0135    9.9600   
 1126  1.4796  0.0004 -0.0042  ...  0.12440  528.4045  1.9943    9.0100   
 1127  1.5456 -0.0285 -0.0166  ...  0.15570  531.4591  1.9611    9.6600   
 1128  1.4854  0.0173 -0.0095  ...  0.19350  531.4064  2.0135    9.9600   
 1129  1.4022  0.0209 -0.0027  ...  0.20170  532.6327  2.1085    7.7700   
 1130  1.3608  0.0250 -0.0060  ...  0.08770  529.5955  2.1619    8.0099   
 1131  1.4693  0.0022  0.0047  ...  0.08770  530.7036  2.1207    8.4000   
 1132  1.4658 -0.0167 -0.0010  ...  0.16360  531.0009  1.8299   13.2800   
 1133  1.5947 -0.0112  0.0025  ...  0.08770  529.9445  1.9740    7.9100   
 1134  1.4669 -0.0080 -0.0173  ...  0.07600  530.2418  2.4302    9.7100   
 1135  1.4048  0.0018  0.0049  ...  0.31960  528.7673  1.9809    6.9500   
 1136  1.5927 -0.0227  0.0059  ...  0.08770  529.6464  2.0320    5.8100   
 1137  1.4135  0.0283 -0.0065  ...  0.12000  531.0364  2.1244    8.9700   
 1138  1.4259  0.0085 -0.0228  ...  0.30030  528.4045  1.9943    9.0100   
 1139  1.4200  0.0070 -0.0077  ...  0.08770  530.5682  2.1417   10.8700   
 1140  1.5319  0.0184  0.0038  ...  0.19810  530.7318  2.2877    9.6300   
 1141  1.4411  0.0002 -0.0038  ...  0.24340  526.7564  2.2090    7.7700   
 1142  1.4309  0.0088 -0.0239  ...  0.13770  528.7673  1.9809    6.9500   
 1143  1.4224 -0.0042 -0.0021  ...  0.08770  528.7673  1.9809    6.9500   
 1145  1.3789  0.0343 -0.0019  ...  0.24160  530.3364  2.0270    9.9200   
 1146  1.3395 -0.0261 -0.0040  ...  0.16630  531.4173  2.1444    6.7200   
 1147  1.3737 -0.0220  0.0163  ...  0.08770  530.7036  2.1207    8.4000   
 1148  1.4205  0.0001  0.0063  ...  0.08770  529.6464  2.0320    5.8100   
 1149  1.5263 -0.0028 -0.0001  ...  0.12040  532.0064  1.9363   10.8900   
 1150  1.4102  0.0215 -0.0056  ...  0.28670  531.7491  1.9611    7.8700   
 1152  1.4144  0.0033 -0.0035  ...  0.17690  530.2409  1.9614   11.5400   
 1153  1.5366  0.0176 -0.0073  ...  0.16530  529.9445  1.9740    7.9100   
 1154  1.3981 -0.0351 -0.0096  ...  0.08770  530.3364  2.0270    9.9200   
 1155  1.4377 -0.0125 -0.0199  ...  0.08770  530.3364  2.0270    9.9200   
 1156  1.4868  0.0005  0.0038  ...  0.20850  531.0418  2.0433    9.8200   
 1157  1.4400  0.0108  0.0029  ...  0.08770  531.8136  2.0854    7.0100   
 1158  1.5905 -0.0142 -0.0100  ...  0.13450  532.6327  2.1085    7.7700   
 1159  1.3561  0.0014 -0.0073  ...  0.12580  531.7491  1.9611    7.8700   
 1160  1.4033  0.0232 -0.0035  ...  0.08770  530.1782  1.9155   10.7800   
 1161  1.3625 -0.0015  0.0163  ...  0.08770  528.7673  1.9809    6.9500   
 1162  1.5535 -0.0061 -0.0117  ...  0.23740  530.2409  1.9614   11.5400   
 1163  1.4080 -0.0138  0.0043  ...  0.08770  532.0064  1.9363   10.8900   
 1164  1.5892 -0.0212  0.0061  ...  0.12970  529.9445  1.9740    7.9100   
 1165  1.5110  0.0011 -0.0016  ...  0.25350  530.7036  2.1207    8.4000   
 1166  1.3435  0.0071 -0.0019  ...  0.08770  529.5182  2.1130   10.6500   
 1167  1.4053 -0.0151  0.0010  ...  0.15930  529.2673  2.0421    8.9400   
 1168  1.4812 -0.0155 -0.0058  ...  0.19890  531.0009  1.8299   13.2800   
 1169  1.4760 -0.0175 -0.0084  ...  0.40550  529.5182  2.0770    8.9900   
 1170  1.3548 -0.0061  0.0015  ...  0.18030  530.3364  2.0270    9.9200   
 1171  1.4476  0.0023  0.0014  ...  0.08770  529.9445  1.9740    7.9100   
 1172  1.4306 -0.0163 -0.0253  ...  0.08770  529.6464  2.0320    5.8100   
 1173  1.3644  0.0237 -0.0174  ...  0.10200  531.0364  2.1244    8.9700   
 1174  1.5459 -0.0368 -0.0002  ...  0.19820  524.4955  1.0912   11.1200   
 1175  1.5106 -0.0165  0.0063  ...  0.08770  524.4955  1.0912   11.1200   
 1176  1.5289 -0.0273 -0.0003  ...  0.08770  528.0491  2.1169    8.9900   
 1177  1.5712 -0.0119  0.0060  ...  0.14490  531.8136  2.0854    7.0100   
 1178  1.4055  0.0244 -0.0074  ...  0.21150  532.3273  2.3967   10.5700   
 1179  1.4382 -0.0182  0.0032  ...  0.08770  529.9445  1.9740    7.9100   
 1180  1.3537  0.0134 -0.0039  ...  0.08770  529.9445  1.9740    7.9100   
 1181  1.3929 -0.0004  0.0059  ...  0.20390  529.9445  1.9740    7.9100   
 1182  1.4146 -0.0260 -0.0004  ...  0.08770  531.3036  1.9250    9.4500   
 1183  1.4925  0.0483 -0.0020  ...  0.22650  531.5200  1.9170    9.7500   
 1184  1.4634 -0.0005 -0.0077  ...  0.08770  528.5973  2.2200    6.2400   
 1186  1.3792  0.0054  0.0039  ...  0.08770  531.8455  2.2544    8.6400   
 1187  1.3406  0.0006 -0.0039  ...  0.11710  529.9445  1.9740    7.9100   
 1188  1.3928  0.0361 -0.0141  ...  0.08770  532.0109  2.0679   11.3500   
 1190  1.3791 -0.0304 -0.0077  ...  0.27740  531.5200  1.9170    9.7500   
 1191  1.5016 -0.0011 -0.0014  ...  0.08770  530.3364  2.0270    9.9200   
 1192  1.3253 -0.0167 -0.0085  ...  0.08770  532.1636  1.7199    8.9800   
 1193  1.4462 -0.0354 -0.0029  ...  0.28100  533.9718  1.9524    5.4400   
 1194  1.4359 -0.0068  0.0058  ...  0.08770  531.0364  2.1244    8.9700   
 1195  1.5517 -0.0230 -0.0026  ...  0.08770  529.6464  2.0320    5.8100   
 1196  1.5755 -0.0072  0.0087  ...  0.18770  531.0182  1.8610    9.2100   
 1197  1.5323  0.0133  0.0066  ...  0.24780  530.5109  1.9739   12.0700   
 1198  1.4002 -0.0151 -0.0006  ...  0.08770  529.6464  2.0320    5.8100   
 1199  1.3892  0.0023  0.0042  ...  0.35950  531.4591  1.9611    9.6600   
 1200  1.3979 -0.0118 -0.0013  ...  0.15580  531.0009  1.8299   13.2800   
 1201  1.5299 -0.0100 -0.0007  ...  0.08770  530.5109  1.9739   12.0700   
 1202  1.5022 -0.0034  0.0118  ...  0.13570  531.3036  1.9250    9.4500   
 1203  1.4093 -0.0010 -0.0045  ...  0.08770  531.4591  1.9611    9.6600   
 1204  1.3993  0.0018 -0.0116  ...  0.08770  531.8827  1.9109    6.9000   
 1205  1.3523 -0.0015 -0.0012  ...  0.08770  528.4045  1.9943    9.0100   
 1206  1.4088  0.0235  0.0120  ...  0.08770  531.5418  2.2320    7.5700   
 1207  1.4573  0.0294 -0.0042  ...  0.08770  530.7036  2.1207    8.4000   
 1208  1.3846  0.0078  0.0097  ...  0.08770  531.7491  1.9611    7.8700   
 1209  1.3694 -0.0054 -0.0074  ...  0.11960  530.7036  2.1207    8.4000   
 1210  1.3728  0.0162  0.0033  ...  0.12340  529.9445  1.9740    7.9100   
 1212  1.6122 -0.0171  0.0078  ...  0.08770  531.5636  2.0805    9.0300   
 1213  1.4741 -0.0142 -0.0060  ...  0.21410  531.8100  1.9298    7.9900   
 1214  1.4989 -0.0041  0.0021  ...  0.29660  528.5973  2.2200    6.2400   
 1215  1.5391  0.0090 -0.0074  ...  0.10320  531.1982  2.2418   10.6300   
 1216  1.4711  0.0033 -0.0020  ...  0.08770  530.7091  1.8681    5.6400   
 1217  1.4031 -0.0050 -0.0012  ...  0.08770  528.5973  2.2200    6.2400   
 1218  1.4670 -0.0134 -0.0103  ...  0.08770  531.3445  2.0359    5.7100   
 1219  1.3896  0.0021 -0.0012  ...  0.27860  531.4591  1.9611    9.6600   
 1220  1.4844 -0.0288 -0.0029  ...  0.16130  528.7673  1.9809    6.9500   
 1221  1.4708 -0.0085 -0.0010  ...  0.19750  527.6991  2.0119    8.4600   
 1222  1.4192  0.0119  0.0016  ...  0.08770  528.5973  2.2200    6.2400   
 1223  1.5380 -0.0097  0.0012  ...  0.20680  530.3364  2.0270    9.9200   
 1224  1.4100  0.0274 -0.0040  ...  0.08770  528.5973  2.2200    6.2400   
 1225  1.4419  0.0029 -0.0062  ...  0.08770  529.9445  1.9740    7.9100   
 1226  1.4052  0.0079 -0.0173  ...  0.08770  531.3445  2.0359    5.7100   
 1228  1.5361  0.0060 -0.0001  ...  0.08770  529.8991  2.0142    7.5600   
 1229  1.3401  0.0182 -0.0085  ...  0.15960  531.1754  2.0396    8.0900   
 1230  1.4752  0.0084  0.0009  ...  0.13990  524.4955  1.0912   11.1200   
 1231  1.4536 -0.0110 -0.0035  ...  0.17310  530.7036  2.1207    8.4000   
 1232  1.4826  0.0136  0.0009  ...  0.17070  524.4955  1.0912   11.1200   
 1233  1.4645  0.0001  0.0067  ...  0.08770  524.4955  1.0912   11.1200   
 1234  1.4845 -0.0037  0.0049  ...  0.23520  495.3682  1.7909  415.0300   
 1235  1.4441 -0.0021  0.0095  ...  0.08770  532.0973  2.0850    7.7400   
 1236  1.4827 -0.0238  0.0059  ...  0.28340  532.2427  1.9313    8.1700   
 1237  1.4889 -0.0076 -0.0017  ...  0.06390  534.3145  1.0024  430.3700   
 1239  1.4294  0.0138  0.0052  ...  0.02890  529.8991  2.0142    7.5600   
 1240  1.4956 -0.0060  0.0038  ...  0.14200  531.0418  2.0433    9.8200   
 1243  1.4051 -0.0204  0.0007  ...  0.27220  531.3445  2.0359    5.7100   
 1244  1.4018 -0.0187  0.0009  ...  0.20260  458.8464  1.0936  271.7300   
 1245  1.3439  0.0055  0.0000  ...  0.12160  531.5200  1.9170    9.7500   
 1246  1.4115 -0.0318  0.0000  ...  0.08770  529.9445  1.9740    7.9100   
 1247  1.5492  0.0102 -0.0056  ...  0.21960  513.8473  1.3216  445.8000   
 1248  1.4630 -0.0293 -0.0005  ...  0.08770  528.5973  2.2200    6.2400   
 1249  1.3823  0.0079  0.0102  ...  0.08770  532.6327  2.1085    7.7700   
 1250  1.4653 -0.0204  0.0038  ...  0.08770  533.1809  1.0744    5.6500   
 1251  1.3453 -0.0072 -0.0082  ...  0.19950  532.0973  2.0850    7.7400   
 1252  1.4346  0.0062 -0.0107  ...  0.08770  524.4955  1.0912   11.1200   
 1253  1.3697 -0.0085 -0.0114  ...  0.24550  532.0973  2.0850    7.7400   
 1255  1.3582 -0.0364  0.0032  ...  0.29900  532.0973  2.0850    7.7400   
 1256  1.4778  0.0095  0.0016  ...  0.08770  527.2555  0.9802    7.4900   
 1257  1.4596 -0.0016  0.0284  ...  0.23870  532.0945  2.0071    5.8600   
 1258  1.3956 -0.0075  0.0079  ...  0.05340  532.0973  2.0850    7.7400   
 1259  1.3873 -0.0135 -0.0086  ...  0.25890  539.0691  1.3198  427.4200   
 1260  1.3584 -0.0024 -0.0048  ...  0.07430  513.8473  1.3216  445.8000   
 1261  1.4248  0.0101 -0.0009  ...  0.08770  532.6345  2.0657   10.1600   
 1262  1.3762 -0.0206 -0.0104  ...  0.08770  524.4955  1.0912   11.1200   
 1263  1.3897 -0.0091 -0.0024  ...  0.10250  531.4591  1.9611    9.6600   
 1264  1.3857 -0.0088 -0.0032  ...  0.08770  530.9445  1.9567    8.0800   
 1265  1.3089  0.0016 -0.0123  ...  0.06150  531.2836  1.8915    7.6200   
 1266  1.4352 -0.0113 -0.0282  ...  0.09030  531.1754  2.0396    8.0900   
 1267  1.3671 -0.0197  0.0056  ...  0.08770  529.5182  2.1130   10.6500   
 1268  1.5132 -0.0217  0.0080  ...  0.20710  528.5973  2.2200    6.2400   
 1269  1.4953 -0.0270  0.0033  ...  0.08770  530.0846  1.9812    9.0100   
 1270  1.4028 -0.0108 -0.0033  ...  0.14530  528.5973  2.2200    6.2400   
 1271  1.5666 -0.0115  0.0084  ...  0.08770  529.2564  1.8034   14.1200   
 1272  1.5030 -0.0061  0.0009  ...  0.08770  531.3445  2.0359    5.7100   
 1273  1.4639 -0.0123 -0.0173  ...  0.08770  532.3427  2.0841    8.1800   
 1274  1.4794 -0.0019 -0.0014  ...  0.08770  514.4364  1.8660  431.7100   
 1275  1.4732 -0.0175 -0.0096  ...  0.20890  532.1382  2.0329    9.0200   
 1276  1.4711  0.0013 -0.0020  ...  0.20290  532.1382  2.0329    9.0200   
 1277  1.4204 -0.0059 -0.0125  ...  0.11170  532.1382  2.0329    9.0200   
 1278  1.5456 -0.0108  0.0094  ...  0.08770  531.7491  1.9611    7.8700   
 1279  1.4617  0.0043  0.0098  ...  0.20130  530.6345  1.9733    7.6000   
 1280  1.3578 -0.0065  0.0004  ...  0.08770  533.1809  1.0744    5.6500   
 1281  1.5958  0.0112  0.0024  ...  0.24260  531.1754  2.0396    8.0900   
 1282  1.4052 -0.0157 -0.0095  ...  0.19950  531.3445  2.0359    5.7100   
 1283  1.3943  0.0076  0.0259  ...  0.34900  532.2427  1.9313    8.1700   
 1284  1.4544  0.0071  0.0030  ...  0.39250  528.5973  2.2200    6.2400   
 1285  1.4915 -0.0176  0.0054  ...  0.21400  528.7618  1.9743   10.0700   
 1286  1.4133  0.0137 -0.0132  ...  0.68920  531.3091  1.9363   10.8500   
 1287  1.5456 -0.0051  0.0062  ...  0.08770  532.3427  2.0841    8.1800   
 1288  1.5240 -0.0091  0.0064  ...  0.08770  532.3427  2.0841    8.1800   
 1289  1.5036  0.0035 -0.0028  ...  0.13320  532.0945  2.0071    5.8600   
 1290  1.4711 -0.0133  0.0001  ...  0.08770  532.1382  2.0329    9.0200   
 1291  1.6147 -0.0057  0.0084  ...  0.16770  514.4364  1.8660  431.7100   
 1292  1.5305  0.0141  0.0187  ...  0.08770  531.1754  2.0396    8.0900   
 1293  1.3979  0.0187  0.0196  ...  0.14840  531.1754  2.0396    8.0900   
 1294  1.3571 -0.0165 -0.0007  ...  0.08770  532.1455  2.0581    9.8900   
 1295  1.3557  0.0024 -0.0050  ...  0.11950  529.8991  2.0142    7.5600   
 1296  1.3841 -0.0264  0.0064  ...  0.15730  532.0973  2.0850    7.7400   
 1297  1.4477  0.0093  0.0079  ...  0.08770  528.0127  1.1073    9.0400   
 1298  1.5418  0.0020  0.0020  ...  0.21740  532.2427  1.9313    8.1700   
 1299  1.3642 -0.0032 -0.0035  ...  0.08770  529.8991  2.0142    7.5600   
 1300  1.5095  0.0097 -0.0053  ...  0.08770  530.7036  2.1207    8.4000   
 1301  1.5761 -0.0022 -0.0069  ...  0.08770  530.0846  1.9812    9.0100   
 1304  1.4347  0.0006  0.0084  ...  0.08770  533.1809  1.0744    5.6500   
 1305  1.3984 -0.0028 -0.0059  ...  0.08770  528.5973  2.2200    6.2400   
 1306  1.4745 -0.0132  0.0054  ...  0.27780  530.2591  0.9847    8.2800   
 1307  1.4317  0.0082  0.0168  ...  0.24170  531.3091  1.9363   10.8500   
 1308  1.5675 -0.0010  0.0099  ...  0.14580  513.8473  1.3216  445.8000   
 1309  1.4214  0.0070  0.0192  ...  0.20050  529.5182  2.1130   10.6500   
 1310  1.5290  0.0014  0.0014  ...  0.10440  539.0691  1.3198  427.4200   
 1311  1.3636  0.0191  0.0040  ...  0.15720  532.0945  2.0071    5.8600   
 1312  1.4418  0.0076  0.0123  ...  0.13070  514.4364  1.8660  431.7100   
 1313  1.5029 -0.0036  0.0052  ...  0.08770  532.0973  2.0850    7.7400   
 1314  1.4245  0.0021  0.0018  ...  0.22260  531.3091  1.9363   10.8500   
 1315  1.5257 -0.0009  0.0039  ...  0.08770  532.1073  1.8126    7.8300   
 1316  1.5986 -0.0048 -0.0027  ...  0.10860  530.0846  1.9812    9.0100   
 1317  1.4564 -0.0182 -0.0095  ...  0.08770  528.5973  2.2200    6.2400   
 1318  1.5757 -0.0037  0.0039  ...  0.37940  530.0846  1.9812    9.0100   
 1319  1.4656  0.0040 -0.0064  ...  0.18330  531.3091  1.9363   10.8500   
 1320  1.4959  0.0007 -0.0135  ...  0.30090  532.2427  1.9313    8.1700   
 1321  1.6193 -0.0127 -0.0164  ...  0.08770  529.2673  2.0421    8.9400   
 1322  1.5480  0.0007  0.0039  ...  0.13660  530.2591  0.9847    8.2800   
 1323  1.5407 -0.0073  0.0108  ...  0.10990  530.2591  0.9847    8.2800   
 1326  1.4323  0.0145 -0.0137  ...  0.08770  530.2591  0.9847    8.2800   
 1330  1.5507 -0.0077  0.0040  ...  0.08770  531.3091  1.9363   10.8500   
 1331  1.4004 -0.0079 -0.0075  ...  0.14400  530.2591  0.9847    8.2800   
 1332  1.3838 -0.0125  0.0073  ...  0.27960  531.1754  2.0396    8.0900   
 1333  1.5840 -0.0058  0.0065  ...  0.08770  534.2564  1.3737   10.8700   
 1334  1.3909 -0.0132  0.0099  ...  0.05370  528.5973  2.2200    6.2400   
 1335  1.5576 -0.0098  0.0105  ...  0.14740  514.4364  1.8660  431.7100   
 1336  1.5714  0.0076  0.0196  ...  0.17740  532.1382  2.0329    9.0200   
 1337  1.5656 -0.0027  0.0092  ...  0.08770  513.8473  1.3216  445.8000   
 1338  1.4319  0.0043  0.0123  ...  0.15300  514.4364  1.8660  431.7100   
 1339  1.3533  0.0013  0.0064  ...  0.23010  532.1073  1.8126    7.8300   
 1340  1.5139 -0.0127  0.0007  ...  0.17650  530.0846  1.9812    9.0100   
 1341  1.5120 -0.0124  0.0090  ...  0.08770  532.8227  2.1206    7.9300   
 1344  1.4424 -0.0242  0.0050  ...  0.08770  532.1073  1.8126    7.8300   
 1345  1.3702 -0.0118  0.0067  ...  0.08770  532.3427  2.0841    8.1800   
 1346  1.4159 -0.0068 -0.0073  ...  0.11290  531.1754  2.0396    8.0900   
 1347  1.4931 -0.0106  0.0015  ...  0.08770  539.0691  1.3198  427.4200   
 1348  1.5267 -0.0067 -0.0034  ...  0.08770  532.1073  1.8126    7.8300   
 1349  1.3997  0.0008  0.0043  ...  0.15370  514.4364  1.8660  431.7100   
 1350  1.3220  0.0145  0.0141  ...  0.08770  531.9873  2.0680   10.2500   
 1351  1.5021 -0.0108 -0.0032  ...  0.13110  514.4364  1.8660  431.7100   
 1352  1.4753 -0.0038  0.0090  ...  0.24770  531.9873  2.0680   10.2500   
 1353  1.3985 -0.0297  0.0005  ...  0.06800  514.4364  1.8660  431.7100   
 1354  1.4001 -0.0156  0.0048  ...  0.10500  532.8227  2.1206    7.9300   
 1355  1.3601 -0.0077  0.0170  ...  0.21930  530.5455  1.9626    9.4200   
 1356  1.4148 -0.0120  0.0022  ...  0.08770  530.2591  0.9847    8.2800   
 1357  1.3914 -0.0072  0.0000  ...  0.08770  531.3091  1.9363   10.8500   
 1358  1.5337 -0.0281  0.0075  ...  0.26180  531.1754  2.0396    8.0900   
 1359  1.5664 -0.0350  0.0170  ...  0.23060  532.1382  2.0329    9.0200   
 1360  1.4288 -0.0063  0.0012  ...  0.08770  532.8227  2.1206    7.9300   
 1361  1.4458 -0.0108  0.0128  ...  0.20110  531.9873  2.0680   10.2500   
 1362  1.4201 -0.0182  0.0055  ...  0.19310  514.4364  1.8660  431.7100   
 1366  1.3717  0.0059  0.0011  ...  0.08770  531.6964  2.0737    8.2100   
 1367  1.4685  0.0057 -0.0007  ...  0.08770  529.6464  2.0320    5.8100   
 1368  1.4888 -0.0128 -0.0141  ...  0.08770  535.9491  1.9753    9.8800   
 1369  1.4022 -0.0058 -0.0039  ...  0.08770  530.0846  1.9812    9.0100   
 1370  1.5111 -0.0106  0.0101  ...  0.12570  514.4364  1.8660  431.7100   
 1371  1.4441 -0.0129  0.0002  ...  0.26640  531.9873  2.0680   10.2500   
 1372  1.3663 -0.0091 -0.0020  ...  0.20250  530.5455  1.9626    9.4200   
 1373  1.4386 -0.0125 -0.0015  ...  0.08770  530.0846  1.9812    9.0100   
 1374  1.3058 -0.0097  0.0073  ...  0.33600  530.2591  0.9847    8.2800   
 1375  1.4252 -0.0267 -0.0216  ...  0.23880  531.9873  2.0680   10.2500   
 1376  1.4509  0.0015 -0.0004  ...  0.08770  532.1382  2.0329    9.0200   
 1377  1.4036 -0.0014  0.0015  ...  0.14960  530.0846  1.9812    9.0100   
 1378  1.4226 -0.0076  0.0293  ...  0.25010  531.1754  2.0396    8.0900   
 1379  1.4454  0.0071  0.0073  ...  0.20660  531.3091  1.9363   10.8500   
 1380  1.4198 -0.0156 -0.0043  ...  0.08770  529.8827  1.9602   11.0500   
 1381  1.5915 -0.0120  0.0085  ...  0.18970  539.0691  1.3198  427.4200   
 1382  1.2877  0.0243  0.0142  ...  0.09100  531.9873  2.0680   10.2500   
 1383  1.5369 -0.0287 -0.0216  ...  0.19710  532.8227  2.1206    7.9300   
 1384  1.5616 -0.0141 -0.0014  ...  0.16400  532.4700  2.1429    9.1400   
 1385  1.3657  0.0273  0.0063  ...  0.08770  531.3091  1.9363   10.8500   
 1386  1.3859 -0.0007 -0.0058  ...  0.08770  530.2591  0.9847    8.2800   
 1387  1.5552 -0.0031  0.0046  ...  0.14720  531.7464  2.2909    9.1299   
 1388  1.4487  0.0014 -0.0063  ...  0.14090  531.9873  2.0680   10.2500   
 1389  1.5903 -0.0062 -0.0010  ...  0.10470  532.4700  2.1429    9.1400   
 1390  1.5002 -0.0122 -0.0110  ...  0.17930  533.1364  2.0133    7.2400   
 1391  1.4906 -0.0051 -0.0164  ...  0.08770  529.4391  2.0075   10.0700   
 1392  1.5521 -0.0085 -0.0152  ...  0.10050  533.1364  2.0133    7.2400   
 1393  1.5083  0.0029  0.0058  ...  0.08770  532.8227  2.1206    7.9300   
 1394  1.3692 -0.0105 -0.0109  ...  0.08770  531.9873  2.0680   10.2500   
 1395  1.4895 -0.0099 -0.0046  ...  0.18460  539.0691  1.3198  427.4200   
 1396  1.5033  0.0192  0.0027  ...  0.08770  532.8227  2.1206    7.9300   
 1397  1.5613  0.0005  0.0023  ...  0.08770  531.9873  2.0680   10.2500   
 1398  1.4832 -0.0191  0.0016  ...  0.08770  531.9873  2.0680   10.2500   
 1399  1.5464 -0.0084  0.0048  ...  0.24190  531.9873  2.0680   10.2500   
 1401  1.4719 -0.0003 -0.0006  ...  0.21820  532.8145  2.1082    6.8800   
 1402  1.6010  0.0075 -0.0032  ...  0.24530  532.1382  2.0329    9.0200   
 1403  1.5697 -0.0122 -0.0050  ...  0.08770  531.7464  2.2909    9.1299   
 1404  1.5471  0.0133  0.0095  ...  0.31170  514.4364  1.8660  431.7100   
 1405  1.5715 -0.0159 -0.0037  ...  0.08770  532.2664  2.0223    9.0300   
 1406  1.5084  0.0112 -0.0010  ...  0.20700  531.9873  2.0680   10.2500   
 1407  1.5242 -0.0006  0.0053  ...  0.20320  529.4391  2.0075   10.0700   
 1408  1.4193  0.0033 -0.0057  ...  0.18570  531.3091  1.9363   10.8500   
 1409  1.4409 -0.0112 -0.0017  ...  0.08770  535.2245  2.0034    8.7400   
 1410  1.5680  0.0165 -0.0113  ...  0.12370  530.5455  1.9626    9.4200   
 1411  1.6162 -0.0063 -0.0108  ...  0.08770  533.1364  2.0133    7.2400   
 1412  1.4298  0.0122  0.0176  ...  0.08770  531.7464  2.2909    9.1299   
 1413  1.3622  0.0387 -0.0202  ...  0.29910  530.5455  1.9626    9.4200   
 1414  1.4425 -0.0291 -0.0025  ...  0.18760  530.5455  1.9626    9.4200   
 1415  1.3987  0.0313 -0.0182  ...  0.08770  531.7464  2.2909    9.1299   
 1416  1.4781  0.0002 -0.0062  ...  0.22890  529.4391  2.0075   10.0700   
 1417  1.5010  0.0259 -0.0228  ...  0.23250  532.4700  2.1429    9.1400   
 1418  1.4440  0.0168 -0.0001  ...  0.25580  532.8227  2.1206    7.9300   
 1419  1.4066 -0.0094 -0.0280  ...  0.17350  532.8145  2.1082    6.8800   
 1420  1.4968 -0.0201 -0.0060  ...  0.08770  532.4527  1.9857    5.8300   
 1421  1.5823  0.0113 -0.0022  ...  0.08770  532.8145  2.1082    6.8800   
 1422  1.4075  0.0280 -0.0349  ...  0.08770  529.8827  1.9602   11.0500   
 1423  1.3188  0.0422 -0.0189  ...  0.07650  531.7464  2.2909    9.1299   
 1424  1.3145  0.0410 -0.0209  ...  0.08770  536.4482  1.4106    7.0500   
 1425  1.4109 -0.0239 -0.0121  ...  0.11150  569.9964  1.7961  441.9200   
 1426  1.3137  0.0345 -0.0137  ...  0.08770  536.4482  1.4106    7.0500   
 1427  1.4339  0.0020 -0.0036  ...  0.10170  531.7464  2.2909    9.1299   
 1428  1.5177  0.0183 -0.0137  ...  0.08770  532.4700  2.1429    9.1400   
 1429  1.5447  0.0036 -0.0125  ...  0.08770  533.1364  2.0133    7.2400   
 1430  1.4855 -0.0034 -0.0103  ...  0.08770  531.7464  2.2909    9.1299   
 1431  1.4089  0.0156 -0.0142  ...  0.08770  528.9927  1.9172    8.9100   
 1432  1.6217 -0.0088  0.0064  ...  0.23520  528.7918  2.0831    6.8100   
 1433  1.4413  0.0086 -0.0173  ...  0.08770  529.4391  2.0075   10.0700   
 1434  1.5547 -0.0048  0.0020  ...  0.09880  532.0782  2.0447    8.3900   
 1435  1.5718  0.0211 -0.0006  ...  0.30400  532.1382  2.0329    9.0200   
 1436  1.4151  0.0213 -0.0179  ...  0.08770  532.1073  1.8126    7.8300   
 1437  1.5172  0.0074 -0.0077  ...  0.09930  529.4391  2.0075   10.0700   
 1439  1.2957  0.0186  0.0070  ...  0.08770  532.8145  2.1082    6.8800   
 1440  1.4411 -0.0039 -0.0065  ...  0.21270  531.7464  2.2909    9.1299   
 1441  1.3134  0.0158  0.0102  ...  0.16220  530.5455  1.9626    9.4200   
 1442  1.3606  0.0030  0.0107  ...  0.08770  532.4527  1.9857    5.8300   
 1444  1.6123 -0.0166  0.0017  ...  0.15520  528.8655  2.0891    6.3600   
 1445  1.3326  0.0314  0.0021  ...  0.12960  531.7464  2.2909    9.1299   
 1446  1.3216  0.0489  0.0035  ...  0.17410  532.1073  1.8126    7.8300   
 1447  1.5576 -0.0205  0.0095  ...  0.08770  531.7464  2.2909    9.1299   
 1448  1.3860  0.0225  0.0018  ...  0.08770  530.0846  1.9812    9.0100   
 1449  1.4857  0.0160  0.0020  ...  0.08770  536.4482  1.4106    7.0500   
 1450  1.3571  0.0324 -0.0007  ...  0.23490  532.0973  2.0850    7.7400   
 1451  1.4471 -0.0130 -0.0070  ...  0.13440  526.2355  2.1114    8.1500   
 1452  1.4120  0.0174  0.0067  ...  0.08770  569.9964  1.7961  441.9200   
 1453  1.4133 -0.0078 -0.0076  ...  0.08770  529.8827  1.9602   11.0500   
 1454  1.5302  0.0133 -0.0051  ...  0.12740  569.9964  1.7961  441.9200   
 1455  1.5124  0.0229  0.0091  ...  0.22160  531.7464  2.2909    9.1299   
 1456  1.4460  0.0107 -0.0026  ...  0.25430  532.1073  1.8126    7.8300   
 1457  1.5169  0.0093  0.0123  ...  0.19920  536.4482  1.4106    7.0500   
 1458  1.3663 -0.0123 -0.0050  ...  0.08770  532.6327  2.1085    7.7700   
 1459  1.4814  0.0180 -0.0001  ...  0.20260  530.9445  1.9567    8.0800   
 1460  1.4714  0.0049 -0.0034  ...  0.08770  532.1073  1.8126    7.8300   
 1461  1.3988  0.0185 -0.0047  ...  0.28150  531.1754  2.0396    8.0900   
 1462  1.4718  0.0100 -0.0122  ...  0.08770  530.0073  2.5161    6.9100   
 1463  1.3068  0.0042 -0.0153  ...  0.08770  530.0073  2.5161    6.9100   
 1464  1.2374  0.0160 -0.0044  ...  0.11640  530.0073  2.5161    6.9100   
 1465  1.2005  0.0081 -0.0110  ...  0.08770  530.0073  2.5161    6.9100   
 1466  1.4498  0.0097  0.0150  ...  0.20400  569.9964  1.7961  441.9200   
 1467  1.4838  0.0033  0.0050  ...  0.19970  530.2591  0.9847    8.2800   
 1468  1.3985 -0.0105 -0.0140  ...  0.08770  532.5155  2.3188    7.3000   
 1469  1.4425  0.0222  0.0037  ...  0.08200  531.1754  2.0396    8.0900   
 1470  1.5442 -0.0135  0.0031  ...  0.20880  536.4482  1.4106    7.0500   
 1471  1.3561  0.0021 -0.0012  ...  0.11860  532.5155  2.3188    7.3000   
 1472  1.2898  0.0187  0.0017  ...  0.14310  536.5927  2.1566    6.8200   
 1473  1.3708  0.0061  0.0033  ...  0.11780  537.9264  2.1814    5.4800   
 1474  1.4120  0.0093 -0.0138  ...  0.08770  536.5927  2.1566    6.8200   
 1475  1.2462  0.0125 -0.0213  ...  0.08770  537.9264  2.1814    5.4800   
 1476  1.4642  0.0749  0.0118  ...  0.13020  534.3264  1.5887    5.4000   
 1477  1.4279 -0.0047 -0.0068  ...  0.17630  531.9600  1.8347   12.7000   
 1478  1.2532  0.0089 -0.0058  ...  0.11180  536.0709  2.1472    8.3701   
 1479  1.4005  0.0486  0.0080  ...  0.11840  532.1618  2.1401    7.1100   
 1480  1.3930  0.0495  0.0078  ...  0.08770  537.9264  2.1814    5.4800   
 1481  1.3639  0.0609  0.0040  ...  0.14000  532.4527  1.9857    5.8300   
 1482  1.5203  0.0424  0.0150  ...  0.19190  498.1891  1.2925  439.2900   
 1483  1.3062 -0.0069 -0.0089  ...  0.22290  534.3264  1.5887    5.4000   
 1484  1.4058  0.0031 -0.0194  ...  0.08770  534.3264  1.5887    5.4000   
 1485  1.4439 -0.0094 -0.0042  ...  0.25190  536.5927  2.1566    6.8200   
 1486  1.3471 -0.0089  0.0135  ...  0.13060  537.1318  2.2109    8.4400   
 1487  1.4197  0.0026 -0.0023  ...  0.16300  534.3936  1.9098    9.1300   
 1488  1.3264 -0.0018  0.0067  ...  0.08770  536.9836  1.8675    7.4700   
 1489  1.3976  0.0102  0.0043  ...  0.17320  525.6236  2.0340   10.7600   
 1490  1.3699 -0.0021  0.0082  ...  0.08770  532.1618  2.1401    7.1100   
 1491  1.3829 -0.0019  0.0017  ...  0.08770  534.3936  1.9098    9.1300   
 1492  1.3508  0.0046 -0.0066  ...  0.37120  526.2355  2.1114    8.1500   
 1493  1.2634  0.0052 -0.0040  ...  0.15170  531.9873  2.0680   10.2500   
 1494  1.4404 -0.0050  0.0046  ...  0.09360  536.5927  2.1566    6.8200   
 1495  1.3573 -0.0060  0.0085  ...  0.18890  526.2355  2.1114    8.1500   
 1496  1.4234 -0.0045  0.0030  ...  0.08770  526.2355  2.1114    8.1500   
 1497  1.4448  0.0042  0.0017  ...  0.08770  526.2355  2.1114    8.1500   
 1498  1.4174  0.0384  0.0080  ...  0.08770  536.5927  2.1566    6.8200   
 1499  1.3207 -0.0116 -0.0128  ...  0.08770  528.7918  2.0831    6.8100   
 1500  1.4119 -0.0053  0.0056  ...  0.08770  537.1318  2.2109    8.4400   
 1501  1.3160 -0.0140  0.0007  ...  0.08770  536.4482  1.4106    7.0500   
 1502  1.4089  0.0447  0.0027  ...  0.08770  538.1373  2.1186    8.4399   
 1503  1.2769 -0.0074  0.0086  ...  0.14790  532.1618  2.1401    7.1100   
 1504  1.4443 -0.0025 -0.0025  ...  0.08920  538.1373  2.1186    8.4399   
 1505  1.4241  0.0008 -0.0031  ...  0.13860  536.2964  2.0719    8.3900   
 1506  1.3881 -0.0146  0.0048  ...  0.08770  536.4482  1.4106    7.0500   
 1507  1.2956  0.0005  0.0046  ...  0.08640  526.2355  2.1114    8.1500   
 1508  1.3636 -0.0103 -0.0056  ...  0.08770  532.3782  2.0882    9.6300   
 1509  1.3488 -0.0018  0.0035  ...  0.08770  537.9264  2.1814    5.4800   
 1510  1.4379 -0.0144 -0.0068  ...  0.13450  532.5155  2.3188    7.3000   
 1511  1.3108 -0.0077 -0.0120  ...  0.17800  531.9600  1.8347   12.7000   
 1512  1.6024 -0.0043  0.0032  ...  0.08770  529.4391  2.0075   10.0700   
 1513  1.3059  0.0076  0.0021  ...  0.08770  536.9836  1.8675    7.4700   
 1514  1.3380 -0.0018 -0.0063  ...  0.08770  535.9345  2.2444    6.3500   
 1515  1.3010 -0.0060  0.0006  ...  0.08590  537.1318  2.2109    8.4400   
 1516  1.3517 -0.0029 -0.0065  ...  0.08770  537.9264  2.1814    5.4800   
 1517  1.3076  0.0046  0.0044  ...  0.08770  536.9836  1.8675    7.4700   
 1518  1.3511  0.0120 -0.0049  ...  0.27590  534.3264  1.5887    5.4000   
 1520  1.3454 -0.0032  0.0028  ...  0.11990  529.3100  2.2860    7.6200   
 1521  1.3214  0.0009 -0.0074  ...  0.24580  536.0709  2.1472    8.3701   
 1522  1.3868  0.0184  0.0035  ...  0.08770  532.1618  2.1401    7.1100   
 1523  1.2872 -0.0117  0.0364  ...  0.08770  536.9836  1.8675    7.4700   
 1524  1.3465 -0.0054 -0.0170  ...  0.23100  526.2355  2.1114    8.1500   
 1525  1.4241 -0.0043 -0.0012  ...  0.08770  534.3264  1.5887    5.4000   
 1526  1.3345  0.0085  0.0097  ...  0.06670  528.7918  2.0831    6.8100   
 1527  1.3226  0.0025  0.0085  ...  0.08770  528.7918  2.0831    6.8100   
 1528  1.3307 -0.0077  0.0046  ...  0.26600  532.1618  2.1401    7.1100   
 1529  1.4994  0.0000 -0.0060  ...  0.24060  532.7627  2.2678    7.5000   
 1530  1.2963 -0.0059 -0.0077  ...  0.12070  532.1618  2.1401    7.1100   
 1531  1.4599 -0.0075 -0.0109  ...  0.08770  528.7918  2.0831    6.8100   
 1532  1.4378 -0.0007  0.0043  ...  0.08770  537.9264  2.1814    5.4800   
 1533  1.3834  0.0002 -0.0026  ...  0.13350  538.5645  2.3008    4.8400   
 1534  1.3297  0.0020  0.0016  ...  0.08770  532.9918  2.0745    8.5200   
 1535  1.3532  0.0006  0.0051  ...  0.37210  534.0200  2.1718    7.3500   
 1536  1.4663 -0.0123 -0.0019  ...  0.14420  525.2427  2.1327   10.1900   
 1537  1.4522 -0.0039 -0.0075  ...  0.14200  533.7318  2.0706    8.1300   
 1538  1.3907  0.0074  0.0078  ...  0.08770  528.7918  2.0831    6.8100   
 1539  1.4167  0.0041 -0.0056  ...  0.21570  569.9964  1.7961  441.9200   
 1540  1.4158 -0.0029  0.0000  ...  0.11850  525.6127  2.0059   10.2500   
 1541  1.3966 -0.0057 -0.0169  ...  0.08770  526.2355  2.1114    8.1500   
 1542  1.3109  0.0174  0.0038  ...  0.14430  532.2627  2.1154   10.1500   
 1543  1.3502  0.0201  0.0037  ...  0.08520  533.0909  2.3195   10.7200   
 1544  1.2901  0.0202  0.0031  ...  0.04880  532.2627  2.1154   10.1500   
 1545  1.4294 -0.0014  0.0135  ...  0.27410  529.3845  2.3706    8.7200   
 1546  1.3482  0.0060  0.0139  ...  0.13740  530.3709  2.3435    6.4900   
 1547  1.2895  0.0023 -0.0024  ...  0.08770  532.1618  2.1401    7.1100   
 1548  1.4410  0.0035  0.0142  ...  0.08770  527.8955  2.3099    9.6600   
 1549  1.4129 -0.0081 -0.0005  ...  0.07330  534.5864  1.8987    7.4600   
 1550  1.3148 -0.0024  0.0039  ...  0.08770  532.1618  2.1401    7.1100   
 1551  1.4386 -0.0179 -0.0048  ...  0.08770  532.1618  2.1401    7.1100   
 1552  1.4366  0.0083 -0.0029  ...  0.05840  532.4527  1.9857    5.8300   
 1553  1.3405  0.0201  0.0018  ...  0.12040  532.2627  2.1154   10.1500   
 1554  1.3485  0.0151  0.0003  ...  0.08770  537.9264  2.1814    5.4800   
 1555  1.3255 -0.0052 -0.0016  ...  0.08770  537.0846  2.1754    7.2800   
 1556  1.3687 -0.0070 -0.0033  ...  0.08770  531.9600  1.8347   12.7000   
 1557  1.4359 -0.0114  0.0096  ...  0.08770  534.3264  1.5887    5.4000   
 1558  1.3832  0.0042  0.0023  ...  0.02640  532.1700  2.1510    9.1600   
 1559  1.3120 -0.0043 -0.0063  ...  0.34200  528.7918  2.0831    6.8100   
 1560  1.2811  0.0037  0.0017  ...  0.08770  536.5927  2.1566    6.8200   
 1561  1.4492 -0.0134  0.0127  ...  0.11955  527.9364  2.3919    9.6900   
 1562  1.3424 -0.0045 -0.0057  ...  0.08770  536.3418  2.0153    7.9800   
 1563  1.4333 -0.0061 -0.0093  ...  0.13080  537.9264  2.1814    5.4800   
 1564  1.4616 -0.0013  0.0004  ...  0.23880  530.3709  2.3435    6.4900   
 1565  1.4622 -0.0072  0.0032  ...  0.08770  534.3936  1.9098    9.1300   
 1566  1.4616 -0.0013  0.0004  ...  0.13070  528.7918  2.0831    6.8100   
 
          582     583     586     587       589  Pass/Fail  
 0     0.5005  0.0118  0.0205  0.0148   71.9005         -1  
 1     0.5019  0.0223  0.0096  0.0201  208.2045         -1  
 3     0.4990  0.0103  0.0202  0.0149   73.8432         -1  
 4     0.4800  0.4766  0.0202  0.0149   73.8432         -1  
 5     0.4949  0.0189  0.0342  0.0151   44.0077         -1  
 6     0.5010  0.0143  0.0342  0.0151   44.0077         -1  
 7     0.4984  0.0106  0.0204  0.0194   95.0310         -1  
 8     0.4993  0.0172  0.0111  0.0124  111.6525         -1  
 9     0.4967  0.0152  0.0212  0.0191   90.2294         -1  
 12    0.4950  0.0153  0.0188  0.0098   52.2039         -1  
 13    0.5034  0.0151  0.0188  0.0098   52.2039         -1  
 15    0.5058  0.0078  0.0174  0.0174  100.2745         -1  
 16    0.5005  0.0108  0.0184  0.0151   82.0989         -1  
 17    0.5015  0.0105  0.0184  0.0151   82.0989         -1  
 18    0.4948  0.0117  0.0184  0.0151   82.0989         -1  
 19    0.5036  0.0169  0.0229  0.0108   47.1586         -1  
 20    0.5011  0.0117  0.0229  0.0108   47.1586         -1  
 21    0.4947  0.0137  0.0175  0.0060   34.4153         -1  
 22    0.4977  0.0114  0.0250  0.0286  114.5979         -1  
 24    0.5032  0.0159  0.0288  0.0361  125.0600         -1  
 25    0.5012  0.0336  0.0288  0.0361  125.0600         -1  
 26    0.5006  0.0083  0.0288  0.0361  125.0600         -1  
 27    0.5069  0.0158  0.0183  0.0397  216.9552         -1  
 28    0.5036  0.0137  0.0130  0.0165  127.5067         -1  
 29    0.5019  0.0139  0.0121  0.0178  146.8715         -1  
 30    0.4981  0.0180  0.0209  0.0171   81.5459         -1  
 31    0.5033  0.0115  0.0082  0.0162  197.9951         -1  
 32    0.4967  0.0113  0.0082  0.0162  197.9951         -1  
 33    0.4992  0.0158  0.0082  0.0162  197.9951         -1  
 34    0.5037  0.0175  0.0268  0.0199   74.1555         -1  
 35    0.5013  0.0138  0.0268  0.0199   74.1555         -1  
 36    0.5032  0.0127  0.0290  0.0165   56.8221         -1  
 37    0.5073  0.0181  0.0104  0.0159  152.6823         -1  
 39    0.5012  0.0099  0.0150  0.0151  100.7279         -1  
 41    0.4976  0.0111  0.0219  0.0162   73.8817         -1  
 42    0.5014  0.0121  0.0173  0.0298  171.8524         -1  
 43    0.4947  0.0127  0.0365  0.0303   83.0563         -1  
 44    0.5009  0.0155  0.0149  0.0158  106.1812         -1  
 46    0.5035  0.0189  0.0227  0.0232  102.2673         -1  
 47    0.4976  0.0127  0.0149  0.0164  110.5454         -1  
 51    0.4943  0.0181  0.0245  0.0157   63.9698         -1  
 52    0.4987  0.0106  0.0358  0.0182   50.8713         -1  
 53    0.5036  0.0150  0.0358  0.0182   50.8713         -1  
 54    0.4967  0.0152  0.0247  0.0054   21.9917         -1  
 55    0.5000  0.0173  0.0097  0.0111  115.1005         -1  
 56    0.5030  0.0118  0.0120  0.0190  157.8567         -1  
 59    0.4997  0.0086  0.0285  0.0132   46.4594         -1  
 60    0.5039  0.0116  0.0201  0.0216  107.8331         -1  
 61    0.5070  0.0186  0.0183  0.0183  100.1756         -1  
 63    0.4984  0.0146  0.0296  0.0062   20.8909         -1  
 65    0.4970  0.0093  0.0343  0.0115   33.4515         -1  
 66    0.5053  0.0121  0.0139  0.0187  134.2014         -1  
 67    0.5001  0.0119  0.0139  0.0187  134.2014         -1  
 68    0.5001  0.0144  0.0110  0.0134  121.5476         -1  
 69    0.4953  0.0148  0.0192  0.0125   65.2312         -1  
 70    0.4969  0.0197  0.0108  0.0106   98.1735         -1  
 71    0.5022  0.0182  0.0108  0.0106   98.1735         -1  
 72    0.5001  0.0129  0.0354  0.0286   80.7240         -1  
 73    0.4980  0.0163  0.0266  0.0086   32.3304         -1  
 74    0.4993  0.0169  0.0266  0.0086   32.3304         -1  
 75    0.4958  0.0120  0.0059  0.0101  169.6401         -1  
 76    0.5032  0.0134  0.0059  0.0101  169.6401         -1  
 77    0.4977  0.0116  0.0461  0.0542  117.6159         -1  
 78    0.4961  0.0124  0.0155  0.0177  114.4127         -1  
 79    0.4959  0.0191  0.0165  0.0116   70.1798         -1  
 80    0.5011  0.0122  0.0218  0.0152   69.4220         -1  
 81    0.4984  0.0137  0.0180  0.0106   59.2072         -1  
 83    0.5003  0.0234  0.0278  0.0042   15.2909         -1  
 84    0.4986  0.0176  0.0298  0.0146   48.9072         -1  
 85    0.5043  0.0131  0.0298  0.0146   48.9072         -1  
 86    0.5032  0.0135  0.0298  0.0146   48.9072         -1  
 87    0.4944  0.0122  0.0200  0.0237  118.3047         -1  
 88    0.4981  0.0086  0.0372  0.0358   96.3971         -1  
 89    0.5008  0.0134  0.0201  0.0083   41.3784         -1  
 90    0.4962  0.0129  0.0201  0.0083   41.3784         -1  
 91    0.5003  0.0136  0.0277  0.0318  114.7497         -1  
 92    0.5007  0.0105  0.0277  0.0318  114.7497         -1  
 93    0.5025  0.0118  0.0277  0.0318  114.7497         -1  
 94    0.5031  0.0146  0.0277  0.0318  114.7497         -1  
 95    0.5002  0.0098  0.0277  0.0318  114.7497         -1  
 97    0.4998  0.0174  0.0095  0.0184  192.2985         -1  
 98    0.4955  0.0110  0.0095  0.0184  192.2985         -1  
 99    0.5012  0.0094  0.0199  0.0107   53.6369         -1  
 100   0.4924  0.0142  0.0199  0.0107   53.6369         -1  
 101   0.4988  0.0126  0.0199  0.0107   53.6369         -1  
 102   0.5026  0.0176  0.0189  0.0137   72.3776         -1  
 103   0.4943  0.0168  0.0313  0.0208   66.4172         -1  
 104   0.4977  0.0138  0.0313  0.0208   66.4172         -1  
 105   0.4984  0.0180  0.0194  0.0223  115.1055         -1  
 106   0.5033  0.0144  0.0120  0.0082   68.6662         -1  
 107   0.5024  0.0120  0.0322  0.0132   40.8703         -1  
 108   0.5021  0.0117  0.0325  0.0073   22.4197         -1  
 109   0.5045  0.0261  0.0349  0.0146   41.7833         -1  
 110   0.4971  0.0151  0.0139  0.0139   99.4503         -1  
 111   0.4987  0.0184  0.0139  0.0139   99.4503         -1  
 112   0.5045  0.0090  0.0139  0.0139   99.4503         -1  
 113   0.5016  0.0160  0.0049  0.0144  293.2614         -1  
 114   0.5010  0.0123  0.0049  0.0144  293.2614         -1  
 116   0.5030  0.0167  0.0364  0.0166   45.6835         -1  
 117   0.5018  0.0106  0.0231  0.0102   44.1220         -1  
 118   0.5001  0.0191  0.0231  0.0102   44.1220         -1  
 119   0.5025  0.0094  0.0254  0.0145   56.8579         -1  
 120   0.5005  0.0128  0.0216  0.0115   53.3157         -1  
 121   0.5040  0.0176  0.0220  0.0547  248.3173         -1  
 122   0.5000  0.0202  0.0241  0.0086   35.5550         -1  
 123   0.4965  0.0113  0.0241  0.0086   35.5550         -1  
 124   0.4950  0.0149  0.0241  0.0086   35.5550         -1  
 125   0.4966  0.0085  0.0201  0.0379  188.2987         -1  
 126   0.5019  0.0130  0.0201  0.0379  188.2987         -1  
 127   0.4998  0.0162  0.0201  0.0379  188.2987         -1  
 128   0.5029  0.0082  0.0153  0.0134   87.6631         -1  
 129   0.5009  0.0153  0.0338  0.0119   35.2148         -1  
 130   0.5059  0.0082  0.0208  0.0149   71.5753         -1  
 132   0.5001  0.0115  0.0058  0.0169  289.9234         -1  
 133   0.5009  0.0100  0.0058  0.0169  289.9234         -1  
 134   0.5002  0.0138  0.0058  0.0169  289.9234         -1  
 135   0.4955  0.0255  0.0058  0.0169  289.9234         -1  
 136   0.5060  0.0264  0.0058  0.0169  289.9234         -1  
 137   0.5037  0.0183  0.0058  0.0169  289.9234         -1  
 138   0.5025  0.0141  0.0058  0.0169  289.9234         -1  
 139   0.5068  0.0160  0.0058  0.0169  289.9234         -1  
 140   0.5027  0.0165  0.0058  0.0169  289.9234         -1  
 141   0.5005  0.0160  0.0058  0.0169  289.9234         -1  
 142   0.5064  0.0117  0.0348  0.0109   31.2736         -1  
 143   0.4980  0.0104  0.0135  0.0153  112.8617         -1  
 144   0.5000  0.0101  0.0055  0.0218  397.5003         -1  
 145   0.4971  0.0132  0.0055  0.0218  397.5003         -1  
 146   0.4973  0.0181  0.0055  0.0218  397.5003         -1  
 147   0.5047  0.0162  0.0055  0.0218  397.5003         -1  
 148   0.5026  0.0089  0.0055  0.0218  397.5003         -1  
 149   0.4992  0.0122  0.0182  0.0187  102.8940         -1  
 150   0.4985  0.0118  0.0182  0.0187  102.8940         -1  
 151   0.4946  0.0141  0.0182  0.0187  102.8940         -1  
 152   0.4957  0.0166  0.0336  0.0153   45.3701         -1  
 153   0.5057  0.0147  0.0199  0.0117   58.5665         -1  
 155   0.5003  0.0150  0.0228  0.0299  131.2543         -1  
 156   0.4939  0.0136  0.0228  0.0299  131.2543         -1  
 159   0.5065  0.0188  0.0082  0.0184  223.6518         -1  
 160   0.5067  0.0126  0.0195  0.0121   62.1248         -1  
 161   0.5028  0.0125  0.0075  0.0140  185.5679         -1  
 162   0.5067  0.0096  0.0215  0.0101   47.0849         -1  
 163   0.5028  0.0228  0.0337  0.0219   64.8236         -1  
 164   0.5023  0.0140  0.0078  0.0112  143.3273         -1  
 165   0.4995  0.0403  0.0078  0.0112  143.3273         -1  
 166   0.5011  0.0100  0.0419  0.0098   23.3511         -1  
 168   0.4975  0.0194  0.0419  0.0098   23.3511         -1  
 170   0.5045  0.0095  0.0240  0.0115   47.8123         -1  
 171   0.5016  0.0454  0.0059  0.0126  213.7714         -1  
 172   0.5043  0.0112  0.0254  0.0160   62.9443         -1  
 173   0.5031  0.0090  0.0254  0.0160   62.9443         -1  
 174   0.5000  0.0115  0.0254  0.0160   62.9443         -1  
 175   0.4974  0.0130  0.0254  0.0160   62.9443         -1  
 176   0.5032  0.0085  0.0254  0.0160   62.9443         -1  
 177   0.5027  0.0245  0.0254  0.0160   62.9443         -1  
 178   0.5043  0.0104  0.0171  0.0173  100.9102         -1  
 179   0.4992  0.0104 -0.0012  0.0220    0.0000         -1  
 181   0.5014  0.0116 -0.0012  0.0220    0.0000         -1  
 183   0.5014  0.0109  0.0077  0.0204  264.7525         -1  
 184   0.4989  0.0109  0.0077  0.0204  264.7525         -1  
 185   0.5003  0.0161  0.0077  0.0204  264.7525         -1  
 187   0.5033  0.0127  0.0104  0.0221  211.6182         -1  
 190   0.4965  0.0143  0.0112  0.0191  170.4645         -1  
 191   0.4929  0.0146  0.0112  0.0191  170.4645         -1  
 192   0.4948  0.0149  0.0112  0.0191  170.4645         -1  
 193   0.4936  0.0289  0.0112  0.0191  170.4645         -1  
 194   0.4996  0.0123  0.0217  0.0244  112.5819         -1  
 195   0.4982  0.0221  0.0217  0.0244  112.5819         -1  
 196   0.4997  0.0110  0.0123  0.0128  104.0012         -1  
 197   0.5000  0.0188  0.0165  0.0129   78.1942         -1  
 198   0.4938  0.0205  0.0105  0.0198  188.2466         -1  
 199   0.4982  0.0097  0.0009  0.0242    0.0000         -1  
 200   0.4952  0.0141  0.0123  0.0094   76.4584         -1  
 201   0.4966  0.0226  0.0123  0.0094   76.4584         -1  
 202   0.4967  0.0160  0.0123  0.0094   76.4584         -1  
 203   0.4965  0.0156  0.0123  0.0094   76.4584         -1  
 204   0.5073  0.0157  0.0123  0.0094   76.4584         -1  
 205   0.5014  0.0103  0.0123  0.0094   76.4584         -1  
 206   0.4991  0.0080  0.0123  0.0094   76.4584         -1  
 207   0.4978  0.0129  0.0123  0.0094   76.4584         -1  
 208   0.5021  0.0249  0.0123  0.0094   76.4584         -1  
 209   0.4970  0.0122  0.0298  0.0116   38.9781         -1  
 210   0.5000  0.0178  0.0298  0.0116   38.9781         -1  
 211   0.5005  0.0147  0.0298  0.0116   38.9781         -1  
 212   0.4919  0.0137  0.0298  0.0116   38.9781         -1  
 213   0.4968  0.0128  0.0298  0.0116   38.9781         -1  
 214   0.4968  0.0115  0.0298  0.0116   38.9781         -1  
 215   0.5025  0.0121  0.0015  0.0207    0.0000         -1  
 216   0.4981  0.0143  0.0334  0.0206   61.6014         -1  
 217   0.5005  0.0163  0.0279  0.0123   44.1754         -1  
 219   0.5040  0.0161  0.0095  0.0162  170.5841         -1  
 220   0.5054  0.0127  0.0315  0.0164   51.9199         -1  
 221   0.4974  0.0141  0.0440  0.0133   30.2219         -1  
 223   0.4958  0.0133  0.0222  0.0048   21.6559         -1  
 224   0.5025  0.0144  0.0287  0.0149   51.8065         -1  
 225   0.5047  0.0124  0.0265  0.0083   31.4679         -1  
 226   0.5047  0.0093  0.0214  0.0127   59.3840         -1  
 227   0.4999  0.0102  0.0327  0.0144   44.1766         -1  
 228   0.5035  0.0130  0.0131  0.0129   98.1126         -1  
 229   0.4978  0.0154  0.0131  0.0129   98.1126         -1  
 230   0.4990  0.0130  0.0131  0.0129   98.1126         -1  
 232   0.5006  0.0164 -0.0034  0.0093  272.3477         -1  
 233   0.5024  0.0151 -0.0034  0.0093  272.3477         -1  
 234   0.4977  0.0184 -0.0034  0.0093  272.3477         -1  
 237   0.5037  0.0115  0.0261  0.0080   30.7439         -1  
 239   0.4915  0.0114  0.0235  0.0184   78.5378         -1  
 242   0.5017  0.0127  0.0154  0.0271  176.0329         -1  
 245   0.5007  0.0151  0.0154  0.0271  176.0329         -1  
 246   0.4989  0.0095  0.0154  0.0271  176.0329         -1  
 247   0.5009  0.0120  0.0154  0.0271  176.0329         -1  
 248   0.5041  0.0145  0.0123  0.0270  220.0378         -1  
 249   0.4964  0.0138  0.0123  0.0270  220.0378         -1  
 250   0.4978  0.0137  0.0123  0.0270  220.0378         -1  
 251   0.4999  0.0135  0.0123  0.0270  220.0378         -1  
 252   0.4924  0.0179  0.0123  0.0270  220.0378         -1  
 253   0.4997  0.0186  0.0137  0.0066   48.5023         -1  
 254   0.4966  0.0169  0.0376  0.0306   81.5102         -1  
 255   0.4988  0.0185  0.0135  0.0114   84.4337         -1  
 256   0.5018  0.0105  0.0135  0.0114   84.4337         -1  
 257   0.5001  0.0191  0.0135  0.0114   84.4337         -1  
 258   0.5016  0.0196  0.0135  0.0114   84.4337         -1  
 259   0.5009  0.0096  0.0135  0.0114   84.4337         -1  
 260   0.4995  0.0147  0.0135  0.0114   84.4337         -1  
 261   0.5077  0.0144  0.0333  0.0125   37.5668         -1  
 262   0.5032  0.0140  0.0333  0.0125   37.5668         -1  
 263   0.5012  0.0160  0.0181  0.0080   44.2988         -1  
 264   0.5000  0.0166  0.0149  0.0076   51.3704         -1  
 265   0.5001  0.0124  0.0195  0.0154   78.7890         -1  
 266   0.4972  0.0185  0.0195  0.0154   78.7890         -1  
 267   0.5022  0.0097  0.0165  0.0286  173.4377         -1  
 268   0.5010  0.0199  0.0165  0.0286  173.4377         -1  
 269   0.4973  0.0153  0.0165  0.0286  173.4377         -1  
 270   0.4999  0.0095  0.0274  0.0142   51.9067         -1  
 271   0.5058  0.0305  0.0274  0.0142   51.9067         -1  
 272   0.5001  0.0170  0.0274  0.0142   51.9067         -1  
 274   0.5033  0.0157  0.0274  0.0142   51.9067         -1  
 275   0.5043  0.0128  0.0239  0.0149   62.5617         -1  
 276   0.5003  0.0124  0.0313  0.0292   93.1579         -1  
 278   0.5032  0.0124  0.0177  0.0184  104.4612         -1  
 279   0.5038  0.0158  0.0177  0.0184  104.4612         -1  
 280   0.5019  0.0267  0.0121  0.0169  139.8330         -1  
 281   0.5015  0.0154  0.0121  0.0169  139.8330         -1  
 283   0.4959  0.0105  0.0121  0.0169  139.8330         -1  
 284   0.5018  0.0245  0.0121  0.0169  139.8330         -1  
 285   0.5044  0.0200  0.0292  0.0197   67.4053         -1  
 286   0.4978  0.0179  0.0292  0.0197   67.4053         -1  
 287   0.5049  0.0350  0.0064  0.0188  294.6251         -1  
 288   0.4976  0.0073  0.0064  0.0188  294.6251         -1  
 289   0.5007  0.0140  0.0197  0.0105   53.0253         -1  
 290   0.5035  0.0166  0.0037  0.0141  382.6619         -1  
 292   0.4968  0.0178  0.0291  0.0237   81.3456         -1  
 293   0.5076  0.0167  0.0291  0.0237   81.3456         -1  
 295   0.5019  0.0181  0.0291  0.0237   81.3456         -1  
 296   0.5025  0.0197  0.0291  0.0237   81.3456         -1  
 297   0.5070  0.0151  0.0079  0.0111  140.6419         -1  
 298   0.4988  0.0152  0.0277  0.0087   31.4497         -1  
 299   0.5017  0.0147  0.0188  0.0133   70.5588         -1  
 300   0.4920  0.0082  0.0188  0.0133   70.5588         -1  
 301   0.5025  0.0147  0.0188  0.0133   70.5588         -1  
 302   0.5007  0.0150  0.0187  0.0108   58.0827         -1  
 303   0.5045  0.0125  0.0187  0.0108   58.0827         -1  
 304   0.4948  0.0160  0.0187  0.0108   58.0827         -1  
 305   0.5051  0.0143  0.0187  0.0108   58.0827         -1  
 306   0.5044  0.0192  0.0187  0.0108   58.0827         -1  
 307   0.4986  0.0154  0.0187  0.0108   58.0827         -1  
 308   0.5005  0.0126  0.0187  0.0108   58.0827         -1  
 309   0.5057  0.0116  0.0187  0.0108   58.0827         -1  
 310   0.5091  0.0227  0.0187  0.0108   58.0827         -1  
 311   0.4999  0.0277  0.0187  0.0108   58.0827         -1  
 312   0.5025  0.0104  0.0187  0.0050   26.7738         -1  
 313   0.5011  0.0151  0.0187  0.0050   26.7738         -1  
 314   0.4967  0.0137  0.0225  0.0053   23.7892         -1  
 315   0.5019  0.0109  0.0225  0.0053   23.7892         -1  
 316   0.4994  0.0303  0.0032  0.0164  510.0410         -1  
 317   0.4992  0.0130  0.0216  0.0183   84.6151         -1  
 318   0.4997  0.0152  0.0262  0.0076   29.1606         -1  
 319   0.5036  0.0150  0.0262  0.0076   29.1606         -1  
 320   0.5003  0.0089  0.0238  0.0174   73.1502         -1  
 322   0.5026  0.0126  0.0227  0.0105   46.1077         -1  
 324   0.5021  0.0092  0.0107  0.0071   66.2997         -1  
 325   0.4932  0.0106  0.0107  0.0071   66.2997         -1  
 328   0.4963  0.0208  0.0226  0.0106   46.9253         -1  
 329   0.5030  0.0173  0.0226  0.0106   46.9253         -1  
 330   0.5098  0.0203  0.0226  0.0106   46.9253         -1  
 331   0.5017  0.0134  0.0226  0.0106   46.9253         -1  
 332   0.4983  0.0093  0.0226  0.0106   46.9253         -1  
 333   0.5031  0.0144  0.0140  0.0085   60.6831         -1  
 334   0.4988  0.0196  0.0140  0.0085   60.6831         -1  
 335   0.5050  0.0265  0.0416  0.0506  121.6592         -1  
 337   0.5068  0.0188  0.0077  0.0098  127.7463         -1  
 338   0.5001  0.0087  0.0270  0.0114   42.1428         -1  
 339   0.5004  0.0140  0.0270  0.0114   42.1428         -1  
 340   0.5014  0.0246 -0.0014  0.0098  706.8240         -1  
 341   0.5034  0.0122 -0.0014  0.0098  706.8240         -1  
 342   0.5025  0.0134  0.0358  0.0162   45.2582         -1  
 343   0.5021  0.0138  0.0358  0.0162   45.2582         -1  
 345   0.4967  0.0155  0.0358  0.0162   45.2582         -1  
 346   0.5024  0.0162  0.0358  0.0162   45.2582         -1  
 347   0.4989  0.0161 -0.0012  0.0151    0.0000         -1  
 348   0.4933  0.0166 -0.0012  0.0151    0.0000         -1  
 349   0.4934  0.0174 -0.0012  0.0151    0.0000         -1  
 350   0.5023  0.0166  0.0275  0.0314  114.1967         -1  
 352   0.4973  0.0157  0.0275  0.0314  114.1967         -1  
 353   0.5012  0.0130  0.0275  0.0314  114.1967         -1  
 354   0.5035  0.0085  0.0484  0.0339   70.0618         -1  
 355   0.4940  0.0126  0.0484  0.0339   70.0618         -1  
 356   0.4982  0.0144  0.0484  0.0339   70.0618         -1  
 357   0.4990  0.0182  0.0484  0.0339   70.0618         -1  
 358   0.5013  0.0169  0.0484  0.0339   70.0618         -1  
 359   0.5051  0.0136  0.0071  0.0100  141.9849         -1  
 360   0.4981  0.0102  0.0094  0.0095  101.3884         -1  
 361   0.4987  0.0123  0.0226  0.0221   97.8001         -1  
 362   0.5047  0.0101  0.0284  0.0141   49.7490         -1  
 363   0.5041  0.0311  0.0284  0.0141   49.7490         -1  
 364   0.4987  0.0195  0.0203  0.0226  110.9041         -1  
 365   0.4976  0.0148  0.0291  0.0135   46.4165         -1  
 366   0.5002  0.0148  0.0046  0.0180  388.9648         -1  
 367   0.5088  0.0136  0.0046  0.0180  388.9648         -1  
 369   0.5003  0.0154  0.0046  0.0180  388.9648         -1  
 370   0.5016  0.0113 -0.0042  0.0138  331.6058         -1  
 371   0.5084  0.0282  0.0121  0.0122  100.7960         -1  
 372   0.4997  0.0097  0.0075  0.0112  150.3448         -1  
 374   0.5017  0.0125  0.0075  0.0112  150.3448         -1  
 375   0.4996  0.0302  0.0075  0.0112  150.3448         -1  
 376   0.5020  0.0098  0.0075  0.0112  150.3448         -1  
 377   0.5026  0.0167  0.0075  0.0112  150.3448         -1  
 378   0.4966  0.0207  0.0075  0.0112  150.3448         -1  
 379   0.5015  0.0207  0.0159  0.0077   48.6584         -1  
 380   0.5077  0.0210  0.0165  0.0318  192.9130         -1  
 381   0.4988  0.0139  0.0165  0.0318  192.9130         -1  
 382   0.4964  0.0107  0.0165  0.0318  192.9130         -1  
 383   0.4996  0.0137  0.0536  0.0607  113.2841         -1  
 384   0.5013  0.0160  0.0536  0.0607  113.2841         -1  
 385   0.4968  0.0096  0.0056  0.0071  127.2483         -1  
 386   0.4992  0.0113  0.0056  0.0071  127.2483         -1  
 387   0.5020  0.0114  0.0056  0.0071  127.2483         -1  
 388   0.4984  0.0196  0.0056  0.0071  127.2483         -1  
 389   0.4994  0.0214  0.0104  0.0083   79.8045         -1  
 390   0.4952  0.0136  0.0104  0.0083   79.8045         -1  
 391   0.4987  0.0269  0.0104  0.0083   79.8045         -1  
 393   0.5036  0.0139  0.0157  0.0191  121.6354         -1  
 394   0.5045  0.0155  0.0157  0.0191  121.6354         -1  
 395   0.4959  0.0126  0.0157  0.0191  121.6354         -1  
 396   0.5043  0.0089  0.0120  0.0104   86.7035         -1  
 397   0.4983  0.0185  0.0120  0.0104   86.7035         -1  
 398   0.5018  0.0136  0.0120  0.0104   86.7035         -1  
 399   0.5022  0.0156  0.0120  0.0104   86.7035         -1  
 400   0.5023  0.0119  0.0219  0.0138   62.8422         -1  
 401   0.4962  0.0098  0.0061  0.0141  233.2441         -1  
 402   0.4998  0.0166  0.0061  0.0141  233.2441         -1  
 403   0.4926  0.0124  0.0061  0.0141  233.2441         -1  
 404   0.5043  0.0177  0.0061  0.0141  233.2441         -1  
 405   0.5010  0.0108  0.0061  0.0141  233.2441         -1  
 407   0.5053  0.0116  0.0061  0.0141  233.2441         -1  
 408   0.5013  0.0101  0.0061  0.0141  233.2441         -1  
 409   0.4998  0.0269  0.0061  0.0141  233.2441         -1  
 410   0.4934  0.0179  0.0145  0.0213  146.5131         -1  
 411   0.4999  0.0115  0.0145  0.0213  146.5131         -1  
 412   0.4976  0.0184  0.0145  0.0213  146.5131         -1  
 413   0.5040  0.0193  0.0145  0.0213  146.5131         -1  
 414   0.5008  0.0109  0.0433  0.0187   43.1616         -1  
 415   0.5076  0.0206  0.0095  0.0173  182.5607         -1  
 416   0.4997  0.0099  0.0005  0.0115    0.0000         -1  
 417   0.5036  0.0164  0.0005  0.0115    0.0000         -1  
 418   0.4969  0.0096  0.0005  0.0115    0.0000         -1  
 419   0.4981  0.0118  0.0005  0.0115    0.0000         -1  
 420   0.5041  0.0172  0.0005  0.0115    0.0000         -1  
 421   0.5008  0.0142  0.0005  0.0115    0.0000         -1  
 422   0.5005  0.0097  0.0005  0.0115    0.0000         -1  
 423   0.4985  0.0144  0.0005  0.0115    0.0000         -1  
 425   0.5080  0.0139  0.0234  0.0073   31.3771         -1  
 426   0.5002  0.0138  0.0234  0.0073   31.3771         -1  
 427   0.4986  0.0147  0.0142  0.0156  109.5996         -1  
 428   0.4981  0.0132  0.0142  0.0156  109.5996         -1  
 429   0.4975  0.0140  0.0142  0.0156  109.5996         -1  
 430   0.5009  0.0124  0.0142  0.0156  109.5996         -1  
 431   0.4981  0.0111  0.0142  0.0156  109.5996         -1  
 432   0.5078  0.0159  0.0142  0.0156  109.5996         -1  
 433   0.5040  0.0164  0.0116  0.0125  107.1020         -1  
 434   0.4998  0.0099  0.0091  0.0111  122.4135         -1  
 435   0.5037  0.0148  0.0254  0.0047   18.5390         -1  
 436   0.5004  0.0316  0.0329  0.0055   16.6695         -1  
 437   0.5033  0.0139  0.0329  0.0055   16.6695         -1  
 438   0.5079  0.0157  0.0329  0.0055   16.6695         -1  
 439   0.5014  0.0109  0.0329  0.0055   16.6695         -1  
 440   0.5002  0.0142  0.0329  0.0055   16.6695         -1  
 442   0.4983  0.0125  0.0329  0.0055   16.6695         -1  
 443   0.5001  0.0110  0.0282  0.0194   68.7444         -1  
 444   0.5036  0.0091  0.0282  0.0194   68.7444         -1  
 445   0.5090  0.0168  0.0282  0.0194   68.7444         -1  
 446   0.5022  0.0111  0.0283  0.0204   72.2002         -1  
 447   0.5037  0.0119  0.0235  0.0355  150.7761         -1  
 449   0.4986  0.0110  0.0235  0.0355  150.7761         -1  
 450   0.5019  0.0139  0.0235  0.0355  150.7761         -1  
 451   0.4960  0.0168  0.0235  0.0355  150.7761         -1  
 452   0.4977  0.0152  0.0235  0.0355  150.7761         -1  
 453   0.4997  0.0126  0.0235  0.0355  150.7761         -1  
 454   0.4941  0.0142  0.0235  0.0355  150.7761         -1  
 455   0.4976  0.0254  0.0262  0.0123   47.1425         -1  
 456   0.4990  0.0139  0.0050  0.0138  276.8808         -1  
 457   0.4954  0.0114  0.0050  0.0138  276.8808         -1  
 458   0.5065  0.0141  0.0050  0.0138  276.8808         -1  
 459   0.5056  0.0137  0.0232  0.0071   30.4587         -1  
 460   0.4995  0.0164  0.0232  0.0071   30.4587         -1  
 461   0.5085  0.0216  0.0232  0.0071   30.4587         -1  
 462   0.4962  0.0138  0.0155  0.0101   64.8825         -1  
 463   0.5037  0.0106  0.0155  0.0101   64.8825         -1  
 464   0.5041  0.0108  0.0155  0.0101   64.8825         -1  
 465   0.5045  0.0111  0.0193  0.0122   63.0838         -1  
 466   0.5023  0.0127  0.0193  0.0122   63.0838         -1  
 467   0.4987  0.0138  0.0193  0.0122   63.0838         -1  
 468   0.4997  0.0188  0.0193  0.0122   63.0838         -1  
 469   0.4993  0.0130  0.0193  0.0122   63.0838         -1  
 470   0.4985  0.0150  0.0193  0.0122   63.0838         -1  
 471   0.5026  0.0111  0.0189  0.0072   37.8921         -1  
 472   0.5015  0.0157  0.0189  0.0072   37.8921         -1  
 473   0.5031  0.0127  0.0189  0.0072   37.8921         -1  
 474   0.4995  0.0247  0.0189  0.0072   37.8921         -1  
 475   0.4959  0.0180  0.0114  0.0220  193.4385         -1  
 476   0.5050  0.0081  0.0208  0.0287  138.2861         -1  
 477   0.5027  0.0115  0.0208  0.0287  138.2861         -1  
 478   0.5015  0.0128  0.0208  0.0287  138.2861         -1  
 479   0.4962  0.0114  0.0208  0.0287  138.2861         -1  
 480   0.4988  0.0154  0.0208  0.0287  138.2861         -1  
 481   0.5024  0.0134  0.0147  0.0100   68.2310         -1  
 482   0.5031  0.0122  0.0316  0.0098   31.0788         -1  
 483   0.4997  0.0125  0.0316  0.0098   31.0788         -1  
 484   0.4989  0.0124  0.0316  0.0098   31.0788         -1  
 485   0.4902  0.0097  0.0316  0.0098   31.0788         -1  
 486   0.4990  0.0095  0.0316  0.0098   31.0788         -1  
 487   0.4952  0.0155  0.0316  0.0098   31.0788         -1  
 488   0.5028  0.0151  0.0316  0.0098   31.0788         -1  
 489   0.5037  0.0102  0.0279  0.0335  119.9899         -1  
 490   0.5038  0.0170  0.0279  0.0335  119.9899         -1  
 491   0.5039  0.0113  0.0279  0.0335  119.9899         -1  
 492   0.4993  0.0134  0.0279  0.0335  119.9899         -1  
 493   0.5055  0.0158  0.0255  0.0260  102.1652         -1  
 494   0.5007  0.0108  0.0255  0.0260  102.1652         -1  
 496   0.4962  0.0187  0.0255  0.0260  102.1652         -1  
 497   0.4987  0.0113  0.0255  0.0260  102.1652         -1  
 498   0.4990  0.0236  0.0048  0.0226  474.0812         -1  
 499   0.4994  0.0151  0.0048  0.0226  474.0812         -1  
 500   0.5038  0.0150  0.0048  0.0226  474.0812         -1  
 501   0.5042  0.0099  0.0048  0.0226  474.0812         -1  
 502   0.4985  0.0126  0.0048  0.0226  474.0812         -1  
 503   0.4982  0.0189  0.0048  0.0226  474.0812         -1  
 504   0.4978  0.0193  0.0048  0.0226  474.0812         -1  
 505   0.4995  0.0100  0.0048  0.0226  474.0812         -1  
 506   0.5002  0.0175  0.0048  0.0226  474.0812         -1  
 507   0.4976  0.0138  0.0048  0.0226  474.0812         -1  
 509   0.5001  0.0149  0.0048  0.0226  474.0812         -1  
 510   0.5011  0.0109  0.0048  0.0226  474.0812         -1  
 511   0.4997  0.0140  0.0048  0.0226  474.0812         -1  
 512   0.4925  0.0232  0.0279  0.0217   77.7519         -1  
 513   0.5052  0.0168  0.0279  0.0217   77.7519         -1  
 514   0.4982  0.0162  0.0279  0.0217   77.7519         -1  
 515   0.4993  0.0111  0.0279  0.0217   77.7519         -1  
 516   0.4999  0.0199  0.0279  0.0217   77.7519         -1  
 517   0.4956  0.0104  0.0331  0.0210   63.2615         -1  
 519   0.4988  0.0140  0.0331  0.0210   63.2615         -1  
 520   0.5036  0.0182  0.0331  0.0210   63.2615         -1  
 521   0.4981  0.0184  0.0331  0.0210   63.2615         -1  
 522   0.4994  0.0107  0.0331  0.0210   63.2615         -1  
 523   0.5020  0.0174  0.0370  0.0178   48.0601         -1  
 524   0.5020  0.0107  0.0091  0.0085   93.0476         -1  
 525   0.5046  0.0162  0.0217  0.0282  130.1233         -1  
 526   0.4972  0.0174  0.0217  0.0282  130.1233         -1  
 527   0.4991  0.0161  0.0222  0.0182   81.9472         -1  
 528   0.4954  0.0103  0.0222  0.0182   81.9472         -1  
 529   0.4998  0.0131  0.0222  0.0182   81.9472         -1  
 530   0.5016  0.0152  0.0465  0.0299   64.2405         -1  
 531   0.4953  0.0105 -0.0012  0.0252    0.0000         -1  
 532   0.4958  0.0111 -0.0012  0.0252    0.0000         -1  
 533   0.4962  0.0086 -0.0012  0.0252    0.0000         -1  
 534   0.4983  0.0159 -0.0012  0.0252    0.0000         -1  
 535   0.5008  0.0115 -0.0012  0.0252    0.0000         -1  
 536   0.4967  0.0110 -0.0012  0.0252    0.0000         -1  
 537   0.5008  0.0154 -0.0012  0.0252    0.0000         -1  
 538   0.5058  0.0104 -0.0012  0.0252    0.0000         -1  
 539   0.5011  0.0153 -0.0012  0.0252    0.0000         -1  
 540   0.4954  0.0136 -0.0012  0.0252    0.0000         -1  
 541   0.4962  0.0343 -0.0012  0.0252    0.0000         -1  
 542   0.5056  0.0184  0.0451  0.0087   19.3621         -1  
 543   0.4999  0.0133  0.0492  0.0182   36.9980         -1  
 544   0.4969  0.0137  0.0334  0.0188   56.2041         -1  
 545   0.5003  0.0233  0.0334  0.0188   56.2041         -1  
 546   0.5029  0.0206  0.0188  0.0172   91.3965         -1  
 547   0.4971  0.0121  0.0188  0.0172   91.3965         -1  
 548   0.4952  0.0120  0.0354  0.0146   41.3226         -1  
 549   0.4924  0.0111  0.0354  0.0146   41.3226         -1  
 550   0.4953  0.0170  0.0335  0.0122   36.2970         -1  
 551   0.5011  0.0179  0.0335  0.0122   36.2970         -1  
 552   0.4968  0.0178  0.0335  0.0122   36.2970         -1  
 553   0.4996  0.0127  0.0335  0.0122   36.2970         -1  
 554   0.5045  0.0212  0.0335  0.0122   36.2970         -1  
 555   0.5008  0.0127  0.0335  0.0122   36.2970         -1  
 556   0.4955  0.0088  0.0335  0.0122   36.2970         -1  
 557   0.5019  0.0124  0.0335  0.0122   36.2970         -1  
 558   0.5033  0.0132  0.0335  0.0122   36.2970         -1  
 559   0.5050  0.0174  0.0335  0.0122   36.2970         -1  
 560   0.5010  0.0157  0.0164  0.0093   56.7050         -1  
 561   0.5020  0.0191  0.0164  0.0093   56.7050         -1  
 562   0.5042  0.0141  0.0267  0.0134   50.0530         -1  
 563   0.4985  0.0155  0.0184  0.0127   68.7057         -1  
 564   0.5022  0.0105  0.0184  0.0127   68.7057         -1  
 565   0.4999  0.0288  0.0184  0.0127   68.7057         -1  
 566   0.4979  0.0165  0.0184  0.0127   68.7057         -1  
 567   0.5036  0.0136  0.0348  0.0160   45.9641         -1  
 568   0.4948  0.0129  0.0348  0.0160   45.9641         -1  
 569   0.4969  0.0116  0.0142  0.0102   71.7780         -1  
 570   0.5031  0.0220  0.0142  0.0102   71.7780         -1  
 571   0.4966  0.0097  0.0142  0.0102   71.7780         -1  
 572   0.5029  0.0198  0.0142  0.0102   71.7780         -1  
 573   0.4991  0.0121  0.0142  0.0102   71.7780         -1  
 574   0.5009  0.0142  0.0277  0.0118   42.7294         -1  
 575   0.5039  0.0104  0.0277  0.0118   42.7294         -1  
 577   0.5023  0.0156  0.0182  0.0139   76.6094         -1  
 578   0.4963  0.0150  0.0182  0.0139   76.6094         -1  
 579   0.4970  0.0143  0.0182  0.0139   76.6094         -1  
 580   0.4999  0.0100  0.0332  0.0216   65.1043         -1  
 581   0.5060  0.0195  0.0332  0.0216   65.1043         -1  
 582   0.5006  0.0152  0.0332  0.0216   65.1043         -1  
 584   0.4968  0.0096  0.0353  0.0298   84.3454         -1  
 585   0.4982  0.0125  0.0353  0.0298   84.3454         -1  
 586   0.4954  0.0096  0.0353  0.0298   84.3454         -1  
 587   0.4936  0.0223  0.0192  0.0167   86.9681         -1  
 588   0.4926  0.0144  0.0256  0.0189   73.8657         -1  
 589   0.5003  0.0214  0.0256  0.0189   73.8657         -1  
 590   0.5098  0.0173  0.0256  0.0189   73.8657         -1  
 591   0.5006  0.0106  0.0256  0.0189   73.8657         -1  
 592   0.4985  0.0233  0.0256  0.0189   73.8657         -1  
 593   0.5004  0.0179  0.0256  0.0189   73.8657         -1  
 594   0.4937  0.0153  0.0256  0.0189   73.8657         -1  
 595   0.5004  0.0151  0.0256  0.0189   73.8657         -1  
 596   0.5011  0.0157  0.0256  0.0189   73.8657         -1  
 597   0.5005  0.0155  0.0256  0.0189   73.8657         -1  
 598   0.5041  0.0082  0.0256  0.0189   73.8657         -1  
 599   0.5018  0.0101  0.0256  0.0189   73.8657         -1  
 600   0.5018  0.0324  0.0225  0.0193   85.7175         -1  
 602   0.4936  0.0150  0.0225  0.0193   85.7175         -1  
 603   0.5043  0.0128  0.0199  0.0159   79.7752         -1  
 604   0.5012  0.0146  0.0199  0.0159   79.7752         -1  
 606   0.5029  0.0152  0.0199  0.0159   79.7752         -1  
 607   0.5027  0.0243  0.0199  0.0159   79.7752         -1  
 608   0.4982  0.0194  0.0252  0.0157   62.3881         -1  
 609   0.4995  0.0154  0.0252  0.0157   62.3881         -1  
 610   0.5015  0.0137  0.0252  0.0157   62.3881         -1  
 611   0.4973  0.0129  0.0252  0.0157   62.3881         -1  
 612   0.5097  0.0438  0.0252  0.0157   62.3881         -1  
 613   0.5022  0.0193  0.0252  0.0157   62.3881         -1  
 614   0.5027  0.0154  0.0252  0.0157   62.3881         -1  
 615   0.4983  0.0100  0.0192  0.0077   40.2536         -1  
 616   0.4977  0.0136  0.0192  0.0077   40.2536         -1  
 617   0.4981  0.0102  0.0192  0.0077   40.2536         -1  
 618   0.4959  0.0126  0.0227  0.0149   65.4831         -1  
 619   0.4980  0.0118  0.0227  0.0149   65.4831         -1  
 620   0.4982  0.0120  0.0227  0.0149   65.4831         -1  
 621   0.5051  0.0086  0.0227  0.0149   65.4831         -1  
 622   0.5035  0.0462  0.0227  0.0149   65.4831         -1  
 623   0.5015  0.0127  0.0227  0.0149   65.4831         -1  
 624   0.4933  0.0126  0.0227  0.0149   65.4831         -1  
 625   0.5071  0.0123  0.0227  0.0149   65.4831         -1  
 626   0.5015  0.0130  0.0227  0.0149   65.4831         -1  
 627   0.4973  0.0073  0.0300  0.0326  108.6076         -1  
 628   0.5006  0.0068  0.0300  0.0326  108.6076         -1  
 629   0.5014  0.0090  0.0300  0.0326  108.6076         -1  
 630   0.5033  0.0209  0.0300  0.0326  108.6076         -1  
 631   0.4988  0.0406  0.0300  0.0326  108.6076         -1  
 632   0.4982  0.0130  0.0300  0.0326  108.6076         -1  
 633   0.5071  0.0141  0.0210  0.0107   50.7912         -1  
 635   0.4998  0.0097  0.0328  0.0235   71.5333         -1  
 636   0.5029  0.0186  0.0328  0.0235   71.5333         -1  
 637   0.4980  0.0130  0.0328  0.0235   71.5333         -1  
 638   0.4989  0.0091  0.0328  0.0235   71.5333         -1  
 639   0.4988  0.0178  0.0111  0.0294  264.1507         -1  
 640   0.5007  0.0135  0.0111  0.0294  264.1507         -1  
 641   0.4957  0.0151  0.0106  0.0277  262.1302         -1  
 642   0.4974  0.0114  0.0106  0.0277  262.1302         -1  
 643   0.5043  0.0155  0.0106  0.0277  262.1302         -1  
 644   0.5044  0.0139  0.0211  0.0106   50.0650         -1  
 645   0.5018  0.0101  0.0211  0.0106   50.0650         -1  
 646   0.4983  0.0103  0.0211  0.0106   50.0650         -1  
 647   0.5034  0.0130  0.0211  0.0106   50.0650         -1  
 648   0.5036  0.0152  0.0211  0.0106   50.0650         -1  
 649   0.5005  0.0169  0.0152  0.0115   75.2700         -1  
 650   0.4995  0.0215  0.0297  0.0115   38.7106         -1  
 651   0.4993  0.0121  0.0297  0.0115   38.7106         -1  
 652   0.4941  0.0097  0.0477  0.0443   92.7605         -1  
 653   0.4991  0.0134  0.0477  0.0443   92.7605         -1  
 654   0.4984  0.0161  0.0140  0.0112   80.4663         -1  
 655   0.5000  0.0131  0.0140  0.0112   80.4663         -1  
 656   0.5052  0.0123  0.0140  0.0112   80.4663         -1  
 657   0.4953  0.0156  0.0140  0.0112   80.4663         -1  
 658   0.5060  0.0134  0.0140  0.0112   80.4663         -1  
 659   0.5006  0.0149  0.0140  0.0112   80.4663         -1  
 660   0.4948  0.0104  0.0140  0.0112   80.4663         -1  
 661   0.5014  0.0125  0.0165  0.0114   68.9871         -1  
 662   0.5010  0.0147  0.0165  0.0114   68.9871         -1  
 663   0.5016  0.0155  0.0263  0.0117   44.3686         -1  
 664   0.5040  0.0115  0.0263  0.0117   44.3686         -1  
 665   0.5037  0.0165  0.0263  0.0117   44.3686         -1  
 666   0.4973  0.0191  0.0263  0.0117   44.3686         -1  
 667   0.5006  0.0216  0.0263  0.0117   44.3686         -1  
 668   0.4977  0.0137  0.0263  0.0117   44.3686         -1  
 669   0.5037  0.0175  0.0263  0.0117   44.3686         -1  
 670   0.4992  0.0128  0.0263  0.0117   44.3686         -1  
 671   0.4924  0.0124  0.0263  0.0117   44.3686         -1  
 672   0.5015  0.0313  0.0106  0.0212  200.1816         -1  
 673   0.5010  0.0146  0.0106  0.0212  200.1816         -1  
 674   0.5010  0.0142  0.0106  0.0212  200.1816         -1  
 675   0.5000  0.0128  0.0106  0.0212  200.1816         -1  
 676   0.4979  0.0125  0.0106  0.0212  200.1816         -1  
 677   0.5020  0.0167  0.0274  0.0072   26.1430         -1  
 678   0.5006  0.0097  0.0274  0.0072   26.1430         -1  
 679   0.5028  0.0164  0.0274  0.0072   26.1430         -1  
 680   0.4934  0.0126  0.0274  0.0072   26.1430         -1  
 681   0.4975  0.0148  0.0274  0.0072   26.1430         -1  
 682   0.5008  0.0147  0.0274  0.0072   26.1430         -1  
 683   0.5022  0.0104  0.0274  0.0072   26.1430         -1  
 684   0.4962  0.0123  0.0274  0.0072   26.1430         -1  
 685   0.4982  0.0161  0.0274  0.0072   26.1430         -1  
 686   0.5026  0.0121  0.0326  0.0078   23.9629         -1  
 687   0.4965  0.0108  0.0326  0.0078   23.9629         -1  
 688   0.4994  0.0182  0.0326  0.0078   23.9629         -1  
 689   0.4984  0.0116  0.0154  0.0113   73.6411         -1  
 690   0.5040  0.0091  0.0154  0.0113   73.6411         -1  
 691   0.5013  0.0095  0.0154  0.0113   73.6411         -1  
 692   0.5004  0.0119  0.0154  0.0113   73.6411         -1  
 693   0.5033  0.0122  0.0123  0.0129  104.7519         -1  
 694   0.5043  0.0214  0.0123  0.0129  104.7519         -1  
 695   0.5032  0.0112  0.0123  0.0129  104.7519         -1  
 696   0.4993  0.0120  0.0123  0.0129  104.7519         -1  
 697   0.4927  0.0095  0.0123  0.0129  104.7519         -1  
 698   0.5031  0.0133  0.0123  0.0129  104.7519         -1  
 699   0.4987  0.0105  0.0123  0.0129  104.7519         -1  
 700   0.5005  0.0099  0.0123  0.0129  104.7519         -1  
 701   0.4965  0.0097  0.0123  0.0129  104.7519         -1  
 702   0.5014  0.0100  0.0123  0.0129  104.7519         -1  
 703   0.4982  0.0132  0.0161  0.0350  217.1506         -1  
 704   0.5017  0.0112  0.0161  0.0350  217.1506         -1  
 705   0.5014  0.0133  0.0161  0.0350  217.1506         -1  
 706   0.4980  0.0174  0.0161  0.0350  217.1506         -1  
 707   0.5043  0.0115  0.0161  0.0350  217.1506         -1  
 708   0.5029  0.0110  0.0161  0.0350  217.1506         -1  
 710   0.4954  0.0150  0.0300  0.0076   25.2223         -1  
 711   0.5017  0.0162  0.0072  0.0210  291.8040         -1  
 712   0.4969  0.0139  0.0072  0.0210  291.8040         -1  
 713   0.5028  0.0239  0.0072  0.0210  291.8040         -1  
 714   0.4997  0.0109  0.0072  0.0210  291.8040         -1  
 715   0.4963  0.0153  0.0072  0.0210  291.8040         -1  
 716   0.4978  0.0135  0.0072  0.0210  291.8040         -1  
 717   0.5001  0.0190  0.0263  0.0131   49.9454         -1  
 718   0.4918  0.0136  0.0121  0.0121  100.3091         -1  
 719   0.4995  0.0101  0.0121  0.0121  100.3091         -1  
 720   0.4944  0.0184  0.0121  0.0121  100.3091         -1  
 721   0.4972  0.0117  0.0121  0.0121  100.3091         -1  
 722   0.4997  0.0151  0.0121  0.0121  100.3091         -1  
 723   0.4948  0.0182  0.0121  0.0121  100.3091         -1  
 724   0.4953  0.0170  0.0472  0.0171   36.1626         -1  
 725   0.5003  0.0142  0.0131  0.0112   85.1551         -1  
 726   0.5005  0.0118  0.0131  0.0112   85.1551         -1  
 727   0.4993  0.0133  0.0131  0.0112   85.1551         -1  
 728   0.5030  0.0105  0.0131  0.0112   85.1551         -1  
 729   0.5013  0.0187  0.0131  0.0112   85.1551         -1  
 730   0.5022  0.0173  0.0335  0.0084   25.1494         -1  
 731   0.5038  0.0103  0.0335  0.0084   25.1494         -1  
 732   0.4941  0.0106  0.0335  0.0084   25.1494         -1  
 733   0.4965  0.0186  0.0335  0.0084   25.1494         -1  
 734   0.4929  0.0155  0.0335  0.0084   25.1494         -1  
 735   0.5000  0.0148  0.0171  0.0173  101.1981         -1  
 736   0.4976  0.0125  0.0270  0.0162   59.9813         -1  
 737   0.4936  0.0115  0.0270  0.0162   59.9813         -1  
 738   0.4991  0.0146  0.0270  0.0162   59.9813         -1  
 739   0.4922  0.0177  0.0270  0.0162   59.9813         -1  
 740   0.4977  0.0097  0.0270  0.0162   59.9813         -1  
 741   0.5007  0.0116  0.0270  0.0162   59.9813         -1  
 742   0.5059  0.0160  0.0270  0.0162   59.9813         -1  
 743   0.5039  0.0169  0.0270  0.0162   59.9813         -1  
 744   0.4939  0.0132  0.0230  0.0064   27.7705         -1  
 745   0.4942  0.0197  0.0419  0.0098   23.3852         -1  
 746   0.5021  0.0131  0.0419  0.0098   23.3852         -1  
 747   0.5010  0.0144  0.0419  0.0098   23.3852         -1  
 748   0.4974  0.0104  0.0419  0.0098   23.3852         -1  
 749   0.4958  0.0170  0.0419  0.0098   23.3852         -1  
 750   0.4965  0.0217  0.0419  0.0098   23.3852         -1  
 751   0.4989  0.0167  0.0119  0.0054   45.7004         -1  
 752   0.5070  0.0135  0.0119  0.0054   45.7004         -1  
 753   0.4992  0.0163  0.0119  0.0054   45.7004         -1  
 754   0.5045  0.0116  0.0136  0.0171  125.5354         -1  
 755   0.4995  0.0121  0.0169  0.0276  163.9998         -1  
 756   0.4979  0.0159  0.0169  0.0276  163.9998         -1  
 757   0.4978  0.0141  0.0169  0.0276  163.9998         -1  
 758   0.4941  0.0164  0.0169  0.0276  163.9998         -1  
 759   0.4997  0.0090  0.0169  0.0276  163.9998         -1  
 760   0.4943  0.0199  0.0169  0.0276  163.9998         -1  
 761   0.5015  0.0145  0.0169  0.0276  163.9998         -1  
 762   0.4934  0.0109  0.0169  0.0276  163.9998         -1  
 763   0.4973  0.0194  0.0169  0.0276  163.9998         -1  
 764   0.4988  0.0167  0.0386  0.0074   19.0852         -1  
 765   0.5010  0.0086  0.0212  0.0109   51.5472         -1  
 766   0.4937  0.0119  0.0212  0.0109   51.5472         -1  
 767   0.4954  0.0151  0.0212  0.0109   51.5472         -1  
 768   0.4995  0.0176  0.0212  0.0109   51.5472         -1  
 769   0.5006  0.0128  0.0212  0.0109   51.5472         -1  
 770   0.4970  0.0118  0.0212  0.0109   51.5472         -1  
 771   0.5031  0.0146  0.0212  0.0109   51.5472         -1  
 772   0.4951  0.0181  0.0262  0.0121   46.1016         -1  
 773   0.4989  0.0082  0.0373  0.0079   21.0599         -1  
 774   0.5013  0.0111  0.0373  0.0079   21.0599         -1  
 775   0.4964  0.0136  0.0373  0.0079   21.0599         -1  
 776   0.4974  0.0181  0.0373  0.0079   21.0599         -1  
 777   0.4995  0.0144  0.0373  0.0079   21.0599         -1  
 778   0.5010  0.0086  0.0373  0.0079   21.0599         -1  
 779   0.5002  0.0143  0.0373  0.0079   21.0599         -1  
 780   0.5019  0.0222  0.0373  0.0079   21.0599         -1  
 781   0.5056  0.0208  0.0373  0.0079   21.0599         -1  
 782   0.4986  0.0172  0.0373  0.0079   21.0599         -1  
 783   0.4977  0.0158  0.0373  0.0079   21.0599         -1  
 784   0.4982  0.0095  0.0373  0.0079   21.0599         -1  
 785   0.4999  0.0121  0.0373  0.0079   21.0599         -1  
 786   0.5012  0.0125  0.0373  0.0079   21.0599         -1  
 787   0.5012  0.0194  0.0373  0.0079   21.0599         -1  
 788   0.5034  0.0141  0.0246  0.0064   25.9900         -1  
 789   0.4983  0.0153  0.0246  0.0064   25.9900         -1  
 790   0.5010  0.0252  0.0246  0.0064   25.9900         -1  
 791   0.4975  0.0142  0.0246  0.0064   25.9900         -1  
 792   0.5008  0.0115  0.0215  0.0071   33.1090         -1  
 793   0.4975  0.0144  0.0215  0.0071   33.1090         -1  
 794   0.4948  0.0124  0.0215  0.0071   33.1090         -1  
 796   0.4949  0.0097  0.0406  0.0268   66.1687         -1  
 798   0.5000  0.0095  0.0122  0.0227  187.0796         -1  
 799   0.4992  0.0098  0.0122  0.0227  187.0796         -1  
 800   0.4942  0.0135  0.0122  0.0227  187.0796         -1  
 801   0.5011  0.0193  0.0122  0.0227  187.0796         -1  
 802   0.4959  0.0150  0.0122  0.0227  187.0796         -1  
 803   0.4967  0.0110  0.0253  0.0058   22.7661         -1  
 804   0.5000  0.0117  0.0253  0.0058   22.7661         -1  
 805   0.4949  0.0155  0.0253  0.0058   22.7661         -1  
 806   0.4962  0.0124  0.0253  0.0058   22.7661         -1  
 807   0.4995  0.0154  0.0253  0.0058   22.7661         -1  
 808   0.4941  0.0096  0.0269  0.0160   59.2045         -1  
 809   0.4968  0.0114  0.0269  0.0160   59.2045         -1  
 810   0.4939  0.0160  0.0269  0.0160   59.2045         -1  
 811   0.4953  0.0128  0.0111  0.0069   62.3602         -1  
 812   0.4974  0.0119  0.0111  0.0069   62.3602         -1  
 813   0.4954  0.0093  0.0111  0.0069   62.3602         -1  
 814   0.4994  0.0073  0.0111  0.0069   62.3602         -1  
 815   0.4963  0.0129  0.0294  0.0051   17.3775         -1  
 816   0.5037  0.0392  0.0294  0.0051   17.3775         -1  
 817   0.5002  0.0124  0.0213  0.0076   35.5725         -1  
 818   0.4984  0.0169  0.0275  0.0215   78.1199         -1  
 819   0.5024  0.0108  0.0275  0.0215   78.1199         -1  
 820   0.4970  0.0150  0.0275  0.0215   78.1199         -1  
 821   0.4979  0.0078  0.0275  0.0215   78.1199         -1  
 822   0.4999  0.0103  0.0275  0.0215   78.1199         -1  
 823   0.5021  0.0086  0.0275  0.0215   78.1199         -1  
 824   0.5004  0.0135  0.0275  0.0215   78.1199         -1  
 825   0.4987  0.0131  0.0275  0.0215   78.1199         -1  
 827   0.4961  0.0157  0.0545  0.0184   33.7876         -1  
 828   0.4954  0.0122  0.0545  0.0184   33.7876         -1  
 829   0.5021  0.0071  0.0545  0.0184   33.7876         -1  
 830   0.5004  0.0120  0.0545  0.0184   33.7876         -1  
 832   0.4987  0.0118  0.0545  0.0184   33.7876         -1  
 833   0.4934  0.0123  0.0545  0.0184   33.7876         -1  
 834   0.4987  0.0145  0.0545  0.0184   33.7876         -1  
 835   0.5033  0.0154  0.0099  0.0113  114.2878         -1  
 836   0.4963  0.0156  0.0099  0.0113  114.2878         -1  
 837   0.4925  0.0145  0.0099  0.0113  114.2878         -1  
 838   0.5032  0.0129  0.0099  0.0113  114.2878         -1  
 839   0.4978  0.0133  0.0128  0.0193  151.1930         -1  
 840   0.4985  0.0101  0.0128  0.0193  151.1930         -1  
 841   0.4989  0.0125  0.0128  0.0193  151.1930         -1  
 842   0.5035  0.0142  0.0128  0.0193  151.1930         -1  
 843   0.4944  0.0123  0.0200  0.0095   47.7832         -1  
 844   0.4959  0.0142  0.0278  0.0135   48.4818         -1  
 845   0.5005  0.0145  0.0278  0.0135   48.4818         -1  
 846   0.4994  0.0125  0.0278  0.0135   48.4818         -1  
 847   0.5027  0.0155  0.0503  0.0334   66.5448         -1  
 848   0.5013  0.0111  0.0221  0.0235  106.1758         -1  
 849   0.5025  0.0118  0.0364  0.0172   47.2136         -1  
 850   0.4999  0.0122  0.0364  0.0172   47.2136         -1  
 851   0.4992  0.0141  0.0364  0.0172   47.2136         -1  
 852   0.4999  0.0098  0.0429  0.0401   93.5134         -1  
 853   0.5033  0.0146  0.0281  0.0227   80.5639         -1  
 854   0.5005  0.0130  0.0281  0.0227   80.5639         -1  
 855   0.4956  0.0437  0.0281  0.0227   80.5639         -1  
 856   0.5025  0.0178  0.0286  0.0154   53.8577         -1  
 857   0.4964  0.0106  0.0286  0.0154   53.8577         -1  
 858   0.5014  0.0081  0.0220  0.0143   65.2186         -1  
 859   0.4938  0.0106  0.0220  0.0143   65.2186         -1  
 860   0.4985  0.0089  0.0220  0.0143   65.2186         -1  
 861   0.4975  0.0134  0.0220  0.0143   65.2186         -1  
 862   0.4948  0.0142  0.0241  0.0121   50.0888         -1  
 863   0.4991  0.0086  0.0140  0.0192  136.9762         -1  
 864   0.4988  0.0192  0.0140  0.0192  136.9762         -1  
 865   0.4927  0.0147  0.0473  0.0280   59.0825         -1  
 866   0.4957  0.0251  0.0473  0.0280   59.0825         -1  
 867   0.5024  0.0096  0.0473  0.0280   59.0825         -1  
 868   0.5032  0.0149  0.0473  0.0280   59.0825         -1  
 869   0.4954  0.0139  0.0117  0.0262  223.1018         -1  
 870   0.5006  0.0113  0.0117  0.0262  223.1018         -1  
 872   0.4993  0.0151  0.0117  0.0262  223.1018         -1  
 873   0.5059  0.0191  0.0117  0.0262  223.1018         -1  
 874   0.4948  0.0145  0.0117  0.0262  223.1018         -1  
 875   0.5009  0.0126  0.0117  0.0262  223.1018         -1  
 876   0.4993  0.0098  0.0117  0.0262  223.1018         -1  
 877   0.5001  0.0168  0.0147  0.0095   65.0365         -1  
 878   0.4981  0.0136  0.0147  0.0095   65.0365         -1  
 879   0.5022  0.0097  0.0147  0.0095   65.0365         -1  
 880   0.4995  0.0139  0.0147  0.0095   65.0365         -1  
 881   0.4968  0.0202  0.0262  0.0123   46.7092         -1  
 882   0.5010  0.0111  0.0262  0.0123   46.7092         -1  
 883   0.4968  0.0217  0.0133  0.0139  104.3034         -1  
 884   0.4981  0.0148  0.0133  0.0139  104.3034         -1  
 885   0.4936  0.0113  0.0133  0.0139  104.3034         -1  
 886   0.4962  0.0106  0.0133  0.0139  104.3034         -1  
 887   0.4948  0.0099  0.0133  0.0139  104.3034         -1  
 888   0.5016  0.0145  0.0309  0.0317  102.4718         -1  
 889   0.5016  0.0112  0.0122  0.0131  107.5257         -1  
 890   0.4973  0.0139  0.0122  0.0131  107.5257         -1  
 891   0.5080  0.0127  0.0226  0.0196   87.0971         -1  
 892   0.4997  0.0177  0.0201  0.0067   33.2533         -1  
 893   0.5020  0.0127  0.0242  0.0094   38.9642         -1  
 894   0.5014  0.0127  0.0242  0.0094   38.9642         -1  
 895   0.5003  0.0175  0.0338  0.0069   20.3091         -1  
 896   0.4977  0.0104  0.0338  0.0069   20.3091         -1  
 897   0.5087  0.0117  0.0338  0.0069   20.3091         -1  
 898   0.5015  0.0126  0.0338  0.0069   20.3091         -1  
 899   0.4974  0.0171  0.0218  0.0054   24.6547         -1  
 900   0.4998  0.0102  0.0218  0.0054   24.6547         -1  
 901   0.4989  0.0131  0.0218  0.0054   24.6547         -1  
 902   0.5000  0.0176  0.0218  0.0054   24.6547         -1  
 903   0.4974  0.0171  0.0218  0.0054   24.6547         -1  
 904   0.5024  0.0185  0.0218  0.0054   24.6547         -1  
 905   0.5048  0.0118  0.0211  0.0115   54.4761         -1  
 906   0.4983  0.0107  0.0211  0.0115   54.4761         -1  
 907   0.5007  0.0099  0.0187  0.0190  101.3876         -1  
 908   0.4989  0.0159  0.0187  0.0190  101.3876         -1  
 909   0.5028  0.0129  0.0118  0.0095   80.9167         -1  
 910   0.4969  0.0114  0.0368  0.0146   39.6224         -1  
 911   0.4973  0.0081  0.0186  0.0095   51.2287         -1  
 912   0.4995  0.0112  0.0134  0.0121   90.4575         -1  
 913   0.4993  0.0117  0.0134  0.0121   90.4575         -1  
 915   0.4997  0.0114  0.0280  0.0078   27.7601         -1  
 916   0.4958  0.0115  0.0280  0.0078   27.7601         -1  
 917   0.4981  0.0092  0.0280  0.0078   27.7601         -1  
 918   0.4940  0.0123  0.0280  0.0078   27.7601         -1  
 919   0.4994  0.0147  0.0280  0.0078   27.7601         -1  
 920   0.5019  0.0128  0.0280  0.0078   27.7601         -1  
 921   0.4950  0.0123  0.0195  0.0149   76.0035         -1  
 922   0.4987  0.0172  0.0195  0.0149   76.0035         -1  
 923   0.4939  0.0210  0.0195  0.0149   76.0035         -1  
 925   0.5013  0.0076  0.0153  0.0048   31.0176         -1  
 927   0.5016  0.0130  0.0153  0.0048   31.0176         -1  
 928   0.5023  0.0140  0.0153  0.0048   31.0176         -1  
 930   0.5007  0.0242  0.0153  0.0048   31.0176         -1  
 931   0.4778  0.4714  0.0299  0.0118   39.5108         -1  
 932   0.5013  0.0118  0.0134  0.0156  116.6826         -1  
 933   0.4992  0.0150  0.0134  0.0156  116.6826         -1  
 934   0.5002  0.0122  0.0134  0.0156  116.6826         -1  
 935   0.5056  0.0110  0.0205  0.0127   62.0056         -1  
 936   0.4996  0.0095  0.0205  0.0127   62.0056         -1  
 937   0.5024  0.0105  0.0205  0.0127   62.0056         -1  
 938   0.5039  0.0215  0.0388  0.0345   88.7443         -1  
 939   0.5047  0.0396  0.0388  0.0345   88.7443         -1  
 940   0.4955  0.0108  0.0388  0.0345   88.7443         -1  
 941   0.5015  0.0134  0.0234  0.0102   43.5691         -1  
 942   0.5013  0.0086  0.0052  0.0203  390.4146         -1  
 943   0.5056  0.0131  0.0052  0.0203  390.4146         -1  
 944   0.5016  0.0197  0.0052  0.0203  390.4146         -1  
 945   0.4987  0.0186  0.0052  0.0203  390.4146         -1  
 946   0.5034  0.0166  0.0052  0.0203  390.4146         -1  
 947   0.4964  0.0120  0.0052  0.0203  390.4146         -1  
 948   0.4976  0.0397  0.0295  0.0244   82.5604         -1  
 949   0.5016  0.0130  0.0295  0.0244   82.5604         -1  
 950   0.4968  0.0173  0.0401  0.0219   54.5609         -1  
 951   0.5034  0.0109  0.0207  0.0084   40.8368         -1  
 952   0.5006  0.0133  0.0220  0.0110   50.1186         -1  
 953   0.5022  0.0163  0.0220  0.0110   50.1186         -1  
 954   0.4989  0.0146  0.0220  0.0110   50.1186         -1  
 955   0.4990  0.0128  0.0208  0.0066   31.8749         -1  
 956   0.5013  0.0227  0.0208  0.0066   31.8749         -1  
 957   0.5039  0.0102  0.0260  0.0268  103.3520         -1  
 958   0.5028  0.0156  0.0260  0.0268  103.3520         -1  
 959   0.5050  0.0221  0.0260  0.0268  103.3520         -1  
 960   0.5000  0.0117  0.0178  0.0119   66.7668         -1  
 961   0.4981  0.0093  0.0178  0.0119   66.7668         -1  
 962   0.5026  0.0125  0.0174  0.0272  156.5532         -1  
 963   0.4974  0.0116  0.0174  0.0272  156.5532         -1  
 964   0.5003  0.0173  0.0219  0.0088   40.4322         -1  
 965   0.4983  0.0096  0.0219  0.0088   40.4322         -1  
 966   0.4990  0.0118  0.0219  0.0088   40.4322         -1  
 967   0.5015  0.0118  0.0219  0.0088   40.4322         -1  
 968   0.5024  0.0105  0.0219  0.0088   40.4322         -1  
 969   0.5034  0.0106  0.0219  0.0088   40.4322         -1  
 970   0.4999  0.0174  0.0223  0.0159   71.0108         -1  
 971   0.4987  0.0112  0.0223  0.0159   71.0108         -1  
 972   0.4998  0.0166  0.0223  0.0159   71.0108         -1  
 973   0.4986  0.0081  0.0223  0.0159   71.0108         -1  
 974   0.5024  0.0094  0.0223  0.0159   71.0108         -1  
 975   0.5058  0.0112  0.0223  0.0159   71.0108         -1  
 976   0.4946  0.0112  0.0223  0.0159   71.0108         -1  
 977   0.4940  0.0199  0.0348  0.0303   86.9740         -1  
 978   0.4981  0.0126  0.0200  0.0078   38.9976         -1  
 979   0.4963  0.0180  0.0237  0.0187   78.6294         -1  
 980   0.5033  0.0146  0.0202  0.0069   34.1793         -1  
 981   0.4980  0.0077  0.0202  0.0069   34.1793         -1  
 982   0.5056  0.0100  0.0202  0.0069   34.1793         -1  
 983   0.4995  0.0125  0.0202  0.0069   34.1793         -1  
 984   0.4970  0.0093  0.0088  0.0123  139.7237         -1  
 985   0.5037  0.0099  0.0162  0.0058   35.6702         -1  
 986   0.5015  0.0119  0.0162  0.0058   35.6702         -1  
 987   0.5017  0.0184  0.0162  0.0058   35.6702         -1  
 988   0.4982  0.0107  0.0162  0.0058   35.6702         -1  
 989   0.4992  0.0122  0.0170  0.0150   88.3754         -1  
 990   0.5017  0.0153  0.0170  0.0150   88.3754         -1  
 991   0.5034  0.0129  0.0200  0.0205  102.5241         -1  
 992   0.4986  0.0111  0.0200  0.0205  102.5241         -1  
 993   0.4960  0.0071  0.0200  0.0205  102.5241         -1  
 994   0.5012  0.0128  0.0200  0.0205  102.5241         -1  
 995   0.5013  0.0208  0.0200  0.0205  102.5241         -1  
 996   0.5004  0.0174  0.0200  0.0205  102.5241         -1  
 997   0.4992  0.0111  0.0260  0.0107   41.2277         -1  
 998   0.4984  0.0110  0.0096  0.0191  197.5077         -1  
 999   0.5008  0.0190  0.0096  0.0191  197.5077         -1  
 1000  0.5006  0.0149  0.0096  0.0191  197.5077         -1  
 1001  0.5012  0.0153  0.0096  0.0191  197.5077         -1  
 1002  0.5010  0.0123  0.0096  0.0191  197.5077         -1  
 1003  0.5035  0.0129  0.0096  0.0191  197.5077         -1  
 1004  0.4936  0.0073  0.0552  0.0178   32.2058         -1  
 1005  0.4976  0.0164  0.0552  0.0178   32.2058         -1  
 1006  0.5025  0.0125  0.0552  0.0178   32.2058         -1  
 1007  0.5020  0.0156  0.0552  0.0178   32.2058         -1  
 1008  0.4955  0.0126  0.0167  0.0132   79.1086         -1  
 1009  0.5014  0.0097  0.0167  0.0132   79.1086         -1  
 1010  0.5015  0.0163  0.0167  0.0132   79.1086         -1  
 1011  0.5022  0.0092  0.0167  0.0132   79.1086         -1  
 1012  0.5039  0.0151  0.0167  0.0132   79.1086         -1  
 1013  0.4999  0.0095  0.0167  0.0132   79.1086         -1  
 1014  0.5036  0.0164  0.0167  0.0132   79.1086         -1  
 1015  0.4987  0.0127  0.0167  0.0132   79.1086         -1  
 1016  0.5045  0.0132  0.0167  0.0132   79.1086         -1  
 1017  0.4982  0.0116  0.0167  0.0132   79.1086         -1  
 1018  0.5036  0.0130  0.0167  0.0132   79.1086         -1  
 1019  0.5025  0.0147  0.0352  0.0108   30.6806         -1  
 1020  0.5010  0.0136  0.0237  0.0090   38.0467         -1  
 1021  0.5012  0.0143  0.0171  0.0071   41.6047         -1  
 1022  0.4964  0.0137  0.0171  0.0071   41.6047         -1  
 1023  0.5002  0.0075  0.0121  0.0139  114.4153         -1  
 1024  0.4965  0.0120  0.0121  0.0139  114.4153         -1  
 1025  0.4945  0.0109  0.0119  0.0123  103.5848         -1  
 1026  0.4957  0.0146  0.0119  0.0123  103.5848         -1  
 1027  0.5028  0.0127  0.0119  0.0123  103.5848         -1  
 1028  0.5023  0.0133  0.0267  0.0174   65.1609         -1  
 1030  0.5046  0.0065  0.0267  0.0174   65.1609         -1  
 1031  0.4984  0.0130  0.0267  0.0174   65.1609         -1  
 1032  0.5062  0.0117  0.0267  0.0174   65.1609         -1  
 1033  0.5038  0.0133  0.0292  0.0311  106.5582         -1  
 1034  0.5001  0.0099  0.0282  0.0217   76.7510         -1  
 1035  0.4964  0.0108  0.0229  0.0182   79.6170         -1  
 1036  0.4966  0.0078  0.0229  0.0182   79.6170         -1  
 1037  0.4942  0.0172  0.0229  0.0182   79.6170         -1  
 1038  0.5020  0.0111  0.0506  0.0245   48.3708         -1  
 1039  0.4937  0.0146  0.0437  0.0200   45.7019         -1  
 1040  0.5091  0.0123  0.0437  0.0200   45.7019         -1  
 1041  0.4956  0.0128  0.0437  0.0200   45.7019         -1  
 1042  0.5012  0.0123  0.0437  0.0200   45.7019         -1  
 1043  0.5068  0.0426  0.0110  0.0096   87.4677         -1  
 1044  0.4998  0.0097  0.0257  0.0174   67.6124         -1  
 1045  0.4941  0.0178  0.0257  0.0174   67.6124         -1  
 1046  0.4964  0.0125  0.0257  0.0174   67.6124         -1  
 1047  0.5044  0.0177  0.0111  0.0195  176.2709         -1  
 1048  0.4963  0.0123  0.0111  0.0195  176.2709         -1  
 1049  0.5021  0.0129  0.0277  0.0116   41.6717         -1  
 1050  0.5023  0.0130  0.0084  0.0097  116.1088         -1  
 1051  0.4979  0.0120  0.0084  0.0097  116.1088         -1  
 1052  0.5049  0.0181  0.0084  0.0097  116.1088         -1  
 1053  0.5084  0.0163  0.0084  0.0097  116.1088         -1  
 1054  0.4976  0.0130  0.0084  0.0097  116.1088         -1  
 1055  0.5011  0.0103  0.0090  0.0166  183.3928         -1  
 1056  0.5028  0.0294  0.0090  0.0166  183.3928         -1  
 1057  0.4980  0.0118  0.0090  0.0166  183.3928         -1  
 1058  0.5054  0.0213  0.0338  0.0058   17.2268         -1  
 1059  0.5005  0.0088  0.0189  0.0059   31.0252         -1  
 1060  0.4965  0.0159  0.0189  0.0059   31.0252         -1  
 1061  0.5027  0.0147  0.0189  0.0059   31.0252         -1  
 1063  0.5053  0.0233  0.0189  0.0059   31.0252         -1  
 1064  0.4983  0.0093  0.0189  0.0059   31.0252         -1  
 1065  0.4978  0.0121  0.0412  0.0176   42.6972         -1  
 1066  0.4962  0.0129  0.0412  0.0176   42.6972         -1  
 1067  0.4996  0.0111  0.0412  0.0176   42.6972         -1  
 1068  0.4997  0.0144  0.0455  0.0072   15.7327         -1  
 1069  0.4977  0.0157  0.0455  0.0072   15.7327         -1  
 1070  0.4967  0.0187  0.0455  0.0072   15.7327         -1  
 1071  0.4953  0.0156  0.0455  0.0072   15.7327         -1  
 1072  0.4987  0.0117  0.0455  0.0072   15.7327         -1  
 1073  0.5021  0.0168  0.0276  0.0160   57.7824         -1  
 1074  0.4965  0.0147  0.0276  0.0160   57.7824         -1  
 1075  0.5016  0.0206  0.0276  0.0160   57.7824         -1  
 1076  0.5024  0.0210  0.0276  0.0160   57.7824         -1  
 1077  0.4991  0.0119  0.0276  0.0160   57.7824         -1  
 1078  0.5004  0.0152  0.0156  0.0093   59.5644         -1  
 1079  0.5040  0.0148  0.0156  0.0093   59.5644         -1  
 1080  0.5013  0.0162  0.0157  0.0081   51.5105         -1  
 1081  0.5054  0.0460  0.0119  0.0173  145.3192         -1  
 1082  0.4983  0.0167  0.0119  0.0173  145.3192         -1  
 1083  0.5022  0.0132  0.0206  0.0185   89.9060         -1  
 1084  0.5045  0.0115  0.0389  0.0172   44.2355         -1  
 1085  0.4999  0.0088  0.0389  0.0172   44.2355         -1  
 1086  0.4980  0.0209  0.0389  0.0172   44.2355         -1  
 1087  0.5092  0.0111  0.0162  0.0172  106.5441         -1  
 1088  0.4966  0.0179  0.0162  0.0172  106.5441         -1  
 1089  0.5066  0.0114  0.0162  0.0172  106.5441         -1  
 1090  0.5032  0.0137  0.0162  0.0172  106.5441         -1  
 1091  0.4981  0.0117  0.0196  0.0137   70.0306         -1  
 1092  0.5023  0.0271  0.0228  0.0132   57.7037         -1  
 1093  0.5003  0.0082  0.0186  0.0146   78.0627         -1  
 1094  0.4972  0.0162  0.0186  0.0146   78.0627         -1  
 1095  0.4969  0.0125  0.0186  0.0146   78.0627         -1  
 1096  0.5021  0.0111  0.0179  0.0079   44.1194         -1  
 1097  0.5063  0.0124  0.0179  0.0079   44.1194         -1  
 1098  0.5025  0.0147  0.0179  0.0079   44.1194         -1  
 1099  0.4981  0.0142  0.0179  0.0079   44.1194         -1  
 1100  0.5032  0.0117  0.0398  0.0202   50.7949         -1  
 1101  0.5015  0.0148  0.0398  0.0202   50.7949         -1  
 1102  0.4979  0.0096  0.0798  0.0280   35.1003         -1  
 1103  0.5056  0.0115  0.0798  0.0280   35.1003         -1  
 1104  0.4990  0.0405  0.0182  0.0200  110.4384         -1  
 1105  0.4981  0.0105  0.0368  0.0177   48.2132         -1  
 1106  0.5031  0.0133  0.0260  0.0149   57.1964         -1  
 1107  0.4994  0.0147  0.0260  0.0149   57.1964         -1  
 1108  0.5021  0.0119  0.0631  0.0315   49.9274         -1  
 1109  0.4953  0.0131  0.0831  0.0198   23.8407         -1  
 1110  0.5063  0.0109  0.0831  0.0198   23.8407         -1  
 1111  0.4981  0.0221  0.0831  0.0198   23.8407         -1  
 1112  0.5012  0.0081  0.0257  0.0402  156.4628         -1  
 1113  0.5042  0.0105  0.0257  0.0402  156.4628         -1  
 1114  0.4997  0.0140  0.0169  0.0236  139.6209         -1  
 1115  0.5023  0.0173  0.0169  0.0236  139.6209         -1  
 1116  0.4965  0.0132  0.0154  0.0123   79.9402         -1  
 1117  0.5032  0.0160  0.0154  0.0123   79.9402         -1  
 1118  0.4996  0.0118  0.0154  0.0123   79.9402         -1  
 1119  0.5024  0.0105  0.0154  0.0123   79.9402         -1  
 1120  0.4966  0.0153  0.0088  0.0179  202.5901         -1  
 1121  0.4984  0.0153  0.0192  0.0503  262.6164         -1  
 1122  0.4963  0.0188  0.0247  0.0087   35.3988         -1  
 1123  0.4997  0.0134  0.0247  0.0087   35.3988         -1  
 1124  0.5016  0.0148  0.0472  0.0327   69.2994         -1  
 1125  0.4979  0.0120  0.0277  0.0047   16.9283         -1  
 1126  0.5023  0.0138  0.0258  0.0117   45.3908         -1  
 1127  0.5026  0.0142  0.0258  0.0117   45.3908         -1  
 1128  0.5023  0.0129  0.0258  0.0117   45.3908         -1  
 1129  0.5013  0.0152  0.0272  0.0171   62.7655         -1  
 1130  0.5048  0.0132  0.0272  0.0171   62.7655         -1  
 1131  0.5014  0.0146  0.0272  0.0171   62.7655         -1  
 1132  0.4960  0.0131  0.0256  0.0098   38.1727         -1  
 1133  0.4996  0.0140  0.0256  0.0098   38.1727         -1  
 1134  0.5029  0.0184  0.0189  0.0060   31.6209         -1  
 1135  0.5004  0.0108  0.0167  0.0182  108.9031         -1  
 1136  0.4978  0.0100  0.0392  0.0076   19.5115         -1  
 1137  0.4996  0.0187  0.0392  0.0076   19.5115         -1  
 1138  0.4981  0.0138  0.0392  0.0076   19.5115         -1  
 1139  0.5020  0.0109  0.0205  0.0172   83.5851         -1  
 1140  0.5055  0.0225  0.0205  0.0172   83.5851         -1  
 1141  0.5028  0.0256  0.0289  0.0096   33.3147         -1  
 1142  0.5042  0.0094  0.0289  0.0096   33.3147         -1  
 1143  0.5010  0.0098  0.0289  0.0096   33.3147         -1  
 1145  0.5006  0.0120  0.0275  0.0108   39.1032         -1  
 1146  0.5034  0.0180  0.0275  0.0108   39.1032         -1  
 1147  0.4999  0.0178  0.0275  0.0108   39.1032         -1  
 1148  0.4980  0.0107  0.0275  0.0108   39.1032         -1  
 1149  0.5052  0.0116  0.0275  0.0108   39.1032         -1  
 1150  0.5024  0.0157  0.0275  0.0108   39.1032         -1  
 1152  0.4990  0.0136  0.0354  0.0181   51.0695         -1  
 1153  0.4993  0.0133  0.0354  0.0181   51.0695         -1  
 1154  0.5041  0.0091  0.0354  0.0181   51.0695         -1  
 1155  0.5002  0.0148  0.0354  0.0181   51.0695         -1  
 1156  0.5040  0.0146  0.0354  0.0181   51.0695         -1  
 1157  0.5007  0.0150  0.0354  0.0181   51.0695         -1  
 1158  0.4992  0.0111  0.0354  0.0181   51.0695         -1  
 1159  0.5015  0.0137  0.0354  0.0181   51.0695         -1  
 1160  0.5004  0.0111  0.0354  0.0181   51.0695         -1  
 1161  0.5004  0.0131  0.0119  0.0089   74.3720         -1  
 1162  0.5039  0.0146  0.0050  0.0367  737.3048         -1  
 1163  0.5035  0.0133  0.0050  0.0367  737.3048         -1  
 1164  0.5018  0.0169  0.0123  0.0281  227.6623         -1  
 1165  0.5020  0.0095  0.0269  0.0207   77.2047         -1  
 1166  0.5029  0.0153  0.0269  0.0207   77.2047         -1  
 1167  0.4978  0.0135  0.0269  0.0207   77.2047         -1  
 1168  0.5033  0.0122  0.0230  0.0181   78.8778         -1  
 1169  0.4983  0.0115  0.0126  0.0115   91.6729         -1  
 1170  0.5033  0.0139  0.1028  0.0233   22.6231         -1  
 1171  0.5007  0.0192  0.1028  0.0233   22.6231         -1  
 1172  0.5067  0.0116  0.1028  0.0233   22.6231         -1  
 1173  0.5046  0.0107  0.1028  0.0233   22.6231         -1  
 1174  0.5060  0.0084  0.1028  0.0233   22.6231         -1  
 1175  0.5041  0.0159  0.0289  0.0032   10.9425         -1  
 1176  0.4996  0.0110  0.0289  0.0032   10.9425         -1  
 1177  0.5009  0.0151  0.0190  0.0150   78.6525         -1  
 1178  0.5027  0.0145  0.0190  0.0150   78.6525         -1  
 1179  0.4962  0.0171  0.0190  0.0150   78.6525         -1  
 1180  0.5006  0.0148  0.0190  0.0150   78.6525         -1  
 1181  0.5002  0.0113  0.0190  0.0150   78.6525         -1  
 1182  0.4979  0.0113  0.0233  0.0138   59.1201         -1  
 1183  0.4987  0.0106  0.0233  0.0138   59.1201         -1  
 1184  0.5028  0.0083  0.0233  0.0138   59.1201         -1  
 1186  0.5053  0.0159  0.0262  0.0104   39.5528         -1  
 1187  0.4982  0.0088  0.0262  0.0104   39.5528         -1  
 1188  0.5029  0.0093  0.0262  0.0104   39.5528         -1  
 1190  0.4996  0.0102  0.0262  0.0104   39.5528         -1  
 1191  0.5027  0.0106  0.0262  0.0104   39.5528         -1  
 1192  0.5069  0.0137  0.0262  0.0104   39.5528         -1  
 1193  0.4970  0.0076  0.0262  0.0104   39.5528         -1  
 1194  0.4964  0.0239  0.0262  0.0104   39.5528         -1  
 1195  0.5045  0.0136  0.0223  0.0105   47.0690         -1  
 1196  0.5056  0.0158  0.0223  0.0105   47.0690         -1  
 1197  0.4971  0.0105  0.0223  0.0105   47.0690         -1  
 1198  0.4974  0.0128  0.0223  0.0105   47.0690         -1  
 1199  0.4969  0.0134  0.0076  0.0099  130.0095         -1  
 1200  0.5019  0.0178  0.0076  0.0099  130.0095         -1  
 1201  0.5008  0.0103  0.0343  0.0140   40.6502         -1  
 1202  0.4968  0.0173  0.0194  0.0211  108.5721         -1  
 1203  0.5008  0.0112  0.0193  0.0091   47.0754         -1  
 1204  0.5014  0.0149  0.0193  0.0091   47.0754         -1  
 1205  0.5030  0.0108  0.0106  0.0178  166.8498         -1  
 1206  0.4989  0.0167  0.0106  0.0178  166.8498         -1  
 1207  0.5035  0.0180  0.0379  0.0155   40.8765         -1  
 1208  0.4994  0.0105  0.0264  0.0128   48.4774         -1  
 1209  0.4992  0.0158  0.0264  0.0128   48.4774         -1  
 1210  0.4999  0.0135  0.0264  0.0128   48.4774         -1  
 1212  0.4983  0.0113  0.0212  0.0087   41.0611         -1  
 1213  0.5063  0.0097  0.0195  0.0117   59.8462         -1  
 1214  0.5071  0.0088  0.0085  0.0064   74.6027         -1  
 1215  0.5017  0.0102  0.0085  0.0064   74.6027         -1  
 1216  0.5009  0.0167  0.0293  0.0045   15.3444         -1  
 1217  0.5077  0.0325  0.0293  0.0045   15.3444         -1  
 1218  0.5073  0.0220  0.0293  0.0045   15.3444         -1  
 1219  0.4979  0.0129  0.0109  0.0061   56.1631         -1  
 1220  0.5030  0.0113  0.0207  0.0112   54.1279         -1  
 1221  0.4989  0.0140  0.0207  0.0112   54.1279         -1  
 1222  0.5002  0.0133  0.0207  0.0112   54.1279         -1  
 1223  0.4949  0.0107  0.0207  0.0112   54.1279         -1  
 1224  0.4989  0.0136  0.0207  0.0112   54.1279         -1  
 1225  0.4992  0.0100  0.0207  0.0112   54.1279         -1  
 1226  0.4981  0.0166  0.0102  0.0133  130.0641         -1  
 1228  0.5004  0.0148  0.0102  0.0133  130.0641         -1  
 1229  0.4944  0.0181  0.0291  0.0102   35.0279         -1  
 1230  0.4948  0.0123  0.0291  0.0102   35.0279         -1  
 1231  0.5063  0.0113  0.0291  0.0102   35.0279         -1  
 1232  0.5033  0.0112  0.0291  0.0102   35.0279         -1  
 1233  0.4954  0.0136  0.0291  0.0102   35.0279         -1  
 1234  0.5058  0.0094  0.0053  0.0188  353.8319         -1  
 1235  0.5011  0.0133  0.0430  0.0283   65.7689         -1  
 1236  0.5005  0.0176  0.0144  0.0080   55.8468         -1  
 1237  0.5050  0.0229  0.0144  0.0080   55.8468         -1  
 1239  0.5019  0.0146  0.0193  0.0072   37.6251         -1  
 1240  0.5071  0.0108  0.0134  0.0104   77.5636         -1  
 1243  0.5009  0.0149  0.0283  0.0112   39.4516         -1  
 1244  0.5003  0.0128  0.0071  0.0170  239.9809         -1  
 1245  0.4980  0.0143  0.0352  0.0251   71.1973         -1  
 1246  0.4973  0.0160  0.0112  0.0121  107.7999         -1  
 1247  0.4991  0.0124  0.0305  0.0092   30.3421         -1  
 1248  0.4997  0.0183  0.0305  0.0092   30.3421         -1  
 1249  0.5006  0.0133  0.0230  0.0095   41.2178         -1  
 1250  0.5055  0.0104  0.0230  0.0095   41.2178         -1  
 1251  0.4940  0.0200  0.0243  0.0100   41.0782         -1  
 1252  0.5001  0.0146  0.0243  0.0100   41.0782         -1  
 1253  0.5007  0.0154  0.0243  0.0100   41.0782         -1  
 1255  0.5001  0.0117  0.0243  0.0100   41.0782         -1  
 1256  0.4928  0.0141  0.0223  0.0152   68.2176         -1  
 1257  0.5038  0.0145  0.0223  0.0152   68.2176         -1  
 1258  0.4991  0.0104  0.0223  0.0152   68.2176         -1  
 1259  0.4982  0.0118  0.0137  0.0326  237.4625         -1  
 1260  0.4968  0.0188  0.0137  0.0326  237.4625         -1  
 1261  0.5006  0.0279  0.0137  0.0326  237.4625         -1  
 1262  0.4949  0.0146  0.0137  0.0326  237.4625         -1  
 1263  0.5016  0.0240  0.0152  0.0077   50.5947         -1  
 1264  0.5019  0.0118 -0.0047  0.0134  286.1303         -1  
 1265  0.5044  0.0166 -0.0047  0.0134  286.1303         -1  
 1266  0.5026  0.0138 -0.0047  0.0134  286.1303         -1  
 1267  0.4985  0.0316 -0.0047  0.0134  286.1303         -1  
 1268  0.4999  0.0150  0.0184  0.0148   80.1759         -1  
 1269  0.4936  0.0218  0.0184  0.0148   80.1759         -1  
 1270  0.5029  0.0161  0.0184  0.0148   80.1759         -1  
 1271  0.5010  0.0202  0.0184  0.0148   80.1759         -1  
 1272  0.5037  0.0138  0.0184  0.0148   80.1759         -1  
 1273  0.4944  0.0142  0.0184  0.0148   80.1759         -1  
 1274  0.5029  0.0200  0.0184  0.0148   80.1759         -1  
 1275  0.4951  0.0285  0.0184  0.0148   80.1759         -1  
 1276  0.4963  0.0178  0.0184  0.0148   80.1759         -1  
 1277  0.5017  0.0169  0.0184  0.0148   80.1759         -1  
 1278  0.5057  0.0160  0.0184  0.0148   80.1759         -1  
 1279  0.5038  0.0107  0.0184  0.0148   80.1759         -1  
 1280  0.5050  0.0132  0.0184  0.0148   80.1759         -1  
 1281  0.4992  0.0144  0.0184  0.0148   80.1759         -1  
 1282  0.5055  0.0195  0.0184  0.0148   80.1759         -1  
 1283  0.5041  0.0282  0.0184  0.0148   80.1759         -1  
 1284  0.5031  0.0158  0.0184  0.0148   80.1759         -1  
 1285  0.4985  0.0190  0.0184  0.0148   80.1759         -1  
 1286  0.4979  0.0142  0.0184  0.0148   80.1759         -1  
 1287  0.4979  0.0169  0.0184  0.0148   80.1759         -1  
 1288  0.4978  0.0114  0.0184  0.0148   80.1759         -1  
 1289  0.5040  0.0160  0.0196  0.0049   25.2265         -1  
 1290  0.5006  0.0320  0.0196  0.0049   25.2265         -1  
 1291  0.4971  0.0093  0.0196  0.0049   25.2265         -1  
 1292  0.4953  0.0167  0.0332  0.0178   53.5775         -1  
 1293  0.5010  0.0079  0.0332  0.0178   53.5775         -1  
 1294  0.4933  0.0128  0.0239  0.0066   27.5232         -1  
 1295  0.5000  0.0103  0.0308  0.0183   59.3775         -1  
 1296  0.4997  0.0131  0.0308  0.0183   59.3775         -1  
 1297  0.5023  0.0196  0.0305  0.0286   93.8008         -1  
 1298  0.4971  0.0141  0.0305  0.0286   93.8008         -1  
 1299  0.5035  0.0116  0.0197  0.0139   70.4993         -1  
 1300  0.5018  0.0172  0.0197  0.0139   70.4993         -1  
 1301  0.4990  0.0117  0.0197  0.0139   70.4993         -1  
 1304  0.5019  0.0189  0.0144  0.0110   76.0785         -1  
 1305  0.5000  0.0155  0.0144  0.0110   76.0785         -1  
 1306  0.5021  0.0117  0.0144  0.0110   76.0785         -1  
 1307  0.5030  0.0102  0.0230  0.0064   27.7025         -1  
 1308  0.5005  0.0135  0.0230  0.0064   27.7025         -1  
 1309  0.5004  0.0163  0.0230  0.0064   27.7025         -1  
 1310  0.4996  0.0137  0.0259  0.0112   43.2861         -1  
 1311  0.4998  0.0108  0.0259  0.0112   43.2861         -1  
 1312  0.4990  0.0146  0.0259  0.0112   43.2861         -1  
 1313  0.5017  0.0254  0.0259  0.0112   43.2861         -1  
 1314  0.5002  0.0147  0.0098  0.0071   72.1094         -1  
 1315  0.5014  0.0155  0.0098  0.0071   72.1094         -1  
 1316  0.5029  0.0181  0.0196  0.0238  121.1691         -1  
 1317  0.4991  0.0134  0.0232  0.0129   55.7204         -1  
 1318  0.4986  0.0126  0.0382  0.0136   35.5800         -1  
 1319  0.4956  0.0134  0.0382  0.0136   35.5800         -1  
 1320  0.5052  0.0139  0.0065  0.0077  118.4201         -1  
 1321  0.4966  0.0124  0.0230  0.0210   91.4264         -1  
 1322  0.4998  0.0150  0.0230  0.0210   91.4264         -1  
 1323  0.5018  0.0127  0.0230  0.0210   91.4264         -1  
 1326  0.4979  0.0192  0.0230  0.0210   91.4264         -1  
 1330  0.4961  0.0143  0.0230  0.0210   91.4264         -1  
 1331  0.4967  0.0127  0.0230  0.0210   91.4264         -1  
 1332  0.4990  0.0137  0.0230  0.0210   91.4264         -1  
 1333  0.4963  0.0160  0.0230  0.0210   91.4264         -1  
 1334  0.5016  0.0135  0.0230  0.0210   91.4264         -1  
 1335  0.5005  0.0099  0.0230  0.0210   91.4264         -1  
 1336  0.4988  0.0185  0.0328  0.0192   58.5087         -1  
 1337  0.5005  0.0117  0.0239  0.0203   85.0589         -1  
 1338  0.4997  0.0129  0.0239  0.0203   85.0589         -1  
 1339  0.4993  0.0198  0.0346  0.0251   72.6469         -1  
 1340  0.4968  0.0164  0.0346  0.0251   72.6469         -1  
 1341  0.4990  0.0177  0.0346  0.0251   72.6469         -1  
 1344  0.5003  0.0153  0.0346  0.0251   72.6469         -1  
 1345  0.4969  0.0164  0.0498  0.0799  160.5042         -1  
 1346  0.4970  0.0124  0.0498  0.0799  160.5042         -1  
 1347  0.4997  0.0122  0.0097  0.0098  101.2092         -1  
 1348  0.5004  0.0107  0.0097  0.0098  101.2092         -1  
 1349  0.5031  0.0077  0.0097  0.0098  101.2092         -1  
 1350  0.5029  0.0133 -0.0060  0.0078  129.6218         -1  
 1351  0.4974  0.0116 -0.0060  0.0078  129.6218         -1  
 1352  0.4987  0.0114 -0.0060  0.0078  129.6218         -1  
 1353  0.5002  0.0106 -0.0060  0.0078  129.6218         -1  
 1354  0.5011  0.0185 -0.0060  0.0078  129.6218         -1  
 1355  0.4981  0.0198 -0.0060  0.0078  129.6218         -1  
 1356  0.4964  0.0139  0.0077  0.0149  193.4633         -1  
 1357  0.4968  0.0150  0.0077  0.0149  193.4633         -1  
 1358  0.4960  0.0123  0.0077  0.0149  193.4633         -1  
 1359  0.5009  0.0102  0.0077  0.0149  193.4633         -1  
 1360  0.5022  0.0112  0.0077  0.0149  193.4633         -1  
 1361  0.5025  0.0110  0.0077  0.0149  193.4633         -1  
 1362  0.5026  0.0085  0.0182  0.0077   42.5048         -1  
 1366  0.4996  0.0141 -0.0006  0.0118    0.0000         -1  
 1367  0.4985  0.0060  0.0331  0.0469  141.6245         -1  
 1368  0.5028  0.0136  0.0331  0.0469  141.6245         -1  
 1369  0.4998  0.0143  0.0331  0.0469  141.6245         -1  
 1370  0.4990  0.0131  0.0331  0.0469  141.6245         -1  
 1371  0.5018  0.0182  0.0236  0.0098   41.4871         -1  
 1372  0.4987  0.0212  0.0236  0.0098   41.4871         -1  
 1373  0.5002  0.0114  0.0236  0.0098   41.4871         -1  
 1374  0.4953  0.0161  0.0236  0.0098   41.4871         -1  
 1375  0.5017  0.0122  0.0236  0.0098   41.4871         -1  
 1376  0.4983  0.0151  0.0236  0.0098   41.4871         -1  
 1377  0.4962  0.0109  0.0261  0.0063   24.0179         -1  
 1378  0.4953  0.0148  0.0025  0.0260    0.0000         -1  
 1379  0.4994  0.0180  0.0025  0.0260    0.0000         -1  
 1380  0.4988  0.0126  0.0203  0.0134   65.9827         -1  
 1381  0.4986  0.0134  0.0295  0.0154   52.2049         -1  
 1382  0.5006  0.0186  0.0295  0.0154   52.2049         -1  
 1383  0.5009  0.0237  0.0295  0.0154   52.2049         -1  
 1384  0.5013  0.0146  0.0295  0.0154   52.2049         -1  
 1385  0.4982  0.0079  0.0295  0.0154   52.2049         -1  
 1386  0.4963  0.0104  0.0295  0.0154   52.2049         -1  
 1387  0.5008  0.0319  0.0295  0.0154   52.2049         -1  
 1388  0.4981  0.0230  0.0295  0.0154   52.2049         -1  
 1389  0.5003  0.0115  0.0295  0.0154   52.2049         -1  
 1390  0.4996  0.0163  0.0134  0.0134  100.1445         -1  
 1391  0.5000  0.0154  0.0095  0.0114  119.8239         -1  
 1392  0.4993  0.0101  0.0042  0.0159  378.7581         -1  
 1393  0.4999  0.0142  0.0366  0.0487  132.9913         -1  
 1394  0.5023  0.0092  0.0366  0.0487  132.9913         -1  
 1395  0.4993  0.0122  0.0106  0.0075   71.0842         -1  
 1396  0.4973  0.0146  0.0106  0.0075   71.0842         -1  
 1397  0.5012  0.0126  0.0106  0.0075   71.0842         -1  
 1398  0.5027  0.0157  0.0211  0.0393  186.4769         -1  
 1399  0.5002  0.0108  0.0268  0.0110   41.1335         -1  
 1401  0.5010  0.0127  0.0111  0.0215  193.8286         -1  
 1402  0.4987  0.0183  0.0458  0.0277   60.5430         -1  
 1403  0.5046  0.0125  0.0458  0.0277   60.5430         -1  
 1404  0.4954  0.0106  0.0458  0.0277   60.5430         -1  
 1405  0.4958  0.0181  0.0458  0.0277   60.5430         -1  
 1406  0.5003  0.0159  0.0458  0.0277   60.5430         -1  
 1407  0.4963  0.0138  0.0458  0.0277   60.5430         -1  
 1408  0.5022  0.0257  0.0299  0.0216   72.0230         -1  
 1409  0.4964  0.0166  0.0299  0.0216   72.0230         -1  
 1410  0.4968  0.0171  0.0299  0.0216   72.0230         -1  
 1411  0.4988  0.0146  0.0156  0.0405  259.2834         -1  
 1412  0.4985  0.0169  0.0378  0.0651  171.9936         -1  
 1413  0.5019  0.0135  0.0378  0.0651  171.9936         -1  
 1414  0.5035  0.0207  0.0122  0.0080   65.4842         -1  
 1415  0.4971  0.0097  0.0122  0.0080   65.4842         -1  
 1416  0.4959  0.0099  0.0149  0.0160  107.2427         -1  
 1417  0.5002  0.0135  0.0149  0.0160  107.2427         -1  
 1418  0.4996  0.0129  0.0191  0.0097   50.6198         -1  
 1419  0.5019  0.0140  0.0092  0.0393  427.4732         -1  
 1420  0.4994  0.0115  0.0299  0.0071   23.6431         -1  
 1421  0.4983  0.0157  0.0299  0.0071   23.6431         -1  
 1422  0.5008  0.0167  0.0232  0.0119   51.4726         -1  
 1423  0.5020  0.0142  0.0232  0.0119   51.4726         -1  
 1424  0.4977  0.0151  0.0232  0.0119   51.4726         -1  
 1425  0.4912  0.0127  0.0166  0.0122   73.6335         -1  
 1426  0.4939  0.0163  0.0166  0.0122   73.6335         -1  
 1427  0.4966  0.0084  0.0166  0.0122   73.6335         -1  
 1428  0.5009  0.0135  0.0178  0.0305  171.3183         -1  
 1429  0.5008  0.0137  0.0048  0.0178  368.8936         -1  
 1430  0.5018  0.0162  0.0048  0.0178  368.8936         -1  
 1431  0.5026  0.0158  0.0091  0.0180  197.1279         -1  
 1432  0.5011  0.0125  0.0157  0.0190  120.6365         -1  
 1433  0.4986  0.0120  0.0157  0.0190  120.6365         -1  
 1434  0.5014  0.0183  0.0157  0.0190  120.6365         -1  
 1435  0.5007  0.0213  0.0157  0.0190  120.6365         -1  
 1436  0.4957  0.0156  0.0178  0.0120   67.7994         -1  
 1437  0.4967  0.0115  0.0178  0.0120   67.7994         -1  
 1439  0.4997  0.0167  0.0059  0.0148  252.0031         -1  
 1440  0.5014  0.0198  0.0059  0.0148  252.0031         -1  
 1441  0.4968  0.0132  0.0059  0.0148  252.0031         -1  
 1442  0.4983  0.0095  0.0209  0.0141   67.2425         -1  
 1444  0.5020  0.0117  0.0274  0.0121   44.0961         -1  
 1445  0.4958  0.0139  0.0274  0.0121   44.0961         -1  
 1446  0.5008  0.0153  0.0250  0.0088   35.1564         -1  
 1447  0.5031  0.0111  0.0250  0.0088   35.1564         -1  
 1448  0.5048  0.0162  0.0250  0.0088   35.1564         -1  
 1449  0.4965  0.0233  0.0250  0.0088   35.1564         -1  
 1450  0.5095  0.0104  0.0250  0.0088   35.1564         -1  
 1451  0.5008  0.0122  0.0132  0.0233  176.4448         -1  
 1452  0.4983  0.0127  0.0152  0.0059   38.9527         -1  
 1453  0.4948  0.0177  0.0152  0.0059   38.9527         -1  
 1454  0.5003  0.0161  0.0232  0.0134   57.7031         -1  
 1455  0.4922  0.0143  0.0251  0.0113   45.0140         -1  
 1456  0.5045  0.0108 -0.0031  0.0168  545.6838         -1  
 1457  0.5021  0.0139 -0.0031  0.0168  545.6838         -1  
 1458  0.4997  0.0142 -0.0031  0.0168  545.6838         -1  
 1459  0.4974  0.0111 -0.0031  0.0168  545.6838         -1  
 1460  0.4973  0.0112  0.0169  0.0157   93.0007         -1  
 1461  0.5000  0.0099  0.0169  0.0157   93.0007         -1  
 1462  0.4966  0.0200  0.0195  0.0701  358.7989         -1  
 1463  0.4980  0.0163  0.0308  0.0412  133.4494         -1  
 1464  0.5004  0.0119  0.0227  0.0284  125.1290         -1  
 1465  0.4986  0.0172  0.0117  0.0373  318.0075         -1  
 1466  0.5020  0.0155  0.0117  0.0373  318.0075         -1  
 1467  0.4993  0.0147  0.0117  0.0373  318.0075         -1  
 1468  0.4972  0.0133  0.0167  0.0310  185.3578         -1  
 1469  0.5036  0.0161  0.0167  0.0310  185.3578         -1  
 1470  0.4998  0.0234  0.0167  0.0310  185.3578         -1  
 1471  0.4986  0.0132  0.0172  0.0222  128.6840         -1  
 1472  0.5008  0.0140  0.0246  0.0145   58.7670         -1  
 1473  0.5030  0.0150  0.0057  0.0109  192.8017         -1  
 1474  0.4979  0.0168  0.0057  0.0328  579.1817         -1  
 1475  0.5005  0.0171  0.0057  0.0328  579.1817         -1  
 1476  0.5024  0.0132  0.0057  0.0328  579.1817         -1  
 1477  0.5008  0.0140  0.0057  0.0328  579.1817         -1  
 1478  0.4954  0.0199  0.0148  0.0197  133.4114         -1  
 1479  0.5051  0.0101  0.0200  0.0169   84.6772         -1  
 1480  0.5027  0.0172  0.0196  0.0165   84.1261         -1  
 1481  0.4977  0.0266  0.0171  0.0194  113.5506         -1  
 1482  0.5033  0.0160  0.0171  0.0194  113.5506         -1  
 1483  0.5030  0.0229 -0.0169  0.0148   87.8627         -1  
 1484  0.4965  0.0123  0.0354  0.0266   75.1897         -1  
 1485  0.4985  0.0153  0.0104  0.0185  178.0405         -1  
 1486  0.4994  0.0118  0.0191  0.0123   64.6789         -1  
 1487  0.4998  0.0154  0.0055  0.0205  372.2183         -1  
 1488  0.4998  0.0161  0.0055  0.0205  372.2183         -1  
 1489  0.4963  0.0191  0.0371  0.0125   33.6823         -1  
 1490  0.4987  0.0135  0.0371  0.0125   33.6823         -1  
 1491  0.4991  0.0125  0.0239  0.0104   43.5641         -1  
 1492  0.5007  0.0148  0.0049  0.0123  248.6235         -1  
 1493  0.5045  0.0181  0.0049  0.0123  248.6235         -1  
 1494  0.5029  0.0111  0.0068  0.0280  414.4256         -1  
 1495  0.5045  0.0249  0.0138  0.0162  117.7603         -1  
 1496  0.4950  0.0115  0.0138  0.0162  117.7603         -1  
 1497  0.4985  0.0132  0.0138  0.0162  117.7603         -1  
 1498  0.5016  0.0112  0.0437  0.0596  136.2608         -1  
 1499  0.4985  0.0138  0.0148  0.0091   61.4005         -1  
 1500  0.4985  0.0096  0.0141  0.0250  176.9394         -1  
 1501  0.5015  0.0241  0.0236  0.0113   47.8577         -1  
 1502  0.5041  0.0243  0.0244  0.0319  130.7654         -1  
 1503  0.5028  0.0112  0.0243  0.0249  102.4834         -1  
 1504  0.5006  0.0110  0.0215  0.0175   81.2810         -1  
 1505  0.4995  0.0178  0.0104  0.0127  122.5713         -1  
 1506  0.4997  0.0136  0.0200  0.0164   82.0875         -1  
 1507  0.5037  0.0145  0.0201  0.0106   52.6841         -1  
 1508  0.5067  0.0471  0.0311  0.0317  101.9627         -1  
 1509  0.5051  0.0137  0.0311  0.0317  101.9627         -1  
 1510  0.4976  0.0160  0.0350  0.0117   33.4695         -1  
 1511  0.5019  0.0126  0.0156  0.0101   64.4686         -1  
 1512  0.4965  0.0173  0.0138  0.0173  125.3032         -1  
 1513  0.5012  0.0166  0.0197  0.0068   34.6093         -1  
 1514  0.4966  0.0134  0.0295  0.0180   61.0329         -1  
 1515  0.5029  0.0185  0.0263  0.0070   26.5137         -1  
 1516  0.4992  0.0117  0.0146  0.0090   61.5936         -1  
 1517  0.4992  0.0177  0.0260  0.0136   52.1827         -1  
 1518  0.4998  0.0243  0.0298  0.0208   69.9837         -1  
 1520  0.5011  0.0151  0.0115  0.0142  122.9917         -1  
 1521  0.5007  0.0135  0.0115  0.0142  122.9917         -1  
 1522  0.5006  0.0196  0.0128  0.0113   88.1365         -1  
 1523  0.4995  0.0127  0.0091  0.0169  185.8369         -1  
 1524  0.5037  0.0252  0.0091  0.0169  185.8369         -1  
 1525  0.5003  0.0165  0.0363  0.0135   37.2277         -1  
 1526  0.4982  0.0143  0.0363  0.0135   37.2277         -1  
 1527  0.4991  0.0115  0.0153  0.0230  149.8127         -1  
 1528  0.4968  0.0189  0.0139  0.0097   70.2425         -1  
 1529  0.4964  0.0193  0.0139  0.0097   70.2425         -1  
 1530  0.5008  0.0108  0.0101  0.0140  138.3371         -1  
 1531  0.5000  0.0169  0.0168  0.0349  207.2955         -1  
 1532  0.5007  0.0247  0.0168  0.0349  207.2955         -1  
 1533  0.4995  0.0195  0.0168  0.0349  207.2955         -1  
 1534  0.5013  0.0158  0.0080  0.0272  337.9120         -1  
 1535  0.4972  0.0145  0.0199  0.0097   48.7045         -1  
 1536  0.5023  0.0148  0.0199  0.0097   48.7045         -1  
 1537  0.4942  0.0175  0.0199  0.0097   48.7045         -1  
 1538  0.4977  0.0144  0.0237  0.0112   47.3376         -1  
 1539  0.4987  0.0118  0.0237  0.0112   47.3376         -1  
 1540  0.5011  0.0163  0.0181  0.0123   67.6676         -1  
 1541  0.5021  0.0103  0.0266  0.0063   23.5979         -1  
 1542  0.4992  0.0136  0.0216  0.0263  121.9426         -1  
 1543  0.4975  0.0109  0.0273  0.0139   50.8827         -1  
 1544  0.4986  0.0192  0.0172  0.0126   72.9676         -1  
 1545  0.4951  0.0165  0.0063  0.0252  402.6874         -1  
 1546  0.5022  0.0249  0.0134  0.0142  105.7142         -1  
 1547  0.5012  0.0160  0.0222  0.0080   36.2975         -1  
 1548  0.4944  0.0171  0.0253  0.0224   88.5812         -1  
 1549  0.5037  0.0117  0.0253  0.0224   88.5812         -1  
 1550  0.5034  0.0178  0.0236  0.0065   27.5514         -1  
 1551  0.4972  0.0157  0.0400  0.0123   30.7574         -1  
 1552  0.5031  0.0111  0.0085  0.0212  247.6285         -1  
 1553  0.4985  0.0126  0.0174  0.0234  134.3983         -1  
 1554  0.5025  0.0138  0.0235  0.0275  117.0945         -1  
 1555  0.5002  0.0130  0.0086  0.0160  184.8703         -1  
 1556  0.5019  0.0158  0.0120  0.0055   46.1076         -1  
 1557  0.5010  0.0132  0.0277  0.0074   26.7330         -1  
 1558  0.4993  0.0130  0.0097  0.0172  176.6783         -1  
 1559  0.4960  0.0157  0.0150  0.0176  117.4564         -1  
 1560  0.5009  0.0155  0.0105  0.0133  127.3154         -1  
 1561  0.4965  0.0118  0.0320  0.0148   46.4573         -1  
 1562  0.4988  0.0143  0.0068  0.0138  203.1720         -1  
 1563  0.4975  0.0131  0.0068  0.0138  203.1720         -1  
 1564  0.4987  0.0153  0.0197  0.0086   43.5231         -1  
 1565  0.5004  0.0178  0.0262  0.0245   93.4941         -1  
 1566  0.4987  0.0181  0.0117  0.0162  137.7844         -1  
 
 [1463 rows x 202 columns],
             0         1          2          3       4         6       7  \
 2     2932.61  2559.940  2186.4111  1698.0172  1.5102   95.4878  0.1241   
 10    2994.05  2548.210  2195.1222  1046.1468  1.3204  103.3400  0.1223   
 11    2928.84  2479.400  2196.2111  1605.7578  0.9959   97.9156  0.1257   
 14    2963.97  2629.480  2224.6222   947.7739  1.2924  104.8489  0.1197   
 23    2884.74  2514.540  2160.3667   899.9488  1.4022  105.4978  0.1240   
 38    2958.09  2542.240  2222.6778  1547.6125  1.4431  110.5644  0.1211   
 40    2962.14  2545.710  2221.5778  1503.6230  1.1878  111.3444  0.1211   
 45    2912.87  2446.250  2166.5222   907.0746  1.0647  104.5211  0.1221   
 48    2993.59  2345.950  2169.4667  1185.4449  1.2412  100.8444  0.1221   
 49    2946.86  2533.910  2174.8666  1039.2291  1.0455  103.6000  0.1221   
 50    2942.31  2446.740  2172.9778  1222.6067  1.3658  101.8400  0.1220   
 57    2935.94  2586.050  2164.4111  1206.6031  0.9799  100.5189  0.1220   
 58    3004.09  2388.740  2223.8000  1503.1248  1.1705  110.0600  0.1211   
 62    2990.22  2483.660  2206.5112  1244.1552  1.2691  101.6667  0.1229   
 64    2980.84  2628.760  2187.5222  1268.6598  1.4503  102.4622  0.1233   
 82    2971.99  2502.620  2239.3000  1192.7495  1.2479  100.1189  0.1187   
 96    2992.40  2467.070  2191.6667  1107.4330  1.3529  103.4233  0.1206   
 115   3002.85  2502.050  2232.5889  1717.2750  1.6700  104.1067  0.1223   
 131   3003.13  2714.300  2171.9000  1811.8799  1.3811   99.2200  0.1276   
 154   2973.86  2359.010  2196.6555  1066.1908  1.2188  101.8900  0.1211   
 157   2959.86  2491.190  2208.0000  1835.9832  1.5714  100.2478  0.1238   
 158   2964.77  2524.440  2181.5111  1177.0830  1.3012  100.9333  0.1215   
 167   2889.26  2529.160  2184.8778   960.8486  1.0160  102.5333  0.1214   
 169   3041.56  2508.560  2184.8778   960.8486  1.0160  102.5333  0.1214   
 180   3058.89  2504.380  2221.9444  1551.6947  1.5296   99.2678  0.1222   
 182   2928.03  2497.030  2221.9444  1551.6947  1.5296   99.2678  0.1222   
 186   2936.64  2509.650  2221.9444  1551.6947  1.5296   99.2678  0.1222   
 188   3074.69  2391.710  2221.9444  1551.6947  1.5296   99.2678  0.1222   
 189   2938.03  2480.900  2138.8778  1046.6043  1.2559  103.3400  0.1240   
 218   3038.21  2521.840  2273.7556  1549.8407  1.4105  105.0467  0.1171   
 222   3037.49  2463.110  2205.2889  1630.3112  1.2733   98.8056  0.1218   
 231   2940.65  2499.405  2214.0556  1150.7775  1.3772  102.9389  0.1205   
 235   3045.71  2490.250  2197.6444  1247.0334  0.7865   99.9211  0.1203   
 236   3035.98  2458.150  2230.0333  1668.6804  1.5739   99.0522  0.1204   
 238   2984.45  2444.090  2230.0333  1668.6804  1.5739   99.0522  0.1204   
 240   2900.69  2483.060  2191.1111  1564.6023  0.9366  102.5100  0.1234   
 241   2916.02  2467.490  2185.9333  1659.6962  1.6290   98.6822  0.1227   
 243   2963.75  2593.630  2204.9223  1787.6757  1.5138  100.4322  0.1216   
 244   3107.79  2470.810  2280.8222  1125.7334  0.6815  101.9111  0.1221   
 273   3075.78  2575.680  2243.7778  1502.9221  1.8160  102.0978  0.1195   
 277   3054.18  2408.460  2178.7333  1039.3641  0.7367  101.4922  0.1219   
 282   3008.84  2522.900  2177.3222  1089.3655  1.3101  101.1478  0.1216   
 291   2983.31  2672.940  2244.1111  1676.7316  0.9197  100.8067  0.1204   
 294   2997.92  2509.240  2201.5777   976.4791  0.7679   99.9956  0.1202   
 321   2936.59  2526.220  2196.6889  1593.1220  1.5925   99.1133  0.1226   
 323   2975.92  2286.250  2238.5444  1659.1424  0.9010   99.3100  0.1204   
 326   2973.53  2499.720  2177.3222  1089.3655  1.3101  101.1478  0.1216   
 327   2954.36  2559.270  2238.5444  1659.1424  0.9010   99.3100  0.1204   
 336   2887.33  2551.170  2201.5777   976.4791  0.7679   99.9956  0.1202   
 344   3098.45  2439.820  2197.6778  1056.7817  1.3168  102.9611  0.1203   
 351   2950.18  2517.340  2196.6889  1593.1220  1.5925   99.1133  0.1226   
 368   2936.04  2574.320  2186.9889   949.2201  1.2981  103.3322  0.1219   
 373   3118.63  2478.880  2170.5444   921.0605  1.4390  104.5300  0.1226   
 392   3013.98  2488.760  2167.5000  1068.4521  1.3719  102.1733  0.1223   
 406   2917.09  2518.960  2213.7556  1113.5599  0.7217  104.1667  0.1211   
 424   2977.43  2297.300  2218.0555  1517.4371  0.8579  105.8133  0.1206   
 441   3102.59  2514.950  2196.0889  1277.8592  1.8246   95.6322  0.1224   
 448   2942.41  2523.710  2207.0444  1269.6078  1.7571   97.0189  0.1221   
 495   2973.55  2536.480  2235.0556  1302.6607  1.6347  109.9856  0.1230   
 508   3003.82  2527.250  2163.5889  1448.3869  1.7014  104.8333  0.1256   
 518   3061.22  2384.190  2170.9667  1600.3858  1.0430  104.9756  0.1249   
 576   2990.72  2425.460  2155.6333  1070.0439  0.8024  101.4333  0.1241   
 583   2949.82  2497.560  2173.4556  1433.6732  1.0304  110.5422  0.1245   
 601   3040.10  2369.950  2201.8222  1288.0857  1.6769   95.9789  0.1209   
 605   2943.90  2436.650  2173.4556  1433.6732  1.0304  110.5422  0.1245   
 634   2990.76  2449.250  2172.9667  1058.2061  0.8433  104.7189  0.1232   
 709   3026.32  2485.810  2205.7222   906.9522  1.3443  105.6600  0.1200   
 795   3069.31  2448.370  2174.7555  1206.3506  1.4202  104.2622  0.1246   
 797   2896.84  2512.610  2190.1555  1298.8207  1.3947  105.7411  0.1242   
 826   3045.05  2443.100  2170.0666  1364.5157  1.5447   96.7700  0.1230   
 831   2961.04  2506.430  2170.0666  1364.5157  1.5447   96.7700  0.1230   
 871   3212.46  2522.410  2200.2333  1173.8377  1.3281  101.6111  0.1211   
 914   2977.29  2471.500  2214.7889  1687.4606  2.2073   97.3378  0.1226   
 924   3091.76  2391.560  2315.2667  2360.1325  1.1259   90.1144  0.1160   
 926   2982.87  2477.010  2315.2667  2360.1325  1.1259   90.1144  0.1160   
 929   2914.86  2465.110  2210.2778  2120.5760  1.0700   95.1089  0.1230   
 1029  3017.86  2584.790  2169.8334  1301.9348  0.8012  101.9733  0.1216   
 1062  3244.74  2422.000  2208.5222  1838.7054  1.1571   95.2056  0.1249   
 1144  2882.22  2518.890  2254.8667  2347.9092  1.2986   87.8044  0.1205   
 1151  2965.73  2539.500  2228.9445  1502.7821  1.2895  100.6867  0.1235   
 1185  2904.26  2500.850  2231.9555  1943.0435  1.2758   96.8789  0.1224   
 1189  3200.74  2534.160  2244.9778  2208.4483  1.9074   87.2789  0.1217   
 1211  3037.63  2524.130  2194.9555  1108.2246  1.2476  102.2822  0.1202   
 1227  3057.45  2457.420  2173.4889  1145.7970  0.9402  104.0556  0.1247   
 1238  3060.00  2571.410  2199.6556  1140.3983  1.3369  103.0967  0.1227   
 1241  2871.62  2530.110  2175.2556  1022.1660  1.2833  100.6222  0.1250   
 1242  3181.59  2512.770  2171.8556  1156.6018  1.4025  100.1367  0.1243   
 1254  2988.41  2598.770  2173.2778  1116.2950  0.8525  103.8200  0.1237   
 1302  3054.34  2473.140  2162.1333   998.9095  0.8826  104.9722  0.1246   
 1303  2848.46  2588.740  2164.3666   958.7313  1.3409  104.1344  0.1248   
 1324  3224.10  2410.530  2208.9778  1285.2144  2.4210   97.8400  0.1238   
 1325  3193.53  2587.860  2162.1333   998.9095  0.8826  104.9722  0.1246   
 1327  3188.42  2565.930  2208.9000   934.7558  1.9469  119.3544  0.1222   
 1328  3177.96  2369.840  2162.1333   998.9095  0.8826  104.9722  0.1246   
 1329  3266.55  2425.300  2211.4000  1511.7842  1.3004   97.4700  0.1237   
 1342  3034.50  2406.110  2183.5889  1108.6491  1.3963  104.0856  0.1233   
 1343  3058.46  2467.730  2228.4778  1721.1108  1.4301   93.6222  0.1221   
 1363  3016.46  2559.420  2167.0889  1253.2140  1.3679  101.6667  0.1243   
 1364  3163.86  2470.600  2211.4000  1511.7842  1.3004   97.4700  0.1237   
 1365  2988.39  2493.720  2206.4000   982.5452  1.1853  116.8167  0.1228   
 1400  3052.98  2515.510  2172.8111   969.3436  1.2736  102.7367  0.1243   
 1438  2951.84  2477.130  2192.1889  1435.9611  2.3870  107.3989  0.1229   
 1443  3173.18  2428.640  2209.4667  1556.3930  1.4884   95.1156  0.1206   
 1519  2903.34  2585.480  2196.1111  1472.6400  1.5599   94.6522  0.1212   
 
            8       9      10  ...      565       570     571       572  \
 2     1.4436  0.0041  0.0013  ...  0.62190  535.0245  2.0293   11.2100   
 10    1.5144 -0.0190  0.0013  ...  0.12240  532.1764  1.8715    9.5699   
 11    1.4690  0.0170 -0.0154  ...  0.08560  533.7464  2.1865    7.7400   
 14    1.4474  0.0144 -0.0119  ...  0.11955  532.6446  2.2808   11.4200   
 23    1.5585 -0.0317 -0.0138  ...  0.11955  536.0054  1.9902    8.7600   
 38    1.4780 -0.0131  0.0079  ...  0.11955  533.9509  1.8946    7.0300   
 40    1.5424 -0.0177  0.0140  ...  0.17180  537.6273  2.2025   12.2100   
 45    1.5336 -0.0283  0.0050  ...  0.12830  528.4727  2.1628    8.6700   
 48    1.5533  0.0173  0.0067  ...  0.13140  530.9973  2.1898    7.2300   
 49    1.5516 -0.0343  0.0065  ...  0.11955  533.3209  2.0680    6.4300   
 50    1.5717 -0.0105  0.0052  ...  0.11955  533.1055  2.3497    6.5900   
 57    1.4786  0.0049  0.0116  ...  0.07640  535.2509  2.2864    9.3200   
 58    1.4441 -0.0177  0.0263  ...  0.15630  533.7473  2.2437    8.2200   
 62    1.4729  0.0206  0.0030  ...  0.13870  535.8027  1.9736    7.3900   
 64    1.4672 -0.0088  0.0071  ...  0.06420  532.2164  2.3626    9.7599   
 82    1.5824 -0.0006  0.0064  ...  0.21770  531.8100  2.2115   13.7600   
 96    1.4993 -0.0091  0.0006  ...  0.04890  533.9136  2.4527    9.2700   
 115   1.4518  0.0066  0.0151  ...  0.11955  532.1818  2.2788   11.6000   
 131   1.4240 -0.0080  0.0074  ...  0.11450  527.1709  2.3873   11.2100   
 154   1.4544 -0.0052 -0.0003  ...  0.05430  535.0655  2.2935    8.1000   
 157   1.4680  0.0033  0.0018  ...  0.25000  535.4327  2.3964   10.4500   
 158   1.4047 -0.0080  0.0038  ...  0.06910  536.0109  2.2047    6.1100   
 167   1.5285 -0.0073  0.0003  ...  0.14530  531.7309  2.3944    9.8400   
 169   1.5473 -0.0128  0.0032  ...  0.11955  530.9354  2.5619    7.6900   
 180   1.4068  0.0057  0.0033  ...  0.15310  536.3600  2.2707   10.1200   
 182   1.4494 -0.0013  0.0042  ...  0.18120  536.0336  2.2646    7.3701   
 186   1.5100 -0.0174  0.0080  ...  0.22840  532.0545  2.5380    9.3800   
 188   1.4746 -0.0110  0.0285  ...  0.16740  532.4191  2.3255   12.4100   
 189   1.4701 -0.0056  0.0107  ...  0.18320  535.6864  1.9080   10.2500   
 218   1.3666 -0.0154  0.0047  ...  0.08360  531.9191  1.7570    8.4500   
 222   1.3605  0.0125 -0.0034  ...  0.11955  533.8327  1.5604    9.2800   
 231   1.4978  0.0221 -0.0055  ...  0.11900  529.0654  2.4848    6.8900   
 235   1.4257 -0.0343 -0.0016  ...  0.11955  530.7709  2.1968    5.0300   
 236   1.3720  0.0005 -0.0181  ...  0.22310  538.0918  2.1799    6.2700   
 238   1.3560 -0.0055 -0.0225  ...  0.19590  536.3273  2.2994    5.8400   
 240   1.5192 -0.0168  0.0104  ...  0.16760  535.9227  1.9110    8.6500   
 241   1.4114 -0.0005 -0.0088  ...  0.16770  535.7882  2.2067    7.6500   
 243   1.5105 -0.0009  0.0134  ...  0.22500  532.2127  1.5093   10.5300   
 244   1.4454 -0.0152 -0.0087  ...  0.10150  533.1445  1.7220    8.1600   
 273   1.4362  0.0062  0.0073  ...  0.11955  531.3791  2.4080    8.0200   
 277   1.4500 -0.0089  0.0004  ...  0.19610  531.8655  2.3718    9.7900   
 282   1.4833  0.0138  0.0012  ...  0.11955  531.2245  2.4840    7.0700   
 291   1.5312  0.0133 -0.0037  ...  0.11955  533.7727  1.7872    6.9800   
 294   1.4932 -0.0012  0.0028  ...  0.11955  535.8300  1.6951    5.3200   
 321   1.4934 -0.0074 -0.0151  ...  0.11955  317.1964  2.5047  286.8200   
 323   1.4610  0.0185  0.0058  ...  0.11955  531.2809  2.3875   10.3200   
 326   1.3955 -0.0259  0.0003  ...  0.11955  534.1736  1.7757    9.3000   
 327   1.4086 -0.0087 -0.0014  ...  0.11955  531.9845  1.8554   10.8900   
 336   1.3901 -0.0010  0.0060  ...  0.21410  533.8264  2.3352    8.4200   
 344   1.5057 -0.0213  0.0039  ...  0.11955  531.1509  2.3937   10.4400   
 351   1.4454 -0.0125 -0.0162  ...  0.15340  529.5736  2.3868   11.5400   
 368   1.5044  0.0038  0.0137  ...  0.35190  532.2545  2.4656   12.0500   
 373   1.4385 -0.0029  0.0099  ...  0.08770  508.2764  2.0687  272.4100   
 392   1.5523 -0.0110  0.0096  ...  0.08770  532.7000  2.3601   10.4600   
 406   1.4628  0.0163 -0.0264  ...  0.11955  537.6618  2.2127    8.3700   
 424   1.4849 -0.0020  0.0082  ...  0.08770  530.6027  2.3970    7.1100   
 441   1.4727 -0.0005 -0.0076  ...  0.11955  532.1854  2.2439    9.1100   
 448   1.5272  0.0181  0.0022  ...  0.23110  534.8673  2.1953    8.7300   
 495   1.4969 -0.0098 -0.0134  ...  0.13710  534.4918  2.3933    8.1800   
 508   1.5240 -0.0016  0.0061  ...  0.02520  538.8482  2.2732    7.0900   
 518   1.4867 -0.0091  0.0005  ...  0.11955  537.0373  2.0049    9.1400   
 576   1.4756  0.0125 -0.0096  ...  0.14100  534.7200  2.3705    7.0400   
 583   1.4031  0.0027  0.0017  ...  0.08260  535.8155  2.3326    7.5800   
 601   1.4542 -0.0002 -0.0007  ...  0.11955  529.7309  2.2346    7.5200   
 605   1.4454  0.0046  0.0101  ...  0.12820  532.2382  1.7897   10.5100   
 634   1.3421 -0.0283  0.0084  ...  0.11890  535.2009  1.6920    8.9000   
 709   1.4943  0.0279  0.0121  ...  0.11955  529.3191  2.0905   11.2500   
 795   1.4372 -0.0047 -0.0148  ...  0.15340  533.3382  2.4033    9.4300   
 797   1.4228 -0.0011 -0.0002  ...  0.08770  533.3382  2.4033    9.4300   
 826   1.4037  0.0013  0.0025  ...  0.08770  535.2482  1.8813    6.8400   
 831   1.3953  0.0084  0.0062  ...  0.08770  535.9527  2.2638    9.6200   
 871   1.4650  0.0035  0.0053  ...  0.18990  534.6682  2.0435    9.4300   
 914   1.5071  0.0045 -0.0011  ...  0.07730  531.8455  2.2544    8.6400   
 924   1.6107  0.0250  0.0125  ...  0.08770  531.8455  2.2544    8.6400   
 926   1.4695  0.0071  0.0215  ...  0.08770  531.8455  2.2544    8.6400   
 929   1.5817  0.0118  0.0000  ...  0.08770  531.8455  2.2544    8.6400   
 1029  1.5374 -0.0343  0.0101  ...  0.08770  531.8455  2.2544    8.6400   
 1062  1.5575  0.0049 -0.0207  ...  0.08770  532.0109  2.0679   11.3500   
 1144  1.4600  0.0029  0.0133  ...  0.36930  524.4955  1.0912   11.1200   
 1151  1.4797 -0.0047 -0.0056  ...  0.08770  530.2409  1.9614   11.5400   
 1185  1.6105 -0.0184  0.0023  ...  0.23550  533.9936  2.2318    8.2700   
 1189  1.4304  0.0214  0.0105  ...  0.29420  529.6464  2.0320    5.8100   
 1211  1.3969 -0.0074 -0.0152  ...  0.11955  533.9936  2.2318    8.2700   
 1227  1.4827  0.0047  0.0145  ...  0.10030  533.1809  1.0744    5.6500   
 1238  1.4300  0.0129 -0.0088  ...  0.32020  524.4955  1.0912   11.1200   
 1241  1.4679 -0.0033 -0.0022  ...  0.08770  529.9445  1.9740    7.9100   
 1242  1.4496  0.0022  0.0008  ...  0.47910  533.1809  1.0744    5.6500   
 1254  1.3858 -0.0053  0.0054  ...  0.20190  529.5182  2.1130   10.6500   
 1302  1.3464  0.0140 -0.0122  ...  0.08990  530.3364  2.0270    9.9200   
 1303  1.5559 -0.0096  0.0028  ...  0.08770  529.9445  1.9740    7.9100   
 1324  1.4115  0.0062 -0.0007  ...  0.13560  531.1982  2.2418   10.6300   
 1325  1.4610 -0.0154 -0.0050  ...  0.08770  531.3445  2.0359    5.7100   
 1327  1.4984  0.0163  0.0191  ...  0.08770  531.3445  2.0359    5.7100   
 1328  1.3719  0.0048  0.0089  ...  0.08770  534.1600  1.1090  432.9400   
 1329  1.4321 -0.0076 -0.0123  ...  0.08770  531.3445  2.0359    5.7100   
 1342  1.5171 -0.0292 -0.0071  ...  0.08770  529.5182  2.1130   10.6500   
 1343  1.4854  0.0070  0.0019  ...  0.16540  528.0127  1.1073    9.0400   
 1363  1.4093 -0.0041  0.0094  ...  0.19810  530.6345  1.9733    7.6000   
 1364  1.4362 -0.0034 -0.0070  ...  0.21760  533.1809  1.0744    5.6500   
 1365  1.5279 -0.0094  0.0001  ...  0.55280  530.0846  1.9812    9.0100   
 1400  1.4065 -0.0037 -0.0058  ...  0.08770  533.1809  1.0744    5.6500   
 1438  1.4613 -0.0027 -0.0039  ...  0.22560  531.7464  2.2909    9.1299   
 1443  1.6073 -0.0278 -0.0057  ...  0.28110  533.1364  2.0133    7.2400   
 1519  1.5134  0.0036 -0.0055  ...  0.19310  530.3364  2.3002   10.0300   
 
          582     583     586     587       589  Pass/Fail  
 2     0.4958  0.0157  0.0584  0.0484   82.8602          1  
 10    0.4925  0.0158  0.0355  0.0205   57.8122          1  
 11    0.4987  0.0427  0.0370  0.0279   75.5077          1  
 14    0.5077  0.0094  0.0202  0.0289  142.9080          1  
 23    0.4982  0.0099  0.0098  0.0213  216.8869          1  
 38    0.5022  0.0105  0.0150  0.0151  100.7279          1  
 40    0.5068  0.0201  0.0189  0.0086   45.4264          1  
 45    0.5018  0.0116  0.0137  0.0155  113.3721          1  
 48    0.4976  0.0087  0.0246  0.0138   56.0357          1  
 49    0.4972  0.0151  0.0296  0.0165   55.8324          1  
 50    0.5026  0.0098  0.0296  0.0165   55.8324          1  
 57    0.5048  0.0138  0.0201  0.0220  109.4273          1  
 58    0.5049  0.0121  0.0207  0.0301  145.6339          1  
 62    0.5053  0.0099  0.0194  0.0169   87.3034          1  
 64    0.5010  0.0289  0.0297  0.0556  187.3554          1  
 82    0.4955  0.0094  0.0278  0.0042   15.2909          1  
 96    0.4996  0.0326  0.0095  0.0184  192.2985          1  
 115   0.4995  0.0093  0.0364  0.0166   45.6835          1  
 131   0.4990  0.0119  0.0058  0.0169  289.9234          1  
 154   0.5025  0.0128  0.0199  0.0117   58.5665          1  
 157   0.4993  0.0138  0.0140  0.0180  128.2819          1  
 158   0.4984  0.0155  0.0140  0.0180  128.2819          1  
 167   0.5046  0.0159  0.0419  0.0098   23.3511          1  
 169   0.5024  0.0389  0.0419  0.0098   23.3511          1  
 180   0.5018  0.0460 -0.0012  0.0220    0.0000          1  
 182   0.5005  0.0203  0.0077  0.0204  264.7525          1  
 186   0.4987  0.0139  0.0158  0.0186  118.2289          1  
 188   0.4982  0.0154  0.0104  0.0221  211.6182          1  
 189   0.4915  0.0112  0.0104  0.0221  211.6182          1  
 218   0.4968  0.0126  0.0279  0.0123   44.1754          1  
 222   0.4995  0.0201  0.0440  0.0133   30.2219          1  
 231   0.5038  0.0188  0.0118  0.0098   83.1192          1  
 235   0.4947  0.0121 -0.0034  0.0093  272.3477          1  
 236   0.5042  0.0116 -0.0034  0.0093  272.3477          1  
 238   0.5008  0.0138  0.0198  0.0069   34.7486          1  
 240   0.5044  0.0115  0.0154  0.0271  176.0329          1  
 241   0.5012  0.0096  0.0154  0.0271  176.0329          1  
 243   0.5014  0.0099  0.0154  0.0271  176.0329          1  
 244   0.4958  0.0112  0.0154  0.0271  176.0329          1  
 273   0.5031  0.0102  0.0274  0.0142   51.9067          1  
 277   0.5036  0.0153  0.0177  0.0184  104.4612          1  
 282   0.5031  0.0153  0.0121  0.0169  139.8330          1  
 291   0.5033  0.0216  0.0291  0.0237   81.3456          1  
 294   0.4915  0.0116  0.0291  0.0237   81.3456          1  
 321   0.5004  0.0174  0.0171  0.0096   56.0858          1  
 323   0.4997  0.0174  0.0107  0.0071   66.2997          1  
 326   0.5013  0.0155  0.0210  0.0156   74.0589          1  
 327   0.4972  0.0260  0.0226  0.0106   46.9253          1  
 336   0.5081  0.0169  0.0353  0.0190   53.7460          1  
 344   0.5004  0.0123  0.0358  0.0162   45.2582          1  
 351   0.5012  0.0110  0.0275  0.0314  114.1967          1  
 368   0.4981  0.0184  0.0046  0.0180  388.9648          1  
 373   0.4994  0.0160  0.0075  0.0112  150.3448          1  
 392   0.5015  0.0129  0.0104  0.0083   79.8045          1  
 406   0.5006  0.0184  0.0061  0.0141  233.2441          1  
 424   0.4970  0.0174  0.0005  0.0115    0.0000          1  
 441   0.5002  0.0162  0.0329  0.0055   16.6695          1  
 448   0.5017  0.0161  0.0235  0.0355  150.7761          1  
 495   0.5018  0.0103  0.0255  0.0260  102.1652          1  
 508   0.5007  0.0169  0.0048  0.0226  474.0812          1  
 518   0.4929  0.0145  0.0331  0.0210   63.2615          1  
 576   0.5088  0.0275  0.0182  0.0139   76.6094          1  
 583   0.4961  0.0185  0.0332  0.0216   65.1043          1  
 601   0.5003  0.0101  0.0225  0.0193   85.7175          1  
 605   0.5049  0.0145  0.0199  0.0159   79.7752          1  
 634   0.4931  0.0364  0.0328  0.0235   71.5333          1  
 709   0.5025  0.0131  0.0161  0.0350  217.1506          1  
 795   0.4994  0.0099  0.0406  0.0268   66.1687          1  
 797   0.5022  0.0346  0.0406  0.0268   66.1687          1  
 826   0.5022  0.0169  0.0275  0.0215   78.1199          1  
 831   0.4994  0.0115  0.0545  0.0184   33.7876          1  
 871   0.4936  0.0131  0.0117  0.0262  223.1018          1  
 914   0.5080  0.0168  0.0134  0.0121   90.4575          1  
 924   0.5087  0.0116  0.0153  0.0048   31.0176          1  
 926   0.5003  0.0106  0.0153  0.0048   31.0176          1  
 929   0.5026  0.0121  0.0153  0.0048   31.0176          1  
 1029  0.5010  0.0116  0.0267  0.0174   65.1609          1  
 1062  0.5027  0.0184  0.0189  0.0059   31.0252          1  
 1144  0.4984  0.0155  0.0275  0.0108   39.1032          1  
 1151  0.5014  0.0121  0.0275  0.0108   39.1032          1  
 1185  0.4968  0.0167  0.0233  0.0138   59.1201          1  
 1189  0.5042  0.0108  0.0262  0.0104   39.5528          1  
 1211  0.5033  0.0268  0.0284  0.0209   73.5120          1  
 1227  0.4926  0.0135  0.0102  0.0133  130.0641          1  
 1238  0.5013  0.0095  0.0193  0.0072   37.6251          1  
 1241  0.5012  0.0131  0.0283  0.0112   39.4516          1  
 1242  0.5046  0.0191  0.0283  0.0112   39.4516          1  
 1254  0.4988  0.0102  0.0243  0.0100   41.0782          1  
 1302  0.5037  0.0118  0.0246  0.0167   68.1642          1  
 1303  0.5019  0.0154  0.0204  0.0159   77.9730          1  
 1324  0.4970  0.0089  0.0230  0.0210   91.4264          1  
 1325  0.5046  0.0107  0.0230  0.0210   91.4264          1  
 1327  0.5042  0.0099  0.0230  0.0210   91.4264          1  
 1328  0.5070  0.0144  0.0230  0.0210   91.4264          1  
 1329  0.4997  0.0126  0.0230  0.0210   91.4264          1  
 1342  0.4966  0.0130  0.0346  0.0251   72.6469          1  
 1343  0.5068  0.0189  0.0346  0.0251   72.6469          1  
 1363  0.4995  0.0122  0.0182  0.0077   42.5048          1  
 1364  0.5028  0.0113  0.0182  0.0077   42.5048          1  
 1365  0.4972  0.0154 -0.0006  0.0118    0.0000          1  
 1400  0.5081  0.0158  0.0302  0.0159   52.7014          1  
 1438  0.4969  0.0118  0.0178  0.0120   67.7994          1  
 1443  0.5021  0.0192  0.0281  0.0247   88.1528          1  
 1519  0.4979  0.0176  0.0157  0.0134   85.3122          1  
 
 [104 rows x 202 columns]]</pre>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[23]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">:</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rug</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;First Sensor Measurements&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">:</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="s1">&#39;2&#39;</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rug</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Second Sensor Measurements&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">:</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="s1">&#39;3&#39;</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rug</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Third Sensor Measurements&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">:</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="s1">&#39;4&#39;</span><span class="p">],</span> <span class="n">hist</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rug</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Fourth Sensor Measurements&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>

<span class="c1">#sns.add_legend()</span>
<span class="c1">#plt.legend()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Pass&#39;</span><span class="p">,</span><span class="s1">&#39;Fail&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)
/opt/anaconda3/lib/python3.9/site-packages/seaborn/distributions.py:2103: FutureWarning: The `axis` variable is no longer used and will be removed. Instead, assign variables directly to `x` or `y`.
  warnings.warn(msg, FutureWarning)
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>




<div class="jp-RenderedImage jp-OutputArea-output ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>3B. Perform bivariate and multivariate analysis with appropriate detailed comments after each analysis.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[24]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">corrMatrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[25]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">50</span><span class="p">))</span>  
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">corrMatrix</span><span class="p">,</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: vmin. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
  warnings.warn(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>




<div class="jp-RenderedImage jp-OutputArea-output ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Observation : 
 Most of features has doesn't have correlation . Becuase already we removed features having more 70% correlation</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="4.-Data-pre-processing">4. Data pre-processing<a class="anchor-link" href="#4.-Data-pre-processing">&#182;</a></h1>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>4.A. Segregate predictors vs target attributes.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[26]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Arrange data into independent variables and dependent variables</span>
<span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Pass/Fail&#39;</span><span class="p">]</span>
<span class="n">X</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Pass/Fail&#39;</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>4.B. Check for target balancing and fix it if found imbalanced.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[27]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[27]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>-1    1463
 1     104
Name: Pass/Fail, dtype: int64</pre>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Data is imbalance between -1,1.So we will do oversample</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[28]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_over</span><span class="p">,</span> <span class="n">y_over</span> <span class="o">=</span> <span class="n">oversample</span><span class="o">.</span><span class="n">fit_resample</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[29]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_over</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[29]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>-1    1463
 1    1463
Name: Pass/Fail, dtype: int64</pre>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>4.C. Perform train-test split and standardise the data or vice versa if required.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[30]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_over</span><span class="p">,</span> <span class="n">y_over</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[31]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pipe_lr</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;scl&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">())])</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[32]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span class="o">=</span><span class="n">pipe_lr</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span><span class="o">=</span><span class="n">pipe_lr</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>4.D. Check if the train and test data have similar statistical characteristics when compared with original data.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[33]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span> 
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[33]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1567.0</td>
      <td>3014.441551</td>
      <td>73.480841</td>
      <td>2743.2400</td>
      <td>2966.66500</td>
      <td>3011.49000</td>
      <td>3056.54000</td>
      <td>3356.3500</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1567.0</td>
      <td>2495.866110</td>
      <td>80.228143</td>
      <td>2158.7500</td>
      <td>2452.88500</td>
      <td>2499.40500</td>
      <td>2538.74500</td>
      <td>2846.4400</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1567.0</td>
      <td>2200.551958</td>
      <td>29.380973</td>
      <td>2060.6600</td>
      <td>2181.09995</td>
      <td>2201.06670</td>
      <td>2218.05550</td>
      <td>2315.2667</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1567.0</td>
      <td>1395.383474</td>
      <td>439.837330</td>
      <td>0.0000</td>
      <td>1083.88580</td>
      <td>1285.21440</td>
      <td>1590.16990</td>
      <td>3715.0417</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1567.0</td>
      <td>4.171281</td>
      <td>56.103721</td>
      <td>0.6815</td>
      <td>1.01770</td>
      <td>1.31680</td>
      <td>1.51880</td>
      <td>1114.5366</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1567.0</td>
      <td>101.116476</td>
      <td>6.209385</td>
      <td>82.1311</td>
      <td>97.93780</td>
      <td>101.51220</td>
      <td>104.53000</td>
      <td>129.2522</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1567.0</td>
      <td>0.121825</td>
      <td>0.008936</td>
      <td>0.0000</td>
      <td>0.12110</td>
      <td>0.12240</td>
      <td>0.12380</td>
      <td>0.1286</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1567.0</td>
      <td>1.462860</td>
      <td>0.073849</td>
      <td>1.1910</td>
      <td>1.41125</td>
      <td>1.46160</td>
      <td>1.51685</td>
      <td>1.6564</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1567.0</td>
      <td>-0.000842</td>
      <td>0.015107</td>
      <td>-0.0534</td>
      <td>-0.01080</td>
      <td>-0.00130</td>
      <td>0.00840</td>
      <td>0.0749</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1567.0</td>
      <td>0.000146</td>
      <td>0.009296</td>
      <td>-0.0349</td>
      <td>-0.00560</td>
      <td>0.00040</td>
      <td>0.00590</td>
      <td>0.0530</td>
    </tr>
    <tr>
      <th>11</th>
      <td>1567.0</td>
      <td>0.964355</td>
      <td>0.012444</td>
      <td>0.6554</td>
      <td>0.95810</td>
      <td>0.96580</td>
      <td>0.97130</td>
      <td>0.9848</td>
    </tr>
    <tr>
      <th>12</th>
      <td>1567.0</td>
      <td>199.956272</td>
      <td>3.255230</td>
      <td>182.0940</td>
      <td>198.13095</td>
      <td>199.53560</td>
      <td>202.00675</td>
      <td>272.0451</td>
    </tr>
    <tr>
      <th>14</th>
      <td>1567.0</td>
      <td>9.005297</td>
      <td>2.793916</td>
      <td>2.2493</td>
      <td>7.09675</td>
      <td>8.96700</td>
      <td>10.85870</td>
      <td>19.5465</td>
    </tr>
    <tr>
      <th>15</th>
      <td>1567.0</td>
      <td>413.084376</td>
      <td>17.204633</td>
      <td>333.4486</td>
      <td>406.13100</td>
      <td>412.21910</td>
      <td>419.08280</td>
      <td>824.9271</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1567.0</td>
      <td>9.907496</td>
      <td>2.401564</td>
      <td>4.4696</td>
      <td>9.56855</td>
      <td>9.85175</td>
      <td>10.12775</td>
      <td>102.8677</td>
    </tr>
    <tr>
      <th>18</th>
      <td>1567.0</td>
      <td>190.046620</td>
      <td>2.778426</td>
      <td>169.1774</td>
      <td>188.30065</td>
      <td>189.66420</td>
      <td>192.17890</td>
      <td>215.5977</td>
    </tr>
    <tr>
      <th>19</th>
      <td>1567.0</td>
      <td>12.481152</td>
      <td>0.217273</td>
      <td>9.8773</td>
      <td>12.46000</td>
      <td>12.49960</td>
      <td>12.54710</td>
      <td>12.9898</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1567.0</td>
      <td>1.405054</td>
      <td>0.016737</td>
      <td>1.1797</td>
      <td>1.39650</td>
      <td>1.40600</td>
      <td>1.41500</td>
      <td>1.4534</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1567.0</td>
      <td>-5618.272176</td>
      <td>626.430997</td>
      <td>-7150.2500</td>
      <td>-5932.62500</td>
      <td>-5523.25000</td>
      <td>-5356.62500</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>23</th>
      <td>1567.0</td>
      <td>-3806.318177</td>
      <td>1379.280633</td>
      <td>-9986.7500</td>
      <td>-4370.62500</td>
      <td>-3820.75000</td>
      <td>-3356.37500</td>
      <td>2363.0000</td>
    </tr>
    <tr>
      <th>24</th>
      <td>1567.0</td>
      <td>-298.317538</td>
      <td>2900.846582</td>
      <td>-14804.5000</td>
      <td>-1474.37500</td>
      <td>-78.75000</td>
      <td>1376.25000</td>
      <td>14106.0000</td>
    </tr>
    <tr>
      <th>25</th>
      <td>1567.0</td>
      <td>1.203946</td>
      <td>0.177510</td>
      <td>0.0000</td>
      <td>1.09490</td>
      <td>1.28300</td>
      <td>1.30430</td>
      <td>1.3828</td>
    </tr>
    <tr>
      <th>28</th>
      <td>1567.0</td>
      <td>69.499093</td>
      <td>3.458992</td>
      <td>59.4000</td>
      <td>67.38335</td>
      <td>69.15560</td>
      <td>72.25555</td>
      <td>77.9000</td>
    </tr>
    <tr>
      <th>29</th>
      <td>1567.0</td>
      <td>2.366212</td>
      <td>0.408433</td>
      <td>0.6667</td>
      <td>2.08890</td>
      <td>2.37780</td>
      <td>2.65560</td>
      <td>3.5111</td>
    </tr>
    <tr>
      <th>31</th>
      <td>1567.0</td>
      <td>3.672880</td>
      <td>0.535050</td>
      <td>2.0698</td>
      <td>3.36270</td>
      <td>3.43100</td>
      <td>3.53125</td>
      <td>4.8044</td>
    </tr>
    <tr>
      <th>32</th>
      <td>1567.0</td>
      <td>85.337340</td>
      <td>2.025908</td>
      <td>83.1829</td>
      <td>84.49050</td>
      <td>85.13545</td>
      <td>85.74190</td>
      <td>105.6038</td>
    </tr>
    <tr>
      <th>33</th>
      <td>1567.0</td>
      <td>8.960157</td>
      <td>1.344036</td>
      <td>7.6032</td>
      <td>8.58000</td>
      <td>8.76980</td>
      <td>9.06060</td>
      <td>23.3453</td>
    </tr>
    <tr>
      <th>37</th>
      <td>1567.0</td>
      <td>66.221280</td>
      <td>0.304044</td>
      <td>64.9193</td>
      <td>66.04080</td>
      <td>66.23180</td>
      <td>66.34305</td>
      <td>67.9586</td>
    </tr>
    <tr>
      <th>38</th>
      <td>1567.0</td>
      <td>86.836566</td>
      <td>0.446613</td>
      <td>84.7327</td>
      <td>86.57830</td>
      <td>86.82070</td>
      <td>87.00240</td>
      <td>88.4188</td>
    </tr>
    <tr>
      <th>40</th>
      <td>1567.0</td>
      <td>68.063966</td>
      <td>23.911897</td>
      <td>1.4340</td>
      <td>74.84000</td>
      <td>78.29000</td>
      <td>80.18000</td>
      <td>86.1200</td>
    </tr>
    <tr>
      <th>41</th>
      <td>1567.0</td>
      <td>3.348792</td>
      <td>2.342518</td>
      <td>-0.0759</td>
      <td>2.69900</td>
      <td>3.07400</td>
      <td>3.51500</td>
      <td>37.8800</td>
    </tr>
    <tr>
      <th>43</th>
      <td>1567.0</td>
      <td>355.537743</td>
      <td>6.232884</td>
      <td>342.7545</td>
      <td>350.80225</td>
      <td>353.72090</td>
      <td>360.77180</td>
      <td>377.2973</td>
    </tr>
    <tr>
      <th>44</th>
      <td>1567.0</td>
      <td>10.031167</td>
      <td>0.174982</td>
      <td>9.4640</td>
      <td>9.92545</td>
      <td>10.03485</td>
      <td>10.15245</td>
      <td>11.0530</td>
    </tr>
    <tr>
      <th>45</th>
      <td>1567.0</td>
      <td>136.742841</td>
      <td>7.846746</td>
      <td>108.8464</td>
      <td>130.73045</td>
      <td>136.40000</td>
      <td>142.09095</td>
      <td>176.3136</td>
    </tr>
    <tr>
      <th>47</th>
      <td>1567.0</td>
      <td>1.178005</td>
      <td>0.189585</td>
      <td>0.4967</td>
      <td>0.98500</td>
      <td>1.25105</td>
      <td>1.34020</td>
      <td>1.5111</td>
    </tr>
    <tr>
      <th>48</th>
      <td>1567.0</td>
      <td>139.972254</td>
      <td>4.522806</td>
      <td>125.7982</td>
      <td>136.93000</td>
      <td>140.00775</td>
      <td>143.19410</td>
      <td>163.2509</td>
    </tr>
    <tr>
      <th>53</th>
      <td>1567.0</td>
      <td>4.592979</td>
      <td>0.054880</td>
      <td>3.7060</td>
      <td>4.57400</td>
      <td>4.59600</td>
      <td>4.61700</td>
      <td>4.7640</td>
    </tr>
    <tr>
      <th>55</th>
      <td>1567.0</td>
      <td>2856.166560</td>
      <td>25.716644</td>
      <td>2801.0000</td>
      <td>2836.00000</td>
      <td>2854.00000</td>
      <td>2874.00000</td>
      <td>2936.0000</td>
    </tr>
    <tr>
      <th>56</th>
      <td>1567.0</td>
      <td>0.928855</td>
      <td>0.006800</td>
      <td>0.8755</td>
      <td>0.92550</td>
      <td>0.93100</td>
      <td>0.93310</td>
      <td>0.9378</td>
    </tr>
    <tr>
      <th>57</th>
      <td>1567.0</td>
      <td>0.949215</td>
      <td>0.004171</td>
      <td>0.9319</td>
      <td>0.94670</td>
      <td>0.94930</td>
      <td>0.95200</td>
      <td>0.9598</td>
    </tr>
    <tr>
      <th>58</th>
      <td>1567.0</td>
      <td>4.593259</td>
      <td>0.084992</td>
      <td>4.2199</td>
      <td>4.53190</td>
      <td>4.57270</td>
      <td>4.66860</td>
      <td>4.8475</td>
    </tr>
    <tr>
      <th>59</th>
      <td>1567.0</td>
      <td>2.951249</td>
      <td>9.511839</td>
      <td>-28.9882</td>
      <td>-1.85545</td>
      <td>0.94725</td>
      <td>4.33770</td>
      <td>168.1455</td>
    </tr>
    <tr>
      <th>61</th>
      <td>1567.0</td>
      <td>10.423195</td>
      <td>0.274351</td>
      <td>9.4611</td>
      <td>10.28405</td>
      <td>10.43670</td>
      <td>10.59055</td>
      <td>11.7849</td>
    </tr>
    <tr>
      <th>62</th>
      <td>1567.0</td>
      <td>116.501216</td>
      <td>8.612494</td>
      <td>81.4900</td>
      <td>112.05545</td>
      <td>116.21180</td>
      <td>120.91820</td>
      <td>287.1509</td>
    </tr>
    <tr>
      <th>63</th>
      <td>1567.0</td>
      <td>13.986604</td>
      <td>7.104106</td>
      <td>1.6591</td>
      <td>10.38365</td>
      <td>13.24605</td>
      <td>16.32550</td>
      <td>188.0923</td>
    </tr>
    <tr>
      <th>64</th>
      <td>1567.0</td>
      <td>20.539783</td>
      <td>4.966452</td>
      <td>6.4482</td>
      <td>17.37730</td>
      <td>20.02135</td>
      <td>22.79955</td>
      <td>48.9882</td>
    </tr>
    <tr>
      <th>67</th>
      <td>1567.0</td>
      <td>16.655186</td>
      <td>306.914183</td>
      <td>0.4137</td>
      <td>0.89150</td>
      <td>0.97830</td>
      <td>1.06490</td>
      <td>7272.8283</td>
    </tr>
    <tr>
      <th>68</th>
      <td>1567.0</td>
      <td>147.438189</td>
      <td>4.231976</td>
      <td>87.0255</td>
      <td>145.24230</td>
      <td>147.59730</td>
      <td>149.93590</td>
      <td>167.8309</td>
    </tr>
    <tr>
      <th>71</th>
      <td>1567.0</td>
      <td>104.322429</td>
      <td>31.591384</td>
      <td>21.4332</td>
      <td>87.58460</td>
      <td>102.60430</td>
      <td>115.43960</td>
      <td>238.4775</td>
    </tr>
    <tr>
      <th>74</th>
      <td>1567.0</td>
      <td>0.002677</td>
      <td>0.105986</td>
      <td>0.0000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>4.1955</td>
    </tr>
    <tr>
      <th>75</th>
      <td>1567.0</td>
      <td>-0.006894</td>
      <td>0.022121</td>
      <td>-0.1049</td>
      <td>-0.01920</td>
      <td>-0.00630</td>
      <td>0.00660</td>
      <td>0.2315</td>
    </tr>
    <tr>
      <th>76</th>
      <td>1567.0</td>
      <td>-0.029383</td>
      <td>0.032948</td>
      <td>-0.1862</td>
      <td>-0.05135</td>
      <td>-0.02890</td>
      <td>-0.00690</td>
      <td>0.0723</td>
    </tr>
    <tr>
      <th>77</th>
      <td>1567.0</td>
      <td>-0.007085</td>
      <td>0.031129</td>
      <td>-0.1046</td>
      <td>-0.02940</td>
      <td>-0.00990</td>
      <td>0.00890</td>
      <td>0.1331</td>
    </tr>
    <tr>
      <th>78</th>
      <td>1567.0</td>
      <td>-0.013625</td>
      <td>0.047504</td>
      <td>-0.3482</td>
      <td>-0.04730</td>
      <td>-0.01250</td>
      <td>0.01205</td>
      <td>0.2492</td>
    </tr>
    <tr>
      <th>79</th>
      <td>1567.0</td>
      <td>0.003415</td>
      <td>0.022905</td>
      <td>-0.0568</td>
      <td>-0.01070</td>
      <td>0.00060</td>
      <td>0.01280</td>
      <td>0.1013</td>
    </tr>
    <tr>
      <th>80</th>
      <td>1567.0</td>
      <td>-0.018380</td>
      <td>0.048862</td>
      <td>-0.1437</td>
      <td>-0.04295</td>
      <td>-0.00870</td>
      <td>0.00870</td>
      <td>0.1186</td>
    </tr>
    <tr>
      <th>81</th>
      <td>1567.0</td>
      <td>-0.021130</td>
      <td>0.016891</td>
      <td>-0.0982</td>
      <td>-0.02710</td>
      <td>-0.01960</td>
      <td>-0.01215</td>
      <td>0.0584</td>
    </tr>
    <tr>
      <th>82</th>
      <td>1567.0</td>
      <td>0.006079</td>
      <td>0.035797</td>
      <td>-0.2129</td>
      <td>-0.01735</td>
      <td>0.00760</td>
      <td>0.02680</td>
      <td>0.1437</td>
    </tr>
    <tr>
      <th>83</th>
      <td>1567.0</td>
      <td>7.452076</td>
      <td>0.516087</td>
      <td>5.8257</td>
      <td>7.10435</td>
      <td>7.46745</td>
      <td>7.80735</td>
      <td>8.9904</td>
    </tr>
    <tr>
      <th>84</th>
      <td>1567.0</td>
      <td>0.133107</td>
      <td>0.005032</td>
      <td>0.1174</td>
      <td>0.12980</td>
      <td>0.13300</td>
      <td>0.13630</td>
      <td>0.1505</td>
    </tr>
    <tr>
      <th>86</th>
      <td>1567.0</td>
      <td>2.401872</td>
      <td>0.037332</td>
      <td>2.2425</td>
      <td>2.37685</td>
      <td>2.40390</td>
      <td>2.42860</td>
      <td>2.5555</td>
    </tr>
    <tr>
      <th>87</th>
      <td>1567.0</td>
      <td>0.982420</td>
      <td>0.012848</td>
      <td>0.7749</td>
      <td>0.97580</td>
      <td>0.98740</td>
      <td>0.98970</td>
      <td>0.9935</td>
    </tr>
    <tr>
      <th>88</th>
      <td>1567.0</td>
      <td>1807.815021</td>
      <td>53.537262</td>
      <td>1627.4714</td>
      <td>1777.47030</td>
      <td>1809.24920</td>
      <td>1841.87300</td>
      <td>2105.1823</td>
    </tr>
    <tr>
      <th>89</th>
      <td>1567.0</td>
      <td>0.188749</td>
      <td>0.051514</td>
      <td>0.1113</td>
      <td>0.16975</td>
      <td>0.19010</td>
      <td>0.20015</td>
      <td>1.4727</td>
    </tr>
    <tr>
      <th>90</th>
      <td>1567.0</td>
      <td>8827.468461</td>
      <td>389.807042</td>
      <td>7397.3100</td>
      <td>8578.56995</td>
      <td>8825.43510</td>
      <td>9055.26000</td>
      <td>10746.6000</td>
    </tr>
    <tr>
      <th>91</th>
      <td>1567.0</td>
      <td>0.002431</td>
      <td>0.087515</td>
      <td>-0.3570</td>
      <td>-0.04265</td>
      <td>0.00000</td>
      <td>0.05035</td>
      <td>0.3627</td>
    </tr>
    <tr>
      <th>92</th>
      <td>1567.0</td>
      <td>0.000507</td>
      <td>0.003229</td>
      <td>-0.0126</td>
      <td>-0.00115</td>
      <td>0.00040</td>
      <td>0.00200</td>
      <td>0.0281</td>
    </tr>
    <tr>
      <th>93</th>
      <td>1567.0</td>
      <td>-0.000540</td>
      <td>0.003008</td>
      <td>-0.0171</td>
      <td>-0.00160</td>
      <td>-0.00020</td>
      <td>0.00100</td>
      <td>0.0133</td>
    </tr>
    <tr>
      <th>94</th>
      <td>1567.0</td>
      <td>-0.000029</td>
      <td>0.000174</td>
      <td>-0.0020</td>
      <td>-0.00010</td>
      <td>0.00000</td>
      <td>0.00010</td>
      <td>0.0011</td>
    </tr>
    <tr>
      <th>95</th>
      <td>1567.0</td>
      <td>0.000060</td>
      <td>0.000104</td>
      <td>-0.0009</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00010</td>
      <td>0.0009</td>
    </tr>
    <tr>
      <th>99</th>
      <td>1567.0</td>
      <td>0.001534</td>
      <td>0.062620</td>
      <td>-0.5283</td>
      <td>-0.02980</td>
      <td>0.00000</td>
      <td>0.02980</td>
      <td>0.8854</td>
    </tr>
    <tr>
      <th>100</th>
      <td>1567.0</td>
      <td>-0.000021</td>
      <td>0.000355</td>
      <td>-0.0030</td>
      <td>-0.00020</td>
      <td>0.00000</td>
      <td>0.00020</td>
      <td>0.0023</td>
    </tr>
    <tr>
      <th>102</th>
      <td>1567.0</td>
      <td>0.001110</td>
      <td>0.062847</td>
      <td>-0.5353</td>
      <td>-0.03530</td>
      <td>0.00000</td>
      <td>0.03360</td>
      <td>0.2979</td>
    </tr>
    <tr>
      <th>103</th>
      <td>1567.0</td>
      <td>-0.009789</td>
      <td>0.003063</td>
      <td>-0.0329</td>
      <td>-0.01180</td>
      <td>-0.01010</td>
      <td>-0.00820</td>
      <td>0.0203</td>
    </tr>
    <tr>
      <th>107</th>
      <td>1567.0</td>
      <td>-0.001759</td>
      <td>0.087307</td>
      <td>-0.5226</td>
      <td>-0.04835</td>
      <td>0.00000</td>
      <td>0.04860</td>
      <td>0.4856</td>
    </tr>
    <tr>
      <th>108</th>
      <td>1567.0</td>
      <td>-0.010790</td>
      <td>0.086591</td>
      <td>-0.3454</td>
      <td>-0.06440</td>
      <td>-0.01120</td>
      <td>0.03785</td>
      <td>0.3938</td>
    </tr>
    <tr>
      <th>113</th>
      <td>1567.0</td>
      <td>0.945424</td>
      <td>0.012133</td>
      <td>0.8534</td>
      <td>0.93860</td>
      <td>0.94640</td>
      <td>0.95230</td>
      <td>0.9763</td>
    </tr>
    <tr>
      <th>114</th>
      <td>1567.0</td>
      <td>0.000123</td>
      <td>0.001668</td>
      <td>0.0000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.0414</td>
    </tr>
    <tr>
      <th>115</th>
      <td>1567.0</td>
      <td>747.383792</td>
      <td>48.949250</td>
      <td>544.0254</td>
      <td>721.02300</td>
      <td>750.86140</td>
      <td>776.78185</td>
      <td>924.5318</td>
    </tr>
    <tr>
      <th>116</th>
      <td>1567.0</td>
      <td>0.987130</td>
      <td>0.009497</td>
      <td>0.8900</td>
      <td>0.98950</td>
      <td>0.99050</td>
      <td>0.99090</td>
      <td>0.9924</td>
    </tr>
    <tr>
      <th>117</th>
      <td>1567.0</td>
      <td>58.625908</td>
      <td>6.485174</td>
      <td>52.8068</td>
      <td>57.97830</td>
      <td>58.54910</td>
      <td>59.13390</td>
      <td>311.7344</td>
    </tr>
    <tr>
      <th>118</th>
      <td>1567.0</td>
      <td>0.598421</td>
      <td>0.008040</td>
      <td>0.5274</td>
      <td>0.59420</td>
      <td>0.59900</td>
      <td>0.60330</td>
      <td>0.6245</td>
    </tr>
    <tr>
      <th>119</th>
      <td>1567.0</td>
      <td>0.970777</td>
      <td>0.008949</td>
      <td>0.8411</td>
      <td>0.96480</td>
      <td>0.96940</td>
      <td>0.97830</td>
      <td>0.9827</td>
    </tr>
    <tr>
      <th>120</th>
      <td>1567.0</td>
      <td>6.310863</td>
      <td>0.124304</td>
      <td>5.1259</td>
      <td>6.24640</td>
      <td>6.31360</td>
      <td>6.37585</td>
      <td>7.5220</td>
    </tr>
    <tr>
      <th>121</th>
      <td>1567.0</td>
      <td>15.796388</td>
      <td>0.099332</td>
      <td>15.4600</td>
      <td>15.73000</td>
      <td>15.79000</td>
      <td>15.86000</td>
      <td>16.0700</td>
    </tr>
    <tr>
      <th>122</th>
      <td>1567.0</td>
      <td>3.898267</td>
      <td>0.901520</td>
      <td>1.6710</td>
      <td>3.20200</td>
      <td>3.87700</td>
      <td>4.39200</td>
      <td>6.8890</td>
    </tr>
    <tr>
      <th>126</th>
      <td>1567.0</td>
      <td>2.750638</td>
      <td>0.252744</td>
      <td>2.3400</td>
      <td>2.57400</td>
      <td>2.73500</td>
      <td>2.87300</td>
      <td>3.9910</td>
    </tr>
    <tr>
      <th>128</th>
      <td>1567.0</td>
      <td>3.192198</td>
      <td>0.263414</td>
      <td>0.0000</td>
      <td>3.07600</td>
      <td>3.19500</td>
      <td>3.31100</td>
      <td>3.8950</td>
    </tr>
    <tr>
      <th>129</th>
      <td>1567.0</td>
      <td>-0.551860</td>
      <td>1.217366</td>
      <td>-3.7790</td>
      <td>-0.89880</td>
      <td>-0.14190</td>
      <td>0.04730</td>
      <td>2.4580</td>
    </tr>
    <tr>
      <th>131</th>
      <td>1567.0</td>
      <td>0.997808</td>
      <td>0.002244</td>
      <td>0.9936</td>
      <td>0.99640</td>
      <td>0.99775</td>
      <td>0.99890</td>
      <td>1.0190</td>
    </tr>
    <tr>
      <th>132</th>
      <td>1567.0</td>
      <td>2.318513</td>
      <td>0.053047</td>
      <td>2.1911</td>
      <td>2.27730</td>
      <td>2.31240</td>
      <td>2.35830</td>
      <td>2.4723</td>
    </tr>
    <tr>
      <th>133</th>
      <td>1567.0</td>
      <td>1004.043129</td>
      <td>6.520981</td>
      <td>980.4510</td>
      <td>1000.04545</td>
      <td>1004.05000</td>
      <td>1008.67060</td>
      <td>1020.9944</td>
    </tr>
    <tr>
      <th>134</th>
      <td>1567.0</td>
      <td>39.389480</td>
      <td>2.983032</td>
      <td>33.3658</td>
      <td>37.36890</td>
      <td>38.90260</td>
      <td>40.80460</td>
      <td>64.1287</td>
    </tr>
    <tr>
      <th>135</th>
      <td>1567.0</td>
      <td>117.932355</td>
      <td>57.454912</td>
      <td>58.0000</td>
      <td>92.00000</td>
      <td>109.00000</td>
      <td>127.00000</td>
      <td>994.0000</td>
    </tr>
    <tr>
      <th>136</th>
      <td>1567.0</td>
      <td>138.180983</td>
      <td>53.806875</td>
      <td>36.1000</td>
      <td>90.15000</td>
      <td>134.60000</td>
      <td>180.90000</td>
      <td>295.8000</td>
    </tr>
    <tr>
      <th>137</th>
      <td>1567.0</td>
      <td>122.670645</td>
      <td>52.137163</td>
      <td>19.2000</td>
      <td>81.40000</td>
      <td>117.70000</td>
      <td>161.60000</td>
      <td>334.7000</td>
    </tr>
    <tr>
      <th>138</th>
      <td>1567.0</td>
      <td>57.587810</td>
      <td>12.291096</td>
      <td>19.8000</td>
      <td>51.00000</td>
      <td>55.90010</td>
      <td>62.90010</td>
      <td>141.7998</td>
    </tr>
    <tr>
      <th>139</th>
      <td>1567.0</td>
      <td>416.077185</td>
      <td>262.221743</td>
      <td>0.0000</td>
      <td>243.84110</td>
      <td>339.56100</td>
      <td>494.80600</td>
      <td>1770.6909</td>
    </tr>
    <tr>
      <th>142</th>
      <td>1567.0</td>
      <td>6.638156</td>
      <td>3.536522</td>
      <td>1.7400</td>
      <td>5.11000</td>
      <td>6.26000</td>
      <td>7.50000</td>
      <td>103.3900</td>
    </tr>
    <tr>
      <th>143</th>
      <td>1567.0</td>
      <td>0.004168</td>
      <td>0.001278</td>
      <td>0.0000</td>
      <td>0.00330</td>
      <td>0.00390</td>
      <td>0.00490</td>
      <td>0.0121</td>
    </tr>
    <tr>
      <th>144</th>
      <td>1567.0</td>
      <td>0.119992</td>
      <td>0.061305</td>
      <td>0.0324</td>
      <td>0.08390</td>
      <td>0.10750</td>
      <td>0.13265</td>
      <td>0.6253</td>
    </tr>
    <tr>
      <th>145</th>
      <td>1567.0</td>
      <td>0.063615</td>
      <td>0.026524</td>
      <td>0.0214</td>
      <td>0.04805</td>
      <td>0.05860</td>
      <td>0.07180</td>
      <td>0.2507</td>
    </tr>
    <tr>
      <th>146</th>
      <td>1567.0</td>
      <td>0.055004</td>
      <td>0.021831</td>
      <td>0.0227</td>
      <td>0.04235</td>
      <td>0.05000</td>
      <td>0.06150</td>
      <td>0.2479</td>
    </tr>
    <tr>
      <th>150</th>
      <td>1567.0</td>
      <td>6.812615</td>
      <td>3.238956</td>
      <td>1.3370</td>
      <td>4.46550</td>
      <td>5.95100</td>
      <td>8.26950</td>
      <td>22.3180</td>
    </tr>
    <tr>
      <th>151</th>
      <td>1567.0</td>
      <td>14.041556</td>
      <td>30.973119</td>
      <td>2.0200</td>
      <td>8.09600</td>
      <td>10.99350</td>
      <td>14.34200</td>
      <td>536.5640</td>
    </tr>
    <tr>
      <th>153</th>
      <td>1567.0</td>
      <td>0.011925</td>
      <td>0.009337</td>
      <td>0.0036</td>
      <td>0.00730</td>
      <td>0.01110</td>
      <td>0.01490</td>
      <td>0.2389</td>
    </tr>
    <tr>
      <th>156</th>
      <td>1567.0</td>
      <td>0.058089</td>
      <td>0.079174</td>
      <td>0.0111</td>
      <td>0.03625</td>
      <td>0.04870</td>
      <td>0.06670</td>
      <td>2.2016</td>
    </tr>
    <tr>
      <th>159</th>
      <td>1567.0</td>
      <td>882.349075</td>
      <td>982.458855</td>
      <td>0.0000</td>
      <td>411.50000</td>
      <td>623.00000</td>
      <td>963.50000</td>
      <td>7791.0000</td>
    </tr>
    <tr>
      <th>160</th>
      <td>1567.0</td>
      <td>555.196554</td>
      <td>574.456703</td>
      <td>0.0000</td>
      <td>295.00000</td>
      <td>438.00000</td>
      <td>624.50000</td>
      <td>4170.0000</td>
    </tr>
    <tr>
      <th>161</th>
      <td>1567.0</td>
      <td>4064.996171</td>
      <td>4236.854877</td>
      <td>0.0000</td>
      <td>1322.50000</td>
      <td>2614.00000</td>
      <td>5033.00000</td>
      <td>37943.0000</td>
    </tr>
    <tr>
      <th>162</th>
      <td>1567.0</td>
      <td>4793.308870</td>
      <td>6550.267099</td>
      <td>0.0000</td>
      <td>451.00000</td>
      <td>1784.00000</td>
      <td>6376.00000</td>
      <td>36871.0000</td>
    </tr>
    <tr>
      <th>166</th>
      <td>1567.0</td>
      <td>2.788641</td>
      <td>1.119061</td>
      <td>0.8000</td>
      <td>2.10000</td>
      <td>2.60000</td>
      <td>3.20000</td>
      <td>21.1000</td>
    </tr>
    <tr>
      <th>167</th>
      <td>1567.0</td>
      <td>1.235737</td>
      <td>0.632364</td>
      <td>0.3000</td>
      <td>0.90000</td>
      <td>1.20000</td>
      <td>1.50000</td>
      <td>16.3000</td>
    </tr>
    <tr>
      <th>168</th>
      <td>1567.0</td>
      <td>0.124390</td>
      <td>0.047609</td>
      <td>0.0330</td>
      <td>0.09000</td>
      <td>0.11900</td>
      <td>0.15050</td>
      <td>0.7250</td>
    </tr>
    <tr>
      <th>169</th>
      <td>1567.0</td>
      <td>0.400468</td>
      <td>0.197792</td>
      <td>0.0460</td>
      <td>0.23050</td>
      <td>0.41200</td>
      <td>0.53600</td>
      <td>1.1430</td>
    </tr>
    <tr>
      <th>170</th>
      <td>1567.0</td>
      <td>0.684331</td>
      <td>0.157418</td>
      <td>0.2979</td>
      <td>0.57560</td>
      <td>0.68600</td>
      <td>0.79730</td>
      <td>1.1530</td>
    </tr>
    <tr>
      <th>171</th>
      <td>1567.0</td>
      <td>0.120059</td>
      <td>0.060766</td>
      <td>0.0089</td>
      <td>0.07980</td>
      <td>0.11250</td>
      <td>0.14030</td>
      <td>0.4940</td>
    </tr>
    <tr>
      <th>172</th>
      <td>1567.0</td>
      <td>0.320115</td>
      <td>0.071220</td>
      <td>0.1287</td>
      <td>0.27660</td>
      <td>0.32385</td>
      <td>0.37020</td>
      <td>0.5484</td>
    </tr>
    <tr>
      <th>173</th>
      <td>1567.0</td>
      <td>0.576193</td>
      <td>0.095703</td>
      <td>0.2538</td>
      <td>0.51690</td>
      <td>0.57760</td>
      <td>0.63450</td>
      <td>0.8643</td>
    </tr>
    <tr>
      <th>175</th>
      <td>1567.0</td>
      <td>0.778037</td>
      <td>0.116285</td>
      <td>0.4616</td>
      <td>0.69220</td>
      <td>0.76820</td>
      <td>0.84390</td>
      <td>1.1720</td>
    </tr>
    <tr>
      <th>176</th>
      <td>1567.0</td>
      <td>0.244717</td>
      <td>0.074894</td>
      <td>0.0735</td>
      <td>0.19630</td>
      <td>0.24290</td>
      <td>0.29375</td>
      <td>0.4411</td>
    </tr>
    <tr>
      <th>177</th>
      <td>1567.0</td>
      <td>0.394699</td>
      <td>0.282823</td>
      <td>0.0470</td>
      <td>0.22200</td>
      <td>0.29900</td>
      <td>0.42300</td>
      <td>1.8580</td>
    </tr>
    <tr>
      <th>180</th>
      <td>1567.0</td>
      <td>19.013050</td>
      <td>3.310585</td>
      <td>9.4000</td>
      <td>16.85000</td>
      <td>18.69000</td>
      <td>20.96500</td>
      <td>48.6700</td>
    </tr>
    <tr>
      <th>181</th>
      <td>1567.0</td>
      <td>0.546756</td>
      <td>0.224331</td>
      <td>0.0930</td>
      <td>0.37800</td>
      <td>0.52400</td>
      <td>0.68850</td>
      <td>3.5730</td>
    </tr>
    <tr>
      <th>182</th>
      <td>1567.0</td>
      <td>10.780153</td>
      <td>4.162749</td>
      <td>3.1700</td>
      <td>7.73500</td>
      <td>10.17000</td>
      <td>13.33500</td>
      <td>55.0000</td>
    </tr>
    <tr>
      <th>183</th>
      <td>1567.0</td>
      <td>26.661515</td>
      <td>6.833931</td>
      <td>5.0140</td>
      <td>21.17200</td>
      <td>27.20050</td>
      <td>31.68700</td>
      <td>72.9470</td>
    </tr>
    <tr>
      <th>184</th>
      <td>1567.0</td>
      <td>0.144808</td>
      <td>0.110163</td>
      <td>0.0297</td>
      <td>0.10220</td>
      <td>0.13260</td>
      <td>0.16910</td>
      <td>3.2283</td>
    </tr>
    <tr>
      <th>188</th>
      <td>1567.0</td>
      <td>43.209503</td>
      <td>21.705075</td>
      <td>6.6130</td>
      <td>24.71400</td>
      <td>40.20950</td>
      <td>57.67250</td>
      <td>191.8300</td>
    </tr>
    <tr>
      <th>195</th>
      <td>1567.0</td>
      <td>0.287013</td>
      <td>0.394684</td>
      <td>0.0800</td>
      <td>0.21950</td>
      <td>0.25900</td>
      <td>0.29600</td>
      <td>4.8380</td>
    </tr>
    <tr>
      <th>200</th>
      <td>1567.0</td>
      <td>17.598561</td>
      <td>8.671307</td>
      <td>3.2100</td>
      <td>14.17500</td>
      <td>17.23500</td>
      <td>20.16000</td>
      <td>199.6200</td>
    </tr>
    <tr>
      <th>201</th>
      <td>1567.0</td>
      <td>7.834537</td>
      <td>5.093583</td>
      <td>0.0000</td>
      <td>5.02500</td>
      <td>6.76000</td>
      <td>9.48500</td>
      <td>126.5300</td>
    </tr>
    <tr>
      <th>208</th>
      <td>1567.0</td>
      <td>73.264254</td>
      <td>28.013323</td>
      <td>5.3590</td>
      <td>56.22050</td>
      <td>73.24800</td>
      <td>90.45250</td>
      <td>172.3490</td>
    </tr>
    <tr>
      <th>210</th>
      <td>1567.0</td>
      <td>0.088725</td>
      <td>0.041756</td>
      <td>0.0319</td>
      <td>0.06590</td>
      <td>0.07970</td>
      <td>0.09905</td>
      <td>0.5164</td>
    </tr>
    <tr>
      <th>211</th>
      <td>1567.0</td>
      <td>0.056701</td>
      <td>0.024817</td>
      <td>0.0022</td>
      <td>0.04395</td>
      <td>0.05320</td>
      <td>0.06410</td>
      <td>0.3227</td>
    </tr>
    <tr>
      <th>212</th>
      <td>1567.0</td>
      <td>0.051281</td>
      <td>0.031358</td>
      <td>0.0071</td>
      <td>0.03260</td>
      <td>0.04160</td>
      <td>0.06210</td>
      <td>0.5941</td>
    </tr>
    <tr>
      <th>213</th>
      <td>1567.0</td>
      <td>0.060280</td>
      <td>0.052624</td>
      <td>0.0037</td>
      <td>0.03695</td>
      <td>0.05600</td>
      <td>0.07340</td>
      <td>1.2837</td>
    </tr>
    <tr>
      <th>214</th>
      <td>1567.0</td>
      <td>0.083148</td>
      <td>0.056030</td>
      <td>0.0193</td>
      <td>0.05705</td>
      <td>0.07540</td>
      <td>0.09325</td>
      <td>0.7615</td>
    </tr>
    <tr>
      <th>215</th>
      <td>1567.0</td>
      <td>0.081097</td>
      <td>0.030203</td>
      <td>0.0059</td>
      <td>0.06360</td>
      <td>0.08250</td>
      <td>0.09810</td>
      <td>0.3429</td>
    </tr>
    <tr>
      <th>216</th>
      <td>1567.0</td>
      <td>0.083501</td>
      <td>0.025566</td>
      <td>0.0097</td>
      <td>0.06980</td>
      <td>0.08460</td>
      <td>0.09730</td>
      <td>0.2828</td>
    </tr>
    <tr>
      <th>217</th>
      <td>1567.0</td>
      <td>0.071483</td>
      <td>0.045944</td>
      <td>0.0079</td>
      <td>0.04610</td>
      <td>0.06170</td>
      <td>0.08605</td>
      <td>0.6744</td>
    </tr>
    <tr>
      <th>218</th>
      <td>1567.0</td>
      <td>3.771376</td>
      <td>1.170068</td>
      <td>1.0340</td>
      <td>2.94620</td>
      <td>3.63075</td>
      <td>4.40340</td>
      <td>8.8015</td>
    </tr>
    <tr>
      <th>219</th>
      <td>1567.0</td>
      <td>0.003252</td>
      <td>0.001640</td>
      <td>0.0007</td>
      <td>0.00230</td>
      <td>0.00300</td>
      <td>0.00380</td>
      <td>0.0163</td>
    </tr>
    <tr>
      <th>221</th>
      <td>1567.0</td>
      <td>0.060718</td>
      <td>0.023305</td>
      <td>0.0200</td>
      <td>0.04020</td>
      <td>0.06090</td>
      <td>0.07650</td>
      <td>0.2305</td>
    </tr>
    <tr>
      <th>222</th>
      <td>1567.0</td>
      <td>0.008821</td>
      <td>0.055937</td>
      <td>0.0003</td>
      <td>0.00140</td>
      <td>0.00230</td>
      <td>0.00550</td>
      <td>0.9911</td>
    </tr>
    <tr>
      <th>223</th>
      <td>1567.0</td>
      <td>122.846571</td>
      <td>55.156003</td>
      <td>32.2637</td>
      <td>95.14735</td>
      <td>119.43600</td>
      <td>144.50280</td>
      <td>1768.8802</td>
    </tr>
    <tr>
      <th>225</th>
      <td>1567.0</td>
      <td>1038.656080</td>
      <td>426.259257</td>
      <td>168.7998</td>
      <td>726.50000</td>
      <td>967.29980</td>
      <td>1252.39965</td>
      <td>3601.2998</td>
    </tr>
    <tr>
      <th>227</th>
      <td>1567.0</td>
      <td>0.019122</td>
      <td>0.010749</td>
      <td>0.0062</td>
      <td>0.01325</td>
      <td>0.01650</td>
      <td>0.02120</td>
      <td>0.1541</td>
    </tr>
    <tr>
      <th>228</th>
      <td>1567.0</td>
      <td>0.017841</td>
      <td>0.010738</td>
      <td>0.0072</td>
      <td>0.01265</td>
      <td>0.01550</td>
      <td>0.02000</td>
      <td>0.2133</td>
    </tr>
    <tr>
      <th>238</th>
      <td>1567.0</td>
      <td>0.004791</td>
      <td>0.001697</td>
      <td>0.0013</td>
      <td>0.00370</td>
      <td>0.00460</td>
      <td>0.00570</td>
      <td>0.0244</td>
    </tr>
    <tr>
      <th>239</th>
      <td>1567.0</td>
      <td>0.004575</td>
      <td>0.001440</td>
      <td>0.0014</td>
      <td>0.00360</td>
      <td>0.00440</td>
      <td>0.00530</td>
      <td>0.0236</td>
    </tr>
    <tr>
      <th>250</th>
      <td>1567.0</td>
      <td>109.650967</td>
      <td>54.597274</td>
      <td>21.0107</td>
      <td>76.13215</td>
      <td>103.09360</td>
      <td>131.75840</td>
      <td>1119.7042</td>
    </tr>
    <tr>
      <th>251</th>
      <td>1567.0</td>
      <td>0.004285</td>
      <td>0.037472</td>
      <td>0.0003</td>
      <td>0.00070</td>
      <td>0.00100</td>
      <td>0.00130</td>
      <td>0.9909</td>
    </tr>
    <tr>
      <th>253</th>
      <td>1567.0</td>
      <td>0.033179</td>
      <td>0.022255</td>
      <td>0.0094</td>
      <td>0.02460</td>
      <td>0.03080</td>
      <td>0.03770</td>
      <td>0.4517</td>
    </tr>
    <tr>
      <th>255</th>
      <td>1567.0</td>
      <td>0.403848</td>
      <td>0.120334</td>
      <td>0.1269</td>
      <td>0.30760</td>
      <td>0.40510</td>
      <td>0.48095</td>
      <td>0.9255</td>
    </tr>
    <tr>
      <th>267</th>
      <td>1567.0</td>
      <td>0.070587</td>
      <td>0.029573</td>
      <td>0.0198</td>
      <td>0.04400</td>
      <td>0.07060</td>
      <td>0.09160</td>
      <td>0.1578</td>
    </tr>
    <tr>
      <th>268</th>
      <td>1567.0</td>
      <td>19.496877</td>
      <td>7.326430</td>
      <td>6.0980</td>
      <td>13.82800</td>
      <td>17.97700</td>
      <td>24.65300</td>
      <td>40.8550</td>
    </tr>
    <tr>
      <th>269</th>
      <td>1567.0</td>
      <td>3.777486</td>
      <td>1.149394</td>
      <td>1.3017</td>
      <td>2.96240</td>
      <td>3.70350</td>
      <td>4.37680</td>
      <td>10.1529</td>
    </tr>
    <tr>
      <th>418</th>
      <td>1567.0</td>
      <td>320.236157</td>
      <td>287.521430</td>
      <td>0.0000</td>
      <td>0.00000</td>
      <td>302.17760</td>
      <td>523.62445</td>
      <td>999.3160</td>
    </tr>
    <tr>
      <th>419</th>
      <td>1567.0</td>
      <td>309.014570</td>
      <td>325.243132</td>
      <td>0.0000</td>
      <td>0.00000</td>
      <td>272.44870</td>
      <td>582.80310</td>
      <td>998.6813</td>
    </tr>
    <tr>
      <th>423</th>
      <td>1567.0</td>
      <td>77.645599</td>
      <td>32.567460</td>
      <td>23.0200</td>
      <td>56.00820</td>
      <td>69.90545</td>
      <td>92.83275</td>
      <td>424.2152</td>
    </tr>
    <tr>
      <th>432</th>
      <td>1567.0</td>
      <td>99.314795</td>
      <td>126.116776</td>
      <td>0.0000</td>
      <td>31.03385</td>
      <td>57.96930</td>
      <td>120.13690</td>
      <td>994.2857</td>
    </tr>
    <tr>
      <th>433</th>
      <td>1567.0</td>
      <td>205.449868</td>
      <td>225.643014</td>
      <td>0.0000</td>
      <td>10.04745</td>
      <td>151.11560</td>
      <td>304.54180</td>
      <td>995.7447</td>
    </tr>
    <tr>
      <th>438</th>
      <td>1567.0</td>
      <td>54.693892</td>
      <td>34.086853</td>
      <td>0.0000</td>
      <td>36.34370</td>
      <td>49.09090</td>
      <td>66.66670</td>
      <td>851.6129</td>
    </tr>
    <tr>
      <th>460</th>
      <td>1567.0</td>
      <td>29.195480</td>
      <td>13.331150</td>
      <td>7.9534</td>
      <td>20.22410</td>
      <td>26.16785</td>
      <td>35.26840</td>
      <td>149.3851</td>
    </tr>
    <tr>
      <th>468</th>
      <td>1567.0</td>
      <td>223.843226</td>
      <td>230.303247</td>
      <td>0.0000</td>
      <td>38.88265</td>
      <td>150.34010</td>
      <td>334.67400</td>
      <td>999.8770</td>
    </tr>
    <tr>
      <th>472</th>
      <td>1567.0</td>
      <td>137.890044</td>
      <td>47.591323</td>
      <td>11.4997</td>
      <td>105.62215</td>
      <td>138.25515</td>
      <td>168.27025</td>
      <td>492.7718</td>
    </tr>
    <tr>
      <th>476</th>
      <td>1567.0</td>
      <td>20.116232</td>
      <td>14.913155</td>
      <td>0.0000</td>
      <td>11.58535</td>
      <td>15.97380</td>
      <td>23.68240</td>
      <td>274.8871</td>
    </tr>
    <tr>
      <th>482</th>
      <td>1567.0</td>
      <td>318.037084</td>
      <td>278.866442</td>
      <td>0.0000</td>
      <td>0.00000</td>
      <td>293.51850</td>
      <td>512.39075</td>
      <td>999.4135</td>
    </tr>
    <tr>
      <th>483</th>
      <td>1567.0</td>
      <td>205.672096</td>
      <td>191.514533</td>
      <td>0.0000</td>
      <td>82.41015</td>
      <td>148.31750</td>
      <td>260.07900</td>
      <td>989.4737</td>
    </tr>
    <tr>
      <th>484</th>
      <td>1567.0</td>
      <td>214.117076</td>
      <td>211.695946</td>
      <td>0.0000</td>
      <td>77.01180</td>
      <td>138.77550</td>
      <td>288.91845</td>
      <td>996.8586</td>
    </tr>
    <tr>
      <th>485</th>
      <td>1567.0</td>
      <td>199.761505</td>
      <td>217.277824</td>
      <td>0.0000</td>
      <td>51.18850</td>
      <td>112.95340</td>
      <td>283.28900</td>
      <td>994.0000</td>
    </tr>
    <tr>
      <th>486</th>
      <td>1567.0</td>
      <td>301.700889</td>
      <td>285.226689</td>
      <td>0.0000</td>
      <td>0.00000</td>
      <td>249.92700</td>
      <td>497.38450</td>
      <td>999.4911</td>
    </tr>
    <tr>
      <th>487</th>
      <td>1567.0</td>
      <td>237.507454</td>
      <td>262.273846</td>
      <td>0.0000</td>
      <td>57.31690</td>
      <td>112.27550</td>
      <td>391.27750</td>
      <td>995.7447</td>
    </tr>
    <tr>
      <th>488</th>
      <td>1567.0</td>
      <td>352.553880</td>
      <td>250.105428</td>
      <td>0.0000</td>
      <td>145.15685</td>
      <td>348.52940</td>
      <td>507.49705</td>
      <td>997.5186</td>
    </tr>
    <tr>
      <th>489</th>
      <td>1567.0</td>
      <td>271.362828</td>
      <td>226.384996</td>
      <td>0.0000</td>
      <td>113.80665</td>
      <td>219.48720</td>
      <td>372.34190</td>
      <td>994.0035</td>
    </tr>
    <tr>
      <th>499</th>
      <td>1567.0</td>
      <td>262.859941</td>
      <td>324.699975</td>
      <td>0.0000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>536.12260</td>
      <td>1000.0000</td>
    </tr>
    <tr>
      <th>500</th>
      <td>1567.0</td>
      <td>240.673807</td>
      <td>322.911797</td>
      <td>0.0000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>505.22575</td>
      <td>999.2337</td>
    </tr>
    <tr>
      <th>510</th>
      <td>1567.0</td>
      <td>55.752306</td>
      <td>37.668964</td>
      <td>0.0000</td>
      <td>35.32440</td>
      <td>46.98610</td>
      <td>64.22845</td>
      <td>451.4851</td>
    </tr>
    <tr>
      <th>511</th>
      <td>1567.0</td>
      <td>275.627217</td>
      <td>329.601505</td>
      <td>0.0000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>554.01070</td>
      <td>1000.0000</td>
    </tr>
    <tr>
      <th>521</th>
      <td>1567.0</td>
      <td>11.610080</td>
      <td>103.122996</td>
      <td>0.0000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1000.0000</td>
    </tr>
    <tr>
      <th>542</th>
      <td>1567.0</td>
      <td>0.111206</td>
      <td>0.002736</td>
      <td>0.1053</td>
      <td>0.10960</td>
      <td>0.10960</td>
      <td>0.11340</td>
      <td>0.1184</td>
    </tr>
    <tr>
      <th>543</th>
      <td>1567.0</td>
      <td>0.008470</td>
      <td>0.001533</td>
      <td>0.0051</td>
      <td>0.00780</td>
      <td>0.00780</td>
      <td>0.00900</td>
      <td>0.0240</td>
    </tr>
    <tr>
      <th>544</th>
      <td>1567.0</td>
      <td>0.002509</td>
      <td>0.000295</td>
      <td>0.0016</td>
      <td>0.00240</td>
      <td>0.00260</td>
      <td>0.00260</td>
      <td>0.0047</td>
    </tr>
    <tr>
      <th>546</th>
      <td>1567.0</td>
      <td>1.018304</td>
      <td>0.358508</td>
      <td>0.4444</td>
      <td>0.81410</td>
      <td>0.91110</td>
      <td>1.21650</td>
      <td>3.9786</td>
    </tr>
    <tr>
      <th>547</th>
      <td>1567.0</td>
      <td>403.476047</td>
      <td>4.627143</td>
      <td>372.8220</td>
      <td>400.81400</td>
      <td>403.12200</td>
      <td>406.76300</td>
      <td>421.7020</td>
    </tr>
    <tr>
      <th>548</th>
      <td>1567.0</td>
      <td>75.415081</td>
      <td>3.152734</td>
      <td>71.0380</td>
      <td>73.25400</td>
      <td>74.08400</td>
      <td>76.96000</td>
      <td>83.7200</td>
    </tr>
    <tr>
      <th>550</th>
      <td>1567.0</td>
      <td>16.901595</td>
      <td>4.542840</td>
      <td>6.1100</td>
      <td>14.82000</td>
      <td>16.34000</td>
      <td>18.40500</td>
      <td>131.6800</td>
    </tr>
    <tr>
      <th>558</th>
      <td>1567.0</td>
      <td>0.994995</td>
      <td>0.083835</td>
      <td>0.8919</td>
      <td>0.95520</td>
      <td>0.97270</td>
      <td>1.00080</td>
      <td>1.5121</td>
    </tr>
    <tr>
      <th>559</th>
      <td>1567.0</td>
      <td>0.325686</td>
      <td>0.201329</td>
      <td>0.0699</td>
      <td>0.14995</td>
      <td>0.29090</td>
      <td>0.44360</td>
      <td>1.0737</td>
    </tr>
    <tr>
      <th>562</th>
      <td>1567.0</td>
      <td>262.998383</td>
      <td>6.958289</td>
      <td>242.2860</td>
      <td>262.10100</td>
      <td>264.27200</td>
      <td>264.73300</td>
      <td>311.4040</td>
    </tr>
    <tr>
      <th>563</th>
      <td>1567.0</td>
      <td>0.674651</td>
      <td>0.111169</td>
      <td>0.3049</td>
      <td>0.56710</td>
      <td>0.65100</td>
      <td>0.73825</td>
      <td>1.2988</td>
    </tr>
    <tr>
      <th>564</th>
      <td>1567.0</td>
      <td>6.221117</td>
      <td>2.442203</td>
      <td>0.9700</td>
      <td>4.98000</td>
      <td>5.16000</td>
      <td>7.31000</td>
      <td>32.5800</td>
    </tr>
    <tr>
      <th>565</th>
      <td>1567.0</td>
      <td>0.141069</td>
      <td>0.074373</td>
      <td>0.0224</td>
      <td>0.08770</td>
      <td>0.11955</td>
      <td>0.16685</td>
      <td>0.6892</td>
    </tr>
    <tr>
      <th>570</th>
      <td>1567.0</td>
      <td>530.523623</td>
      <td>17.499736</td>
      <td>317.1964</td>
      <td>530.70270</td>
      <td>532.39820</td>
      <td>534.35640</td>
      <td>589.5082</td>
    </tr>
    <tr>
      <th>571</th>
      <td>1567.0</td>
      <td>2.101836</td>
      <td>0.275112</td>
      <td>0.9802</td>
      <td>1.98290</td>
      <td>2.11860</td>
      <td>2.29065</td>
      <td>2.7395</td>
    </tr>
    <tr>
      <th>572</th>
      <td>1567.0</td>
      <td>28.450165</td>
      <td>86.304681</td>
      <td>3.5400</td>
      <td>7.50000</td>
      <td>8.65000</td>
      <td>10.13000</td>
      <td>454.5600</td>
    </tr>
    <tr>
      <th>582</th>
      <td>1567.0</td>
      <td>0.500096</td>
      <td>0.003403</td>
      <td>0.4778</td>
      <td>0.49790</td>
      <td>0.50020</td>
      <td>0.50235</td>
      <td>0.5098</td>
    </tr>
    <tr>
      <th>583</th>
      <td>1567.0</td>
      <td>0.015317</td>
      <td>0.017174</td>
      <td>0.0060</td>
      <td>0.01160</td>
      <td>0.01380</td>
      <td>0.01650</td>
      <td>0.4766</td>
    </tr>
    <tr>
      <th>586</th>
      <td>1567.0</td>
      <td>0.021458</td>
      <td>0.012354</td>
      <td>-0.0169</td>
      <td>0.01345</td>
      <td>0.02050</td>
      <td>0.02760</td>
      <td>0.1028</td>
    </tr>
    <tr>
      <th>587</th>
      <td>1567.0</td>
      <td>0.016474</td>
      <td>0.008805</td>
      <td>0.0032</td>
      <td>0.01060</td>
      <td>0.01480</td>
      <td>0.02030</td>
      <td>0.0799</td>
    </tr>
    <tr>
      <th>589</th>
      <td>1567.0</td>
      <td>99.652345</td>
      <td>93.864558</td>
      <td>0.0000</td>
      <td>44.36860</td>
      <td>71.90050</td>
      <td>114.74970</td>
      <td>737.3048</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[34]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train_df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>   
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[34]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2194.0</td>
      <td>5.788948e-16</td>
      <td>1.000228</td>
      <td>-3.317875</td>
      <td>-0.680283</td>
      <td>-0.136262</td>
      <td>0.543866</td>
      <td>4.341457</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2194.0</td>
      <td>1.521243e-15</td>
      <td>1.000228</td>
      <td>-4.399281</td>
      <td>-0.542260</td>
      <td>0.054060</td>
      <td>0.538736</td>
      <td>4.590805</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2194.0</td>
      <td>1.237956e-14</td>
      <td>1.000228</td>
      <td>-4.552223</td>
      <td>-0.759917</td>
      <td>-0.033931</td>
      <td>0.588571</td>
      <td>3.724207</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2194.0</td>
      <td>-2.276362e-16</td>
      <td>1.000228</td>
      <td>-1.709379</td>
      <td>-0.743939</td>
      <td>-0.236848</td>
      <td>0.488838</td>
      <td>5.956214</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2194.0</td>
      <td>8.653060e-18</td>
      <td>1.000228</td>
      <td>-0.049400</td>
      <td>-0.039004</td>
      <td>-0.030471</td>
      <td>-0.024130</td>
      <td>33.139103</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2194.0</td>
      <td>-6.924978e-16</td>
      <td>1.000228</td>
      <td>-3.326588</td>
      <td>-0.445971</td>
      <td>0.027301</td>
      <td>0.517347</td>
      <td>4.763205</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2194.0</td>
      <td>-1.525386e-15</td>
      <td>1.000228</td>
      <td>-21.956885</td>
      <td>-0.187450</td>
      <td>0.010291</td>
      <td>0.279937</td>
      <td>1.160781</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2194.0</td>
      <td>1.695190e-15</td>
      <td>1.000228</td>
      <td>-4.113134</td>
      <td>-0.657638</td>
      <td>-0.028553</td>
      <td>0.692957</td>
      <td>2.787422</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2194.0</td>
      <td>4.675689e-17</td>
      <td>1.000228</td>
      <td>-3.575289</td>
      <td>-0.605486</td>
      <td>0.010627</td>
      <td>0.557514</td>
      <td>5.306429</td>
    </tr>
    <tr>
      <th>9</th>
      <td>2194.0</td>
      <td>7.362691e-18</td>
      <td>1.000228</td>
      <td>-3.776959</td>
      <td>-0.635218</td>
      <td>0.039519</td>
      <td>0.619370</td>
      <td>5.490123</td>
    </tr>
    <tr>
      <th>10</th>
      <td>2194.0</td>
      <td>-3.446452e-15</td>
      <td>1.000228</td>
      <td>-26.541209</td>
      <td>-0.614520</td>
      <td>0.109016</td>
      <td>0.634440</td>
      <td>1.831719</td>
    </tr>
    <tr>
      <th>11</th>
      <td>2194.0</td>
      <td>1.002914e-15</td>
      <td>1.000228</td>
      <td>-6.797573</td>
      <td>-0.592897</td>
      <td>-0.102826</td>
      <td>0.767429</td>
      <td>9.914739</td>
    </tr>
    <tr>
      <th>12</th>
      <td>2194.0</td>
      <td>2.892450e-16</td>
      <td>1.000228</td>
      <td>-2.275035</td>
      <td>-0.751849</td>
      <td>0.016649</td>
      <td>0.696288</td>
      <td>3.912580</td>
    </tr>
    <tr>
      <th>13</th>
      <td>2194.0</td>
      <td>1.359027e-15</td>
      <td>1.000228</td>
      <td>-5.329464</td>
      <td>-0.419682</td>
      <td>-0.029693</td>
      <td>0.351441</td>
      <td>27.574395</td>
    </tr>
    <tr>
      <th>14</th>
      <td>2194.0</td>
      <td>2.504783e-15</td>
      <td>1.000228</td>
      <td>-10.238797</td>
      <td>-0.532742</td>
      <td>0.007969</td>
      <td>0.524973</td>
      <td>4.363118</td>
    </tr>
    <tr>
      <th>15</th>
      <td>2194.0</td>
      <td>2.766814e-15</td>
      <td>1.000228</td>
      <td>-6.769610</td>
      <td>-0.614417</td>
      <td>-0.164173</td>
      <td>0.749835</td>
      <td>9.657058</td>
    </tr>
    <tr>
      <th>16</th>
      <td>2194.0</td>
      <td>-3.223897e-15</td>
      <td>1.000228</td>
      <td>-14.749173</td>
      <td>-0.163522</td>
      <td>0.040789</td>
      <td>0.322986</td>
      <td>2.817611</td>
    </tr>
    <tr>
      <th>17</th>
      <td>2194.0</td>
      <td>-4.010769e-16</td>
      <td>1.000228</td>
      <td>-10.932438</td>
      <td>-0.604693</td>
      <td>0.051710</td>
      <td>0.669101</td>
      <td>3.213299</td>
    </tr>
    <tr>
      <th>18</th>
      <td>2194.0</td>
      <td>-5.033197e-16</td>
      <td>1.000228</td>
      <td>-2.021659</td>
      <td>-0.313541</td>
      <td>0.021001</td>
      <td>0.223908</td>
      <td>6.708483</td>
    </tr>
    <tr>
      <th>19</th>
      <td>2194.0</td>
      <td>-1.334899e-16</td>
      <td>1.000228</td>
      <td>-4.300106</td>
      <td>-0.456785</td>
      <td>-0.017734</td>
      <td>0.356972</td>
      <td>4.214688</td>
    </tr>
    <tr>
      <th>20</th>
      <td>2194.0</td>
      <td>-4.478338e-18</td>
      <td>1.000228</td>
      <td>-5.259198</td>
      <td>-0.414176</td>
      <td>0.126221</td>
      <td>0.642760</td>
      <td>5.417190</td>
    </tr>
    <tr>
      <th>21</th>
      <td>2194.0</td>
      <td>-4.548676e-16</td>
      <td>1.000228</td>
      <td>-5.348678</td>
      <td>0.164724</td>
      <td>0.429648</td>
      <td>0.528769</td>
      <td>0.881551</td>
    </tr>
    <tr>
      <th>22</th>
      <td>2194.0</td>
      <td>3.641876e-16</td>
      <td>1.000228</td>
      <td>-2.704752</td>
      <td>-0.569155</td>
      <td>-0.053123</td>
      <td>0.634392</td>
      <td>2.582616</td>
    </tr>
    <tr>
      <th>23</th>
      <td>2194.0</td>
      <td>-7.914261e-17</td>
      <td>1.000228</td>
      <td>-4.236385</td>
      <td>-0.759532</td>
      <td>0.013074</td>
      <td>0.702981</td>
      <td>2.827587</td>
    </tr>
    <tr>
      <th>24</th>
      <td>2194.0</td>
      <td>1.608407e-16</td>
      <td>1.000228</td>
      <td>-3.082948</td>
      <td>-0.526427</td>
      <td>-0.401108</td>
      <td>-0.222095</td>
      <td>2.305362</td>
    </tr>
    <tr>
      <th>25</th>
      <td>2194.0</td>
      <td>2.546833e-15</td>
      <td>1.000228</td>
      <td>-0.919015</td>
      <td>-0.380389</td>
      <td>-0.137299</td>
      <td>0.076931</td>
      <td>7.822077</td>
    </tr>
    <tr>
      <th>26</th>
      <td>2194.0</td>
      <td>6.672597e-16</td>
      <td>1.000228</td>
      <td>-0.778309</td>
      <td>-0.276718</td>
      <td>-0.183726</td>
      <td>0.000129</td>
      <td>7.014540</td>
    </tr>
    <tr>
      <th>27</th>
      <td>2194.0</td>
      <td>2.593793e-15</td>
      <td>1.000228</td>
      <td>-4.561817</td>
      <td>-0.542511</td>
      <td>-0.018224</td>
      <td>0.431949</td>
      <td>5.866623</td>
    </tr>
    <tr>
      <th>28</th>
      <td>2194.0</td>
      <td>-2.195438e-14</td>
      <td>1.000228</td>
      <td>-3.768379</td>
      <td>-0.513097</td>
      <td>-0.089933</td>
      <td>0.241878</td>
      <td>2.682508</td>
    </tr>
    <tr>
      <th>29</th>
      <td>2194.0</td>
      <td>3.273994e-16</td>
      <td>1.000228</td>
      <td>-2.475696</td>
      <td>0.011789</td>
      <td>0.494782</td>
      <td>0.563221</td>
      <td>0.818543</td>
    </tr>
    <tr>
      <th>30</th>
      <td>2194.0</td>
      <td>7.276667e-17</td>
      <td>1.000228</td>
      <td>-1.716256</td>
      <td>-0.381103</td>
      <td>-0.156454</td>
      <td>0.180264</td>
      <td>17.654802</td>
    </tr>
    <tr>
      <th>31</th>
      <td>2194.0</td>
      <td>-3.319536e-16</td>
      <td>1.000228</td>
      <td>-2.090494</td>
      <td>-0.741270</td>
      <td>-0.228676</td>
      <td>0.860742</td>
      <td>3.699374</td>
    </tr>
    <tr>
      <th>32</th>
      <td>2194.0</td>
      <td>-2.215557e-15</td>
      <td>1.000228</td>
      <td>-3.337912</td>
      <td>-0.692068</td>
      <td>-0.014549</td>
      <td>0.680787</td>
      <td>6.178799</td>
    </tr>
    <tr>
      <th>33</th>
      <td>2194.0</td>
      <td>6.107745e-16</td>
      <td>1.000228</td>
      <td>-3.449858</td>
      <td>-0.764456</td>
      <td>-0.042998</td>
      <td>0.618751</td>
      <td>3.943352</td>
    </tr>
    <tr>
      <th>34</th>
      <td>2194.0</td>
      <td>-9.219557e-16</td>
      <td>1.000228</td>
      <td>-2.471688</td>
      <td>-0.998139</td>
      <td>0.393372</td>
      <td>0.865693</td>
      <td>1.746032</td>
    </tr>
    <tr>
      <th>35</th>
      <td>2194.0</td>
      <td>-5.557187e-15</td>
      <td>1.000228</td>
      <td>-3.019890</td>
      <td>-0.647372</td>
      <td>0.012745</td>
      <td>0.669996</td>
      <td>5.099316</td>
    </tr>
    <tr>
      <th>36</th>
      <td>2194.0</td>
      <td>-1.275016e-14</td>
      <td>1.000228</td>
      <td>-12.392614</td>
      <td>-0.401412</td>
      <td>0.058105</td>
      <td>0.430095</td>
      <td>3.734240</td>
    </tr>
    <tr>
      <th>37</th>
      <td>2194.0</td>
      <td>6.176413e-15</td>
      <td>1.000228</td>
      <td>-1.955335</td>
      <td>-0.877995</td>
      <td>-0.052034</td>
      <td>0.702104</td>
      <td>2.892696</td>
    </tr>
    <tr>
      <th>38</th>
      <td>2194.0</td>
      <td>-1.631532e-15</td>
      <td>1.000228</td>
      <td>-8.905011</td>
      <td>-0.431901</td>
      <td>0.324921</td>
      <td>0.637521</td>
      <td>1.344985</td>
    </tr>
    <tr>
      <th>39</th>
      <td>2194.0</td>
      <td>5.012128e-14</td>
      <td>1.000228</td>
      <td>-4.000372</td>
      <td>-0.648336</td>
      <td>0.096560</td>
      <td>0.608677</td>
      <td>2.494197</td>
    </tr>
    <tr>
      <th>40</th>
      <td>2194.0</td>
      <td>8.912298e-15</td>
      <td>1.000228</td>
      <td>-4.638327</td>
      <td>-0.794304</td>
      <td>-0.128104</td>
      <td>0.829863</td>
      <td>2.991366</td>
    </tr>
    <tr>
      <th>41</th>
      <td>2194.0</td>
      <td>-7.873779e-17</td>
      <td>1.000228</td>
      <td>-3.294258</td>
      <td>-0.629851</td>
      <td>-0.294496</td>
      <td>0.377076</td>
      <td>15.535384</td>
    </tr>
    <tr>
      <th>42</th>
      <td>2194.0</td>
      <td>-4.067432e-15</td>
      <td>1.000228</td>
      <td>-3.048503</td>
      <td>-0.584095</td>
      <td>0.014616</td>
      <td>0.649355</td>
      <td>4.524203</td>
    </tr>
    <tr>
      <th>43</th>
      <td>2194.0</td>
      <td>-1.162344e-15</td>
      <td>1.000228</td>
      <td>-3.760381</td>
      <td>-0.536094</td>
      <td>-0.006174</td>
      <td>0.510171</td>
      <td>18.468054</td>
    </tr>
    <tr>
      <th>44</th>
      <td>2194.0</td>
      <td>6.993845e-18</td>
      <td>1.000228</td>
      <td>-1.765064</td>
      <td>-0.552816</td>
      <td>-0.178454</td>
      <td>0.360735</td>
      <td>23.670851</td>
    </tr>
    <tr>
      <th>45</th>
      <td>2194.0</td>
      <td>7.914261e-17</td>
      <td>1.000228</td>
      <td>-2.333936</td>
      <td>-0.675198</td>
      <td>-0.146969</td>
      <td>0.468112</td>
      <td>4.439086</td>
    </tr>
    <tr>
      <th>46</th>
      <td>2194.0</td>
      <td>-6.983171e-18</td>
      <td>1.000228</td>
      <td>-0.069304</td>
      <td>-0.068165</td>
      <td>-0.067916</td>
      <td>-0.067725</td>
      <td>17.201556</td>
    </tr>
    <tr>
      <th>47</th>
      <td>2194.0</td>
      <td>4.397020e-15</td>
      <td>1.000228</td>
      <td>-11.363856</td>
      <td>-0.400171</td>
      <td>0.041035</td>
      <td>0.487393</td>
      <td>3.940000</td>
    </tr>
    <tr>
      <th>48</th>
      <td>2194.0</td>
      <td>1.874324e-16</td>
      <td>1.000228</td>
      <td>-2.443582</td>
      <td>-0.531475</td>
      <td>-0.050037</td>
      <td>0.278915</td>
      <td>3.883048</td>
    </tr>
    <tr>
      <th>49</th>
      <td>2194.0</td>
      <td>4.254516e-16</td>
      <td>1.000228</td>
      <td>-0.021354</td>
      <td>-0.021354</td>
      <td>-0.021354</td>
      <td>-0.021354</td>
      <td>46.829478</td>
    </tr>
    <tr>
      <th>50</th>
      <td>2194.0</td>
      <td>-1.467478e-17</td>
      <td>1.000228</td>
      <td>-3.746250</td>
      <td>-0.546377</td>
      <td>0.014030</td>
      <td>0.513852</td>
      <td>9.598503</td>
    </tr>
    <tr>
      <th>51</th>
      <td>2194.0</td>
      <td>-2.656641e-17</td>
      <td>1.000228</td>
      <td>-4.597860</td>
      <td>-0.633605</td>
      <td>-0.010822</td>
      <td>0.623938</td>
      <td>3.142018</td>
    </tr>
    <tr>
      <th>52</th>
      <td>2194.0</td>
      <td>4.033034e-17</td>
      <td>1.000228</td>
      <td>-3.002400</td>
      <td>-0.710907</td>
      <td>-0.063043</td>
      <td>0.532251</td>
      <td>4.348313</td>
    </tr>
    <tr>
      <th>53</th>
      <td>2194.0</td>
      <td>3.567490e-17</td>
      <td>1.000228</td>
      <td>-6.754550</td>
      <td>-0.790235</td>
      <td>0.001353</td>
      <td>0.555720</td>
      <td>5.488666</td>
    </tr>
    <tr>
      <th>54</th>
      <td>2194.0</td>
      <td>-4.453037e-18</td>
      <td>1.000228</td>
      <td>-2.637438</td>
      <td>-0.673460</td>
      <td>-0.169266</td>
      <td>0.538885</td>
      <td>3.909845</td>
    </tr>
    <tr>
      <th>55</th>
      <td>2194.0</td>
      <td>2.087361e-17</td>
      <td>1.000228</td>
      <td>-2.291803</td>
      <td>-1.060862</td>
      <td>0.239454</td>
      <td>0.589611</td>
      <td>2.626335</td>
    </tr>
    <tr>
      <th>56</th>
      <td>2194.0</td>
      <td>6.588470e-17</td>
      <td>1.000228</td>
      <td>-4.479310</td>
      <td>-0.361715</td>
      <td>0.044891</td>
      <td>0.537399</td>
      <td>4.488916</td>
    </tr>
    <tr>
      <th>57</th>
      <td>2194.0</td>
      <td>6.020455e-17</td>
      <td>1.000228</td>
      <td>-3.696888</td>
      <td>-0.760411</td>
      <td>0.066174</td>
      <td>0.646184</td>
      <td>3.916097</td>
    </tr>
    <tr>
      <th>58</th>
      <td>2194.0</td>
      <td>-1.337429e-15</td>
      <td>1.000228</td>
      <td>-2.976134</td>
      <td>-0.618277</td>
      <td>-0.017028</td>
      <td>0.670532</td>
      <td>2.822647</td>
    </tr>
    <tr>
      <th>59</th>
      <td>2194.0</td>
      <td>-3.226111e-16</td>
      <td>1.000228</td>
      <td>-3.256312</td>
      <td>-0.710739</td>
      <td>-0.027780</td>
      <td>0.634483</td>
      <td>3.593971</td>
    </tr>
    <tr>
      <th>60</th>
      <td>2194.0</td>
      <td>-8.024973e-15</td>
      <td>1.000228</td>
      <td>-4.622857</td>
      <td>-0.709679</td>
      <td>0.117333</td>
      <td>0.670595</td>
      <td>4.163064</td>
    </tr>
    <tr>
      <th>61</th>
      <td>2194.0</td>
      <td>1.257044e-15</td>
      <td>1.000228</td>
      <td>-10.372585</td>
      <td>-0.439060</td>
      <td>0.308625</td>
      <td>0.424233</td>
      <td>0.585080</td>
    </tr>
    <tr>
      <th>62</th>
      <td>2194.0</td>
      <td>2.696415e-15</td>
      <td>1.000228</td>
      <td>-3.390159</td>
      <td>-0.568960</td>
      <td>-0.004629</td>
      <td>0.628715</td>
      <td>5.598967</td>
    </tr>
    <tr>
      <th>63</th>
      <td>2194.0</td>
      <td>-1.809999e-16</td>
      <td>1.000228</td>
      <td>-0.813545</td>
      <td>-0.304087</td>
      <td>-0.019296</td>
      <td>0.106236</td>
      <td>15.052673</td>
    </tr>
    <tr>
      <th>64</th>
      <td>2194.0</td>
      <td>-1.871287e-15</td>
      <td>1.000228</td>
      <td>-3.848354</td>
      <td>-0.631593</td>
      <td>0.002445</td>
      <td>0.583173</td>
      <td>5.359076</td>
    </tr>
    <tr>
      <th>65</th>
      <td>2194.0</td>
      <td>2.413748e-17</td>
      <td>1.000228</td>
      <td>-4.266187</td>
      <td>-0.502905</td>
      <td>-0.025524</td>
      <td>0.615281</td>
      <td>4.631598</td>
    </tr>
    <tr>
      <th>66</th>
      <td>2194.0</td>
      <td>-2.841341e-17</td>
      <td>1.000228</td>
      <td>-4.218170</td>
      <td>-0.544942</td>
      <td>-0.061623</td>
      <td>0.453918</td>
      <td>8.895896</td>
    </tr>
    <tr>
      <th>67</th>
      <td>2194.0</td>
      <td>2.636845e-17</td>
      <td>1.000228</td>
      <td>-5.692734</td>
      <td>-0.372075</td>
      <td>0.102379</td>
      <td>0.509054</td>
      <td>3.830230</td>
    </tr>
    <tr>
      <th>68</th>
      <td>2194.0</td>
      <td>3.091824e-17</td>
      <td>1.000228</td>
      <td>-11.654924</td>
      <td>-0.437026</td>
      <td>0.153390</td>
      <td>0.153390</td>
      <td>6.647962</td>
    </tr>
    <tr>
      <th>69</th>
      <td>2194.0</td>
      <td>-1.206368e-16</td>
      <td>1.000228</td>
      <td>-8.680023</td>
      <td>-0.615100</td>
      <td>0.281002</td>
      <td>0.281002</td>
      <td>7.449822</td>
    </tr>
    <tr>
      <th>70</th>
      <td>2194.0</td>
      <td>3.537128e-17</td>
      <td>1.000228</td>
      <td>-4.405853</td>
      <td>-0.446783</td>
      <td>-0.087718</td>
      <td>0.271346</td>
      <td>10.267851</td>
    </tr>
    <tr>
      <th>71</th>
      <td>2194.0</td>
      <td>1.988180e-16</td>
      <td>1.000228</td>
      <td>-7.590891</td>
      <td>-0.574496</td>
      <td>-0.073325</td>
      <td>0.427846</td>
      <td>5.690142</td>
    </tr>
    <tr>
      <th>72</th>
      <td>2194.0</td>
      <td>-8.931375e-18</td>
      <td>1.000228</td>
      <td>-7.803259</td>
      <td>-0.592747</td>
      <td>-0.041924</td>
      <td>0.435651</td>
      <td>4.402751</td>
    </tr>
    <tr>
      <th>73</th>
      <td>2194.0</td>
      <td>-2.133789e-16</td>
      <td>1.000228</td>
      <td>-7.044210</td>
      <td>-0.649311</td>
      <td>-0.089389</td>
      <td>0.676820</td>
      <td>4.478395</td>
    </tr>
    <tr>
      <th>74</th>
      <td>2194.0</td>
      <td>-2.732545e-17</td>
      <td>1.000228</td>
      <td>-5.780091</td>
      <td>-0.531136</td>
      <td>0.056300</td>
      <td>0.544062</td>
      <td>5.479475</td>
    </tr>
    <tr>
      <th>75</th>
      <td>2194.0</td>
      <td>-6.021720e-18</td>
      <td>1.000228</td>
      <td>-3.884405</td>
      <td>-0.606792</td>
      <td>0.017321</td>
      <td>0.566401</td>
      <td>4.714748</td>
    </tr>
    <tr>
      <th>76</th>
      <td>2194.0</td>
      <td>-2.287242e-17</td>
      <td>1.000228</td>
      <td>-7.991130</td>
      <td>-0.578355</td>
      <td>0.114488</td>
      <td>0.557561</td>
      <td>2.659983</td>
    </tr>
    <tr>
      <th>77</th>
      <td>2194.0</td>
      <td>2.074963e-16</td>
      <td>1.000228</td>
      <td>-0.128385</td>
      <td>-0.128385</td>
      <td>-0.128385</td>
      <td>-0.128385</td>
      <td>9.391683</td>
    </tr>
    <tr>
      <th>78</th>
      <td>2194.0</td>
      <td>-2.262041e-15</td>
      <td>1.000228</td>
      <td>-3.541344</td>
      <td>-0.487823</td>
      <td>0.090799</td>
      <td>0.595858</td>
      <td>3.686565</td>
    </tr>
    <tr>
      <th>79</th>
      <td>2194.0</td>
      <td>1.311394e-14</td>
      <td>1.000228</td>
      <td>-10.303727</td>
      <td>0.279947</td>
      <td>0.397071</td>
      <td>0.439661</td>
      <td>0.556784</td>
    </tr>
    <tr>
      <th>80</th>
      <td>2194.0</td>
      <td>-9.959305e-16</td>
      <td>1.000228</td>
      <td>-1.023707</td>
      <td>-0.100695</td>
      <td>0.013124</td>
      <td>0.112889</td>
      <td>45.807391</td>
    </tr>
    <tr>
      <th>81</th>
      <td>2194.0</td>
      <td>-9.553535e-15</td>
      <td>1.000228</td>
      <td>-7.989388</td>
      <td>-0.578107</td>
      <td>0.063190</td>
      <td>0.592095</td>
      <td>3.501073</td>
    </tr>
    <tr>
      <th>82</th>
      <td>2194.0</td>
      <td>-2.774596e-15</td>
      <td>1.000228</td>
      <td>-16.037953</td>
      <td>-0.758361</td>
      <td>-0.118575</td>
      <td>0.910100</td>
      <td>1.487162</td>
    </tr>
    <tr>
      <th>83</th>
      <td>2194.0</td>
      <td>-1.704299e-15</td>
      <td>1.000228</td>
      <td>-9.433215</td>
      <td>-0.593925</td>
      <td>0.006417</td>
      <td>0.563706</td>
      <td>9.670081</td>
    </tr>
    <tr>
      <th>84</th>
      <td>2194.0</td>
      <td>-8.553069e-15</td>
      <td>1.000228</td>
      <td>-3.583282</td>
      <td>-0.730477</td>
      <td>-0.119161</td>
      <td>0.695926</td>
      <td>2.631759</td>
    </tr>
    <tr>
      <th>85</th>
      <td>2194.0</td>
      <td>-6.957870e-17</td>
      <td>1.000228</td>
      <td>-2.528268</td>
      <td>-0.778116</td>
      <td>-0.149641</td>
      <td>0.639244</td>
      <td>3.718171</td>
    </tr>
    <tr>
      <th>86</th>
      <td>2194.0</td>
      <td>1.891529e-16</td>
      <td>1.000228</td>
      <td>-1.648517</td>
      <td>-0.696198</td>
      <td>-0.100062</td>
      <td>0.417340</td>
      <td>4.541554</td>
    </tr>
    <tr>
      <th>87</th>
      <td>2194.0</td>
      <td>5.696345e-16</td>
      <td>1.000228</td>
      <td>-13.103748</td>
      <td>-0.507915</td>
      <td>0.022866</td>
      <td>0.512817</td>
      <td>2.799257</td>
    </tr>
    <tr>
      <th>88</th>
      <td>2194.0</td>
      <td>-7.501849e-18</td>
      <td>1.000228</td>
      <td>-3.236881</td>
      <td>-0.278679</td>
      <td>0.252096</td>
      <td>0.517437</td>
      <td>2.594443</td>
    </tr>
    <tr>
      <th>89</th>
      <td>2194.0</td>
      <td>3.615977e-14</td>
      <td>1.000228</td>
      <td>-2.054129</td>
      <td>-0.682000</td>
      <td>-0.044940</td>
      <td>0.543116</td>
      <td>10.393046</td>
    </tr>
    <tr>
      <th>90</th>
      <td>2194.0</td>
      <td>7.600423e-15</td>
      <td>1.000228</td>
      <td>-2.297082</td>
      <td>-0.745726</td>
      <td>-0.094517</td>
      <td>0.794806</td>
      <td>2.775491</td>
    </tr>
    <tr>
      <th>91</th>
      <td>2194.0</td>
      <td>-1.096722e-14</td>
      <td>1.000228</td>
      <td>-3.868753</td>
      <td>-0.609136</td>
      <td>-0.020435</td>
      <td>0.799012</td>
      <td>2.553411</td>
    </tr>
    <tr>
      <th>92</th>
      <td>2194.0</td>
      <td>3.486272e-16</td>
      <td>1.000228</td>
      <td>-2.099950</td>
      <td>-0.705563</td>
      <td>-0.146359</td>
      <td>0.539911</td>
      <td>8.935937</td>
    </tr>
    <tr>
      <th>93</th>
      <td>2194.0</td>
      <td>9.540188e-18</td>
      <td>1.000228</td>
      <td>-0.920820</td>
      <td>-0.425618</td>
      <td>-0.154056</td>
      <td>0.149455</td>
      <td>14.031073</td>
    </tr>
    <tr>
      <th>94</th>
      <td>2194.0</td>
      <td>-3.599875e-16</td>
      <td>1.000228</td>
      <td>-1.918416</td>
      <td>-0.898570</td>
      <td>-0.108654</td>
      <td>0.799936</td>
      <td>2.867441</td>
    </tr>
    <tr>
      <th>95</th>
      <td>2194.0</td>
      <td>-3.219090e-16</td>
      <td>1.000228</td>
      <td>-2.130860</td>
      <td>-0.719485</td>
      <td>-0.017790</td>
      <td>0.754773</td>
      <td>4.167426</td>
    </tr>
    <tr>
      <th>96</th>
      <td>2194.0</td>
      <td>2.991125e-16</td>
      <td>1.000228</td>
      <td>-3.022715</td>
      <td>-0.517422</td>
      <td>-0.171190</td>
      <td>0.369549</td>
      <td>6.469373</td>
    </tr>
    <tr>
      <th>97</th>
      <td>2194.0</td>
      <td>-5.930635e-17</td>
      <td>1.000228</td>
      <td>-1.350009</td>
      <td>-0.625970</td>
      <td>-0.295474</td>
      <td>0.292154</td>
      <td>4.948770</td>
    </tr>
    <tr>
      <th>98</th>
      <td>2194.0</td>
      <td>-1.327815e-16</td>
      <td>1.000228</td>
      <td>-2.018984</td>
      <td>-0.621768</td>
      <td>-0.113496</td>
      <td>0.401156</td>
      <td>19.502819</td>
    </tr>
    <tr>
      <th>99</th>
      <td>2194.0</td>
      <td>-2.820088e-16</td>
      <td>1.000228</td>
      <td>-3.567739</td>
      <td>-0.748245</td>
      <td>-0.219590</td>
      <td>0.573392</td>
      <td>7.093472</td>
    </tr>
    <tr>
      <th>100</th>
      <td>2194.0</td>
      <td>1.751865e-16</td>
      <td>1.000228</td>
      <td>-1.233035</td>
      <td>-0.585514</td>
      <td>-0.217896</td>
      <td>0.165704</td>
      <td>7.911009</td>
    </tr>
    <tr>
      <th>101</th>
      <td>2194.0</td>
      <td>-8.147033e-18</td>
      <td>1.000228</td>
      <td>-1.761999</td>
      <td>-0.669463</td>
      <td>-0.151027</td>
      <td>0.451450</td>
      <td>8.248730</td>
    </tr>
    <tr>
      <th>102</th>
      <td>2194.0</td>
      <td>7.691609e-17</td>
      <td>1.000228</td>
      <td>-1.580215</td>
      <td>-0.635905</td>
      <td>-0.196928</td>
      <td>0.318614</td>
      <td>9.047105</td>
    </tr>
    <tr>
      <th>103</th>
      <td>2194.0</td>
      <td>5.475211e-17</td>
      <td>1.000228</td>
      <td>-1.638872</td>
      <td>-0.698055</td>
      <td>-0.260900</td>
      <td>0.392852</td>
      <td>4.874130</td>
    </tr>
    <tr>
      <th>104</th>
      <td>2194.0</td>
      <td>5.203063e-17</td>
      <td>1.000228</td>
      <td>-0.528610</td>
      <td>-0.215893</td>
      <td>-0.070077</td>
      <td>0.069331</td>
      <td>26.130788</td>
    </tr>
    <tr>
      <th>105</th>
      <td>2194.0</td>
      <td>-1.470767e-16</td>
      <td>1.000228</td>
      <td>-1.183131</td>
      <td>-0.720093</td>
      <td>-0.025537</td>
      <td>0.491976</td>
      <td>23.916238</td>
    </tr>
    <tr>
      <th>106</th>
      <td>2194.0</td>
      <td>5.649790e-17</td>
      <td>1.000228</td>
      <td>-1.108837</td>
      <td>-0.472554</td>
      <td>-0.213686</td>
      <td>0.233890</td>
      <td>28.440828</td>
    </tr>
    <tr>
      <th>107</th>
      <td>2194.0</td>
      <td>-3.633273e-17</td>
      <td>1.000228</td>
      <td>-0.816758</td>
      <td>-0.464770</td>
      <td>-0.282317</td>
      <td>-0.020142</td>
      <td>5.473016</td>
    </tr>
    <tr>
      <th>108</th>
      <td>2194.0</td>
      <td>-5.657697e-17</td>
      <td>1.000228</td>
      <td>-0.917554</td>
      <td>-0.451776</td>
      <td>-0.238057</td>
      <td>-0.014753</td>
      <td>5.004085</td>
    </tr>
    <tr>
      <th>109</th>
      <td>2194.0</td>
      <td>-1.958324e-17</td>
      <td>1.000228</td>
      <td>-1.044427</td>
      <td>-0.675684</td>
      <td>-0.267640</td>
      <td>0.226325</td>
      <td>8.963593</td>
    </tr>
    <tr>
      <th>110</th>
      <td>2194.0</td>
      <td>-1.773624e-17</td>
      <td>1.000228</td>
      <td>-0.753014</td>
      <td>-0.677455</td>
      <td>-0.461243</td>
      <td>0.362390</td>
      <td>5.277204</td>
    </tr>
    <tr>
      <th>111</th>
      <td>2194.0</td>
      <td>1.870781e-16</td>
      <td>1.000228</td>
      <td>-1.199902</td>
      <td>-0.453509</td>
      <td>-0.166435</td>
      <td>0.178054</td>
      <td>10.455311</td>
    </tr>
    <tr>
      <th>112</th>
      <td>2194.0</td>
      <td>1.115347e-16</td>
      <td>1.000228</td>
      <td>-1.239095</td>
      <td>-0.461434</td>
      <td>-0.072603</td>
      <td>0.316228</td>
      <td>19.498536</td>
    </tr>
    <tr>
      <th>113</th>
      <td>2194.0</td>
      <td>3.600887e-16</td>
      <td>1.000228</td>
      <td>-1.811372</td>
      <td>-0.675678</td>
      <td>-0.137718</td>
      <td>0.499864</td>
      <td>11.976344</td>
    </tr>
    <tr>
      <th>114</th>
      <td>2194.0</td>
      <td>-2.712304e-17</td>
      <td>1.000228</td>
      <td>-1.768168</td>
      <td>-0.846491</td>
      <td>0.034223</td>
      <td>0.705000</td>
      <td>3.848945</td>
    </tr>
    <tr>
      <th>115</th>
      <td>2194.0</td>
      <td>8.086310e-17</td>
      <td>1.000228</td>
      <td>-2.427326</td>
      <td>-0.739264</td>
      <td>-0.026533</td>
      <td>0.656260</td>
      <td>2.770593</td>
    </tr>
    <tr>
      <th>116</th>
      <td>2194.0</td>
      <td>2.493701e-16</td>
      <td>1.000228</td>
      <td>-1.952329</td>
      <td>-0.656779</td>
      <td>-0.085471</td>
      <td>0.429584</td>
      <td>6.575096</td>
    </tr>
    <tr>
      <th>117</th>
      <td>2194.0</td>
      <td>4.062384e-16</td>
      <td>1.000228</td>
      <td>-2.935237</td>
      <td>-0.568347</td>
      <td>0.016660</td>
      <td>0.762096</td>
      <td>3.328217</td>
    </tr>
    <tr>
      <th>118</th>
      <td>2194.0</td>
      <td>-1.760164e-15</td>
      <td>1.000228</td>
      <td>-3.544499</td>
      <td>-0.626334</td>
      <td>-0.026183</td>
      <td>0.555248</td>
      <td>3.178293</td>
    </tr>
    <tr>
      <th>119</th>
      <td>2194.0</td>
      <td>7.500331e-16</td>
      <td>1.000228</td>
      <td>-2.613084</td>
      <td>-0.718133</td>
      <td>-0.007526</td>
      <td>0.628087</td>
      <td>3.528131</td>
    </tr>
    <tr>
      <th>120</th>
      <td>2194.0</td>
      <td>-7.531198e-16</td>
      <td>1.000228</td>
      <td>-2.277351</td>
      <td>-0.629279</td>
      <td>-0.072734</td>
      <td>0.680555</td>
      <td>2.676299</td>
    </tr>
    <tr>
      <th>121</th>
      <td>2194.0</td>
      <td>9.503185e-17</td>
      <td>1.000228</td>
      <td>-1.220369</td>
      <td>-0.578681</td>
      <td>-0.364785</td>
      <td>0.091526</td>
      <td>5.235724</td>
    </tr>
    <tr>
      <th>122</th>
      <td>2194.0</td>
      <td>5.721140e-16</td>
      <td>1.000228</td>
      <td>-2.974103</td>
      <td>-0.665215</td>
      <td>-0.083946</td>
      <td>0.643046</td>
      <td>9.742589</td>
    </tr>
    <tr>
      <th>123</th>
      <td>2194.0</td>
      <td>-1.709517e-16</td>
      <td>1.000228</td>
      <td>-1.865439</td>
      <td>-0.742468</td>
      <td>-0.101958</td>
      <td>0.606286</td>
      <td>14.457454</td>
    </tr>
    <tr>
      <th>124</th>
      <td>2194.0</td>
      <td>5.428404e-16</td>
      <td>1.000228</td>
      <td>-1.966924</td>
      <td>-0.733762</td>
      <td>-0.147810</td>
      <td>0.645223</td>
      <td>11.837567</td>
    </tr>
    <tr>
      <th>125</th>
      <td>2194.0</td>
      <td>5.586537e-17</td>
      <td>1.000228</td>
      <td>-3.397618</td>
      <td>-0.797405</td>
      <td>0.214343</td>
      <td>0.719757</td>
      <td>6.985100</td>
    </tr>
    <tr>
      <th>126</th>
      <td>2194.0</td>
      <td>2.265988e-16</td>
      <td>1.000228</td>
      <td>-1.151961</td>
      <td>-0.430659</td>
      <td>-0.142732</td>
      <td>0.245129</td>
      <td>30.496287</td>
    </tr>
    <tr>
      <th>127</th>
      <td>2194.0</td>
      <td>-2.727485e-17</td>
      <td>1.000228</td>
      <td>-1.756583</td>
      <td>-0.815443</td>
      <td>-0.066852</td>
      <td>0.571446</td>
      <td>6.756976</td>
    </tr>
    <tr>
      <th>128</th>
      <td>2194.0</td>
      <td>-6.965460e-17</td>
      <td>1.000228</td>
      <td>-0.489764</td>
      <td>-0.139400</td>
      <td>-0.074684</td>
      <td>0.001191</td>
      <td>10.128247</td>
    </tr>
    <tr>
      <th>129</th>
      <td>2194.0</td>
      <td>1.496828e-16</td>
      <td>1.000228</td>
      <td>-1.506127</td>
      <td>-0.337387</td>
      <td>-0.070275</td>
      <td>0.243508</td>
      <td>16.412237</td>
    </tr>
    <tr>
      <th>130</th>
      <td>2194.0</td>
      <td>2.945077e-17</td>
      <td>1.000228</td>
      <td>-1.477156</td>
      <td>-0.550154</td>
      <td>-0.218692</td>
      <td>0.251183</td>
      <td>21.566754</td>
    </tr>
    <tr>
      <th>131</th>
      <td>2194.0</td>
      <td>-2.126831e-16</td>
      <td>1.000228</td>
      <td>-2.422044</td>
      <td>-0.592485</td>
      <td>0.022388</td>
      <td>0.660430</td>
      <td>3.448330</td>
    </tr>
    <tr>
      <th>132</th>
      <td>2194.0</td>
      <td>5.589067e-17</td>
      <td>1.000228</td>
      <td>-1.111152</td>
      <td>-0.515057</td>
      <td>-0.245009</td>
      <td>0.236383</td>
      <td>7.604470</td>
    </tr>
    <tr>
      <th>133</th>
      <td>2194.0</td>
      <td>5.748466e-17</td>
      <td>1.000228</td>
      <td>-1.696269</td>
      <td>-0.468163</td>
      <td>-0.192238</td>
      <td>0.192143</td>
      <td>8.402910</td>
    </tr>
    <tr>
      <th>134</th>
      <td>2194.0</td>
      <td>-1.943143e-16</td>
      <td>1.000228</td>
      <td>-1.479332</td>
      <td>-0.661578</td>
      <td>-0.335470</td>
      <td>0.509597</td>
      <td>17.954729</td>
    </tr>
    <tr>
      <th>135</th>
      <td>2194.0</td>
      <td>5.957202e-17</td>
      <td>1.000228</td>
      <td>-1.014653</td>
      <td>-0.402577</td>
      <td>-0.135519</td>
      <td>0.181300</td>
      <td>20.217253</td>
    </tr>
    <tr>
      <th>136</th>
      <td>2194.0</td>
      <td>-4.156378e-17</td>
      <td>1.000228</td>
      <td>-1.283080</td>
      <td>-0.508262</td>
      <td>-0.115487</td>
      <td>0.272995</td>
      <td>14.633956</td>
    </tr>
    <tr>
      <th>137</th>
      <td>2194.0</td>
      <td>1.267091e-16</td>
      <td>1.000228</td>
      <td>-2.455899</td>
      <td>-0.619590</td>
      <td>0.047320</td>
      <td>0.580848</td>
      <td>8.791807</td>
    </tr>
    <tr>
      <th>138</th>
      <td>2194.0</td>
      <td>-2.651581e-17</td>
      <td>1.000228</td>
      <td>-2.766396</td>
      <td>-0.568492</td>
      <td>-0.028383</td>
      <td>0.556506</td>
      <td>7.216933</td>
    </tr>
    <tr>
      <th>139</th>
      <td>2194.0</td>
      <td>-2.651581e-17</td>
      <td>1.000228</td>
      <td>-1.539500</td>
      <td>-0.612290</td>
      <td>-0.263697</td>
      <td>0.428747</td>
      <td>14.265756</td>
    </tr>
    <tr>
      <th>140</th>
      <td>2194.0</td>
      <td>2.179964e-16</td>
      <td>1.000228</td>
      <td>-2.305560</td>
      <td>-0.727063</td>
      <td>-0.107191</td>
      <td>0.575102</td>
      <td>4.275483</td>
    </tr>
    <tr>
      <th>141</th>
      <td>2194.0</td>
      <td>-1.052283e-16</td>
      <td>1.000228</td>
      <td>-1.723667</td>
      <td>-0.640175</td>
      <td>-0.166147</td>
      <td>0.375599</td>
      <td>8.840377</td>
    </tr>
    <tr>
      <th>142</th>
      <td>2194.0</td>
      <td>4.153469e-16</td>
      <td>1.000228</td>
      <td>-1.544242</td>
      <td>-0.799674</td>
      <td>-0.088781</td>
      <td>0.569731</td>
      <td>6.331711</td>
    </tr>
    <tr>
      <th>143</th>
      <td>2194.0</td>
      <td>2.163265e-17</td>
      <td>1.000228</td>
      <td>-0.152129</td>
      <td>-0.140865</td>
      <td>-0.129600</td>
      <td>-0.083416</td>
      <td>11.008750</td>
    </tr>
    <tr>
      <th>144</th>
      <td>2194.0</td>
      <td>1.731118e-16</td>
      <td>1.000228</td>
      <td>-1.779081</td>
      <td>-0.517167</td>
      <td>-0.063069</td>
      <td>0.437273</td>
      <td>32.553305</td>
    </tr>
    <tr>
      <th>145</th>
      <td>2194.0</td>
      <td>-1.603931e-16</td>
      <td>1.000228</td>
      <td>-1.966004</td>
      <td>-0.651278</td>
      <td>-0.213912</td>
      <td>0.412264</td>
      <td>5.559085</td>
    </tr>
    <tr>
      <th>146</th>
      <td>2194.0</td>
      <td>-5.417018e-17</td>
      <td>1.000228</td>
      <td>-1.126959</td>
      <td>-0.518946</td>
      <td>-0.246941</td>
      <td>0.177069</td>
      <td>10.705293</td>
    </tr>
    <tr>
      <th>147</th>
      <td>2194.0</td>
      <td>-6.515096e-17</td>
      <td>1.000228</td>
      <td>-1.050223</td>
      <td>-0.518788</td>
      <td>-0.253070</td>
      <td>0.249895</td>
      <td>18.508498</td>
    </tr>
    <tr>
      <th>148</th>
      <td>2194.0</td>
      <td>9.918127e-18</td>
      <td>1.000228</td>
      <td>-2.049055</td>
      <td>-0.649079</td>
      <td>-0.143933</td>
      <td>0.548839</td>
      <td>11.286800</td>
    </tr>
    <tr>
      <th>149</th>
      <td>2194.0</td>
      <td>-1.140079e-16</td>
      <td>1.000228</td>
      <td>-1.962502</td>
      <td>-0.586070</td>
      <td>-0.148114</td>
      <td>0.414971</td>
      <td>11.926948</td>
    </tr>
    <tr>
      <th>150</th>
      <td>2194.0</td>
      <td>2.208807e-16</td>
      <td>1.000228</td>
      <td>-1.706693</td>
      <td>-0.623585</td>
      <td>-0.131595</td>
      <td>0.444682</td>
      <td>19.035589</td>
    </tr>
    <tr>
      <th>151</th>
      <td>2194.0</td>
      <td>3.581405e-17</td>
      <td>1.000228</td>
      <td>-0.120579</td>
      <td>-0.106974</td>
      <td>-0.096770</td>
      <td>-0.086567</td>
      <td>33.572458</td>
    </tr>
    <tr>
      <th>152</th>
      <td>2194.0</td>
      <td>-1.677100e-16</td>
      <td>1.000228</td>
      <td>-1.025282</td>
      <td>-0.387510</td>
      <td>-0.130566</td>
      <td>0.190615</td>
      <td>19.154023</td>
    </tr>
    <tr>
      <th>153</th>
      <td>2194.0</td>
      <td>6.069793e-17</td>
      <td>1.000228</td>
      <td>-2.414753</td>
      <td>-0.783292</td>
      <td>-0.022989</td>
      <td>0.622321</td>
      <td>4.542314</td>
    </tr>
    <tr>
      <th>154</th>
      <td>2194.0</td>
      <td>3.821009e-16</td>
      <td>1.000228</td>
      <td>-1.608524</td>
      <td>-0.972208</td>
      <td>0.009721</td>
      <td>0.710637</td>
      <td>2.848917</td>
    </tr>
    <tr>
      <th>155</th>
      <td>2194.0</td>
      <td>-2.649051e-16</td>
      <td>1.000228</td>
      <td>-1.817495</td>
      <td>-0.842162</td>
      <td>-0.216955</td>
      <td>0.725184</td>
      <td>2.848664</td>
    </tr>
    <tr>
      <th>156</th>
      <td>2194.0</td>
      <td>-1.559069e-16</td>
      <td>1.000228</td>
      <td>-2.096666</td>
      <td>-0.710952</td>
      <td>-0.095721</td>
      <td>0.496963</td>
      <td>5.315245</td>
    </tr>
    <tr>
      <th>157</th>
      <td>2194.0</td>
      <td>8.845350e-17</td>
      <td>1.000228</td>
      <td>-1.057830</td>
      <td>-1.057830</td>
      <td>-0.099840</td>
      <td>0.683457</td>
      <td>2.402380</td>
    </tr>
    <tr>
      <th>158</th>
      <td>2194.0</td>
      <td>1.979577e-16</td>
      <td>1.000228</td>
      <td>-1.014199</td>
      <td>-1.014199</td>
      <td>-0.007414</td>
      <td>0.791701</td>
      <td>2.107307</td>
    </tr>
    <tr>
      <th>159</th>
      <td>2194.0</td>
      <td>8.668241e-17</td>
      <td>1.000228</td>
      <td>-1.723441</td>
      <td>-0.683069</td>
      <td>-0.278605</td>
      <td>0.549946</td>
      <td>10.608134</td>
    </tr>
    <tr>
      <th>160</th>
      <td>2194.0</td>
      <td>-1.139826e-16</td>
      <td>1.000228</td>
      <td>-0.860489</td>
      <td>-0.586710</td>
      <td>-0.339348</td>
      <td>0.171051</td>
      <td>7.647334</td>
    </tr>
    <tr>
      <th>161</th>
      <td>2194.0</td>
      <td>-5.404367e-17</td>
      <td>1.000228</td>
      <td>-0.909059</td>
      <td>-0.867240</td>
      <td>-0.285144</td>
      <td>0.415882</td>
      <td>3.172550</td>
    </tr>
    <tr>
      <th>162</th>
      <td>2194.0</td>
      <td>-1.025970e-16</td>
      <td>1.000228</td>
      <td>-1.336052</td>
      <td>-0.475452</td>
      <td>-0.157814</td>
      <td>0.268443</td>
      <td>19.103647</td>
    </tr>
    <tr>
      <th>163</th>
      <td>2194.0</td>
      <td>8.389926e-17</td>
      <td>1.000228</td>
      <td>-1.633675</td>
      <td>-0.658614</td>
      <td>-0.278395</td>
      <td>0.472951</td>
      <td>7.311566</td>
    </tr>
    <tr>
      <th>164</th>
      <td>2194.0</td>
      <td>-2.692063e-17</td>
      <td>1.000228</td>
      <td>-0.889621</td>
      <td>-0.749303</td>
      <td>-0.388207</td>
      <td>0.405611</td>
      <td>3.450791</td>
    </tr>
    <tr>
      <th>165</th>
      <td>2194.0</td>
      <td>3.926769e-17</td>
      <td>1.000228</td>
      <td>-2.648596</td>
      <td>-0.693006</td>
      <td>-0.015308</td>
      <td>0.651986</td>
      <td>7.349617</td>
    </tr>
    <tr>
      <th>166</th>
      <td>2194.0</td>
      <td>-3.411886e-17</td>
      <td>1.000228</td>
      <td>-1.311472</td>
      <td>-0.564329</td>
      <td>-0.273325</td>
      <td>0.283704</td>
      <td>16.162206</td>
    </tr>
    <tr>
      <th>167</th>
      <td>2194.0</td>
      <td>4.527929e-16</td>
      <td>1.000228</td>
      <td>-1.109917</td>
      <td>-1.109917</td>
      <td>-0.092794</td>
      <td>0.713752</td>
      <td>2.447038</td>
    </tr>
    <tr>
      <th>168</th>
      <td>2194.0</td>
      <td>5.303162e-17</td>
      <td>1.000228</td>
      <td>-1.132982</td>
      <td>-0.638322</td>
      <td>-0.267473</td>
      <td>0.330843</td>
      <td>4.739949</td>
    </tr>
    <tr>
      <th>169</th>
      <td>2194.0</td>
      <td>1.597843e-16</td>
      <td>1.000228</td>
      <td>-1.002200</td>
      <td>-0.654587</td>
      <td>-0.312842</td>
      <td>0.395796</td>
      <td>3.949625</td>
    </tr>
    <tr>
      <th>170</th>
      <td>2194.0</td>
      <td>-2.226518e-17</td>
      <td>1.000228</td>
      <td>-0.902646</td>
      <td>-0.656604</td>
      <td>-0.371591</td>
      <td>0.297826</td>
      <td>3.753302</td>
    </tr>
    <tr>
      <th>171</th>
      <td>2194.0</td>
      <td>2.722425e-17</td>
      <td>1.000228</td>
      <td>-1.142204</td>
      <td>-1.142204</td>
      <td>-0.190854</td>
      <td>0.668321</td>
      <td>2.517876</td>
    </tr>
    <tr>
      <th>172</th>
      <td>2194.0</td>
      <td>4.819906e-17</td>
      <td>1.000228</td>
      <td>-0.887463</td>
      <td>-0.614237</td>
      <td>-0.464505</td>
      <td>0.497745</td>
      <td>2.937577</td>
    </tr>
    <tr>
      <th>173</th>
      <td>2194.0</td>
      <td>1.805504e-16</td>
      <td>1.000228</td>
      <td>-1.294781</td>
      <td>-0.980161</td>
      <td>0.035022</td>
      <td>0.655616</td>
      <td>2.613527</td>
    </tr>
    <tr>
      <th>174</th>
      <td>2194.0</td>
      <td>1.619286e-16</td>
      <td>1.000228</td>
      <td>-1.221308</td>
      <td>-0.686209</td>
      <td>-0.226447</td>
      <td>0.393966</td>
      <td>3.160511</td>
    </tr>
    <tr>
      <th>175</th>
      <td>2194.0</td>
      <td>6.942689e-17</td>
      <td>1.000228</td>
      <td>-0.746360</td>
      <td>-0.746360</td>
      <td>-0.746360</td>
      <td>0.825867</td>
      <td>2.357846</td>
    </tr>
    <tr>
      <th>176</th>
      <td>2194.0</td>
      <td>1.138561e-16</td>
      <td>1.000228</td>
      <td>-0.740204</td>
      <td>-0.740204</td>
      <td>-0.740204</td>
      <td>0.747247</td>
      <td>2.232419</td>
    </tr>
    <tr>
      <th>177</th>
      <td>2194.0</td>
      <td>-6.537868e-17</td>
      <td>1.000228</td>
      <td>-1.349572</td>
      <td>-0.562671</td>
      <td>-0.270185</td>
      <td>0.124802</td>
      <td>8.175457</td>
    </tr>
    <tr>
      <th>178</th>
      <td>2194.0</td>
      <td>-6.193453e-17</td>
      <td>1.000228</td>
      <td>-0.896319</td>
      <td>-0.896319</td>
      <td>-0.691461</td>
      <td>0.988376</td>
      <td>2.034152</td>
    </tr>
    <tr>
      <th>179</th>
      <td>2194.0</td>
      <td>-6.527747e-16</td>
      <td>1.000228</td>
      <td>-0.140396</td>
      <td>-0.140396</td>
      <td>-0.140396</td>
      <td>-0.140396</td>
      <td>7.898866</td>
    </tr>
    <tr>
      <th>180</th>
      <td>2194.0</td>
      <td>-5.705858e-15</td>
      <td>1.000228</td>
      <td>-2.243642</td>
      <td>-0.537110</td>
      <td>-0.537110</td>
      <td>0.613807</td>
      <td>2.955327</td>
    </tr>
    <tr>
      <th>181</th>
      <td>2194.0</td>
      <td>-3.305368e-16</td>
      <td>1.000228</td>
      <td>-2.407297</td>
      <td>-0.398175</td>
      <td>-0.398175</td>
      <td>0.048297</td>
      <td>11.656558</td>
    </tr>
    <tr>
      <th>182</th>
      <td>2194.0</td>
      <td>8.762362e-16</td>
      <td>1.000228</td>
      <td>-3.377761</td>
      <td>-0.451191</td>
      <td>0.280452</td>
      <td>0.280452</td>
      <td>7.962699</td>
    </tr>
    <tr>
      <th>183</th>
      <td>2194.0</td>
      <td>-6.391120e-17</td>
      <td>1.000228</td>
      <td>-1.621381</td>
      <td>-0.606436</td>
      <td>-0.302341</td>
      <td>0.477658</td>
      <td>8.372954</td>
    </tr>
    <tr>
      <th>184</th>
      <td>2194.0</td>
      <td>7.210074e-15</td>
      <td>1.000228</td>
      <td>-7.194216</td>
      <td>-0.697835</td>
      <td>-0.162195</td>
      <td>0.714023</td>
      <td>4.149850</td>
    </tr>
    <tr>
      <th>185</th>
      <td>2194.0</td>
      <td>-5.192057e-16</td>
      <td>1.000228</td>
      <td>-1.363359</td>
      <td>-0.670686</td>
      <td>-0.411246</td>
      <td>0.463504</td>
      <td>2.600757</td>
    </tr>
    <tr>
      <th>186</th>
      <td>2194.0</td>
      <td>1.811070e-16</td>
      <td>1.000228</td>
      <td>-1.668507</td>
      <td>-0.423216</td>
      <td>-0.155523</td>
      <td>0.222722</td>
      <td>16.902883</td>
    </tr>
    <tr>
      <th>187</th>
      <td>2194.0</td>
      <td>3.774961e-17</td>
      <td>1.000228</td>
      <td>-1.096079</td>
      <td>-0.447228</td>
      <td>-0.240170</td>
      <td>0.004815</td>
      <td>5.261223</td>
    </tr>
    <tr>
      <th>188</th>
      <td>2194.0</td>
      <td>-2.306471e-16</td>
      <td>1.000228</td>
      <td>-1.277654</td>
      <td>-0.842064</td>
      <td>-0.189910</td>
      <td>0.623068</td>
      <td>3.662967</td>
    </tr>
    <tr>
      <th>189</th>
      <td>2194.0</td>
      <td>3.056833e-15</td>
      <td>1.000228</td>
      <td>-2.887032</td>
      <td>-0.089554</td>
      <td>0.250035</td>
      <td>0.256385</td>
      <td>5.293657</td>
    </tr>
    <tr>
      <th>190</th>
      <td>2194.0</td>
      <td>4.693906e-16</td>
      <td>1.000228</td>
      <td>-3.794341</td>
      <td>-0.862554</td>
      <td>-0.226196</td>
      <td>0.475887</td>
      <td>6.452349</td>
    </tr>
    <tr>
      <th>191</th>
      <td>2194.0</td>
      <td>-1.487719e-17</td>
      <td>1.000228</td>
      <td>-2.167262</td>
      <td>-0.535293</td>
      <td>-0.462037</td>
      <td>0.485197</td>
      <td>10.697214</td>
    </tr>
    <tr>
      <th>192</th>
      <td>2194.0</td>
      <td>8.957941e-17</td>
      <td>1.000228</td>
      <td>-1.416675</td>
      <td>-0.675988</td>
      <td>-0.302714</td>
      <td>0.260419</td>
      <td>5.584696</td>
    </tr>
    <tr>
      <th>193</th>
      <td>2194.0</td>
      <td>-1.475113e-15</td>
      <td>1.000228</td>
      <td>-11.081199</td>
      <td>0.016802</td>
      <td>0.085720</td>
      <td>0.201461</td>
      <td>3.058086</td>
    </tr>
    <tr>
      <th>194</th>
      <td>2194.0</td>
      <td>-3.828347e-16</td>
      <td>1.000228</td>
      <td>-3.483387</td>
      <td>-0.369614</td>
      <td>0.171020</td>
      <td>0.631444</td>
      <td>2.028850</td>
    </tr>
    <tr>
      <th>195</th>
      <td>2194.0</td>
      <td>-5.566296e-18</td>
      <td>1.000228</td>
      <td>-0.270755</td>
      <td>-0.220065</td>
      <td>-0.203126</td>
      <td>-0.182546</td>
      <td>5.698335</td>
    </tr>
    <tr>
      <th>196</th>
      <td>2194.0</td>
      <td>1.365868e-14</td>
      <td>1.000228</td>
      <td>-6.281437</td>
      <td>-0.630429</td>
      <td>0.037671</td>
      <td>0.622258</td>
      <td>2.626556</td>
    </tr>
    <tr>
      <th>197</th>
      <td>2194.0</td>
      <td>-6.184281e-17</td>
      <td>1.000228</td>
      <td>-0.813403</td>
      <td>-0.332476</td>
      <td>-0.131361</td>
      <td>0.139707</td>
      <td>39.881784</td>
    </tr>
    <tr>
      <th>198</th>
      <td>2194.0</td>
      <td>-1.282272e-16</td>
      <td>1.000228</td>
      <td>-3.187290</td>
      <td>-0.608299</td>
      <td>-0.074139</td>
      <td>0.541397</td>
      <td>6.803173</td>
    </tr>
    <tr>
      <th>199</th>
      <td>2194.0</td>
      <td>-3.518911e-16</td>
      <td>1.000228</td>
      <td>-1.609150</td>
      <td>-0.718013</td>
      <td>-0.139353</td>
      <td>0.482707</td>
      <td>7.267498</td>
    </tr>
    <tr>
      <th>200</th>
      <td>2194.0</td>
      <td>-6.730158e-17</td>
      <td>1.000228</td>
      <td>-1.127377</td>
      <td>-0.618918</td>
      <td>-0.286974</td>
      <td>0.188515</td>
      <td>7.078848</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[35]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">X_test_df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>    
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[35]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>732.0</td>
      <td>7.276890e-15</td>
      <td>1.000684</td>
      <td>-2.185075</td>
      <td>-0.659571</td>
      <td>-0.144246</td>
      <td>0.533917</td>
      <td>3.427766</td>
    </tr>
    <tr>
      <th>1</th>
      <td>732.0</td>
      <td>-6.716698e-16</td>
      <td>1.000684</td>
      <td>-3.090055</td>
      <td>-0.560818</td>
      <td>0.045983</td>
      <td>0.546470</td>
      <td>3.668981</td>
    </tr>
    <tr>
      <th>2</th>
      <td>732.0</td>
      <td>-5.575382e-16</td>
      <td>1.000684</td>
      <td>-4.851195</td>
      <td>-0.750024</td>
      <td>-0.061808</td>
      <td>0.562047</td>
      <td>4.019677</td>
    </tr>
    <tr>
      <th>3</th>
      <td>732.0</td>
      <td>1.158757e-16</td>
      <td>1.000684</td>
      <td>-3.415972</td>
      <td>-0.726693</td>
      <td>-0.267616</td>
      <td>0.578864</td>
      <td>5.684693</td>
    </tr>
    <tr>
      <th>4</th>
      <td>732.0</td>
      <td>-3.036240e-17</td>
      <td>1.000684</td>
      <td>-0.063052</td>
      <td>-0.057284</td>
      <td>-0.052366</td>
      <td>-0.049429</td>
      <td>19.107991</td>
    </tr>
    <tr>
      <th>5</th>
      <td>732.0</td>
      <td>2.201866e-15</td>
      <td>1.000684</td>
      <td>-3.425556</td>
      <td>-0.467071</td>
      <td>0.062325</td>
      <td>0.522790</td>
      <td>3.391505</td>
    </tr>
    <tr>
      <th>6</th>
      <td>732.0</td>
      <td>6.291643e-16</td>
      <td>1.000684</td>
      <td>-13.222649</td>
      <td>-0.083778</td>
      <td>0.057500</td>
      <td>0.220513</td>
      <td>0.644348</td>
    </tr>
    <tr>
      <th>7</th>
      <td>732.0</td>
      <td>6.819075e-16</td>
      <td>1.000684</td>
      <td>-3.435507</td>
      <td>-0.743003</td>
      <td>0.003613</td>
      <td>0.735872</td>
      <td>2.897130</td>
    </tr>
    <tr>
      <th>8</th>
      <td>732.0</td>
      <td>2.896893e-17</td>
      <td>1.000684</td>
      <td>-3.135579</td>
      <td>-0.560479</td>
      <td>0.004618</td>
      <td>0.634048</td>
      <td>3.082219</td>
    </tr>
    <tr>
      <th>9</th>
      <td>732.0</td>
      <td>1.171649e-17</td>
      <td>1.000684</td>
      <td>-3.497506</td>
      <td>-0.617140</td>
      <td>0.036029</td>
      <td>0.624951</td>
      <td>4.212024</td>
    </tr>
    <tr>
      <th>10</th>
      <td>732.0</td>
      <td>-8.576170e-16</td>
      <td>1.000684</td>
      <td>-8.180336</td>
      <td>-0.708438</td>
      <td>0.135470</td>
      <td>0.716249</td>
      <td>1.974998</td>
    </tr>
    <tr>
      <th>11</th>
      <td>732.0</td>
      <td>-1.629139e-14</td>
      <td>1.000684</td>
      <td>-2.380168</td>
      <td>-0.433949</td>
      <td>-0.085992</td>
      <td>0.526910</td>
      <td>19.553673</td>
    </tr>
    <tr>
      <th>12</th>
      <td>732.0</td>
      <td>3.190185e-16</td>
      <td>1.000684</td>
      <td>-2.345378</td>
      <td>-0.690853</td>
      <td>0.034786</td>
      <td>0.672560</td>
      <td>3.411210</td>
    </tr>
    <tr>
      <th>13</th>
      <td>732.0</td>
      <td>-2.762931e-15</td>
      <td>1.000684</td>
      <td>-2.551107</td>
      <td>-0.715358</td>
      <td>-0.095542</td>
      <td>0.776909</td>
      <td>4.256996</td>
    </tr>
    <tr>
      <th>14</th>
      <td>732.0</td>
      <td>1.034672e-16</td>
      <td>1.000684</td>
      <td>-0.570599</td>
      <td>-0.118443</td>
      <td>-0.038858</td>
      <td>0.035118</td>
      <td>26.776608</td>
    </tr>
    <tr>
      <th>15</th>
      <td>732.0</td>
      <td>1.578974e-14</td>
      <td>1.000684</td>
      <td>-7.874998</td>
      <td>-0.546228</td>
      <td>-0.110609</td>
      <td>0.769809</td>
      <td>3.911860</td>
    </tr>
    <tr>
      <th>16</th>
      <td>732.0</td>
      <td>-1.501304e-14</td>
      <td>1.000684</td>
      <td>-21.977564</td>
      <td>-0.308487</td>
      <td>0.001682</td>
      <td>0.409513</td>
      <td>3.458809</td>
    </tr>
    <tr>
      <th>17</th>
      <td>732.0</td>
      <td>-2.991232e-15</td>
      <td>1.000684</td>
      <td>-12.389230</td>
      <td>-0.493644</td>
      <td>0.046314</td>
      <td>0.582138</td>
      <td>2.520199</td>
    </tr>
    <tr>
      <th>18</th>
      <td>732.0</td>
      <td>-1.656993e-17</td>
      <td>1.000684</td>
      <td>-2.047355</td>
      <td>-0.310341</td>
      <td>0.014714</td>
      <td>0.216412</td>
      <td>5.071232</td>
    </tr>
    <tr>
      <th>19</th>
      <td>732.0</td>
      <td>-4.299650e-16</td>
      <td>1.000684</td>
      <td>-3.876199</td>
      <td>-0.426771</td>
      <td>-0.047622</td>
      <td>0.334715</td>
      <td>4.430227</td>
    </tr>
    <tr>
      <th>20</th>
      <td>732.0</td>
      <td>-2.938603e-17</td>
      <td>1.000684</td>
      <td>-5.614495</td>
      <td>-0.449679</td>
      <td>0.130314</td>
      <td>0.607380</td>
      <td>5.569701</td>
    </tr>
    <tr>
      <th>21</th>
      <td>732.0</td>
      <td>4.606971e-16</td>
      <td>1.000684</td>
      <td>-4.443771</td>
      <td>0.250044</td>
      <td>0.412176</td>
      <td>0.511085</td>
      <td>0.868856</td>
    </tr>
    <tr>
      <th>22</th>
      <td>732.0</td>
      <td>2.496485e-16</td>
      <td>1.000684</td>
      <td>-2.639376</td>
      <td>-0.587840</td>
      <td>-0.057983</td>
      <td>0.600969</td>
      <td>2.581156</td>
    </tr>
    <tr>
      <th>23</th>
      <td>732.0</td>
      <td>-3.608225e-16</td>
      <td>1.000684</td>
      <td>-3.637893</td>
      <td>-0.733707</td>
      <td>0.050431</td>
      <td>0.718516</td>
      <td>2.606198</td>
    </tr>
    <tr>
      <th>24</th>
      <td>732.0</td>
      <td>6.699256e-16</td>
      <td>1.000684</td>
      <td>-2.938705</td>
      <td>-0.516527</td>
      <td>-0.405051</td>
      <td>-0.227947</td>
      <td>2.335483</td>
    </tr>
    <tr>
      <th>25</th>
      <td>732.0</td>
      <td>-2.254420e-15</td>
      <td>1.000684</td>
      <td>-0.911798</td>
      <td>-0.361151</td>
      <td>-0.125767</td>
      <td>0.074081</td>
      <td>7.669562</td>
    </tr>
    <tr>
      <th>26</th>
      <td>732.0</td>
      <td>-2.054747e-16</td>
      <td>1.000684</td>
      <td>-0.799442</td>
      <td>-0.267178</td>
      <td>-0.179685</td>
      <td>0.000155</td>
      <td>7.286988</td>
    </tr>
    <tr>
      <th>27</th>
      <td>732.0</td>
      <td>-1.081284e-14</td>
      <td>1.000684</td>
      <td>-4.275707</td>
      <td>-0.655529</td>
      <td>0.027309</td>
      <td>0.430981</td>
      <td>3.154671</td>
    </tr>
    <tr>
      <th>28</th>
      <td>732.0</td>
      <td>-4.406008e-17</td>
      <td>1.000684</td>
      <td>-3.775545</td>
      <td>-0.538025</td>
      <td>-0.033807</td>
      <td>0.297125</td>
      <td>2.775740</td>
    </tr>
    <tr>
      <th>29</th>
      <td>732.0</td>
      <td>-1.354411e-16</td>
      <td>1.000684</td>
      <td>-2.457605</td>
      <td>0.021847</td>
      <td>0.491472</td>
      <td>0.564731</td>
      <td>0.763685</td>
    </tr>
    <tr>
      <th>30</th>
      <td>732.0</td>
      <td>1.921657e-16</td>
      <td>1.000684</td>
      <td>-2.105704</td>
      <td>-0.422285</td>
      <td>-0.175847</td>
      <td>0.254348</td>
      <td>21.104615</td>
    </tr>
    <tr>
      <th>31</th>
      <td>732.0</td>
      <td>-3.463835e-15</td>
      <td>1.000684</td>
      <td>-2.037386</td>
      <td>-0.737076</td>
      <td>-0.248721</td>
      <td>0.850270</td>
      <td>3.481195</td>
    </tr>
    <tr>
      <th>32</th>
      <td>732.0</td>
      <td>6.074945e-16</td>
      <td>1.000684</td>
      <td>-3.112456</td>
      <td>-0.686249</td>
      <td>-0.043301</td>
      <td>0.696652</td>
      <td>3.555083</td>
    </tr>
    <tr>
      <th>33</th>
      <td>732.0</td>
      <td>-2.534611e-15</td>
      <td>1.000684</td>
      <td>-2.302729</td>
      <td>-0.773028</td>
      <td>-0.072406</td>
      <td>0.645383</td>
      <td>4.593731</td>
    </tr>
    <tr>
      <th>34</th>
      <td>732.0</td>
      <td>2.457051e-16</td>
      <td>1.000684</td>
      <td>-3.763727</td>
      <td>-0.926510</td>
      <td>0.373915</td>
      <td>0.837536</td>
      <td>1.754579</td>
    </tr>
    <tr>
      <th>35</th>
      <td>732.0</td>
      <td>-5.362286e-15</td>
      <td>1.000684</td>
      <td>-2.455293</td>
      <td>-0.723254</td>
      <td>0.017568</td>
      <td>0.642579</td>
      <td>2.312778</td>
    </tr>
    <tr>
      <th>36</th>
      <td>732.0</td>
      <td>6.394399e-16</td>
      <td>1.000684</td>
      <td>-18.744682</td>
      <td>-0.391638</td>
      <td>0.051876</td>
      <td>0.395072</td>
      <td>3.536631</td>
    </tr>
    <tr>
      <th>37</th>
      <td>732.0</td>
      <td>6.977570e-16</td>
      <td>1.000684</td>
      <td>-1.918338</td>
      <td>-0.831044</td>
      <td>-0.094489</td>
      <td>0.650834</td>
      <td>2.781581</td>
    </tr>
    <tr>
      <th>38</th>
      <td>732.0</td>
      <td>5.837619e-15</td>
      <td>1.000684</td>
      <td>-9.155001</td>
      <td>-0.463451</td>
      <td>0.312580</td>
      <td>0.657483</td>
      <td>1.399024</td>
    </tr>
    <tr>
      <th>39</th>
      <td>732.0</td>
      <td>9.530097e-15</td>
      <td>1.000684</td>
      <td>-3.166583</td>
      <td>-0.618171</td>
      <td>0.061406</td>
      <td>0.595359</td>
      <td>2.318571</td>
    </tr>
    <tr>
      <th>40</th>
      <td>732.0</td>
      <td>1.405372e-15</td>
      <td>1.000684</td>
      <td>-4.640294</td>
      <td>-0.780281</td>
      <td>-0.140817</td>
      <td>0.811653</td>
      <td>2.996589</td>
    </tr>
    <tr>
      <th>41</th>
      <td>732.0</td>
      <td>1.698702e-17</td>
      <td>1.000684</td>
      <td>-2.552386</td>
      <td>-0.619970</td>
      <td>-0.332963</td>
      <td>0.427176</td>
      <td>7.823519</td>
    </tr>
    <tr>
      <th>42</th>
      <td>732.0</td>
      <td>2.775558e-17</td>
      <td>1.000684</td>
      <td>-3.009798</td>
      <td>-0.506307</td>
      <td>0.038131</td>
      <td>0.570059</td>
      <td>4.168450</td>
    </tr>
    <tr>
      <th>43</th>
      <td>732.0</td>
      <td>1.286686e-15</td>
      <td>1.000684</td>
      <td>-3.193962</td>
      <td>-0.497806</td>
      <td>-0.006672</td>
      <td>0.499370</td>
      <td>7.652856</td>
    </tr>
    <tr>
      <th>44</th>
      <td>732.0</td>
      <td>-2.442642e-16</td>
      <td>1.000684</td>
      <td>-1.556241</td>
      <td>-0.612820</td>
      <td>-0.217335</td>
      <td>0.367036</td>
      <td>10.373750</td>
    </tr>
    <tr>
      <th>45</th>
      <td>732.0</td>
      <td>-8.148461e-17</td>
      <td>1.000684</td>
      <td>-2.397078</td>
      <td>-0.687895</td>
      <td>-0.182378</td>
      <td>0.477407</td>
      <td>3.362541</td>
    </tr>
    <tr>
      <th>46</th>
      <td>732.0</td>
      <td>1.630450e-18</td>
      <td>1.000684</td>
      <td>-0.084019</td>
      <td>-0.083166</td>
      <td>-0.082982</td>
      <td>-0.082787</td>
      <td>12.058247</td>
    </tr>
    <tr>
      <th>47</th>
      <td>732.0</td>
      <td>-1.777570e-16</td>
      <td>1.000684</td>
      <td>-10.212381</td>
      <td>-0.362795</td>
      <td>0.059622</td>
      <td>0.492789</td>
      <td>1.947710</td>
    </tr>
    <tr>
      <th>48</th>
      <td>732.0</td>
      <td>-1.771503e-16</td>
      <td>1.000684</td>
      <td>-2.395015</td>
      <td>-0.543190</td>
      <td>-0.096982</td>
      <td>0.247487</td>
      <td>3.745319</td>
    </tr>
    <tr>
      <th>49</th>
      <td>732.0</td>
      <td>0.000000e+00</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50</th>
      <td>732.0</td>
      <td>1.846580e-17</td>
      <td>1.000684</td>
      <td>-3.212554</td>
      <td>-0.492907</td>
      <td>-0.009342</td>
      <td>0.415806</td>
      <td>7.704981</td>
    </tr>
    <tr>
      <th>51</th>
      <td>732.0</td>
      <td>5.353944e-17</td>
      <td>1.000684</td>
      <td>-4.505535</td>
      <td>-0.519077</td>
      <td>0.014948</td>
      <td>0.611411</td>
      <td>2.543304</td>
    </tr>
    <tr>
      <th>52</th>
      <td>732.0</td>
      <td>-1.941374e-17</td>
      <td>1.000684</td>
      <td>-2.568568</td>
      <td>-0.692804</td>
      <td>-0.106580</td>
      <td>0.489659</td>
      <td>3.162718</td>
    </tr>
    <tr>
      <th>53</th>
      <td>732.0</td>
      <td>-3.185066e-18</td>
      <td>1.000684</td>
      <td>-2.036358</td>
      <td>-0.804255</td>
      <td>0.042950</td>
      <td>0.511707</td>
      <td>3.208141</td>
    </tr>
    <tr>
      <th>54</th>
      <td>732.0</td>
      <td>2.305381e-17</td>
      <td>1.000684</td>
      <td>-1.872838</td>
      <td>-0.687628</td>
      <td>-0.183502</td>
      <td>0.518158</td>
      <td>3.732216</td>
    </tr>
    <tr>
      <th>55</th>
      <td>732.0</td>
      <td>1.236109e-17</td>
      <td>1.000684</td>
      <td>-2.088963</td>
      <td>-1.074517</td>
      <td>0.217898</td>
      <td>0.553180</td>
      <td>2.513295</td>
    </tr>
    <tr>
      <th>56</th>
      <td>732.0</td>
      <td>-1.918623e-17</td>
      <td>1.000684</td>
      <td>-3.711700</td>
      <td>-0.334124</td>
      <td>0.044040</td>
      <td>0.582794</td>
      <td>2.924304</td>
    </tr>
    <tr>
      <th>57</th>
      <td>732.0</td>
      <td>-7.280151e-18</td>
      <td>1.000684</td>
      <td>-6.041211</td>
      <td>-0.663720</td>
      <td>0.058424</td>
      <td>0.598654</td>
      <td>3.589211</td>
    </tr>
    <tr>
      <th>58</th>
      <td>732.0</td>
      <td>5.770515e-16</td>
      <td>1.000684</td>
      <td>-2.982964</td>
      <td>-0.625841</td>
      <td>0.004367</td>
      <td>0.594927</td>
      <td>3.013489</td>
    </tr>
    <tr>
      <th>59</th>
      <td>732.0</td>
      <td>-2.019029e-15</td>
      <td>1.000684</td>
      <td>-3.291545</td>
      <td>-0.700361</td>
      <td>-0.104019</td>
      <td>0.677031</td>
      <td>2.830194</td>
    </tr>
    <tr>
      <th>60</th>
      <td>732.0</td>
      <td>3.254796e-15</td>
      <td>1.000684</td>
      <td>-3.516490</td>
      <td>-0.707048</td>
      <td>0.062082</td>
      <td>0.692537</td>
      <td>3.910299</td>
    </tr>
    <tr>
      <th>61</th>
      <td>732.0</td>
      <td>-4.406615e-15</td>
      <td>1.000684</td>
      <td>-14.329567</td>
      <td>-0.590495</td>
      <td>0.341553</td>
      <td>0.500346</td>
      <td>0.762700</td>
    </tr>
    <tr>
      <th>62</th>
      <td>732.0</td>
      <td>-3.627942e-16</td>
      <td>1.000684</td>
      <td>-3.341563</td>
      <td>-0.608094</td>
      <td>-0.015574</td>
      <td>0.628301</td>
      <td>2.724604</td>
    </tr>
    <tr>
      <th>63</th>
      <td>732.0</td>
      <td>-1.920140e-16</td>
      <td>1.000684</td>
      <td>-0.972258</td>
      <td>-0.294237</td>
      <td>-0.024131</td>
      <td>0.116740</td>
      <td>14.643658</td>
    </tr>
    <tr>
      <th>64</th>
      <td>732.0</td>
      <td>1.465434e-15</td>
      <td>1.000684</td>
      <td>-3.923223</td>
      <td>-0.630289</td>
      <td>-0.003250</td>
      <td>0.537239</td>
      <td>4.443237</td>
    </tr>
    <tr>
      <th>65</th>
      <td>732.0</td>
      <td>7.735160e-18</td>
      <td>1.000684</td>
      <td>-4.246654</td>
      <td>-0.407436</td>
      <td>0.022406</td>
      <td>0.592886</td>
      <td>3.398897</td>
    </tr>
    <tr>
      <th>66</th>
      <td>732.0</td>
      <td>1.137524e-17</td>
      <td>1.000684</td>
      <td>-3.499116</td>
      <td>-0.575995</td>
      <td>-0.067626</td>
      <td>0.440743</td>
      <td>5.270247</td>
    </tr>
    <tr>
      <th>67</th>
      <td>732.0</td>
      <td>2.199212e-18</td>
      <td>1.000684</td>
      <td>-4.343274</td>
      <td>-0.404920</td>
      <td>0.011329</td>
      <td>0.459597</td>
      <td>4.365932</td>
    </tr>
    <tr>
      <th>68</th>
      <td>732.0</td>
      <td>-5.088522e-17</td>
      <td>1.000684</td>
      <td>-6.006282</td>
      <td>-0.469004</td>
      <td>0.223155</td>
      <td>0.223155</td>
      <td>5.760433</td>
    </tr>
    <tr>
      <th>69</th>
      <td>732.0</td>
      <td>-3.354936e-16</td>
      <td>1.000684</td>
      <td>-4.484188</td>
      <td>-0.723173</td>
      <td>0.217080</td>
      <td>0.217080</td>
      <td>5.858602</td>
    </tr>
    <tr>
      <th>70</th>
      <td>732.0</td>
      <td>-1.719556e-17</td>
      <td>1.000684</td>
      <td>-6.548086</td>
      <td>-0.473827</td>
      <td>-0.050757</td>
      <td>0.329576</td>
      <td>10.838388</td>
    </tr>
    <tr>
      <th>71</th>
      <td>732.0</td>
      <td>-1.213358e-17</td>
      <td>1.000684</td>
      <td>-5.858533</td>
      <td>-0.553864</td>
      <td>-0.048657</td>
      <td>0.456549</td>
      <td>5.508615</td>
    </tr>
    <tr>
      <th>72</th>
      <td>732.0</td>
      <td>-3.511156e-17</td>
      <td>1.000684</td>
      <td>-4.374931</td>
      <td>-0.567847</td>
      <td>-0.050930</td>
      <td>0.483243</td>
      <td>4.662676</td>
    </tr>
    <tr>
      <th>73</th>
      <td>732.0</td>
      <td>-9.737202e-17</td>
      <td>1.000684</td>
      <td>-2.105933</td>
      <td>-0.652182</td>
      <td>-0.107957</td>
      <td>0.667377</td>
      <td>8.718921</td>
    </tr>
    <tr>
      <th>74</th>
      <td>732.0</td>
      <td>-1.334694e-17</td>
      <td>1.000684</td>
      <td>-5.724386</td>
      <td>-0.540771</td>
      <td>0.093822</td>
      <td>0.574776</td>
      <td>3.093104</td>
    </tr>
    <tr>
      <th>75</th>
      <td>732.0</td>
      <td>-4.095085e-18</td>
      <td>1.000684</td>
      <td>-2.608984</td>
      <td>-0.598263</td>
      <td>-0.071736</td>
      <td>0.521888</td>
      <td>3.695334</td>
    </tr>
    <tr>
      <th>76</th>
      <td>732.0</td>
      <td>-9.529414e-16</td>
      <td>1.000684</td>
      <td>-7.242029</td>
      <td>-0.500005</td>
      <td>0.185692</td>
      <td>0.554303</td>
      <td>2.405288</td>
    </tr>
    <tr>
      <th>77</th>
      <td>732.0</td>
      <td>-2.478285e-16</td>
      <td>1.000684</td>
      <td>-0.149048</td>
      <td>-0.149048</td>
      <td>-0.149048</td>
      <td>-0.149048</td>
      <td>12.171939</td>
    </tr>
    <tr>
      <th>78</th>
      <td>732.0</td>
      <td>1.031639e-16</td>
      <td>1.000684</td>
      <td>-4.043446</td>
      <td>-0.437831</td>
      <td>0.078679</td>
      <td>0.601231</td>
      <td>3.514551</td>
    </tr>
    <tr>
      <th>79</th>
      <td>732.0</td>
      <td>1.354336e-14</td>
      <td>1.000684</td>
      <td>-7.983195</td>
      <td>0.260041</td>
      <td>0.391407</td>
      <td>0.435196</td>
      <td>0.599404</td>
    </tr>
    <tr>
      <th>80</th>
      <td>732.0</td>
      <td>-5.788478e-16</td>
      <td>1.000684</td>
      <td>-4.268388</td>
      <td>-0.375433</td>
      <td>0.140430</td>
      <td>0.657114</td>
      <td>2.793325</td>
    </tr>
    <tr>
      <th>81</th>
      <td>732.0</td>
      <td>4.349587e-15</td>
      <td>1.000684</td>
      <td>-9.224776</td>
      <td>-0.570810</td>
      <td>0.065893</td>
      <td>0.585650</td>
      <td>3.028512</td>
    </tr>
    <tr>
      <th>82</th>
      <td>732.0</td>
      <td>1.016157e-14</td>
      <td>1.000684</td>
      <td>-14.960777</td>
      <td>-0.709055</td>
      <td>-0.097442</td>
      <td>0.883445</td>
      <td>1.310420</td>
    </tr>
    <tr>
      <th>83</th>
      <td>732.0</td>
      <td>6.285197e-15</td>
      <td>1.000684</td>
      <td>-4.488376</td>
      <td>-0.624212</td>
      <td>0.057147</td>
      <td>0.565816</td>
      <td>3.748231</td>
    </tr>
    <tr>
      <th>84</th>
      <td>732.0</td>
      <td>8.796849e-15</td>
      <td>1.000684</td>
      <td>-3.581480</td>
      <td>-0.616514</td>
      <td>-0.105313</td>
      <td>0.610368</td>
      <td>2.655173</td>
    </tr>
    <tr>
      <th>85</th>
      <td>732.0</td>
      <td>-1.242176e-16</td>
      <td>1.000684</td>
      <td>-2.573434</td>
      <td>-0.778604</td>
      <td>-0.169188</td>
      <td>0.587265</td>
      <td>3.767408</td>
    </tr>
    <tr>
      <th>86</th>
      <td>732.0</td>
      <td>1.269419e-15</td>
      <td>1.000684</td>
      <td>-1.739294</td>
      <td>-0.719423</td>
      <td>-0.078420</td>
      <td>0.488857</td>
      <td>5.022981</td>
    </tr>
    <tr>
      <th>87</th>
      <td>732.0</td>
      <td>-2.536223e-15</td>
      <td>1.000684</td>
      <td>-2.473528</td>
      <td>-0.671204</td>
      <td>0.022616</td>
      <td>0.645310</td>
      <td>3.699732</td>
    </tr>
    <tr>
      <th>88</th>
      <td>732.0</td>
      <td>-4.580428e-17</td>
      <td>1.000684</td>
      <td>-3.012376</td>
      <td>-0.209860</td>
      <td>0.292981</td>
      <td>0.544357</td>
      <td>2.512052</td>
    </tr>
    <tr>
      <th>89</th>
      <td>732.0</td>
      <td>1.999969e-14</td>
      <td>1.000684</td>
      <td>-1.845192</td>
      <td>-0.587676</td>
      <td>-0.067325</td>
      <td>0.409664</td>
      <td>9.125549</td>
    </tr>
    <tr>
      <th>90</th>
      <td>732.0</td>
      <td>1.647134e-16</td>
      <td>1.000684</td>
      <td>-2.232987</td>
      <td>-0.716892</td>
      <td>-0.063832</td>
      <td>0.779630</td>
      <td>2.770839</td>
    </tr>
    <tr>
      <th>91</th>
      <td>732.0</td>
      <td>3.715653e-14</td>
      <td>1.000684</td>
      <td>-3.026880</td>
      <td>-0.602058</td>
      <td>-0.003985</td>
      <td>0.805272</td>
      <td>2.570626</td>
    </tr>
    <tr>
      <th>92</th>
      <td>732.0</td>
      <td>1.007088e-15</td>
      <td>1.000684</td>
      <td>-2.013700</td>
      <td>-0.751895</td>
      <td>-0.153074</td>
      <td>0.523045</td>
      <td>8.564228</td>
    </tr>
    <tr>
      <th>93</th>
      <td>732.0</td>
      <td>2.066501e-17</td>
      <td>1.000684</td>
      <td>-1.074865</td>
      <td>-0.491660</td>
      <td>-0.161178</td>
      <td>0.213045</td>
      <td>10.103221</td>
    </tr>
    <tr>
      <th>94</th>
      <td>732.0</td>
      <td>3.178241e-16</td>
      <td>1.000684</td>
      <td>-1.816204</td>
      <td>-0.935039</td>
      <td>-0.042791</td>
      <td>0.770485</td>
      <td>2.861173</td>
    </tr>
    <tr>
      <th>95</th>
      <td>732.0</td>
      <td>-4.010150e-16</td>
      <td>1.000684</td>
      <td>-2.045929</td>
      <td>-0.784083</td>
      <td>0.038519</td>
      <td>0.746408</td>
      <td>3.174015</td>
    </tr>
    <tr>
      <th>96</th>
      <td>732.0</td>
      <td>6.830189e-16</td>
      <td>1.000684</td>
      <td>-3.212951</td>
      <td>-0.617010</td>
      <td>-0.213757</td>
      <td>0.508730</td>
      <td>4.524474</td>
    </tr>
    <tr>
      <th>97</th>
      <td>732.0</td>
      <td>7.477322e-17</td>
      <td>1.000684</td>
      <td>-1.620439</td>
      <td>-0.560350</td>
      <td>-0.318274</td>
      <td>0.314667</td>
      <td>5.143995</td>
    </tr>
    <tr>
      <th>98</th>
      <td>732.0</td>
      <td>1.115200e-16</td>
      <td>1.000684</td>
      <td>-1.150203</td>
      <td>-0.370684</td>
      <td>-0.104578</td>
      <td>0.165639</td>
      <td>22.718225</td>
    </tr>
    <tr>
      <th>99</th>
      <td>732.0</td>
      <td>-1.680502e-16</td>
      <td>1.000684</td>
      <td>-3.561399</td>
      <td>-0.767247</td>
      <td>-0.156026</td>
      <td>0.629829</td>
      <td>7.003990</td>
    </tr>
    <tr>
      <th>100</th>
      <td>732.0</td>
      <td>-2.910544e-16</td>
      <td>1.000684</td>
      <td>-1.401795</td>
      <td>-0.580015</td>
      <td>-0.193899</td>
      <td>0.192645</td>
      <td>5.854124</td>
    </tr>
    <tr>
      <th>101</th>
      <td>732.0</td>
      <td>-2.944859e-16</td>
      <td>1.000684</td>
      <td>-1.811842</td>
      <td>-0.661575</td>
      <td>-0.126165</td>
      <td>0.375853</td>
      <td>8.094964</td>
    </tr>
    <tr>
      <th>102</th>
      <td>732.0</td>
      <td>-2.875660e-16</td>
      <td>1.000684</td>
      <td>-1.506428</td>
      <td>-0.601184</td>
      <td>-0.189710</td>
      <td>0.320776</td>
      <td>10.045720</td>
    </tr>
    <tr>
      <th>103</th>
      <td>732.0</td>
      <td>9.024354e-17</td>
      <td>1.000684</td>
      <td>-1.610976</td>
      <td>-0.710332</td>
      <td>-0.290053</td>
      <td>0.497854</td>
      <td>4.075785</td>
    </tr>
    <tr>
      <th>104</th>
      <td>732.0</td>
      <td>2.051334e-17</td>
      <td>1.000684</td>
      <td>-0.363210</td>
      <td>-0.197859</td>
      <td>-0.084565</td>
      <td>0.022617</td>
      <td>15.055099</td>
    </tr>
    <tr>
      <th>105</th>
      <td>732.0</td>
      <td>5.293276e-17</td>
      <td>1.000684</td>
      <td>-0.799593</td>
      <td>-0.542823</td>
      <td>-0.064700</td>
      <td>0.254048</td>
      <td>20.034165</td>
    </tr>
    <tr>
      <th>106</th>
      <td>732.0</td>
      <td>3.715910e-17</td>
      <td>1.000684</td>
      <td>-0.479430</td>
      <td>-0.248446</td>
      <td>-0.141689</td>
      <td>0.038827</td>
      <td>20.746782</td>
    </tr>
    <tr>
      <th>107</th>
      <td>732.0</td>
      <td>3.814496e-17</td>
      <td>1.000684</td>
      <td>-0.800093</td>
      <td>-0.478590</td>
      <td>-0.307135</td>
      <td>0.011251</td>
      <td>5.547678</td>
    </tr>
    <tr>
      <th>108</th>
      <td>732.0</td>
      <td>4.095085e-17</td>
      <td>1.000684</td>
      <td>-0.834243</td>
      <td>-0.452966</td>
      <td>-0.228359</td>
      <td>-0.022816</td>
      <td>4.890459</td>
    </tr>
    <tr>
      <th>109</th>
      <td>732.0</td>
      <td>-4.474259e-17</td>
      <td>1.000684</td>
      <td>-0.988733</td>
      <td>-0.636606</td>
      <td>-0.301653</td>
      <td>0.189673</td>
      <td>6.282106</td>
    </tr>
    <tr>
      <th>110</th>
      <td>732.0</td>
      <td>4.550094e-17</td>
      <td>1.000684</td>
      <td>-0.772857</td>
      <td>-0.686351</td>
      <td>-0.466765</td>
      <td>0.368430</td>
      <td>5.352990</td>
    </tr>
    <tr>
      <th>111</th>
      <td>732.0</td>
      <td>2.191629e-16</td>
      <td>1.000684</td>
      <td>-0.951776</td>
      <td>-0.442947</td>
      <td>-0.160264</td>
      <td>0.178955</td>
      <td>10.298997</td>
    </tr>
    <tr>
      <th>112</th>
      <td>732.0</td>
      <td>-2.070293e-17</td>
      <td>1.000684</td>
      <td>-1.202954</td>
      <td>-0.518011</td>
      <td>-0.107046</td>
      <td>0.303920</td>
      <td>9.756126</td>
    </tr>
    <tr>
      <th>113</th>
      <td>732.0</td>
      <td>3.006854e-16</td>
      <td>1.000684</td>
      <td>-1.576970</td>
      <td>-0.683686</td>
      <td>-0.139731</td>
      <td>0.499041</td>
      <td>4.770832</td>
    </tr>
    <tr>
      <th>114</th>
      <td>732.0</td>
      <td>1.607700e-17</td>
      <td>1.000684</td>
      <td>-1.720467</td>
      <td>-0.814756</td>
      <td>0.077998</td>
      <td>0.710312</td>
      <td>3.125542</td>
    </tr>
    <tr>
      <th>115</th>
      <td>732.0</td>
      <td>2.068776e-16</td>
      <td>1.000684</td>
      <td>-2.328501</td>
      <td>-0.660873</td>
      <td>-0.030029</td>
      <td>0.668798</td>
      <td>2.827384</td>
    </tr>
    <tr>
      <th>116</th>
      <td>732.0</td>
      <td>-2.032375e-16</td>
      <td>1.000684</td>
      <td>-1.848083</td>
      <td>-0.642807</td>
      <td>-0.142179</td>
      <td>0.372378</td>
      <td>6.024311</td>
    </tr>
    <tr>
      <th>117</th>
      <td>732.0</td>
      <td>8.296339e-17</td>
      <td>1.000684</td>
      <td>-2.795032</td>
      <td>-0.533253</td>
      <td>-0.015939</td>
      <td>0.771789</td>
      <td>2.582387</td>
    </tr>
    <tr>
      <th>118</th>
      <td>732.0</td>
      <td>3.166866e-16</td>
      <td>1.000684</td>
      <td>-3.429627</td>
      <td>-0.637522</td>
      <td>0.013648</td>
      <td>0.604842</td>
      <td>3.108847</td>
    </tr>
    <tr>
      <th>119</th>
      <td>732.0</td>
      <td>-1.570086e-15</td>
      <td>1.000684</td>
      <td>-2.883343</td>
      <td>-0.776394</td>
      <td>-0.011401</td>
      <td>0.628090</td>
      <td>3.166539</td>
    </tr>
    <tr>
      <th>120</th>
      <td>732.0</td>
      <td>-9.979874e-17</td>
      <td>1.000684</td>
      <td>-2.273299</td>
      <td>-0.655759</td>
      <td>0.002312</td>
      <td>0.630112</td>
      <td>2.564842</td>
    </tr>
    <tr>
      <th>121</th>
      <td>732.0</td>
      <td>-2.727023e-16</td>
      <td>1.000684</td>
      <td>-1.251713</td>
      <td>-0.592459</td>
      <td>-0.375123</td>
      <td>0.081283</td>
      <td>5.308224</td>
    </tr>
    <tr>
      <th>122</th>
      <td>732.0</td>
      <td>3.574099e-16</td>
      <td>1.000684</td>
      <td>-2.571139</td>
      <td>-0.734427</td>
      <td>-0.119066</td>
      <td>0.668096</td>
      <td>2.735959</td>
    </tr>
    <tr>
      <th>123</th>
      <td>732.0</td>
      <td>1.235351e-16</td>
      <td>1.000684</td>
      <td>-2.186794</td>
      <td>-0.818505</td>
      <td>-0.105594</td>
      <td>0.704874</td>
      <td>3.341394</td>
    </tr>
    <tr>
      <th>124</th>
      <td>732.0</td>
      <td>1.011638e-16</td>
      <td>1.000684</td>
      <td>-2.066563</td>
      <td>-0.764664</td>
      <td>-0.068333</td>
      <td>0.747127</td>
      <td>3.472888</td>
    </tr>
    <tr>
      <th>125</th>
      <td>732.0</td>
      <td>-9.653784e-17</td>
      <td>1.000684</td>
      <td>-3.237960</td>
      <td>-0.781040</td>
      <td>0.190342</td>
      <td>0.755363</td>
      <td>2.416291</td>
    </tr>
    <tr>
      <th>126</th>
      <td>732.0</td>
      <td>1.547032e-16</td>
      <td>1.000684</td>
      <td>-1.792324</td>
      <td>-0.698020</td>
      <td>-0.185280</td>
      <td>0.474142</td>
      <td>7.560878</td>
    </tr>
    <tr>
      <th>127</th>
      <td>732.0</td>
      <td>3.494472e-16</td>
      <td>1.000684</td>
      <td>-1.557295</td>
      <td>-0.826489</td>
      <td>-0.044746</td>
      <td>0.563395</td>
      <td>3.147231</td>
    </tr>
    <tr>
      <th>128</th>
      <td>732.0</td>
      <td>-4.923107e-17</td>
      <td>1.000684</td>
      <td>-0.410399</td>
      <td>-0.196292</td>
      <td>-0.113139</td>
      <td>-0.058508</td>
      <td>7.234791</td>
    </tr>
    <tr>
      <th>129</th>
      <td>732.0</td>
      <td>-1.202552e-16</td>
      <td>1.000684</td>
      <td>-1.162652</td>
      <td>-0.283215</td>
      <td>-0.085544</td>
      <td>0.124752</td>
      <td>13.641746</td>
    </tr>
    <tr>
      <th>130</th>
      <td>732.0</td>
      <td>-1.448447e-16</td>
      <td>1.000684</td>
      <td>-1.224237</td>
      <td>-0.598922</td>
      <td>-0.240229</td>
      <td>0.250816</td>
      <td>8.832594</td>
    </tr>
    <tr>
      <th>131</th>
      <td>732.0</td>
      <td>-1.765437e-16</td>
      <td>1.000684</td>
      <td>-2.410812</td>
      <td>-0.588096</td>
      <td>-0.058888</td>
      <td>0.579604</td>
      <td>3.543727</td>
    </tr>
    <tr>
      <th>132</th>
      <td>732.0</td>
      <td>-4.783476e-16</td>
      <td>1.000684</td>
      <td>-1.098408</td>
      <td>-0.509865</td>
      <td>-0.286452</td>
      <td>0.264994</td>
      <td>6.979465</td>
    </tr>
    <tr>
      <th>133</th>
      <td>732.0</td>
      <td>2.208312e-16</td>
      <td>1.000684</td>
      <td>-2.066437</td>
      <td>-0.507329</td>
      <td>-0.145586</td>
      <td>0.264991</td>
      <td>7.845298</td>
    </tr>
    <tr>
      <th>134</th>
      <td>732.0</td>
      <td>-1.038938e-16</td>
      <td>1.000684</td>
      <td>-1.533388</td>
      <td>-0.691497</td>
      <td>-0.307350</td>
      <td>0.442094</td>
      <td>4.060071</td>
    </tr>
    <tr>
      <th>135</th>
      <td>732.0</td>
      <td>-4.656263e-17</td>
      <td>1.000684</td>
      <td>-1.137652</td>
      <td>-0.446616</td>
      <td>-0.156947</td>
      <td>0.225880</td>
      <td>9.483251</td>
    </tr>
    <tr>
      <th>136</th>
      <td>732.0</td>
      <td>2.843809e-16</td>
      <td>1.000684</td>
      <td>-1.455679</td>
      <td>-0.516600</td>
      <td>-0.132864</td>
      <td>0.286623</td>
      <td>10.773802</td>
    </tr>
    <tr>
      <th>137</th>
      <td>732.0</td>
      <td>2.972728e-17</td>
      <td>1.000684</td>
      <td>-2.545539</td>
      <td>-0.647078</td>
      <td>0.060941</td>
      <td>0.599764</td>
      <td>3.264377</td>
    </tr>
    <tr>
      <th>138</th>
      <td>732.0</td>
      <td>4.052428e-17</td>
      <td>1.000684</td>
      <td>-2.757049</td>
      <td>-0.667331</td>
      <td>0.027341</td>
      <td>0.584219</td>
      <td>4.841579</td>
    </tr>
    <tr>
      <th>139</th>
      <td>732.0</td>
      <td>-5.975791e-17</td>
      <td>1.000684</td>
      <td>-1.424839</td>
      <td>-0.604164</td>
      <td>-0.238226</td>
      <td>0.456322</td>
      <td>10.384732</td>
    </tr>
    <tr>
      <th>140</th>
      <td>732.0</td>
      <td>-2.629955e-16</td>
      <td>1.000684</td>
      <td>-2.066389</td>
      <td>-0.740187</td>
      <td>-0.083187</td>
      <td>0.656964</td>
      <td>3.371040</td>
    </tr>
    <tr>
      <th>141</th>
      <td>732.0</td>
      <td>1.579641e-16</td>
      <td>1.000684</td>
      <td>-1.551490</td>
      <td>-0.570706</td>
      <td>-0.166853</td>
      <td>0.294692</td>
      <td>7.448650</td>
    </tr>
    <tr>
      <th>142</th>
      <td>732.0</td>
      <td>-5.475280e-17</td>
      <td>1.000684</td>
      <td>-1.497465</td>
      <td>-0.793353</td>
      <td>-0.098350</td>
      <td>0.532891</td>
      <td>6.143013</td>
    </tr>
    <tr>
      <th>143</th>
      <td>732.0</td>
      <td>1.145107e-17</td>
      <td>1.000684</td>
      <td>-0.157033</td>
      <td>-0.140227</td>
      <td>-0.121555</td>
      <td>-0.052465</td>
      <td>18.295463</td>
    </tr>
    <tr>
      <th>144</th>
      <td>732.0</td>
      <td>-3.064489e-16</td>
      <td>1.000684</td>
      <td>-1.872614</td>
      <td>-0.680935</td>
      <td>-0.066748</td>
      <td>0.630338</td>
      <td>4.021518</td>
    </tr>
    <tr>
      <th>145</th>
      <td>732.0</td>
      <td>3.982849e-16</td>
      <td>1.000684</td>
      <td>-1.777655</td>
      <td>-0.684209</td>
      <td>-0.154057</td>
      <td>0.437419</td>
      <td>5.738040</td>
    </tr>
    <tr>
      <th>146</th>
      <td>732.0</td>
      <td>3.321569e-16</td>
      <td>1.000684</td>
      <td>-1.193634</td>
      <td>-0.603619</td>
      <td>-0.258687</td>
      <td>0.297289</td>
      <td>8.328304</td>
    </tr>
    <tr>
      <th>147</th>
      <td>732.0</td>
      <td>-1.176958e-16</td>
      <td>1.000684</td>
      <td>-1.179019</td>
      <td>-0.586888</td>
      <td>-0.279857</td>
      <td>0.315015</td>
      <td>9.468377</td>
    </tr>
    <tr>
      <th>148</th>
      <td>732.0</td>
      <td>3.417121e-16</td>
      <td>1.000684</td>
      <td>-1.999803</td>
      <td>-0.629579</td>
      <td>-0.115745</td>
      <td>0.512274</td>
      <td>6.278632</td>
    </tr>
    <tr>
      <th>149</th>
      <td>732.0</td>
      <td>-8.220504e-17</td>
      <td>1.000684</td>
      <td>-2.059287</td>
      <td>-0.603695</td>
      <td>-0.140552</td>
      <td>0.454918</td>
      <td>7.666715</td>
    </tr>
    <tr>
      <th>150</th>
      <td>732.0</td>
      <td>-2.396383e-16</td>
      <td>1.000684</td>
      <td>-1.791074</td>
      <td>-0.717212</td>
      <td>-0.106650</td>
      <td>0.564592</td>
      <td>5.408623</td>
    </tr>
    <tr>
      <th>151</th>
      <td>732.0</td>
      <td>5.740702e-17</td>
      <td>1.000684</td>
      <td>-0.135567</td>
      <td>-0.117813</td>
      <td>-0.104498</td>
      <td>-0.091182</td>
      <td>26.060148</td>
    </tr>
    <tr>
      <th>152</th>
      <td>732.0</td>
      <td>-2.883622e-17</td>
      <td>1.000684</td>
      <td>-0.896846</td>
      <td>-0.326160</td>
      <td>-0.147603</td>
      <td>0.118484</td>
      <td>14.588680</td>
    </tr>
    <tr>
      <th>153</th>
      <td>732.0</td>
      <td>2.523786e-16</td>
      <td>1.000684</td>
      <td>-2.106356</td>
      <td>-0.788034</td>
      <td>-0.058329</td>
      <td>0.686653</td>
      <td>2.703228</td>
    </tr>
    <tr>
      <th>154</th>
      <td>732.0</td>
      <td>1.273268e-16</td>
      <td>1.000684</td>
      <td>-1.624878</td>
      <td>-0.919622</td>
      <td>-0.003939</td>
      <td>0.714469</td>
      <td>2.912436</td>
    </tr>
    <tr>
      <th>155</th>
      <td>732.0</td>
      <td>-4.356004e-17</td>
      <td>1.000684</td>
      <td>-1.832710</td>
      <td>-0.818288</td>
      <td>-0.195494</td>
      <td>0.721314</td>
      <td>2.608406</td>
    </tr>
    <tr>
      <th>156</th>
      <td>732.0</td>
      <td>-4.169403e-16</td>
      <td>1.000684</td>
      <td>-2.065731</td>
      <td>-0.733325</td>
      <td>-0.083605</td>
      <td>0.494990</td>
      <td>3.919758</td>
    </tr>
    <tr>
      <th>157</th>
      <td>732.0</td>
      <td>-8.523843e-17</td>
      <td>1.000684</td>
      <td>-1.116422</td>
      <td>-1.116422</td>
      <td>-0.087548</td>
      <td>0.699474</td>
      <td>2.417869</td>
    </tr>
    <tr>
      <th>158</th>
      <td>732.0</td>
      <td>3.139565e-17</td>
      <td>1.000684</td>
      <td>-0.955571</td>
      <td>-0.955571</td>
      <td>-0.089399</td>
      <td>0.823201</td>
      <td>2.257063</td>
    </tr>
    <tr>
      <th>159</th>
      <td>732.0</td>
      <td>4.712381e-16</td>
      <td>1.000684</td>
      <td>-1.534798</td>
      <td>-0.720042</td>
      <td>-0.307936</td>
      <td>0.649725</td>
      <td>3.695483</td>
    </tr>
    <tr>
      <th>160</th>
      <td>732.0</td>
      <td>-3.018229e-17</td>
      <td>1.000684</td>
      <td>-0.787345</td>
      <td>-0.533760</td>
      <td>-0.307827</td>
      <td>0.148465</td>
      <td>6.862925</td>
    </tr>
    <tr>
      <th>161</th>
      <td>732.0</td>
      <td>-1.011638e-16</td>
      <td>1.000684</td>
      <td>-0.924464</td>
      <td>-0.880745</td>
      <td>-0.274999</td>
      <td>0.398950</td>
      <td>3.087230</td>
    </tr>
    <tr>
      <th>162</th>
      <td>732.0</td>
      <td>1.501531e-16</td>
      <td>1.000684</td>
      <td>-1.118656</td>
      <td>-0.491374</td>
      <td>-0.181162</td>
      <td>0.233943</td>
      <td>9.907672</td>
    </tr>
    <tr>
      <th>163</th>
      <td>732.0</td>
      <td>-2.517719e-16</td>
      <td>1.000684</td>
      <td>-1.494034</td>
      <td>-0.614811</td>
      <td>-0.237595</td>
      <td>0.335852</td>
      <td>8.830148</td>
    </tr>
    <tr>
      <th>164</th>
      <td>732.0</td>
      <td>-2.714890e-17</td>
      <td>1.000684</td>
      <td>-0.879252</td>
      <td>-0.755221</td>
      <td>-0.393478</td>
      <td>0.359591</td>
      <td>3.344192</td>
    </tr>
    <tr>
      <th>165</th>
      <td>732.0</td>
      <td>1.314598e-16</td>
      <td>1.000684</td>
      <td>-2.358341</td>
      <td>-0.670253</td>
      <td>-0.048151</td>
      <td>0.624369</td>
      <td>5.875308</td>
    </tr>
    <tr>
      <th>166</th>
      <td>732.0</td>
      <td>7.826162e-17</td>
      <td>1.000684</td>
      <td>-1.094712</td>
      <td>-0.568016</td>
      <td>-0.256006</td>
      <td>0.319337</td>
      <td>8.326555</td>
    </tr>
    <tr>
      <th>167</th>
      <td>732.0</td>
      <td>-1.298294e-16</td>
      <td>1.000684</td>
      <td>-1.095838</td>
      <td>-1.095838</td>
      <td>-0.092652</td>
      <td>0.777247</td>
      <td>2.449112</td>
    </tr>
    <tr>
      <th>168</th>
      <td>732.0</td>
      <td>3.078897e-17</td>
      <td>1.000684</td>
      <td>-1.147523</td>
      <td>-0.656415</td>
      <td>-0.329805</td>
      <td>0.368593</td>
      <td>4.252493</td>
    </tr>
    <tr>
      <th>169</th>
      <td>732.0</td>
      <td>1.048038e-16</td>
      <td>1.000684</td>
      <td>-0.993273</td>
      <td>-0.640915</td>
      <td>-0.336524</td>
      <td>0.439800</td>
      <td>3.954924</td>
    </tr>
    <tr>
      <th>170</th>
      <td>732.0</td>
      <td>-8.815808e-17</td>
      <td>1.000684</td>
      <td>-0.916054</td>
      <td>-0.677538</td>
      <td>-0.393733</td>
      <td>0.339789</td>
      <td>3.533812</td>
    </tr>
    <tr>
      <th>171</th>
      <td>732.0</td>
      <td>1.126907e-16</td>
      <td>1.000684</td>
      <td>-1.134947</td>
      <td>-1.134947</td>
      <td>-0.210565</td>
      <td>0.723722</td>
      <td>2.460720</td>
    </tr>
    <tr>
      <th>172</th>
      <td>732.0</td>
      <td>6.552136e-17</td>
      <td>1.000684</td>
      <td>-0.888294</td>
      <td>-0.611983</td>
      <td>-0.464544</td>
      <td>0.499819</td>
      <td>3.132154</td>
    </tr>
    <tr>
      <th>173</th>
      <td>732.0</td>
      <td>4.590856e-17</td>
      <td>1.000684</td>
      <td>-1.193387</td>
      <td>-0.942949</td>
      <td>-0.019612</td>
      <td>0.633836</td>
      <td>2.635098</td>
    </tr>
    <tr>
      <th>174</th>
      <td>732.0</td>
      <td>2.256847e-16</td>
      <td>1.000684</td>
      <td>-1.223295</td>
      <td>-0.709895</td>
      <td>-0.226157</td>
      <td>0.409456</td>
      <td>3.086447</td>
    </tr>
    <tr>
      <th>175</th>
      <td>732.0</td>
      <td>-1.835205e-17</td>
      <td>1.000684</td>
      <td>-0.770401</td>
      <td>-0.770401</td>
      <td>-0.770401</td>
      <td>0.751589</td>
      <td>2.334492</td>
    </tr>
    <tr>
      <th>176</th>
      <td>732.0</td>
      <td>2.214379e-16</td>
      <td>1.000684</td>
      <td>-0.765128</td>
      <td>-0.765128</td>
      <td>-0.765128</td>
      <td>0.776577</td>
      <td>2.226177</td>
    </tr>
    <tr>
      <th>177</th>
      <td>732.0</td>
      <td>2.309931e-16</td>
      <td>1.000684</td>
      <td>-1.378420</td>
      <td>-0.580687</td>
      <td>-0.280955</td>
      <td>0.152892</td>
      <td>7.965921</td>
    </tr>
    <tr>
      <th>178</th>
      <td>732.0</td>
      <td>-1.164824e-16</td>
      <td>1.000684</td>
      <td>-0.924582</td>
      <td>-0.924582</td>
      <td>-0.228407</td>
      <td>0.968038</td>
      <td>1.943023</td>
    </tr>
    <tr>
      <th>179</th>
      <td>732.0</td>
      <td>-5.429779e-17</td>
      <td>1.000684</td>
      <td>-0.170927</td>
      <td>-0.170927</td>
      <td>-0.170927</td>
      <td>-0.170927</td>
      <td>6.321682</td>
    </tr>
    <tr>
      <th>180</th>
      <td>732.0</td>
      <td>-7.979045e-15</td>
      <td>1.000684</td>
      <td>-2.006551</td>
      <td>-0.585329</td>
      <td>-0.585329</td>
      <td>0.723691</td>
      <td>2.705922</td>
    </tr>
    <tr>
      <th>181</th>
      <td>732.0</td>
      <td>2.162812e-16</td>
      <td>1.000684</td>
      <td>-2.571308</td>
      <td>-0.422681</td>
      <td>-0.422681</td>
      <td>0.213950</td>
      <td>4.431625</td>
    </tr>
    <tr>
      <th>182</th>
      <td>732.0</td>
      <td>1.075946e-15</td>
      <td>1.000684</td>
      <td>-3.409557</td>
      <td>-0.398039</td>
      <td>0.354841</td>
      <td>0.354841</td>
      <td>3.366359</td>
    </tr>
    <tr>
      <th>183</th>
      <td>732.0</td>
      <td>1.510631e-16</td>
      <td>1.000684</td>
      <td>-1.682038</td>
      <td>-0.585549</td>
      <td>-0.256999</td>
      <td>0.585139</td>
      <td>3.822698</td>
    </tr>
    <tr>
      <th>184</th>
      <td>732.0</td>
      <td>1.500075e-14</td>
      <td>1.000684</td>
      <td>-4.782607</td>
      <td>-0.760595</td>
      <td>-0.228995</td>
      <td>0.706142</td>
      <td>3.822038</td>
    </tr>
    <tr>
      <th>185</th>
      <td>732.0</td>
      <td>2.371509e-15</td>
      <td>1.000684</td>
      <td>-1.337159</td>
      <td>-0.705722</td>
      <td>-0.460045</td>
      <td>0.675259</td>
      <td>2.388590</td>
    </tr>
    <tr>
      <th>186</th>
      <td>732.0</td>
      <td>-2.426717e-18</td>
      <td>1.000684</td>
      <td>-1.762837</td>
      <td>-0.451032</td>
      <td>-0.133604</td>
      <td>0.247837</td>
      <td>12.907276</td>
    </tr>
    <tr>
      <th>187</th>
      <td>732.0</td>
      <td>4.977803e-16</td>
      <td>1.000684</td>
      <td>-0.925451</td>
      <td>-0.444112</td>
      <td>-0.274832</td>
      <td>-0.004350</td>
      <td>4.574652</td>
    </tr>
    <tr>
      <th>188</th>
      <td>732.0</td>
      <td>4.368091e-17</td>
      <td>1.000684</td>
      <td>-1.340175</td>
      <td>-0.865005</td>
      <td>-0.124061</td>
      <td>0.543895</td>
      <td>3.237366</td>
    </tr>
    <tr>
      <th>189</th>
      <td>732.0</td>
      <td>3.635525e-15</td>
      <td>1.000684</td>
      <td>-2.715001</td>
      <td>-0.184915</td>
      <td>0.231004</td>
      <td>0.279377</td>
      <td>6.546437</td>
    </tr>
    <tr>
      <th>190</th>
      <td>732.0</td>
      <td>2.390316e-16</td>
      <td>1.000684</td>
      <td>-3.187463</td>
      <td>-0.939153</td>
      <td>-0.216587</td>
      <td>0.551201</td>
      <td>4.311492</td>
    </tr>
    <tr>
      <th>191</th>
      <td>732.0</td>
      <td>3.976782e-16</td>
      <td>1.000684</td>
      <td>-2.017019</td>
      <td>-0.513640</td>
      <td>-0.443895</td>
      <td>0.467625</td>
      <td>5.155030</td>
    </tr>
    <tr>
      <th>192</th>
      <td>732.0</td>
      <td>7.894414e-17</td>
      <td>1.000684</td>
      <td>-1.355163</td>
      <td>-0.666843</td>
      <td>-0.331115</td>
      <td>0.339813</td>
      <td>5.673505</td>
    </tr>
    <tr>
      <th>193</th>
      <td>732.0</td>
      <td>1.316703e-15</td>
      <td>1.000684</td>
      <td>-13.369234</td>
      <td>0.005480</td>
      <td>0.097649</td>
      <td>0.230072</td>
      <td>0.511306</td>
    </tr>
    <tr>
      <th>194</th>
      <td>732.0</td>
      <td>-2.865043e-16</td>
      <td>1.000684</td>
      <td>-3.500528</td>
      <td>-0.355553</td>
      <td>0.154718</td>
      <td>0.663081</td>
      <td>2.081914</td>
    </tr>
    <tr>
      <th>195</th>
      <td>732.0</td>
      <td>3.503573e-17</td>
      <td>1.000684</td>
      <td>-0.236055</td>
      <td>-0.191684</td>
      <td>-0.173872</td>
      <td>-0.150229</td>
      <td>6.793760</td>
    </tr>
    <tr>
      <th>196</th>
      <td>732.0</td>
      <td>-2.127973e-14</td>
      <td>1.000684</td>
      <td>-5.594854</td>
      <td>-0.627799</td>
      <td>0.003374</td>
      <td>0.634547</td>
      <td>2.582950</td>
    </tr>
    <tr>
      <th>197</th>
      <td>732.0</td>
      <td>1.816246e-17</td>
      <td>1.000684</td>
      <td>-0.506310</td>
      <td>-0.240788</td>
      <td>-0.119090</td>
      <td>0.046861</td>
      <td>25.481641</td>
    </tr>
    <tr>
      <th>198</th>
      <td>732.0</td>
      <td>-5.930290e-17</td>
      <td>1.000684</td>
      <td>-2.230140</td>
      <td>-0.656224</td>
      <td>-0.083891</td>
      <td>0.555776</td>
      <td>6.817772</td>
    </tr>
    <tr>
      <th>199</th>
      <td>732.0</td>
      <td>-3.139565e-17</td>
      <td>1.000684</td>
      <td>-1.563968</td>
      <td>-0.723800</td>
      <td>-0.159688</td>
      <td>0.452435</td>
      <td>4.605263</td>
    </tr>
    <tr>
      <th>200</th>
      <td>732.0</td>
      <td>3.185066e-18</td>
      <td>1.000684</td>
      <td>-1.177220</td>
      <td>-0.621775</td>
      <td>-0.294523</td>
      <td>0.223711</td>
      <td>7.660460</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[&nbsp;]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Observation</span> <span class="p">:</span> 
    <span class="n">Train</span><span class="p">,</span><span class="n">Test</span> <span class="n">data</span> <span class="nb">all</span> <span class="n">the</span> <span class="n">features</span> <span class="n">are</span> <span class="n">scaled</span> <span class="n">to</span> <span class="n">new</span> <span class="n">values</span> 
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="5.-Model-training,-testing-and-tuning:">5. Model training, testing and tuning:<a class="anchor-link" href="#5.-Model-training,-testing-and-tuning:">&#182;</a></h1>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>5.A. Use any Supervised Learning technique to train a model.</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>LogisticRegression Model</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[36]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">logit</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">logit</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">logit_pred</span> <span class="o">=</span> <span class="n">logit</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">Accuracy</span> <span class="o">=</span> <span class="n">logit</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy on Training data:&#39;</span><span class="p">,</span><span class="n">logit</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy on Test data:&#39;</span><span class="p">,</span><span class="n">logit</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Accuracy on Training data: 0.9567000911577028
Accuracy on Test data: 0.8579234972677595
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[37]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">logit</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">logit_pred</span><span class="p">))</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>              precision    recall  f1-score   support

          -1       0.86      0.85      0.86       363
           1       0.86      0.86      0.86       369

    accuracy                           0.86       732
   macro avg       0.86      0.86      0.86       732
weighted avg       0.86      0.86      0.86       732

</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[38]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;Logit Regression&#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="n">Accuracy</span><span class="p">},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;1&#39;</span><span class="p">})</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[38]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>5.B. Use cross validation techniques</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[39]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_folds</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">kfold</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">num_folds</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span><span class="p">)</span>
<span class="n">accuracy</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">results</span><span class="p">))</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[40]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average accuracy: &#39;</span><span class="p">,</span><span class="n">accuracy</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Standard Deviation: &#39;</span><span class="p">,</span><span class="n">results</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Average accuracy:  0.9335080645161291
Standard Deviation:  0.07012100252276122
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[41]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[42]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">tempResultsDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;Logit Regression K Fold &#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">accuracy</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;2&#39;</span><span class="p">})</span>
<span class="n">tempResultsDf</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">tempResultsDf</span><span class="p">])</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[42]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Bootstrap samples - To Give Confidence to Deploy in Production</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[43]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">values</span> <span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">values</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[44]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">50</span>              <span class="c1"># Number of bootstrap samples to create</span>
<span class="n">n_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.50</span><span class="p">)</span>    <span class="c1"># picking only 50 % of the given data in every bootstrap sample</span>

<span class="c1"># run bootstrap</span>
<span class="n">stats</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
	<span class="c1"># prepare train and test sets</span>
	<span class="n">train</span> <span class="o">=</span> <span class="n">resample</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">n_size</span><span class="p">)</span>  <span class="c1"># Sampling with replacement </span>
	<span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">train</span><span class="o">.</span><span class="n">tolist</span><span class="p">()])</span>  <span class="c1"># picking rest of the data not considered in sample</span>
    <span class="c1"># fit model</span>
	<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
	<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">train</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># evaluate model</span>
	<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
	<span class="n">score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">test</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">predictions</span><span class="p">)</span>    <span class="c1"># caution, overall accuracy score can mislead when classes are imbalanced</span>
	<span class="nb">print</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
	<span class="n">stats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9290254237288136
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9265477439664218
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9315508021390374
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.934020618556701
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9295624332977588
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9272918861959958
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9216101694915254
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9294605809128631
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.916241062308478
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9091869060190074
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9368983957219251
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9291754756871036
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9252136752136753
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9454926624737946
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9232386961093586
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9188900747065102
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9141039236479321
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9118895966029724
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9140708915145005
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9241306638566913
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9365750528541226
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9157894736842105
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9234042553191489
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9219409282700421
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9233193277310925
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9318658280922432
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9236401673640168
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9135416666666667
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9158004158004158
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9288747346072187
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9197080291970803
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9258872651356994
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9190871369294605
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9336188436830836
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9262381454162276
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9288702928870293
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9257322175732218
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9271047227926078
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9109947643979057
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9342379958246346
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9241164241164241
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9138297872340425
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.911578947368421
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9256900212314225
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9228362877997914
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9094827586206896
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9242902208201893
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9224318658280922
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9372384937238494
0.9221748400852878
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[45]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># plot scores</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># confidence intervals</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.95</span>                             <span class="c1"># for 95% confidence </span>
<span class="n">p</span> <span class="o">=</span> <span class="p">((</span><span class="mf">1.0</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span><span class="o">/</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>              <span class="c1"># tail regions on right and left .25 on each side indicated by P value (border)</span>
<span class="n">lower</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>  
<span class="n">p</span> <span class="o">=</span> <span class="p">(</span><span class="n">alpha</span><span class="o">+</span><span class="p">((</span><span class="mf">1.0</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span><span class="o">/</span><span class="mf">2.0</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">upper</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%.1f</span><span class="s1"> confidence interval </span><span class="si">%.1f%%</span><span class="s1"> and </span><span class="si">%.1f%%</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">alpha</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="n">lower</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="n">upper</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>




<div class="jp-RenderedImage jp-OutputArea-output ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>95.0 confidence interval 91.0% and 93.7%
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[46]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loocv</span> <span class="o">=</span> <span class="n">LeaveOneOut</span><span class="p">()</span>
<span class="c1"># enumerate splits</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[47]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">results</span><span class="o">=</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="n">loocv</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[48]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">results</span><span class="p">))</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9329929802169751
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[49]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[50]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">tempResultsDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;Logit Regression loocv &#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">accuracy</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;3&#39;</span><span class="p">})</span>
<span class="n">tempResultsDf</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">tempResultsDf</span><span class="p">])</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[50]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Logit Regression loocv</td>
      <td>0.932993</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[&nbsp;]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">StratifiedKFold</span> <span class="n">Is</span> <span class="ow">not</span> <span class="n">tested</span> <span class="n">here</span> <span class="n">because</span> <span class="n">Target</span> <span class="n">imbalance</span> <span class="n">already</span> <span class="n">done</span> 
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>5.C. Apply hyper-parameter tuning techniques to get the best accuracy.</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Grid Search</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[51]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">param_grid</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;C&quot;</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span> <span class="s2">&quot;penalty&quot;</span><span class="p">:[</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span><span class="s2">&quot;l2&quot;</span><span class="p">]}</span><span class="c1"># l1 lasso l2 ridge</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[52]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">logit_model_GV</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span><span class="n">param_grid</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[53]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">logit_model_GV</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: 
70 fits failed out of a total of 140.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score=&#39;raise&#39;.

Below are more details about the failures:
--------------------------------------------------------------------------------
70 fits failed with the following error:
Traceback (most recent call last):
  File &#34;/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py&#34;, line 680, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File &#34;/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py&#34;, line 1461, in fit
    solver = _check_solver(self.solver, self.penalty, self.dual)
  File &#34;/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py&#34;, line 447, in _check_solver
    raise ValueError(
ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty.

  warnings.warn(some_fits_failed_message, FitFailedWarning)
/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.78852636        nan 0.85003736        nan 0.89287879
        nan 0.91748028        nan 0.92020756        nan 0.92066418
        nan 0.92157534]
  warnings.warn(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[53]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>GridSearchCV(cv=10, estimator=LogisticRegression(),
             param_grid={&#39;C&#39;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),
                         &#39;penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;]})</pre>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[54]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># print the best score and estimator </span>
<span class="nb">print</span><span class="p">(</span><span class="n">logit_model_GV</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logit_model_GV</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">get_params</span><span class="p">())</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9215753424657533
{&#39;C&#39;: 1000.0, &#39;class_weight&#39;: None, &#39;dual&#39;: False, &#39;fit_intercept&#39;: True, &#39;intercept_scaling&#39;: 1, &#39;l1_ratio&#39;: None, &#39;max_iter&#39;: 100, &#39;multi_class&#39;: &#39;auto&#39;, &#39;n_jobs&#39;: None, &#39;penalty&#39;: &#39;l2&#39;, &#39;random_state&#39;: None, &#39;solver&#39;: &#39;lbfgs&#39;, &#39;tol&#39;: 0.0001, &#39;verbose&#39;: 0, &#39;warm_start&#39;: False}
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[55]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy</span><span class="o">=</span><span class="n">logit_model_GV</span><span class="o">.</span><span class="n">best_score_</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[56]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">tempResultsDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;Logit Regression Grid SearchCV &#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">accuracy</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;4&#39;</span><span class="p">})</span>
<span class="n">tempResultsDf</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">tempResultsDf</span><span class="p">])</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[56]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Logit Regression loocv</td>
      <td>0.932993</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logit Regression Grid SearchCV</td>
      <td>0.921575</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>RandomizedSearchCV</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[57]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">param_grid</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;C&quot;</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span> <span class="s2">&quot;penalty&quot;</span><span class="p">:[</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span><span class="s2">&quot;l2&quot;</span><span class="p">]}</span><span class="c1"># l1 lasso l2 ridge</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[58]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">randomCV_model</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">param_distributions</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> <span class="c1">#default cv = 3</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[59]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">randomCV_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:292: UserWarning: The total space of parameters 14 is smaller than n_iter=20. Running 14 iterations. For exhaustive searches, use GridSearchCV.
  warnings.warn(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: 
35 fits failed out of a total of 70.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score=&#39;raise&#39;.

Below are more details about the failures:
--------------------------------------------------------------------------------
35 fits failed with the following error:
Traceback (most recent call last):
  File &#34;/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py&#34;, line 680, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File &#34;/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py&#34;, line 1461, in fit
    solver = _check_solver(self.solver, self.penalty, self.dual)
  File &#34;/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py&#34;, line 447, in _check_solver
    raise ValueError(
ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty.

  warnings.warn(some_fits_failed_message, FitFailedWarning)
/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.78988049        nan 0.84821044        nan 0.88877794
        nan 0.91157051        nan 0.91794968        nan 0.91658086
        nan 0.91703852]
  warnings.warn(
/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[59]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>RandomizedSearchCV(estimator=LogisticRegression(), n_iter=20,
                   param_distributions={&#39;C&#39;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),
                                        &#39;penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;]})</pre>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[60]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># print the best score and estimator </span>
<span class="nb">print</span><span class="p">(</span><span class="n">randomCV_model</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">randomCV_model</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">get_params</span><span class="p">())</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>0.9179496780769911
{&#39;C&#39;: 10.0, &#39;class_weight&#39;: None, &#39;dual&#39;: False, &#39;fit_intercept&#39;: True, &#39;intercept_scaling&#39;: 1, &#39;l1_ratio&#39;: None, &#39;max_iter&#39;: 100, &#39;multi_class&#39;: &#39;auto&#39;, &#39;n_jobs&#39;: None, &#39;penalty&#39;: &#39;l2&#39;, &#39;random_state&#39;: None, &#39;solver&#39;: &#39;lbfgs&#39;, &#39;tol&#39;: 0.0001, &#39;verbose&#39;: 0, &#39;warm_start&#39;: False}
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[61]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span><span class="n">randomCV_model</span><span class="o">.</span><span class="n">best_score_</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[62]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">tempResultsDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;Logit Regression Random SearchCV &#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">accuracy</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;5&#39;</span><span class="p">})</span>
<span class="n">tempResultsDf</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">tempResultsDf</span><span class="p">])</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[62]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Logit Regression loocv</td>
      <td>0.932993</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logit Regression Grid SearchCV</td>
      <td>0.921575</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Logit Regression Random SearchCV</td>
      <td>0.917950</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>5.D. Use any other technique/method which can enhance the model performance.</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>standardisation/normalisation  - Already done 
target balancing  - Already done using upsampling</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>PCA</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[63]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_over</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[63]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>(2926, 201)</pre>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[64]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_over</span><span class="p">)</span>
<span class="n">pca_model</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_over</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[65]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_PCA</span><span class="o">=</span><span class="n">pca_model</span>
<span class="n">y_PCA</span><span class="o">=</span> <span class="n">y_over</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[66]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Split X and y into training and test set in 70:30 ratio</span>
<span class="n">X_train_PCA</span><span class="p">,</span> <span class="n">X_test_PCA</span><span class="p">,</span> <span class="n">y_train_PCA</span><span class="p">,</span> <span class="n">y_test_PCA</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_PCA</span><span class="p">,</span> <span class="n">y_PCA</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[67]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">logit_PCA</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">logit_PCA</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_PCA</span><span class="p">,</span> <span class="n">y_train_PCA</span><span class="p">)</span>
<span class="n">logit_pred_PCA</span> <span class="o">=</span> <span class="n">logit_PCA</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_PCA</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy on Training data:&#39;</span><span class="p">,</span><span class="n">logit_PCA</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_PCA</span><span class="p">,</span> <span class="n">y_train_PCA</span><span class="p">)</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy on Test data:&#39;</span><span class="p">,</span><span class="n">logit_PCA</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_PCA</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Accuracy on Training data: 0.6841385597082954
Accuracy on Test data: 0.6338797814207651
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr">
<pre>/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[68]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span><span class="n">logit_PCA</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_PCA</span><span class="p">,</span> <span class="n">y_test_PCA</span><span class="p">)</span>
<span class="n">accuracy</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[68]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>0.6338797814207651</pre>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[69]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">tempResultsDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;Logit Regression PCA &#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">accuracy</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;6&#39;</span><span class="p">})</span>
<span class="n">tempResultsDf</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">tempResultsDf</span><span class="p">])</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[69]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Logit Regression loocv</td>
      <td>0.932993</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logit Regression Grid SearchCV</td>
      <td>0.921575</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Logit Regression Random SearchCV</td>
      <td>0.917950</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Logit Regression PCA</td>
      <td>0.633880</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>5.E Display and explain the classification report in detail</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[70]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">logit</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">logit_pred</span><span class="p">))</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>              precision    recall  f1-score   support

          -1       0.86      0.85      0.86       363
           1       0.86      0.86      0.86       369

    accuracy                           0.86       732
   macro avg       0.86      0.86      0.86       732
weighted avg       0.86      0.86      0.86       732

</pre>
</div>
</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Observartion : 
    Without K-fold/Grid Search CV accuracy score is low as 86
    PCA reduced accuracy due to features reduction</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>KNeighborsClassifier</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>5.F. Apply the above steps for all possible models that you have learnt so far</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>KNeighborsClassifier</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[71]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">KNN</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span> <span class="mi">5</span> <span class="p">,</span> <span class="n">metric</span> <span class="o">=</span> <span class="s1">&#39;euclidean&#39;</span> <span class="p">)</span> 
<span class="n">fitted_model</span><span class="o">=</span><span class="n">KNN</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[72]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">fitted_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="n">accuracy_score</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy Score&quot;</span><span class="p">,</span><span class="n">accuracy_score</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>              precision    recall  f1-score   support

          -1       1.00      0.83      0.91       363
           1       0.86      1.00      0.92       369

    accuracy                           0.92       732
   macro avg       0.93      0.92      0.92       732
weighted avg       0.93      0.92      0.92       732

Accuracy Score 91.66666666666666
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[73]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">tempResultsDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;KNN &#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">accuracy_score</span><span class="o">/</span><span class="mi">100</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;7&#39;</span><span class="p">})</span>
<span class="n">tempResultsDf</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">tempResultsDf</span><span class="p">])</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[73]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Logit Regression loocv</td>
      <td>0.932993</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logit Regression Grid SearchCV</td>
      <td>0.921575</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Logit Regression Random SearchCV</td>
      <td>0.917950</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Logit Regression PCA</td>
      <td>0.633880</td>
    </tr>
    <tr>
      <th>7</th>
      <td>KNN</td>
      <td>0.916667</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[74]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_folds</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">7</span>

<span class="n">kfold</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">num_folds</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[75]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%.3f%%</span><span class="s2"> (</span><span class="si">%.3f%%</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">*</span><span class="mf">100.0</span><span class="p">,</span> <span class="n">results</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">*</span><span class="mf">100.0</span><span class="p">))</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Accuracy: 93.347% (6.638%)
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[76]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span><span class="n">results</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[77]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">tempResultsDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;KNN  K Fold&#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">accuracy</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;8&#39;</span><span class="p">})</span>
<span class="n">tempResultsDf</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">tempResultsDf</span><span class="p">])</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[77]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Logit Regression loocv</td>
      <td>0.932993</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logit Regression Grid SearchCV</td>
      <td>0.921575</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Logit Regression Random SearchCV</td>
      <td>0.917950</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Logit Regression PCA</td>
      <td>0.633880</td>
    </tr>
    <tr>
      <th>7</th>
      <td>KNN</td>
      <td>0.916667</td>
    </tr>
    <tr>
      <th>8</th>
      <td>KNN  K Fold</td>
      <td>0.933468</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[78]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">k_range</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">31</span><span class="p">))</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k_range</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[79]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">KNN</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[80]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">grid_search</span><span class="o">=</span><span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Fitting 10 folds for each of 30 candidates, totalling 300 fits
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[81]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_score_</span> <span class="o">*</span><span class="mi">100</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for our training dataset with tuning is : </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span> <span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Accuracy for our training dataset with tuning is : 96.81%
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[82]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">tempResultsDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;KNN  Grid Serach CV&#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">accuracy</span><span class="o">/</span><span class="mi">100</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;9&#39;</span><span class="p">})</span>
<span class="n">tempResultsDf</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">tempResultsDf</span><span class="p">])</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[82]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Logit Regression loocv</td>
      <td>0.932993</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logit Regression Grid SearchCV</td>
      <td>0.921575</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Logit Regression Random SearchCV</td>
      <td>0.917950</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Logit Regression PCA</td>
      <td>0.633880</td>
    </tr>
    <tr>
      <th>7</th>
      <td>KNN</td>
      <td>0.916667</td>
    </tr>
    <tr>
      <th>8</th>
      <td>KNN  K Fold</td>
      <td>0.933468</td>
    </tr>
    <tr>
      <th>9</th>
      <td>KNN  Grid Serach CV</td>
      <td>0.968095</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>DecisionTreeClassifier</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[83]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span> <span class="n">dTree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span> <span class="o">=</span> <span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[84]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fitted_model</span><span class="o">=</span><span class="n">dTree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>  
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[85]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">fitted_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="n">accuracy_score</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy &quot;</span><span class="p">,</span><span class="n">accuracy_score</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>              precision    recall  f1-score   support

          -1       0.71      0.94      0.81       363
           1       0.91      0.62      0.74       369

    accuracy                           0.78       732
   macro avg       0.81      0.78      0.77       732
weighted avg       0.81      0.78      0.77       732

Accuracy  77.8688524590164
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[86]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">tempResultsDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;Decision Tree&#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">accuracy_score</span><span class="o">/</span><span class="mi">100</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;9&#39;</span><span class="p">})</span>
<span class="n">tempResultsDf</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">tempResultsDf</span><span class="p">])</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[86]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Logit Regression loocv</td>
      <td>0.932993</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logit Regression Grid SearchCV</td>
      <td>0.921575</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Logit Regression Random SearchCV</td>
      <td>0.917950</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Logit Regression PCA</td>
      <td>0.633880</td>
    </tr>
    <tr>
      <th>7</th>
      <td>KNN</td>
      <td>0.916667</td>
    </tr>
    <tr>
      <th>8</th>
      <td>KNN  K Fold</td>
      <td>0.933468</td>
    </tr>
    <tr>
      <th>9</th>
      <td>KNN  Grid Serach CV</td>
      <td>0.968095</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Decision Tree</td>
      <td>0.778689</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[87]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_folds</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">7</span>

<span class="n">kfold</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">num_folds</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[88]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%.3f%%</span><span class="s2"> (</span><span class="si">%.3f%%</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">*</span><span class="mf">100.0</span><span class="p">,</span> <span class="n">results</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">*</span><span class="mf">100.0</span><span class="p">))</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Accuracy: 86.667% (10.604%)
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[89]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy_score</span> <span class="o">=</span><span class="n">results</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[90]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">tempResultsDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;Decision Tree K fold&#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">accuracy_score</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;10&#39;</span><span class="p">})</span>
<span class="n">tempResultsDf</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">tempResultsDf</span><span class="p">])</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[90]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Logit Regression loocv</td>
      <td>0.932993</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logit Regression Grid SearchCV</td>
      <td>0.921575</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Logit Regression Random SearchCV</td>
      <td>0.917950</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Logit Regression PCA</td>
      <td>0.633880</td>
    </tr>
    <tr>
      <th>7</th>
      <td>KNN</td>
      <td>0.916667</td>
    </tr>
    <tr>
      <th>8</th>
      <td>KNN  K Fold</td>
      <td>0.933468</td>
    </tr>
    <tr>
      <th>9</th>
      <td>KNN  Grid Serach CV</td>
      <td>0.968095</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Decision Tree</td>
      <td>0.778689</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Decision Tree K fold</td>
      <td>0.866673</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Grid Search CV</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[91]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_components</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="s1">&#39;entropy&#39;</span><span class="p">]</span>
<span class="n">max_depth</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">12</span><span class="p">]</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[92]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;max_leaf_nodes&#39;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)),</span> <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]}</span>
<span class="n">grid_search_cv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> <span class="n">params</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">grid_search_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Fitting 3 folds for each of 294 candidates, totalling 882 fits
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[92]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>GridSearchCV(cv=3, estimator=DecisionTreeClassifier(random_state=42),
             param_grid={&#39;max_leaf_nodes&#39;: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
                                            13, 14, 15, 16, 17, 18, 19, 20, 21,
                                            22, 23, 24, 25, 26, 27, 28, 29, 30,
                                            31, ...],
                         &#39;min_samples_split&#39;: [2, 3, 4]},
             verbose=1)</pre>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[93]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_score_</span> <span class="o">*</span><span class="mi">100</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for our training dataset with tuning is : </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span> <span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Accuracy for our training dataset with tuning is : 96.81%
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[94]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">tempResultsDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;Decision Tree Grid Search CV&#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">accuracy</span><span class="o">/</span><span class="mi">100</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;11&#39;</span><span class="p">})</span>
<span class="n">tempResultsDf</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">tempResultsDf</span><span class="p">])</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[94]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Logit Regression loocv</td>
      <td>0.932993</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logit Regression Grid SearchCV</td>
      <td>0.921575</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Logit Regression Random SearchCV</td>
      <td>0.917950</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Logit Regression PCA</td>
      <td>0.633880</td>
    </tr>
    <tr>
      <th>7</th>
      <td>KNN</td>
      <td>0.916667</td>
    </tr>
    <tr>
      <th>8</th>
      <td>KNN  K Fold</td>
      <td>0.933468</td>
    </tr>
    <tr>
      <th>9</th>
      <td>KNN  Grid Serach CV</td>
      <td>0.968095</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Decision Tree</td>
      <td>0.778689</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Decision Tree K fold</td>
      <td>0.866673</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Decision Tree Grid Search CV</td>
      <td>0.968095</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>SVM</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[95]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">svm_m</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>  
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[96]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fitted_model</span><span class="o">=</span><span class="n">svm_m</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">fitted_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="n">accuracy_score</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>              precision    recall  f1-score   support

          -1       0.99      1.00      1.00       363
           1       1.00      0.99      1.00       369

    accuracy                           1.00       732
   macro avg       1.00      1.00      1.00       732
weighted avg       1.00      1.00      1.00       732

</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[97]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy_score</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[97]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>99.72677595628416</pre>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[98]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">tempResultsDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;SVM&#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">accuracy_score</span><span class="o">/</span><span class="mi">100</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;12&#39;</span><span class="p">})</span>
<span class="n">tempResultsDf</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">tempResultsDf</span><span class="p">])</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[98]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Logit Regression loocv</td>
      <td>0.932993</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logit Regression Grid SearchCV</td>
      <td>0.921575</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Logit Regression Random SearchCV</td>
      <td>0.917950</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Logit Regression PCA</td>
      <td>0.633880</td>
    </tr>
    <tr>
      <th>7</th>
      <td>KNN</td>
      <td>0.916667</td>
    </tr>
    <tr>
      <th>8</th>
      <td>KNN  K Fold</td>
      <td>0.933468</td>
    </tr>
    <tr>
      <th>9</th>
      <td>KNN  Grid Serach CV</td>
      <td>0.968095</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Decision Tree</td>
      <td>0.778689</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Decision Tree K fold</td>
      <td>0.866673</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Decision Tree Grid Search CV</td>
      <td>0.968095</td>
    </tr>
    <tr>
      <th>12</th>
      <td>SVM</td>
      <td>0.997268</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[99]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_folds</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">7</span>

<span class="n">kfold</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">num_folds</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">svm_m</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[100]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%.3f%%</span><span class="s2"> (</span><span class="si">%.3f%%</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">*</span><span class="mf">100.0</span><span class="p">,</span> <span class="n">results</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">*</span><span class="mf">100.0</span><span class="p">))</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Accuracy: 100.000% (0.000%)
</pre>
</div>
</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[101]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy_score</span> <span class="o">=</span><span class="n">results</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[102]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">tempResultsDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;SVM K Fold&#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">accuracy_score</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;12&#39;</span><span class="p">})</span>
<span class="n">tempResultsDf</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">tempResultsDf</span><span class="p">])</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[102]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Logit Regression loocv</td>
      <td>0.932993</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logit Regression Grid SearchCV</td>
      <td>0.921575</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Logit Regression Random SearchCV</td>
      <td>0.917950</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Logit Regression PCA</td>
      <td>0.633880</td>
    </tr>
    <tr>
      <th>7</th>
      <td>KNN</td>
      <td>0.916667</td>
    </tr>
    <tr>
      <th>8</th>
      <td>KNN  K Fold</td>
      <td>0.933468</td>
    </tr>
    <tr>
      <th>9</th>
      <td>KNN  Grid Serach CV</td>
      <td>0.968095</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Decision Tree</td>
      <td>0.778689</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Decision Tree K fold</td>
      <td>0.866673</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Decision Tree Grid Search CV</td>
      <td>0.968095</td>
    </tr>
    <tr>
      <th>12</th>
      <td>SVM</td>
      <td>0.997268</td>
    </tr>
    <tr>
      <th>12</th>
      <td>SVM K Fold</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Grid Search CV</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[103]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">],</span>
              <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">],</span>
              <span class="s1">&#39;kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;rbf&#39;</span><span class="p">]}</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[104]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">SVC</span><span class="p">(),</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">refit</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[105]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt"></div>


<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain">
<pre>Fitting 5 folds for each of 25 candidates, totalling 125 fits
[CV 1/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.672 total time=   0.4s
[CV 2/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.663 total time=   0.4s
[CV 3/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.649 total time=   0.4s
[CV 4/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.672 total time=   0.4s
[CV 5/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.662 total time=   0.4s
[CV 1/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.672 total time=   0.4s
[CV 2/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.663 total time=   0.4s
[CV 3/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.649 total time=   0.4s
[CV 4/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.672 total time=   0.4s
[CV 5/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.662 total time=   0.4s
[CV 1/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.941 total time=   0.4s
[CV 2/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.938 total time=   0.4s
[CV 3/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.952 total time=   0.4s
[CV 4/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.916 total time=   0.4s
[CV 5/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.913 total time=   0.4s
[CV 1/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.738 total time=   0.4s
[CV 2/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.777 total time=   0.4s
[CV 3/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.747 total time=   0.4s
[CV 4/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.770 total time=   0.4s
[CV 5/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.749 total time=   0.4s
[CV 1/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.501 total time=   0.4s
[CV 2/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.501 total time=   0.4s
[CV 3/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.501 total time=   0.4s
[CV 4/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.501 total time=   0.4s
[CV 5/5] END ...C=0.1, gamma=0.0001, kernel=rbf;, score=0.502 total time=   0.4s
[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=1.000 total time=   0.3s
[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=1.000 total time=   0.3s
[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=1.000 total time=   0.3s
[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=1.000 total time=   0.3s
[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=1.000 total time=   0.3s
[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.3s
[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.3s
[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.3s
[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.3s
[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.3s
[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.2s
[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.3s
[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s
[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.995 total time=   0.2s
[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s
[CV 1/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.847 total time=   0.3s
[CV 2/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.884 total time=   0.3s
[CV 3/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.870 total time=   0.3s
[CV 4/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.854 total time=   0.3s
[CV 5/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.849 total time=   0.3s
[CV 1/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.738 total time=   0.4s
[CV 2/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.784 total time=   0.4s
[CV 3/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.761 total time=   0.4s
[CV 4/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.765 total time=   0.4s
[CV 5/5] END .....C=1, gamma=0.0001, kernel=rbf;, score=0.758 total time=   0.4s
[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.2s
[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.2s
[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s
[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s
[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s
[CV 1/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.959 total time=   0.2s
[CV 2/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.968 total time=   0.2s
[CV 3/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.959 total time=   0.2s
[CV 4/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.957 total time=   0.2s
[CV 5/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.938 total time=   0.2s
[CV 1/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.829 total time=   0.3s
[CV 2/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.854 total time=   0.3s
[CV 3/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.843 total time=   0.3s
[CV 4/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.820 total time=   0.3s
[CV 5/5] END ....C=10, gamma=0.0001, kernel=rbf;, score=0.840 total time=   0.3s
[CV 1/5] END ........C=100, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 2/5] END ........C=100, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 3/5] END ........C=100, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 4/5] END ........C=100, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 5/5] END ........C=100, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 1/5] END ......C=100, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 2/5] END ......C=100, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 3/5] END ......C=100, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 4/5] END ......C=100, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 5/5] END ......C=100, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 1/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.2s
[CV 2/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.2s
[CV 3/5] END .....C=100, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s
[CV 4/5] END .....C=100, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s
[CV 5/5] END .....C=100, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s
[CV 1/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.977 total time=   0.2s
[CV 2/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.989 total time=   0.2s
[CV 3/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.975 total time=   0.2s
[CV 4/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.970 total time=   0.2s
[CV 5/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.950 total time=   0.2s
[CV 1/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.893 total time=   0.2s
[CV 2/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.916 total time=   0.2s
[CV 3/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.895 total time=   0.2s
[CV 4/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.911 total time=   0.2s
[CV 5/5] END ...C=100, gamma=0.0001, kernel=rbf;, score=0.897 total time=   0.2s
[CV 1/5] END .......C=1000, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 2/5] END .......C=1000, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 3/5] END .......C=1000, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 4/5] END .......C=1000, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 5/5] END .......C=1000, gamma=1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 1/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 2/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 3/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 4/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 5/5] END .....C=1000, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.2s
[CV 1/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.2s
[CV 2/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=0.998 total time=   0.2s
[CV 3/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s
[CV 4/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s
[CV 5/5] END ....C=1000, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.2s
[CV 1/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.977 total time=   0.2s
[CV 2/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.989 total time=   0.2s
[CV 3/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.975 total time=   0.2s
[CV 4/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.970 total time=   0.2s
[CV 5/5] END ...C=1000, gamma=0.001, kernel=rbf;, score=0.950 total time=   0.2s
[CV 1/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.941 total time=   0.2s
[CV 2/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.952 total time=   0.2s
[CV 3/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.938 total time=   0.2s
[CV 4/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.932 total time=   0.2s
[CV 5/5] END ..C=1000, gamma=0.0001, kernel=rbf;, score=0.929 total time=   0.2s
</pre>
</div>
</div>

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[105]:</div>




<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain">
<pre>GridSearchCV(estimator=SVC(),
             param_grid={&#39;C&#39;: [0.1, 1, 10, 100, 1000],
                         &#39;gamma&#39;: [1, 0.1, 0.01, 0.001, 0.0001],
                         &#39;kernel&#39;: [&#39;rbf&#39;]},
             verbose=3)</pre>
</div>

</div>

</div>

</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[106]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">grid_predictions</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[107]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">grid_predictions_Train</span><span class="o">=</span><span class="n">grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[108]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Store the accuracy results for each model in a dataframe for final comparison</span>
<span class="n">tempResultsDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Method&#39;</span><span class="p">:[</span><span class="s1">&#39;SVM K Fold Grid Search CV &#39;</span><span class="p">],</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">accuracy</span><span class="o">/</span><span class="mi">100</span><span class="p">]},</span><span class="n">index</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;13&#39;</span><span class="p">})</span>
<span class="n">tempResultsDf</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">results_df</span><span class="p">,</span> <span class="n">tempResultsDf</span><span class="p">])</span>
<span class="n">results_df</span> <span class="o">=</span> <span class="n">results_df</span><span class="p">[[</span><span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]]</span>
<span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[108]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Logit Regression loocv</td>
      <td>0.932993</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logit Regression Grid SearchCV</td>
      <td>0.921575</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Logit Regression Random SearchCV</td>
      <td>0.917950</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Logit Regression PCA</td>
      <td>0.633880</td>
    </tr>
    <tr>
      <th>7</th>
      <td>KNN</td>
      <td>0.916667</td>
    </tr>
    <tr>
      <th>8</th>
      <td>KNN  K Fold</td>
      <td>0.933468</td>
    </tr>
    <tr>
      <th>9</th>
      <td>KNN  Grid Serach CV</td>
      <td>0.968095</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Decision Tree</td>
      <td>0.778689</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Decision Tree K fold</td>
      <td>0.866673</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Decision Tree Grid Search CV</td>
      <td>0.968095</td>
    </tr>
    <tr>
      <th>12</th>
      <td>SVM</td>
      <td>0.997268</td>
    </tr>
    <tr>
      <th>12</th>
      <td>SVM K Fold</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>13</th>
      <td>SVM K Fold Grid Search CV</td>
      <td>0.968095</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="6.-Post-Training-and-Conclusion:">6. Post Training and Conclusion:<a class="anchor-link" href="#6.-Post-Training-and-Conclusion:">&#182;</a></h1>
</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>6.A. Display and compare all the models designed with their train and test accuracies.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell   ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[109]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">results_df</span>
</pre></div>

     </div>
</div>
</div>
</div>

<div class="jp-Cell-outputWrapper">


<div class="jp-OutputArea jp-Cell-outputArea">

<div class="jp-OutputArea-child">

    
    <div class="jp-OutputPrompt jp-OutputArea-prompt">Out[109]:</div>



<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Method</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Logit Regression</td>
      <td>0.857923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logit Regression K Fold</td>
      <td>0.933508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Logit Regression loocv</td>
      <td>0.932993</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Logit Regression Grid SearchCV</td>
      <td>0.921575</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Logit Regression Random SearchCV</td>
      <td>0.917950</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Logit Regression PCA</td>
      <td>0.633880</td>
    </tr>
    <tr>
      <th>7</th>
      <td>KNN</td>
      <td>0.916667</td>
    </tr>
    <tr>
      <th>8</th>
      <td>KNN  K Fold</td>
      <td>0.933468</td>
    </tr>
    <tr>
      <th>9</th>
      <td>KNN  Grid Serach CV</td>
      <td>0.968095</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Decision Tree</td>
      <td>0.778689</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Decision Tree K fold</td>
      <td>0.866673</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Decision Tree Grid Search CV</td>
      <td>0.968095</td>
    </tr>
    <tr>
      <th>12</th>
      <td>SVM</td>
      <td>0.997268</td>
    </tr>
    <tr>
      <th>12</th>
      <td>SVM K Fold</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>13</th>
      <td>SVM K Fold Grid Search CV</td>
      <td>0.968095</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>

</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>6.B. Select the final best trained model along with your detailed comments for selecting this model.</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>1.SVM model is overfitting
2.Logit is  best model with all folding/Hyper testing
3.Also confidence level 95 % in boot strap sampling</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>6.C. Pickle the selected model for future use.</p>

</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs  ">
<div class="jp-Cell-inputWrapper">
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In&nbsp;[110]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
     <div class="CodeMirror cm-s-jupyter">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;models/Best_performing_model_&#39;</span><span class="o">+</span><span class="s1">&#39;logit&#39;</span><span class="o">+</span><span class="s1">&#39;.p&#39;</span><span class="p">,</span><span class="s1">&#39;wb&#39;</span><span class="p">))</span>
</pre></div>

     </div>
</div>
</div>
</div>

</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>6.D  Write your conclusion on the results.</p>

</div>
</div>
<div class="jp-Cell-inputWrapper"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>We selected the logit model. Becuase bootstrap smapling giving 95.0 confidence interval 91.0% and 93.7%
Actual Data set has lot of features which is continuous values
Lot features having Std as 0 which menas it has unique values .It is removed to build the model</p>

</div>
</div>
</body>







</html>
